<!DOCTYPE HTML>
<html lang="ja" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>コードリーディング: vLLM</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-ae938929.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/custom-9cb14b51.css">


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex-bb2bb663.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc-9e89a2b8.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">コードリーディング: vLLM</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="はじめに"><a class="header" href="#はじめに">はじめに</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="テキスト推論データフロー"><a class="header" href="#テキスト推論データフロー">テキスト推論データフロー</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11 (Phase 2bでマルチモーダル差分追加)</p>
</blockquote>
<h2 id="概要"><a class="header" href="#概要">概要</a></h2>
<p>テキスト推論リクエストは、APIエントリポイントからエンジン層を経てGPUで実行され、生成されたトークンがデトークナイズされてユーザーに返却される。フロー全体は5つの境界データ構造（EngineCoreRequest → SchedulerOutput → ModelRunnerOutput → EngineCoreOutput → RequestOutput）で区切られ、ZMQ IPCによるプロセス分離とasyncioによる非同期パイプラインで高スループットを実現する。</p>
<h2 id="フロー全体図"><a class="header" href="#フロー全体図">フロー全体図</a></h2>
<pre class="mermaid">graph TD
    subgraph フロントエンドプロセス
        API["API Server / LLM"]
        AsyncLLM["AsyncLLM&lt;br&gt;generate() / add_request()"]
        IP["InputProcessor&lt;br&gt;process_inputs()"]
        OP["OutputProcessor&lt;br&gt;process_outputs()"]
        Client["EngineCoreClient&lt;br&gt;AsyncMPClient"]
    end

    subgraph バックエンドプロセス ["EngineCore プロセス"]
        EC["EngineCore&lt;br&gt;step()"]
        Sched["Scheduler&lt;br&gt;schedule()"]
        KV["KVCacheManager&lt;br&gt;allocate_slots()"]
        Exec["Executor&lt;br&gt;execute_model()"]
        Worker["Worker"]
        MR["GPUModelRunner&lt;br&gt;execute_model()"]
    end

    API --&gt;|"prompt, params"| AsyncLLM
    AsyncLLM --&gt;|"prompt, params"| IP
    IP --&gt;|"EngineCoreRequest"| AsyncLLM
    AsyncLLM --&gt;|"EngineCoreRequest"| Client

    Client --&gt;|"ZMQ ROUTER\nmsgpack"| EC
    EC --&gt; Sched
    Sched --&gt;|"allocate_slots()"| KV
    Sched --&gt;|"SchedulerOutput"| EC
    EC --&gt;|"SchedulerOutput"| Exec
    Exec --&gt;|"MessageQueue\n共有メモリ"| Worker
    Worker --&gt; MR
    MR --&gt;|"ModelRunnerOutput"| Worker
    Worker --&gt;|"ModelRunnerOutput"| Exec
    Exec --&gt;|"ModelRunnerOutput"| EC
    EC --&gt;|"update_from_output()"| Sched
    Sched --&gt;|"EngineCoreOutputs"| EC

    EC --&gt;|"ZMQ PUSH\nmsgpack"| Client
    Client --&gt;|"EngineCoreOutputs"| OP
    OP --&gt;|"RequestOutput"| AsyncLLM
    AsyncLLM --&gt;|"RequestOutput"| API
</pre>

<h2 id="境界データ構造"><a class="header" href="#境界データ構造">境界データ構造</a></h2>
<p>フローは以下の5つのデータ構造で区切られる。各構造はプロセス間またはコンポーネント間の境界を定義する。</p>
<h3 id="enginecorerequest"><a class="header" href="#enginecorerequest">EngineCoreRequest</a></h3>
<p>フロントエンド → バックエンドの境界。ユーザー入力を正規化した内部表現。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/__init__.py:55</code> (EngineCoreRequest)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>request_id</code></td><td><code>str</code></td><td>内部リクエストID（外部IDに8文字ランダムサフィックス付与）</td></tr>
<tr><td><code>prompt_token_ids</code></td><td><code>list[int] | None</code></td><td>トークナイズ済みプロンプト</td></tr>
<tr><td><code>mm_features</code></td><td><code>list[MultiModalFeatureSpec] | None</code></td><td>マルチモーダル入力（テキスト推論ではNone）</td></tr>
<tr><td><code>sampling_params</code></td><td><code>SamplingParams | None</code></td><td>サンプリングパラメータ（clone済み）</td></tr>
<tr><td><code>eos_token_id</code></td><td><code>int | None</code></td><td>終了トークンID</td></tr>
<tr><td><code>arrival_time</code></td><td><code>float</code></td><td>リクエスト到着時刻</td></tr>
<tr><td><code>lora_request</code></td><td><code>LoRARequest | None</code></td><td>LoRAアダプタ情報</td></tr>
<tr><td><code>priority</code></td><td><code>int</code></td><td>優先度（デフォルト0）</td></tr>
<tr><td><code>data_parallel_rank</code></td><td><code>int | None</code></td><td>データ並列ランク指定</td></tr>
</tbody>
</table>
</div>
<p><code>msgspec.Struct</code> を継承し、<code>array_like=True</code> + <code>omit_defaults=True</code> で効率的にmsgpackシリアライズされる。</p>
<h3 id="scheduleroutput"><a class="header" href="#scheduleroutput">SchedulerOutput</a></h3>
<p>Scheduler → Executor の境界。各ステップのスケジュール結果を含む。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/output.py:184</code> (SchedulerOutput)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>scheduled_new_reqs</code></td><td><code>list[NewRequestData]</code></td><td>初回スケジュールされたリクエスト（フルデータ）</td></tr>
<tr><td><code>scheduled_cached_reqs</code></td><td><code>CachedRequestData</code></td><td>既スケジュール済みリクエスト（差分のみ）</td></tr>
<tr><td><code>num_scheduled_tokens</code></td><td><code>dict[str, int]</code></td><td>リクエストごとのスケジュールトークン数</td></tr>
<tr><td><code>total_num_scheduled_tokens</code></td><td><code>int</code></td><td>合計スケジュールトークン数</td></tr>
<tr><td><code>scheduled_spec_decode_tokens</code></td><td><code>dict[str, list[int]]</code></td><td>Speculative Decoding用トークン</td></tr>
<tr><td><code>scheduled_encoder_inputs</code></td><td><code>dict[str, list[int]]</code></td><td>エンコーダ入力インデックス（マルチモーダル）</td></tr>
<tr><td><code>num_common_prefix_blocks</code></td><td><code>list[int]</code></td><td>共通プレフィックスブロック数（Cascade Attention用）</td></tr>
<tr><td><code>finished_req_ids</code></td><td><code>set[str]</code></td><td>このステップで完了したリクエストID</td></tr>
<tr><td><code>free_encoder_mm_hashes</code></td><td><code>list[str]</code></td><td>解放するエンコーダキャッシュのmm_hash</td></tr>
<tr><td><code>preempted_req_ids</code></td><td><code>set[str] | None</code></td><td>プリエンプションされたリクエスト</td></tr>
<tr><td><code>has_structured_output_requests</code></td><td><code>bool</code></td><td>構造化出力リクエストの有無</td></tr>
<tr><td><code>pending_structured_output_tokens</code></td><td><code>bool</code></td><td>Grammar bitmask準備状態</td></tr>
<tr><td><code>num_invalid_spec_tokens</code></td><td><code>dict[str, int] | None</code></td><td>無効スペキュレーショントークン数</td></tr>
<tr><td><code>kv_connector_metadata</code></td><td><code>KVConnectorMetadata | None</code></td><td>KV Transfer メタデータ</td></tr>
<tr><td><code>ec_connector_metadata</code></td><td><code>ECConnectorMetadata | None</code></td><td>EC Transfer メタデータ</td></tr>
</tbody>
</table>
</div>
<p><strong>NewRequestData</strong> は初回スケジュール時のフルデータ（プロンプトトークン、サンプリングパラメータ、ブロックID等）を含む。<strong>CachedRequestData</strong> は既スケジュール済みリクエストの差分（新規ブロックID、新トークンID、計算済みトークン数の更新）のみを含み、プロセス間通信コストを最小化する。</p>
<h3 id="modelrunneroutput"><a class="header" href="#modelrunneroutput">ModelRunnerOutput</a></h3>
<p>GPUModelRunner → EngineCore の境界。モデル推論結果を含む。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/outputs.py:160</code> (ModelRunnerOutput)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>req_ids</code></td><td><code>list[str]</code></td><td>バッチ内のリクエストID一覧</td></tr>
<tr><td><code>req_id_to_index</code></td><td><code>dict[str, int]</code></td><td>リクエストID → バッチインデックス</td></tr>
<tr><td><code>sampled_token_ids</code></td><td><code>list[list[int]]</code></td><td>サンプリング済みトークンID [num_reqs, num_generated]</td></tr>
<tr><td><code>logprobs</code></td><td><code>LogprobsLists | None</code></td><td>生成トークンの対数確率</td></tr>
<tr><td><code>prompt_logprobs_dict</code></td><td><code>dict[str, LogprobsTensors | None]</code></td><td>プロンプトトークンの対数確率</td></tr>
<tr><td><code>pooler_output</code></td><td><code>list[Tensor | None] | None</code></td><td>プーリング出力（埋め込みモデル用）</td></tr>
<tr><td><code>kv_connector_output</code></td><td><code>KVConnectorOutput | None</code></td><td>KV Transfer出力</td></tr>
<tr><td><code>ec_connector_output</code></td><td><code>ECConnectorOutput | None</code></td><td>EC Transfer出力</td></tr>
<tr><td><code>num_nans_in_logits</code></td><td><code>dict[str, int] | None</code></td><td>logits内のNaN数</td></tr>
<tr><td><code>cudagraph_stats</code></td><td><code>CUDAGraphStat | None</code></td><td>CUDAGraph実行統計</td></tr>
</tbody>
</table>
</div>
<p>Worker→Executorへの転送ではPythonリスト形式を使用し、torch.Tensorの高コストなシリアライゼーションを回避する。</p>
<h3 id="enginecoreoutput"><a class="header" href="#enginecoreoutput">EngineCoreOutput</a></h3>
<p>バックエンド → フロントエンドの境界。リクエスト単位の推論結果。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/__init__.py:130</code> (EngineCoreOutput)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>request_id</code></td><td><code>str</code></td><td>対応するリクエストID</td></tr>
<tr><td><code>new_token_ids</code></td><td><code>list[int]</code></td><td>新たに生成されたトークンID</td></tr>
<tr><td><code>finish_reason</code></td><td><code>FinishReason | None</code></td><td>完了理由（stop/length/abort/error）</td></tr>
<tr><td><code>new_logprobs</code></td><td><code>LogprobsLists | None</code></td><td>生成トークンのlogprobs</td></tr>
<tr><td><code>num_cached_tokens</code></td><td><code>int</code></td><td>プレフィックスキャッシュヒット数</td></tr>
</tbody>
</table>
</div>
<p><code>EngineCoreOutputs</code>（複数形）がこれを<code>list[EngineCoreOutput]</code>としてバッチ化し、<code>scheduler_stats</code>やタイムスタンプと共にZMQ経由で送信される。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/__init__.py:176</code> (EngineCoreOutputs)</p>
<h3 id="requestoutput"><a class="header" href="#requestoutput">RequestOutput</a></h3>
<p>OutputProcessor → API の境界。ユーザーに返却される最終出力。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/outputs.py:86</code> (RequestOutput)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>request_id</code></td><td><code>str</code></td><td>外部リクエストID（クライアントが指定したID）</td></tr>
<tr><td><code>prompt</code></td><td><code>str | None</code></td><td>元のプロンプト文字列</td></tr>
<tr><td><code>prompt_token_ids</code></td><td><code>list[int] | None</code></td><td>トークナイズ済みプロンプト</td></tr>
<tr><td><code>prompt_logprobs</code></td><td><code>PromptLogprobs | None</code></td><td>プロンプトトークンの対数確率</td></tr>
<tr><td><code>outputs</code></td><td><code>list[CompletionOutput]</code></td><td>サンプルごとの出力（n&gt;1で複数）</td></tr>
<tr><td><code>finished</code></td><td><code>bool</code></td><td>リクエスト完了フラグ</td></tr>
<tr><td><code>metrics</code></td><td><code>RequestStateStats | None</code></td><td>レイテンシ等の統計情報</td></tr>
<tr><td><code>num_cached_tokens</code></td><td><code>int | None</code></td><td>プレフィックスキャッシュヒット数</td></tr>
<tr><td><code>kv_transfer_params</code></td><td><code>dict[str, Any] | None</code></td><td>KV Transfer情報（完了時）</td></tr>
</tbody>
</table>
</div>
<p><strong>CompletionOutput</strong> (<code>target/vllm/vllm/outputs.py:23</code>) は各サンプルの出力を表す:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>index</code></td><td><code>int</code></td><td>サンプルインデックス</td></tr>
<tr><td><code>text</code></td><td><code>str</code></td><td>デトークナイズ済みテキスト</td></tr>
<tr><td><code>token_ids</code></td><td><code>GenericSequence[int]</code></td><td>生成トークンID列</td></tr>
<tr><td><code>cumulative_logprob</code></td><td><code>float | None</code></td><td>累積対数確率</td></tr>
<tr><td><code>logprobs</code></td><td><code>SampleLogprobs | None</code></td><td>各トークンのlogprobs</td></tr>
<tr><td><code>finish_reason</code></td><td><code>str | None</code></td><td>完了理由（“stop” / “length”）</td></tr>
<tr><td><code>stop_reason</code></td><td><code>int | str | None</code></td><td>停止トークン/文字列</td></tr>
</tbody>
</table>
</div>
<p><strong>出力モード</strong>（<code>RequestOutputKind</code>、<code>target/vllm/vllm/sampling_params.py:108</code>）:</p>
<ul>
<li><code>CUMULATIVE</code>: 毎回全出力を返す（デフォルト）</li>
<li><code>DELTA</code>: 差分のみ返す（ストリーミング向け）</li>
<li><code>FINAL_ONLY</code>: 完了時のみ返す</li>
</ul>
<h2 id="上流パス-リクエスト受信--enginecore"><a class="header" href="#上流パス-リクエスト受信--enginecore">上流パス: リクエスト受信 → EngineCore</a></h2>
<h3 id="エントリポイント-llm--asyncllm"><a class="header" href="#エントリポイント-llm--asyncllm">エントリポイント (LLM / AsyncLLM)</a></h3>
<p>vLLMには同期パス（<code>LLM</code>）と非同期パス（<code>AsyncLLM</code>）の2つのエントリポイントがある。内部的にはどちらも<code>InputProcessor</code>と<code>EngineCoreClient</code>を使用する。</p>
<h4 id="非同期パス主パス-asyncllm"><a class="header" href="#非同期パス主パス-asyncllm">非同期パス（主パス）: AsyncLLM</a></h4>
<p>APIサーバー（OpenAI互換API等）が使用する主要パス。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/async_llm.py:71</code> (AsyncLLM)</p>
<pre><code>AsyncLLM.generate(prompt, sampling_params, request_id)    # L537
  │
  ├─ add_request(request_id, prompt, params)               # L286
  │   ├─ input_processor.process_inputs(prompt, params)    # L364
  │   │   → EngineCoreRequest を生成
  │   ├─ input_processor.assign_request_id(request)        # L378
  │   │   → 内部IDを付与（外部ID + 8文字ランダムサフィックス）
  │   ├─ output_processor.add_request(request, ...)        # L423
  │   │   → フロントエンド側でリクエストを登録
  │   └─ engine_core.add_request_async(request)            # L426
  │       → ZMQ経由でバックエンドへ送信
  │
  └─ while not finished:                                    # L586
      out = q.get_nowait() or await q.get()                # L589
      yield out                                             # L596
</code></pre>
<p><code>generate()</code>はAsyncGeneratorで、バックグラウンドの<code>output_handler</code>タスクがEngineCoreからの出力を<code>RequestOutputCollector</code>キューにpushし、<code>generate()</code>がそれをyieldする。</p>
<p><strong>output_handler（バックグラウンドタスク）</strong>:</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/async_llm.py:647</code> (_run_output_handler)</p>
<pre><code>output_handler():                                           # L662
  while True:
    outputs = await engine_core.get_output_async()          # L666
    for chunk in outputs.outputs:                           # L677
      output_processor.process_outputs(chunk, ...)          # L681
      → RequestOutputをキューにpush
    if reqs_to_abort:
      await engine_core.abort_requests_async(...)           # L693
</code></pre>
<h4 id="同期パス-llm"><a class="header" href="#同期パス-llm">同期パス: LLM</a></h4>
<p>オフライン推論（バッチ処理）で使用される。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/entrypoints/llm.py:396</code> (generate)</p>
<pre><code>LLM.generate(prompts, sampling_params)                      # L396
  → _run_completion(prompts, params)                        # L449
    → _add_request(prompt, params) × N                      # L1850
    │  ├─ input_processor.process_inputs(prompt, params)    # L1879
    │  └─ llm_engine.add_request(request_id, request, ...)  # L1889
    → _run_engine()                                         # L1900
       while has_unfinished_requests():                     # L1918
         step_outputs = llm_engine.step()                   # L1919
</code></pre>
<p>同期パスとの主な違い:</p>
<ul>
<li><code>LLM</code>は<code>_run_engine()</code>でポーリングループを回す（AsyncGeneratorではない）</li>
<li><code>llm_engine</code>（=<code>AsyncLLM</code>のラッパー）の<code>step()</code>を直接呼ぶ</li>
<li>プログレスバー（tqdm）でバッチ処理の進捗を表示</li>
</ul>
<h3 id="入力処理-inputprocessor"><a class="header" href="#入力処理-inputprocessor">入力処理 (InputProcessor)</a></h3>
<p><code>InputProcessor</code>はユーザー入力（テキストプロンプト、パラメータ）を<code>EngineCoreRequest</code>に変換する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/input_processor.py:56</code> (InputProcessor)</p>
<pre><code>InputProcessor.process_inputs(request_id, prompt, params)   # L521
  ├─ _validate_lora(lora_request)                           # L535
  ├─ _validate_params(params)                               # L536
  ├─ data_parallel_rank の範囲チェック                       # L542
  ├─ arrival_time 設定（未指定なら time.time()）             # L548
  │
  ├─ input_preprocessor.preprocess(prompt, ...)             # L581
  │   → テキストをトークナイズ（tokenizer.encode()）
  │   → ProcessorInputs を返す
  │
  ├─ split_enc_dec_inputs(processed_inputs)                 # L597
  │   → エンコーダ/デコーダ入力を分離
  │
  ├─ SamplingParams の正規化                                # L608-623
  │   ├─ params.clone()                                     # L612
  │   ├─ max_tokens 未設定時: max_model_len - seq_len       # L614-618
  │   ├─ update_from_generation_config()                    # L619
  │   └─ update_from_tokenizer()                            # L623
  │
  └─ EngineCoreRequest を構築して返す                        # L656-671
</code></pre>
<p>テキスト推論の場合、マルチモーダル関連処理（L630-654）はスキップされる（<code>mm_features</code>はNone）。</p>
<h3 id="プロセス間通信-enginecoreclient--zmq-ipc"><a class="header" href="#プロセス間通信-enginecoreclient--zmq-ipc">プロセス間通信 (EngineCoreClient / ZMQ IPC)</a></h3>
<p><code>EngineCoreClient</code>はフロントエンドプロセスとバックエンドプロセス（EngineCore）間のZMQ IPC通信を担当する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core_client.py:63</code> (EngineCoreClient)</p>
<h4 id="クライアント階層"><a class="header" href="#クライアント階層">クライアント階層</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>クラス</th><th>用途</th><th>トランスポート</th></tr>
</thead>
<tbody>
<tr><td><code>EngineCoreClient</code> (ABC)</td><td>抽象インターフェース</td><td>—</td></tr>
<tr><td><code>InprocClient</code></td><td>インプロセス（デバッグ用）</td><td>直接呼び出し</td></tr>
<tr><td><code>SyncMPClient</code></td><td>同期マルチプロセス（LLM用）</td><td>ZMQ同期</td></tr>
<tr><td><code>AsyncMPClient</code></td><td>非同期マルチプロセス（AsyncLLM用）</td><td>ZMQ非同期</td></tr>
<tr><td><code>DPAsyncMPClient</code></td><td>データ並列（外部LB）</td><td>複数ZMQ</td></tr>
<tr><td><code>DPLBAsyncMPClient</code></td><td>データ並列（内部LB）</td><td>複数ZMQ</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core_client.py:442</code> (MPClient)</p>
<h4 id="zmqソケット構成"><a class="header" href="#zmqソケット構成">ZMQソケット構成</a></h4>
<pre><code>フロントエンド                 バックエンド
┌──────────────┐              ┌──────────────┐
│ AsyncMPClient│              │ EngineCore   │
│              │              │              │
│ input_socket ├─── ROUTER ──→│ (受信)       │
│ (zmq.ROUTER) │    msgpack   │              │
│              │              │              │
│ output_socket│←── PULL ─────┤ (送信)       │
│ (zmq.PULL)   │    msgpack   │              │
└──────────────┘              └──────────────┘
</code></pre>
<ul>
<li><strong>シリアライゼーション</strong>: <code>MsgpackEncoder</code> / <code>MsgpackDecoder</code>（<code>msgspec</code>ライブラリ）
<ul>
<li><code>EngineCoreRequest</code>のシリアライズ → 入力ソケット経由で送信</li>
<li><code>EngineCoreOutputs</code>のデシリアライズ ← 出力ソケット経由で受信</li>
</ul>
</li>
<li><strong>非同期出力受信</strong>: <code>process_outputs_socket()</code>タスクがZMQソケットをポーリングし、受信したOutputsを<code>asyncio.Queue</code>にpush</li>
</ul>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core_client.py:822</code> (AsyncMPClient)</p>
<h4 id="enginecore側のリクエスト受信"><a class="header" href="#enginecore側のリクエスト受信">EngineCore側のリクエスト受信</a></h4>
<p><code>EngineCore.add_request()</code>はリクエストをバリデーションしてSchedulerに登録する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:288</code> (add_request)</p>
<pre><code>EngineCore.add_request(request)                             # L288
  ├─ request_id の型チェック                                 # L295
  ├─ pooling_params のバリデーション                          # L300
  ├─ kv_transfer_params の互換性チェック                      # L311
  └─ scheduler.add_request(request)                          # L319
</code></pre>
<h4 id="enginecorestep-コアループ概要"><a class="header" href="#enginecorestep-コアループ概要">EngineCore.step() （コアループ概要）</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:389</code> (step)</p>
<pre><code>EngineCore.step()                                            # L389
  ├─ scheduler.schedule()        → SchedulerOutput           # L404
  ├─ executor.execute_model()    → Future[ModelRunnerOutput]  # L405
  ├─ grammar_output 取得                                      # L406
  ├─ future.result()             → ModelRunnerOutput          # L411
  ├─ sample_tokens()（非同期スケジューリング時）              # L413
  └─ scheduler.update_from_output() → EngineCoreOutputs      # L418
</code></pre>
<h2 id="コアループ-enginecorestep"><a class="header" href="#コアループ-enginecorestep">コアループ: EngineCore.step()</a></h2>
<p>EngineCoreの<code>step()</code>メソッドは、各ステップで <strong>schedule → execute → update</strong> のサイクルを実行し、待機中のリクエストから生成トークンを生産する。</p>
<h3 id="step-実行フロー"><a class="header" href="#step-実行フロー">step() 実行フロー</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:389</code> (step)</p>
<pre><code>EngineCore.step()                                            # L389
  │
  ├─ if _scheduler_paused: return {}, False                  # L397
  ├─ if not scheduler.has_requests(): return {}, False       # L402
  │
  ├─ 1. scheduler_output = scheduler.schedule()              # L404
  │      → SchedulerOutput
  │      （RUNNINGリクエストの予算割当 → WAITINGリクエストの受け入れ
  │        → KVキャッシュブロック確保 → SchedulerOutput構築）
  │
  ├─ 2. future = executor.execute_model(                     # L405
  │         scheduler_output, non_block=True)
  │      → Future[ModelRunnerOutput | None]
  │      （非ブロッキング。ワーカープロセスで並行実行）
  │
  ├─ 3. grammar_output = scheduler.get_grammar_bitmask(      # L406
  │         scheduler_output)
  │      （構造化出力有効時のみ使用）
  │
  ├─ 4. model_output = future.result()                       # L411
  │      → ModelRunnerOutput（ブロッキング待機）
  │
  ├─ 5. if model_output is None:                             # L413
  │        model_output = executor.sample_tokens(grammar_output)
  │      （非同期スケジューリング時: execute_modelとsamplingが分離）
  │
  ├─ 6. _process_aborts_queue()                              # L417
  │
  └─ 7. engine_core_outputs = scheduler.update_from_output(  # L418
  │         scheduler_output, model_output)
  │      → dict[int, EngineCoreOutputs]
  │      （生成トークンの追加、完了判定、出力構築）
  │
  └─ return (engine_core_outputs,                            # L422
             total_num_scheduled_tokens &gt; 0)
</code></pre>
<h3 id="scheduler-と-kvcachemanager-の相互作用"><a class="header" href="#scheduler-と-kvcachemanager-の相互作用">Scheduler と KVCacheManager の相互作用</a></h3>
<pre class="mermaid">sequenceDiagram
    participant EC as EngineCore
    participant S as Scheduler
    participant KV as KVCacheManager
    participant Ex as Executor

    EC-&gt;&gt;S: schedule()

    Note over S: Phase 1: RUNNINGリクエスト処理
    loop 各RUNNINGリクエスト
        S-&gt;&gt;KV: allocate_slots(request, num_new_tokens)
        KV--&gt;&gt;S: KVCacheBlocks or None
        alt 割り当て失敗 (None)
            S-&gt;&gt;KV: free(低優先度request)
            Note over S: プリエンプション → 再試行
        end
    end

    Note over S: Phase 2: WAITINGリクエスト受け入れ
    loop 各WAITINGリクエスト
        S-&gt;&gt;KV: get_computed_blocks(request)
        KV--&gt;&gt;S: (cached_blocks, num_hits)
        S-&gt;&gt;KV: allocate_slots(request, num_new_tokens, ...)
        KV--&gt;&gt;S: KVCacheBlocks or None
        alt 割り当て失敗 (None)
            Note over S: break（ループ終了）
        end
    end

    Note over S: Phase 3: SchedulerOutput構築
    S--&gt;&gt;EC: SchedulerOutput

    EC-&gt;&gt;Ex: execute_model(scheduler_output)
    Ex--&gt;&gt;EC: Future[ModelRunnerOutput]
    EC-&gt;&gt;EC: future.result()（待機）

    EC-&gt;&gt;S: update_from_output(scheduler_output, model_output)
    Note over S: トークン追加、完了判定
    S--&gt;&gt;EC: dict[int, EngineCoreOutputs]
</pre>

<h3 id="schedulerschedule-の3フェーズ"><a class="header" href="#schedulerschedule-の3フェーズ">Scheduler.schedule() の3フェーズ</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:321</code> (schedule)</p>
<p><code>schedule()</code> は Unified Compute Model を採用し、Prefill/Decodeを区別せず <code>num_computed_tokens</code> の進捗で統一的にトークンを割り当てる。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フェーズ</th><th>行</th><th>対象</th><th>処理</th></tr>
</thead>
<tbody>
<tr><td>Phase 1</td><td>L350-517</td><td>RUNNINGリクエスト</td><td>トークン予算割当。ブロック不足時はプリエンプション</td></tr>
<tr><td>Phase 2</td><td>L532-800</td><td>WAITINGリクエスト</td><td>新規受け入れ。プレフィックスキャッシュ検索 + ブロック割当</td></tr>
<tr><td>Phase 3</td><td>L827-896</td><td>出力構築</td><td>NewRequestData + CachedRequestData → SchedulerOutput</td></tr>
</tbody>
</table>
</div>
<p><strong>トークン予算</strong>: <code>token_budget = max_num_scheduled_tokens</code>（ステップあたり上限）で、各リクエストのスケジュール時に消費される。</p>
<p>詳細は <a href="#scheduler-サマリー">Scheduler サマリー</a> を参照。</p>
<h3 id="kvcachemanager-のブロック割り当て"><a class="header" href="#kvcachemanager-のブロック割り当て">KVCacheManager のブロック割り当て</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_manager.py:206</code> (allocate_slots)</p>
<p><code>allocate_slots()</code> は以下のブロック配置に基づいてGPUメモリブロックを確保する:</p>
<pre><code>|  comp  | new_comp | ext_comp |   new   | lookahead |
|&lt;------ 既計算トークン ------&gt;|&lt;-- 新規計算対象 --&gt;|
                               |&lt;- 割り当て対象 -&gt;|
</code></pre>
<ul>
<li>成功時: <code>KVCacheBlocks</code>（割り当てたブロック情報）を返す</li>
<li>失敗時: <code>None</code> を返す → Schedulerがプリエンプション（RUNNING）またはスキップ（WAITING）</li>
</ul>
<p>プレフィックスキャッシュ検索は <code>get_computed_blocks()</code> で行い、過去に計算済みのブロックを再利用する。</p>
<p>詳細は <a href="#kvcachemanager-サマリー">KVCacheManager サマリー</a> を参照。</p>
<h3 id="update_from_output--enginecoreoutputs"><a class="header" href="#update_from_output--enginecoreoutputs">update_from_output() → EngineCoreOutputs</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:1241</code> (update_from_output)</p>
<p><code>ModelRunnerOutput</code>を受けてSchedulerの状態を更新し、クライアントに返す<code>EngineCoreOutputs</code>を構築する。</p>
<pre><code>update_from_output(scheduler_output, model_runner_output)
  for each scheduled request:
    ├─ Speculative Decodingリジェクション処理
    │   → 不採用分の num_computed_tokens 巻き戻し
    ├─ 生成トークンをリクエストに追加
    ├─ 完了判定（EOS、max_tokens、stop_token）
    │   → 完了時: kv_cache_manager.free(request) でブロック解放
    └─ EngineCoreOutput 構築（request_id, new_token_ids, finish_reason, ...）
  → dict[int, EngineCoreOutputs]（クライアントインデックス別）
</code></pre>
<h2 id="下流パス-実行--ユーザー応答"><a class="header" href="#下流パス-実行--ユーザー応答">下流パス: 実行 → ユーザー応答</a></h2>
<h3 id="実行層-executor--worker--gpumodelrunner"><a class="header" href="#実行層-executor--worker--gpumodelrunner">実行層: Executor → Worker → GPUModelRunner</a></h3>
<p>EngineCore.step()は<code>executor.execute_model()</code>を<strong>非ブロッキング</strong>で呼び出し、GPUでの推論実行を開始する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py:202</code> (execute_model)</p>
<h4 id="collective_rpc-パターン"><a class="header" href="#collective_rpc-パターン">collective_rpc パターン</a></h4>
<p>Executorは<code>collective_rpc()</code>パターンで全Workerに同一メソッドを実行させ、出力ランクのWorkerの結果のみを返す。</p>
<pre><code>EngineCore.step()
  │
  ├─ executor.execute_model(scheduler_output, non_block=True)
  │   └─ collective_rpc("execute_model", args=(scheduler_output,))
  │       └─ Worker.execute_model(scheduler_output)                # L604
  │           └─ model_runner.execute_model(scheduler_output)      # L652
  │               → ExecuteModelState を内部保存、None を返す
  │
  ├─ grammar_output = scheduler.get_grammar_bitmask(...)           # 並行処理
  │
  ├─ future.result()  → None                                       # 待機
  │
  └─ executor.sample_tokens(grammar_output)                         # L222
      └─ collective_rpc("sample_tokens", args=(grammar_output,))
          └─ Worker.sample_tokens(grammar_output)                  # L598
              └─ model_runner.sample_tokens(grammar_output)        # L3621
                  → ModelRunnerOutput を返す
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_worker.py:604</code> (Worker.execute_model)</p>
<h4 id="gpumodelrunner-の2フェーズ実行"><a class="header" href="#gpumodelrunner-の2フェーズ実行">GPUModelRunner の2フェーズ実行</a></h4>
<p>GPUModelRunnerは <code>execute_model()</code> と <code>sample_tokens()</code> を分離する<strong>2フェーズ実行パターン</strong>を採用する。これにより、モデルフォワード中にgrammar bitmask計算を並行実行できる。</p>
<p><strong>Phase 1: execute_model()</strong> (<code>target/vllm/vllm/v1/worker/gpu_model_runner.py:3312</code>)</p>
<pre><code>execute_model(scheduler_output)
  ├─ _update_states(scheduler_output)          # バッチ状態更新
  ├─ _prepare_inputs(scheduler_output)         # 入力ID・位置計算
  ├─ _build_attention_metadata(...)            # Attention メタデータ構築
  ├─ _model_forward(...)                       # model.forward() 実行
  │   → hidden_states
  ├─ compute_logits(hidden_states)             # logits 計算
  │   → logits
  └─ ExecuteModelState に保存 → None を返す
</code></pre>
<p><strong>Phase 2: sample_tokens()</strong> (<code>target/vllm/vllm/v1/worker/gpu_model_runner.py:3621</code>)</p>
<pre><code>sample_tokens(grammar_output)
  ├─ ExecuteModelState を復元
  ├─ grammar bitmask 適用（構造化出力時）
  ├─ _sample(logits) → SamplerOutput
  ├─ バッチ状態更新（生成トークン反映）
  └─ ModelRunnerOutput を構築して返す
</code></pre>
<p><strong>ExecuteModelState</strong> (<code>target/vllm/vllm/v1/worker/gpu_model_runner.py:313</code>) はGPUテンソル（logits, hidden_states等）を保持するNamedTupleで、2フェーズ間の一時状態転送に使用される。</p>
<h3 id="出力処理-enginecoreoutput--requestoutput"><a class="header" href="#出力処理-enginecoreoutput--requestoutput">出力処理: EngineCoreOutput → RequestOutput</a></h3>
<p>ModelRunnerOutputはバックエンドプロセス（EngineCore）で<code>EngineCoreOutput</code>に変換され、ZMQ経由でフロントエンドプロセスの<code>OutputProcessor</code>に送られてデトークナイズされる。</p>
<pre class="mermaid">sequenceDiagram
    participant MR as GPUModelRunner
    participant S as Scheduler
    participant EC as EngineCore
    participant ZMQ as ZMQ IPC
    participant OP as OutputProcessor
    participant Client as API Client

    MR-&gt;&gt;EC: ModelRunnerOutput
    EC-&gt;&gt;S: update_from_output()
    Note over S: トークン追加、完了判定&lt;br&gt;KVキャッシュ解放
    S-&gt;&gt;EC: dict[int, EngineCoreOutputs]

    EC-&gt;&gt;ZMQ: msgpack シリアライズ
    ZMQ-&gt;&gt;OP: EngineCoreOutputs

    Note over OP: デトークナイズ&lt;br&gt;停止文字列判定&lt;br&gt;logprobs処理
    OP-&gt;&gt;Client: RequestOutput (yield)
</pre>

<h4 id="outputprocessorprocess_outputs"><a class="header" href="#outputprocessorprocess_outputs">OutputProcessor.process_outputs()</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/output_processor.py:582</code> (process_outputs)</p>
<p>OutputProcessorは<strong>フロントエンドプロセス</strong>で動作し、<code>EngineCoreOutput</code>をユーザー向け<code>RequestOutput</code>に変換する。</p>
<pre><code>OutputProcessor.process_outputs(engine_core_outputs)       # L582
  for each engine_core_output:
    ├─ req_state = request_states[req_id]                  # RequestState取得
    │
    ├─ detokenizer.update(new_token_ids, stop_terminated)  # L637
    │   ├─ トークン→テキスト変換（インクリメンタル）
    │   └─ 停止文字列チェック → stop_string or None
    │
    ├─ logprobs_processor.update_from_output(output)       # L646
    │
    ├─ req_state.make_request_output(...)                   # L649
    │   ├─ _new_completion_output(token_ids, finish_reason, ...)
    │   │   ├─ detokenizer.get_next_output_text(finished, delta)
    │   │   └─ CompletionOutput(text, token_ids, logprobs, ...)
    │   └─ RequestOutput(request_id, outputs, finished, ...)
    │
    └─ req_state.queue.put(request_output)                 # L661
        → AsyncLLM.generate() が yield
</code></pre>
<h4 id="detokenizerインクリメンタルデトークナイズ"><a class="header" href="#detokenizerインクリメンタルデトークナイズ">Detokenizer（インクリメンタルデトークナイズ）</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/detokenizer.py:30</code> (IncrementalDetokenizer)</p>
<p>トークンからテキストへの変換はインクリメンタルに行われ、ストリーミング出力を実現する。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>クラス</th><th>条件</th><th>方式</th></tr>
</thead>
<tbody>
<tr><td><code>FastIncrementalDetokenizer</code></td><td><code>PreTrainedTokenizerFast</code> 使用時</td><td>HF tokenizersの<code>DecodeStream</code>で高速変換</td></tr>
<tr><td><code>SlowIncrementalDetokenizer</code></td><td>その他のトークナイザ</td><td><code>detokenize_incrementally()</code>でPython変換</td></tr>
<tr><td><code>IncrementalDetokenizer</code></td><td>トークナイザなし</td><td>No-op（テキスト出力なし）</td></tr>
</tbody>
</table>
</div>
<p><code>update()</code>メソッドで各トークンをインクリメンタルにデコードし、同時に<code>check_stop_strings()</code>で停止文字列を検出する（<code>target/vllm/vllm/v1/engine/detokenizer.py:316</code>）。</p>
<h2 id="prefill-vs-decode"><a class="header" href="#prefill-vs-decode">Prefill vs Decode</a></h2>
<p>vLLM v1は<strong>Unified Compute Model</strong>を採用し、PrefillとDecodeを明示的に区別しない。両者は<code>num_computed_tokens</code>の進捗によって暗黙的に区分される。</p>
<h3 id="統一管理の仕組み"><a class="header" href="#統一管理の仕組み">統一管理の仕組み</a></h3>
<p>各リクエストは<code>num_computed_tokens</code>フィールドで計算済みトークン数を追跡する:</p>
<pre><code>プロンプト: [A, B, C, D, E]    (len=5)
num_computed_tokens: 0 → 5 → 6 → 7 → ...

Prefillフェーズ: num_computed_tokens &lt; len(prompt_token_ids)
  → 複数トークンを一度に計算（チャンクプリフィル可能）

Decodeフェーズ: num_computed_tokens &gt;= len(prompt_token_ids)
  → 1トークンずつ生成
</code></pre>
<h3 id="schedulerでの扱い"><a class="header" href="#schedulerでの扱い">Schedulerでの扱い</a></h3>
<p>Scheduler.schedule()はPrefill/Decodeを区別せず、トークン予算の範囲内で各リクエストに計算トークン数を割り当てる:</p>
<ul>
<li><strong>新規リクエスト（WAITING→RUNNING）</strong>: <code>num_tokens = len(prompt_token_ids) - num_computed_tokens</code>（プレフィックスキャッシュヒット分を差し引き）</li>
<li><strong>継続リクエスト（RUNNING）</strong>: <code>num_tokens = 1</code>（Decode 1トークン）</li>
<li>予算不足時は部分的なPrefill（チャンクプリフィル）も可能</li>
</ul>
<h3 id="gpumodelrunner内での違い"><a class="header" href="#gpumodelrunner内での違い">GPUModelRunner内での違い</a></h3>
<p>GPUModelRunner.execute_model()は入力準備の段階で暗黙的にPrefill/Decodeを処理する:</p>
<ul>
<li><strong>_prepare_inputs()</strong>: <code>num_scheduled_tokens</code>に基づいて入力トークンと位置を計算。Prefillなら複数トークン、Decodeなら1トークン</li>
<li><strong>_build_attention_metadata()</strong>: Prefillはフルattention、Decodeはキャッシュ済みKVに対するattentionのメタデータを構築</li>
<li><strong>モデルフォワード</strong>: 入力テンソルのサイズが異なるだけで、同一のforward()を実行</li>
</ul>
<p>この統一モデルにより、同一バッチ内にPrefillリクエストとDecodeリクエストを混在させるContinuous Batchingが自然に実現される。</p>
<h2 id="コンポーネント優先度確定"><a class="header" href="#コンポーネント優先度確定">コンポーネント優先度（確定）</a></h2>
<p>Phase 2での深堀り順序。ユーザー関心領域とフロー上の重要度に基づく。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>優先度</th><th>コンポーネント</th><th>理由</th><th>現在の深度</th></tr>
</thead>
<tbody>
<tr><td><strong>S</strong></td><td>KVCacheManager</td><td>ユーザー関心1位（メモリ管理/KVキャッシュ）。PagedAttention、ブロック管理、Eviction</td><td>[MEDIUM]</td></tr>
<tr><td><strong>A</strong></td><td>Scheduler</td><td>KVCacheManagerと密連携、推論パイプライン全体を制御。Continuous Batching</td><td>[MEDIUM]</td></tr>
<tr><td><strong>A</strong></td><td>GPUModelRunner</td><td>推論実行の中核。6277行の巨大クラス。将来のプラグイン開発に重要</td><td>[SHALLOW]</td></tr>
<tr><td><strong>B</strong></td><td>EngineCore</td><td>step()サイクル、batch_queueパイプライン。全体の統合ポイント</td><td>[MEDIUM]</td></tr>
<tr><td><strong>B</strong></td><td>OutputProcessor</td><td>デトークナイズ、停止判定。ストリーミング出力の仕組み</td><td>[SHALLOW]</td></tr>
<tr><td><strong>C</strong></td><td>AsyncLLM, InputProcessor</td><td>エントリポイント。薄いレイヤー</td><td>[SHALLOW]</td></tr>
<tr><td><strong>C</strong></td><td>Executor, Worker</td><td>委譲パターン。分散推論時のみ詳細が必要</td><td>[SHALLOW]</td></tr>
<tr><td><strong>C</strong></td><td>EngineCoreClient</td><td>ZMQ IPC通信層。プロトコルは把握済み</td><td>[SHALLOW]</td></tr>
</tbody>
</table>
</div>
<h2 id="参照ファイル一覧"><a class="header" href="#参照ファイル一覧">参照ファイル一覧</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th><th>役割</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/entrypoints/llm.py</code></td><td><code>LLM.generate()</code> (L396), <code>_add_request()</code> (L1850)</td><td>同期エントリポイント</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/async_llm.py</code></td><td><code>AsyncLLM.generate()</code> (L537), <code>add_request()</code> (L286)</td><td>非同期エントリポイント</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/input_processor.py</code></td><td><code>InputProcessor.process_inputs()</code> (L521)</td><td>入力処理</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/__init__.py</code></td><td><code>EngineCoreRequest</code> (L55), <code>EngineCoreOutput</code> (L130), <code>EngineCoreOutputs</code> (L176)</td><td>境界データ構造</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core_client.py</code></td><td><code>EngineCoreClient</code> (L63), <code>MPClient</code> (L442), <code>AsyncMPClient</code> (L822)</td><td>ZMQ IPC通信</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core.py</code></td><td><code>EngineCore.add_request()</code> (L288), <code>step()</code> (L389)</td><td>推論ループ本体</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/sched/scheduler.py</code></td><td><code>Scheduler.schedule()</code> (L321), <code>update_from_output()</code> (L1241)</td><td>スケジューリング</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/sched/output.py</code></td><td><code>SchedulerOutput</code> (L184), <code>NewRequestData</code> (L34), <code>CachedRequestData</code> (L114)</td><td>スケジュール出力データ構造</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/kv_cache_manager.py</code></td><td><code>KVCacheManager.allocate_slots()</code> (L206), <code>get_computed_blocks()</code> (L164)</td><td>KVキャッシュ管理</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/block_pool.py</code></td><td><code>BlockPool</code> (L128)</td><td>物理ブロック管理</td></tr>
<tr><td><code>target/vllm/vllm/v1/request.py</code></td><td><code>Request</code></td><td>リクエスト内部状態</td></tr>
<tr><td><code>target/vllm/vllm/v1/outputs.py</code></td><td><code>ModelRunnerOutput</code> (L160)</td><td>モデル推論出力</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/abstract.py</code></td><td><code>Executor</code> (ABC), <code>execute_model()</code> (L202), <code>collective_rpc()</code> (L180)</td><td>実行層抽象</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/uniproc_executor.py</code></td><td><code>UniProcExecutor</code> (L26)</td><td>単一プロセス実行</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/multiproc_executor.py</code></td><td><code>MultiprocExecutor</code> (L93)</td><td>マルチプロセス実行</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_worker.py</code></td><td><code>Worker.execute_model()</code> (L604), <code>sample_tokens()</code> (L598)</td><td>GPU Worker</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_model_runner.py</code></td><td><code>GPUModelRunner.execute_model()</code> (L3312), <code>sample_tokens()</code> (L3621), <code>ExecuteModelState</code> (L313)</td><td>モデル実行</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/output_processor.py</code></td><td><code>OutputProcessor.process_outputs()</code> (L582), <code>RequestState.make_request_output()</code> (L269)</td><td>出力処理</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/detokenizer.py</code></td><td><code>IncrementalDetokenizer</code> (L30), <code>FastIncrementalDetokenizer</code> (L169), <code>check_stop_strings()</code> (L316)</td><td>デトークナイズ</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/logprobs.py</code></td><td><code>LogprobsProcessor</code> (L28)</td><td>logprobs処理</td></tr>
<tr><td><code>target/vllm/vllm/outputs.py</code></td><td><code>RequestOutput</code> (L86), <code>CompletionOutput</code> (L23)</td><td>最終出力データ構造</td></tr>
</tbody>
</table>
</div>
<h2 id="マルチモーダル推論パスの差分"><a class="header" href="#マルチモーダル推論パスの差分">マルチモーダル推論パスの差分</a></h2>
<p>テキスト推論フローに対し、画像等のマルチモーダル入力がある場合の主要な差分を以下に示す。詳細は <a href="#マルチモーダル処理パイプライン-サマリー">マルチモーダル処理パイプライン</a> を参照。</p>
<h3 id="フロントエンドp0の差分"><a class="header" href="#フロントエンドp0の差分">フロントエンド（P0）の差分</a></h3>
<ol>
<li><strong>チャットテンプレート</strong>: プレースホルダー（<code>&lt;start_of_image&gt;</code> 等）がプロンプトに挿入される</li>
<li><strong>HF Processor実行</strong>: 画像を <code>pixel_values</code> テンソルに変換（リサイズ、正規化、パッチ分割）</li>
<li><strong>MMハッシュ計算</strong>: <code>MultiModalHasher</code> でコンテンツベースのblake3ハッシュを生成</li>
<li><strong>ProcessorCache</strong>: HF処理結果をキャッシュ（4種類の実装: processor_only/lru/shm/none）</li>
<li><strong>EngineCoreRequest</strong>: <code>mm_features: list[MultiModalFeatureSpec]</code> にテンソルデータ・位置情報・ハッシュを格納</li>
</ol>
<h3 id="バックエンドp1の差分"><a class="header" href="#バックエンドp1の差分">バックエンド（P1）の差分</a></h3>
<ol>
<li><strong>EncoderCacheManager</strong>: エンコーダ出力をリファレンスカウント方式で管理。キャッシュヒットでエンコーダ計算スキップ</li>
<li><strong>Scheduler</strong>: <code>encoder_compute_budget</code> でステップあたりのエンコーダ計算量を制御</li>
<li><strong>GPUModelRunner</strong>:
<ul>
<li><code>_execute_mm_encoder()</code>: ビジョンエンコーダ実行（<code>model.embed_multimodal()</code>）</li>
<li><code>_gather_mm_embeddings()</code>: キャッシュからプレースホルダー位置に対応する埋め込みを取得</li>
<li><code>embed_input_ids()</code>: <code>masked_scatter_</code> でテキスト埋め込みとビジョン埋め込みをマージ</li>
</ul>
</li>
<li><strong>モデルforward</strong>: <code>input_ids</code> ではなく <code>inputs_embeds</code>（マージ済み）が渡される</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="アーキテクチャ概要"><a class="header" href="#アーキテクチャ概要">アーキテクチャ概要</a></h1>
<blockquote>
<p><strong>深度</strong>: [SHALLOW]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-09</p>
</blockquote>
<h2 id="概要-1"><a class="header" href="#概要-1">概要</a></h2>
<p>vLLMはUC Berkeley Sky Computing Lab発のLLM推論・サービングライブラリである。PagedAttentionによるKVキャッシュの効率的メモリ管理、Continuous Batchingによる動的バッチスケジューリングを中核技術とし、高スループット・低レイテンシのLLM推論を実現する。OpenAI互換APIサーバー、マルチモーダル対応、分散推論（Tensor/Pipeline/Data/Expert並列）を備える。</p>
<h2 id="全体構造"><a class="header" href="#全体構造">全体構造</a></h2>
<pre class="mermaid">graph TD
    subgraph エントリポイント層
        CLI["CLI&lt;br&gt;vllm.entrypoints.cli"]
        LLM["LLM&lt;br&gt;vllm.entrypoints.llm:101"]
        OpenAI["OpenAI互換API&lt;br&gt;vllm.entrypoints.openai"]
    end

    subgraph エンジン層
        AsyncLLM["AsyncLLM&lt;br&gt;vllm.v1.engine.async_llm:71"]
        LLMEngine["LLMEngine&lt;br&gt;vllm.v1.engine.llm_engine"]
        EngineCore["EngineCore&lt;br&gt;vllm.v1.engine.core:79"]
        InputProc["InputProcessor"]
        OutputProc["OutputProcessor"]
    end

    subgraph コア層
        Scheduler["Scheduler&lt;br&gt;vllm.v1.core.sched.scheduler:63"]
        KVCacheMgr["KVCacheManager&lt;br&gt;vllm.v1.core.kv_cache_manager:94"]
        BlockPool["BlockPool&lt;br&gt;vllm.v1.core.block_pool"]
    end

    subgraph 実行層
        Executor["Executor&lt;br&gt;vllm.v1.executor"]
        Worker["Worker&lt;br&gt;vllm.v1.worker.gpu_worker:70"]
        ModelRunner["GPUModelRunner&lt;br&gt;vllm.v1.worker.gpu_model_runner:329"]
    end

    subgraph モデル層
        Models["Models&lt;br&gt;vllm.model_executor.models&lt;br&gt;241ファイル"]
        Attention["Attention&lt;br&gt;2層構造"]
        Layers["Layers&lt;br&gt;vllm.model_executor.layers"]
    end

    CLI --&gt; AsyncLLM
    LLM --&gt; LLMEngine
    OpenAI --&gt; AsyncLLM

    AsyncLLM --&gt;|"ZMQ IPC"| EngineCore
    LLMEngine --&gt; EngineCore
    EngineCore --&gt; InputProc
    EngineCore --&gt; OutputProc

    EngineCore --&gt; Scheduler
    EngineCore --&gt; KVCacheMgr
    KVCacheMgr --&gt; BlockPool

    EngineCore --&gt; Executor
    Executor --&gt; Worker
    Worker --&gt; ModelRunner

    ModelRunner --&gt; Models
    ModelRunner --&gt; Attention
    Models --&gt; Layers
</pre>

<h2 id="アーキテクチャの世代"><a class="header" href="#アーキテクチャの世代">アーキテクチャの世代</a></h2>
<p><code>vllm/engine/</code> は <code>vllm/v1/</code> への薄いラッパーである。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/engine/llm_engine.py:4</code> — <code>LLMEngine = V1LLMEngine</code> の1行エイリアス</p>
<p>v1が現行アーキテクチャの本体であり、コードリーディングでは <code>vllm/v1/</code> を中心に読む。ただし <code>vllm/model_executor/</code>、<code>vllm/distributed/</code>、<code>vllm/multimodal/</code> 等はv1からも直接利用されるため調査対象に含む。</p>
<h2 id="主要コンポーネント"><a class="header" href="#主要コンポーネント">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>クラス</th><th>パス</th><th>役割</th></tr>
</thead>
<tbody>
<tr><td>AsyncLLM</td><td><code>AsyncLLM(EngineClient)</code></td><td><code>target/vllm/vllm/v1/engine/async_llm.py:71</code></td><td>非同期APIトップレベル</td></tr>
<tr><td>EngineCore</td><td><code>EngineCore</code></td><td><code>target/vllm/vllm/v1/engine/core.py:79</code></td><td>推論ループ内側。ZMQで外側と通信</td></tr>
<tr><td>Scheduler</td><td><code>Scheduler(SchedulerInterface)</code></td><td><code>target/vllm/vllm/v1/core/sched/scheduler.py:63</code></td><td>Continuous Batchingスケジューラ</td></tr>
<tr><td>KVCacheManager</td><td><code>KVCacheManager</code></td><td><code>target/vllm/vllm/v1/core/kv_cache_manager.py:94</code></td><td>KVキャッシュブロック管理</td></tr>
<tr><td>Executor</td><td><code>Executor</code>(ABC)</td><td><code>target/vllm/vllm/v1/executor/abstract.py</code></td><td>Worker群を束ねる実行層</td></tr>
<tr><td>Worker</td><td><code>Worker(WorkerBase)</code></td><td><code>target/vllm/vllm/v1/worker/gpu_worker.py:70</code></td><td>1 GPUデバイスを担当</td></tr>
<tr><td>GPUModelRunner</td><td><code>GPUModelRunner</code></td><td><code>target/vllm/vllm/v1/worker/gpu_model_runner.py:329</code></td><td>GPU上のフォワードパス実行</td></tr>
<tr><td>VllmConfig</td><td><code>VllmConfig</code></td><td><code>target/vllm/vllm/config/vllm.py</code></td><td>全設定の集約クラス</td></tr>
</tbody>
</table>
</div>
<h2 id="設計原則"><a class="header" href="#設計原則">設計原則</a></h2>
<h3 id="pagedattention"><a class="header" href="#pagedattention">PagedAttention</a></h3>
<p>KVキャッシュをOSの仮想メモリページングに着想を得てブロック単位で管理する。連続したGPUメモリ確保が不要になり、メモリ断片化を大幅に抑制する。</p>
<h3 id="continuous-batching"><a class="header" href="#continuous-batching">Continuous Batching</a></h3>
<p>リクエストの到着・完了に応じてバッチを動的に更新する。固定バッチサイズと異なり、GPU稼働率を最大化できる。</p>
<h3 id="zmq-ipc-によるプロセス分離"><a class="header" href="#zmq-ipc-によるプロセス分離">ZMQ IPC によるプロセス分離</a></h3>
<p>EngineCoreは別プロセス（<code>EngineCoreProc</code>）として動作し、ZeroMQソケットで上位エンジン層と通信する。これによりスケジューリングと推論処理を並行実行できる。</p>
<h3 id="プラグインシステム"><a class="header" href="#プラグインシステム">プラグインシステム</a></h3>
<p><code>vllm/plugins/</code> によるプラグイン機構を備え、起動時に <code>load_general_plugins()</code> で拡張を読み込む。</p>
<h2 id="ccuda-拡張"><a class="header" href="#ccuda-拡張">C++/CUDA 拡張</a></h2>
<p><code>target/vllm/csrc/</code> にパフォーマンスクリティカルなネイティブコードが配置されている。PagedAttentionカーネル、LayerNorm、量子化カーネル、カスタムAllReduce等が含まれる。Pythonからは <code>vllm._custom_ops</code> 等のバインディング経由で呼び出される。</p>
<h2 id="参照ファイル"><a class="header" href="#参照ファイル">参照ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/engine/llm_engine.py</code></td><td><code>LLMEngine</code>（v1への薄いラッパー）</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/async_llm.py</code></td><td><code>AsyncLLM</code></td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core.py</code></td><td><code>EngineCore</code>, <code>EngineCoreProc</code></td></tr>
<tr><td><code>target/vllm/vllm/v1/core/sched/scheduler.py</code></td><td><code>Scheduler</code></td></tr>
<tr><td><code>target/vllm/vllm/v1/core/kv_cache_manager.py</code></td><td><code>KVCacheManager</code></td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/abstract.py</code></td><td><code>Executor</code>(ABC)</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_worker.py</code></td><td><code>Worker</code></td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_model_runner.py</code></td><td><code>GPUModelRunner</code></td></tr>
<tr><td><code>target/vllm/vllm/config/vllm.py</code></td><td><code>VllmConfig</code></td></tr>
<tr><td><code>target/vllm/vllm/entrypoints/llm.py</code></td><td><code>LLM</code></td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ecconnectorencoder-cache-connector"><a class="header" href="#ecconnectorencoder-cache-connector">ECConnector（Encoder Cache Connector）</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM] | <strong>確信度</strong>: [VERIFIED] | <strong>最終更新</strong>: 2026-02-14</p>
</blockquote>
<h2 id="概要-2"><a class="header" href="#概要-2">概要</a></h2>
<p>ECConnectorは、マルチモーダルモデルのエンコーダ出力を<strong>vLLMインスタンス間</strong>または<strong>外部ストレージ</strong>と転送するためのプラグインフレームワークである。KV Transfer（デコーダKVキャッシュ用）とは完全に独立した系統で、エンコーダキャッシュに特化している。</p>
<p>主なユースケースは**Encoder-Prefill-Decode分離（EPD）**で、エンコーダ専用インスタンスが画像処理を行い、その結果をデコーダインスタンスに転送する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/</code> (パッケージ全体)</p>
<h2 id="アーキテクチャ"><a class="header" href="#アーキテクチャ">アーキテクチャ</a></h2>
<h3 id="ファイル構成"><a class="header" href="#ファイル構成">ファイル構成</a></h3>
<pre><code>vllm/distributed/ec_transfer/
├── __init__.py                          # get_ec_transfer(), has_ec_transfer() 公開API
├── ec_transfer_state.py                 # グローバルシングルトン管理
└── ec_connector/
    ├── __init__.py
    ├── base.py                          # ECConnectorBase 抽象基底クラス
    ├── factory.py                       # ECConnectorFactory レジストリ + 動的ロード
    └── example_connector.py             # ECExampleConnector 参照実装（safetensors）

vllm/v1/worker/
└── ec_connector_model_runner_mixin.py   # GPUModelRunner統合Mixin

vllm/config/
└── ec_transfer.py                       # ECTransferConfig 設定クラス
</code></pre>
<h3 id="2ロール分離アーキテクチャ"><a class="header" href="#2ロール分離アーキテクチャ">2ロール分離アーキテクチャ</a></h3>
<p>ECConnectorは<strong>Scheduler側</strong>と<strong>Worker側</strong>に分離され、同じクラスが両方のロールを担う：</p>
<pre class="mermaid">graph TB
    subgraph "Scheduler Process (ECConnectorRole.SCHEDULER)"
        SC[ECConnector Scheduler側]
        SCH[Scheduler]
        SCH --&gt;|has_cache_item| SC
        SCH --&gt;|update_state_after_alloc| SC
        SCH --&gt;|build_connector_meta| SC
    end

    subgraph "Worker Process (ECConnectorRole.WORKER)"
        WC[ECConnector Worker側]
        GMR[GPUModelRunner]
        GMR --&gt;|bind_connector_metadata| WC
        GMR --&gt;|start_load_caches| WC
        GMR --&gt;|save_caches| WC
        GMR --&gt;|get_finished| WC
    end

    SC --&gt;|ECConnectorMetadata&lt;br/&gt;SchedulerOutput経由| WC
</pre>

<div class="table-wrapper">
<table>
<thead>
<tr><th>ロール</th><th>生成場所</th><th>主な責務</th></tr>
</thead>
<tbody>
<tr><td>SCHEDULER</td><td><code>Scheduler.__init__()</code> via <code>ECConnectorFactory</code></td><td>キャッシュ存在チェック、メタデータ構築</td></tr>
<tr><td>WORKER</td><td><code>gpu_worker.py</code> の <code>ensure_ec_transfer_initialized()</code></td><td>キャッシュのロード/セーブ</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:135-138</code> (Scheduler側生成)
<strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_transfer_state.py:26-43</code> (Worker側生成)</p>
<h2 id="ecconnectorbase-抽象基底クラス"><a class="header" href="#ecconnectorbase-抽象基底クラス">ECConnectorBase 抽象基底クラス</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_connector/base.py:59-253</code></p>
<h3 id="プロパティ"><a class="header" href="#プロパティ">プロパティ</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>プロパティ</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>role</code></td><td><code>ECConnectorRole</code></td><td>SCHEDULER or WORKER</td></tr>
<tr><td><code>is_producer</code></td><td><code>bool</code></td><td>エンコーダキャッシュを生成する側か</td></tr>
<tr><td><code>is_consumer</code></td><td><code>bool</code></td><td>エンコーダキャッシュを消費する側か</td></tr>
</tbody>
</table>
</div>
<h3 id="抽象メソッド実装必須"><a class="header" href="#抽象メソッド実装必須">抽象メソッド（実装必須）</a></h3>
<h4 id="worker側3メソッド"><a class="header" href="#worker側3メソッド">Worker側（3メソッド）</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>シグネチャ</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>start_load_caches</code></td><td><code>(encoder_cache: dict[str, Tensor], **kwargs) → None</code></td><td>メタデータに基づきキャッシュをロード</td></tr>
<tr><td><code>save_caches</code></td><td><code>(encoder_cache: dict[str, Tensor], mm_hash: str, **kwargs) → None</code></td><td>エンコーダ出力を外部に保存</td></tr>
</tbody>
</table>
</div>
<h4 id="scheduler側3メソッド"><a class="header" href="#scheduler側3メソッド">Scheduler側（3メソッド）</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>シグネチャ</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>has_cache_item</code></td><td><code>(identifier: str) → bool</code></td><td>外部にキャッシュが存在するか判定</td></tr>
<tr><td><code>update_state_after_alloc</code></td><td><code>(request: Request, index: int) → None</code></td><td>割当後の内部状態更新</td></tr>
<tr><td><code>build_connector_meta</code></td><td><code>(scheduler_output: SchedulerOutput) → ECConnectorMetadata</code></td><td>Worker転送用メタデータ構築</td></tr>
</tbody>
</table>
</div>
<h3 id="具象メソッドオーバーライド任意"><a class="header" href="#具象メソッドオーバーライド任意">具象メソッド（オーバーライド任意）</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>デフォルト動作</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>bind_connector_metadata</code></td><td>メタデータ保持</td><td>Worker側: 毎step実行前に呼ばれる</td></tr>
<tr><td><code>clear_connector_metadata</code></td><td>Noneに設定</td><td>Worker側: 毎step実行後に呼ばれる</td></tr>
<tr><td><code>register_caches</code></td><td>no-op</td><td>将来のP2P機能用</td></tr>
<tr><td><code>get_finished</code></td><td><code>(None, None)</code></td><td>非同期転送完了通知</td></tr>
<tr><td><code>update_connector_output</code></td><td>no-op</td><td>Worker出力からScheduler状態を更新</td></tr>
<tr><td><code>request_finished</code></td><td><code>(False, None)</code></td><td>リクエスト完了時のフック</td></tr>
</tbody>
</table>
</div>
<h2 id="ectransferconfig-設定"><a class="header" href="#ectransferconfig-設定">ECTransferConfig 設定</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/config/ec_transfer.py:16-108</code></p>
<h3 id="ecロール"><a class="header" href="#ecロール">ECロール</a></h3>
<pre><code class="language-python">ECRole = Literal["ec_producer", "ec_consumer", "ec_both"]
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ロール</th><th>説明</th><th><code>is_producer</code></th><th><code>is_consumer</code></th></tr>
</thead>
<tbody>
<tr><td><code>ec_producer</code></td><td>エンコーダ計算+キャッシュ保存</td><td>True</td><td>False</td></tr>
<tr><td><code>ec_consumer</code></td><td>キャッシュ読み込み+デコーダ実行</td><td>False</td><td>True</td></tr>
<tr><td><code>ec_both</code></td><td>両方の機能</td><td>True</td><td>True</td></tr>
</tbody>
</table>
</div>
<h3 id="主要設定パラメータ"><a class="header" href="#主要設定パラメータ">主要設定パラメータ</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>ec_connector</code></td><td>None</td><td>コネクタ名（例: “ECExampleConnector”）</td></tr>
<tr><td><code>ec_role</code></td><td>None</td><td>ECロール</td></tr>
<tr><td><code>ec_connector_module_path</code></td><td>None</td><td>カスタムコネクタのPythonモジュールパス</td></tr>
<tr><td><code>ec_connector_extra_config</code></td><td><code>{}</code></td><td>コネクタ固有の追加設定</td></tr>
<tr><td><code>ec_buffer_device</code></td><td>“cuda”</td><td>バッファデバイス</td></tr>
<tr><td><code>ec_buffer_size</code></td><td>1e9</td><td>バッファサイズ（バイト）</td></tr>
<tr><td><code>ec_ip</code> / <code>ec_port</code></td><td>127.0.0.1:14579</td><td>P2P接続用</td></tr>
<tr><td><code>ec_rank</code> / <code>ec_parallel_size</code></td><td>None / 1</td><td>分散接続設定</td></tr>
</tbody>
</table>
</div>
<h2 id="ecconnectorfactory"><a class="header" href="#ecconnectorfactory">ECConnectorFactory</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_connector/factory.py:20-85</code></p>
<h3 id="コネクタ登録方式"><a class="header" href="#コネクタ登録方式">コネクタ登録方式</a></h3>
<p>2つの登録方法がある：</p>
<ol>
<li><strong>静的登録</strong>: <code>ECConnectorFactory.register_connector()</code> でモジュール遅延ロード登録</li>
<li><strong>動的ロード</strong>: <code>ec_connector_module_path</code> で任意のPythonモジュールからロード</li>
</ol>
<pre><code class="language-python"># 静的登録（factory.py末尾）
ECConnectorFactory.register_connector(
    "ECExampleConnector",
    "vllm.distributed.ec_transfer.ec_connector.example_connector",
    "ECExampleConnector",
)

# 動的ロード（ec_connector_module_pathが設定されている場合）
connector_module = importlib.import_module(connector_module_path)
connector_cls = getattr(connector_module, connector_name)
</code></pre>
<h3 id="現在登録済みコネクタ"><a class="header" href="#現在登録済みコネクタ">現在登録済みコネクタ</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>名前</th><th>実装</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>ECExampleConnector</code></td><td><code>example_connector.py</code></td><td>参照実装（safetensorsディスク保存）</td></tr>
</tbody>
</table>
</div>
<h2 id="ecexampleconnector-参照実装"><a class="header" href="#ecexampleconnector-参照実装">ECExampleConnector 参照実装</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_connector/example_connector.py:45-199</code></p>
<p>safetensorsフォーマットでディスクにエンコーダキャッシュを保存/読み込みする参照実装。</p>
<h3 id="メタデータ"><a class="header" href="#メタデータ">メタデータ</a></h3>
<pre><code class="language-python">@dataclass
class MMMeta:
    mm_hash: str      # マルチモーダルデータのハッシュ
    num_token: int     # エンコーダトークン数

@dataclass
class ECExampleConnectorMetadata(ECConnectorMetadata):
    mm_datas: list[MMMeta]  # ロードすべきエントリ一覧
</code></pre>
<h3 id="ストレージ構造"><a class="header" href="#ストレージ構造">ストレージ構造</a></h3>
<pre><code>{shared_storage_path}/
└── {mm_hash}/
    └── encoder_cache.safetensors    # {"ec_cache": Tensor} 形式
</code></pre>
<h3 id="動作フロー"><a class="header" href="#動作フロー">動作フロー</a></h3>
<h4 id="保存producer側"><a class="header" href="#保存producer側">保存（Producer側）</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_connector/example_connector.py:98-118</code></p>
<ol>
<li>GPUModelRunnerが <code>_execute_mm_encoder()</code> 完了後に <code>maybe_save_ec_to_connector()</code> を呼ぶ</li>
<li><code>save_caches()</code>: テンソルを <code>.detach().cpu()</code> してsafetensorsで保存</li>
</ol>
<h4 id="読み込みconsumer側"><a class="header" href="#読み込みconsumer側">読み込み（Consumer側）</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_connector/example_connector.py:63-96</code></p>
<ol>
<li>Scheduler側: <code>has_cache_item()</code> でファイル存在確認 (<code>os.path.exists</code>)</li>
<li>Scheduler側: <code>build_connector_meta()</code> でロード対象リストを構築</li>
<li>Worker側: <code>start_load_caches()</code> でsafetensorsからGPUにロード</li>
</ol>
<h4 id="存在確認"><a class="header" href="#存在確認">存在確認</a></h4>
<p><code>has_cache_item()</code> は <code>os.path.exists()</code> でsafetensorsファイルの存在をチェック。</p>
<h2 id="schedulerとの統合"><a class="header" href="#schedulerとの統合">Schedulerとの統合</a></h2>
<h3 id="_schedule_encoder_inputs-内の分岐"><a class="header" href="#_schedule_encoder_inputs-内の分岐">_schedule_encoder_inputs() 内の分岐</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:1212-1218</code></p>
<pre><code class="language-python">if self.ec_connector is not None and self.ec_connector.has_cache_item(identifier):
    # 外部キャッシュにヒット → エンコーダ計算不要、compute_budget消費なし
    mm_hashes_to_schedule.add(item_identifier)
    external_load_encoder_input.append(i)
    num_embeds_to_schedule += num_encoder_embeds
    continue
</code></pre>
<p>ECConnectorにキャッシュがある場合：</p>
<ul>
<li><code>encoder_compute_budget</code> は<strong>消費しない</strong>（エンコーダ計算不要のため）</li>
<li><code>external_load_encoder_input</code> リストに追加</li>
<li><code>encoder_cache_manager.allocate()</code> は実行される（GPU側に空きが必要）</li>
</ul>
<h3 id="割当後の状態更新"><a class="header" href="#割当後の状態更新">割当後の状態更新</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:523-527</code></p>
<pre><code class="language-python">if external_load_encoder_input:
    for i in external_load_encoder_input:
        self.encoder_cache_manager.allocate(request, i)
        if self.ec_connector is not None:
            self.ec_connector.update_state_after_alloc(request, i)
</code></pre>
<h3 id="メタデータ構築"><a class="header" href="#メタデータ構築">メタデータ構築</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:899-904</code></p>
<p><code>build_connector_meta()</code> がSchedulerOutputに <code>ec_connector_metadata</code> を設定。Worker側はこのメタデータを使ってロード対象を特定する。</p>
<h2 id="gpumodelrunnerとの統合"><a class="header" href="#gpumodelrunnerとの統合">GPUModelRunnerとの統合</a></h2>
<h3 id="ecconnectormodelrunnermixin"><a class="header" href="#ecconnectormodelrunnermixin">ECConnectorModelRunnerMixin</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/ec_connector_model_runner_mixin.py:25-87</code></p>
<p>GPUModelRunnerに3つのstatic methodを提供：</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>maybe_save_ec_to_connector</code></td><td>エンコーダ出力保存（Producer時）</td></tr>
<tr><td><code>get_finished_ec_transfers</code></td><td>非同期転送完了確認</td></tr>
<tr><td><code>maybe_get_ec_connector_output</code></td><td>コンテキストマネージャでライフサイクル管理</td></tr>
</tbody>
</table>
</div>
<h3 id="コンテキストマネージャのライフサイクル"><a class="header" href="#コンテキストマネージャのライフサイクル">コンテキストマネージャのライフサイクル</a></h3>
<pre><code class="language-python">with self.maybe_get_ec_connector_output(scheduler_output, encoder_cache) as output:
    # 1. bind_connector_metadata() → メタデータ設定
    # 2. Consumer時: start_load_caches() → 外部からロード
    # 3. yield → エンコーダ実行、gather処理
    # 4. get_finished() → 非同期完了確認
    # 5. clear_connector_metadata() → クリーンアップ
</code></pre>
<h3 id="producer専用モード"><a class="header" href="#producer専用モード">Producer専用モード</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:3343-3349</code></p>
<p>Producer専用インスタンスは、エンコーダ実行後にデコーダ実行をスキップし、空のModelRunnerOutputを返す：</p>
<pre><code class="language-python">if has_ec_transfer() and get_ec_transfer().is_producer:
    with self.maybe_get_ec_connector_output(...) as ec_connector_output:
        self._execute_mm_encoder(scheduler_output)
        return make_empty_encoder_model_runner_output(scheduler_output)
</code></pre>
<p>また、Producer専用インスタンスはKVキャッシュを確保しない：</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:6160-6161</code></p>
<pre><code class="language-python">if has_ec_transfer() and get_ec_transfer().is_producer:
    return {}  # KVCacheSpec空 → KVキャッシュ確保なし
</code></pre>
<h2 id="ecconnectoroutput"><a class="header" href="#ecconnectoroutput">ECConnectorOutput</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/outputs.py:151-154</code></p>
<pre><code class="language-python">@dataclass
class ECConnectorOutput:
    finished_sending: set[str] | None = None
    finished_recving: set[str] | None = None
</code></pre>
<p>ModelRunnerOutputに含まれてScheduler側に返されるが、<strong>現時点ではScheduler側で未消費</strong>（<code>ec_connector_output</code>を読み取るコードがSchedulerにない）。非同期転送完了フィードバックは将来実装予定。</p>
<h2 id="グローバルシングルトン管理"><a class="header" href="#グローバルシングルトン管理">グローバルシングルトン管理</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_transfer_state.py:14-43</code></p>
<pre><code class="language-python">_EC_CONNECTOR_AGENT: ECConnectorBase | None = None
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>関数</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>has_ec_transfer()</code></td><td>ECConnectorが初期化済みか</td></tr>
<tr><td><code>get_ec_transfer()</code></td><td>シングルトン取得（未初期化ならassert）</td></tr>
<tr><td><code>ensure_ec_transfer_initialized(config)</code></td><td>Workerロールで初期化（冪等）</td></tr>
</tbody>
</table>
</div>
<p>Worker側のシングルトン初期化は <code>gpu_worker.py</code> で <code>ensure_ec_transfer_initialized()</code> を呼ぶことで行われる。Scheduler側は <code>ECConnectorFactory.create_connector()</code> で直接生成し、<code>self.ec_connector</code> に保持する（シングルトンではない）。</p>
<h2 id="カスタムecconnector実装ガイド"><a class="header" href="#カスタムecconnector実装ガイド">カスタムECConnector実装ガイド</a></h2>
<h3 id="最小実装"><a class="header" href="#最小実装">最小実装</a></h3>
<ol>
<li><code>ECConnectorBase</code> を継承</li>
<li>5つの抽象メソッドを実装</li>
<li>ECTransferConfigの<code>ec_connector</code>にクラス名、<code>ec_connector_module_path</code>にモジュールパスを指定</li>
</ol>
<h3 id="実装の要点"><a class="header" href="#実装の要点">実装の要点</a></h3>
<ul>
<li><code>has_cache_item()</code> はSchedulerのホットパスで呼ばれるため<strong>高速</strong>であるべき</li>
<li><code>start_load_caches()</code> は <code>encoder_cache</code> dict に直接テンソルを追加する</li>
<li><code>save_caches()</code> は <code>encoder_cache[mm_hash]</code> からGPUテンソルを取得して保存する</li>
<li><code>build_connector_meta()</code> は内部状態をリセットすること</li>
</ul>
<h3 id="起動コマンド例"><a class="header" href="#起動コマンド例">起動コマンド例</a></h3>
<pre><code class="language-bash"># Producer（エンコーダ専用インスタンス）
vllm serve model_name \
    --ec-connector ECExampleConnector \
    --ec-role ec_producer \
    --ec-connector-extra-config '{"shared_storage_path": "/shared/cache"}'

# Consumer（デコーダインスタンス）
vllm serve model_name \
    --ec-connector ECExampleConnector \
    --ec-role ec_consumer \
    --ec-connector-extra-config '{"shared_storage_path": "/shared/cache"}'
</code></pre>
<h2 id="上流下流依存関係"><a class="header" href="#上流下流依存関係">上流・下流依存関係</a></h2>
<h3 id="上流"><a class="header" href="#上流">上流</a></h3>
<ul>
<li><strong>ECTransferConfig</strong>: <code>ec_connector</code>, <code>ec_role</code> 等の設定</li>
<li><strong>Scheduler</strong>: キャッシュ存在確認、状態更新、メタデータ構築の呼び出し</li>
<li><strong>GPUModelRunner</strong>: エンコーダ実行結果の保存、ロード済みキャッシュの利用</li>
</ul>
<h3 id="下流"><a class="header" href="#下流">下流</a></h3>
<ul>
<li><strong>外部ストレージ</strong>: safetensors（例）、共有メモリ、ネットワーク等（実装依存）</li>
</ul>
<h2 id="開発状況未実装機能"><a class="header" href="#開発状況未実装機能">開発状況・未実装機能</a></h2>
<ol>
<li><strong>ECConnectorOutput未消費</strong>: Worker→Scheduler方向の非同期転送完了フィードバックが未実装</li>
<li><strong>request_finished未統合</strong>: Schedulerから<code>ec_connector.request_finished()</code>が呼ばれていない</li>
<li><strong>register_caches未実装</strong>: P2P直接転送のためのキャッシュ登録（TODO）</li>
<li><strong>エンコーダキャッシュ事前割り当て未対応</strong>: <code>encoder_cache</code> が <code>dict</code> のため、固定バッファへの移行が必要</li>
<li><strong>登録済みコネクタが1つのみ</strong>: ECExampleConnector（デバッグ用）のみ。SHMConnector等は外部PR待ち</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="encodercacheエンコーダキャッシュ"><a class="header" href="#encodercacheエンコーダキャッシュ">EncoderCache（エンコーダキャッシュ）</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM] | <strong>確信度</strong>: [VERIFIED] | <strong>最終更新</strong>: 2026-02-14</p>
</blockquote>
<h2 id="概要-3"><a class="header" href="#概要-3">概要</a></h2>
<p>EncoderCacheは、マルチモーダルモデルにおけるエンコーダ出力（例: ビジョンエンコーダの画像埋め込み）のGPUメモリ上キャッシュを管理するコンポーネントである。2層構造で、Scheduler側の論理管理（<code>EncoderCacheManager</code>）とWorker側の物理ストレージ（<code>GPUModelRunner.encoder_cache</code>）に分離されている。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py</code> (EncoderCacheManager)</p>
<h2 id="アーキテクチャ-1"><a class="header" href="#アーキテクチャ-1">アーキテクチャ</a></h2>
<h3 id="2層構造"><a class="header" href="#2層構造">2層構造</a></h3>
<pre class="mermaid">graph TB
    subgraph Scheduler Process
        ECM[EncoderCacheManager&lt;br/&gt;論理管理]
    end
    subgraph Worker Process
        EC["encoder_cache: dict[str, Tensor]&lt;br/&gt;GPU物理ストレージ"]
    end
    ECM --&gt;|"free_encoder_mm_hashes&lt;br/&gt;(SchedulerOutput経由)"| EC
    ECM --&gt;|"scheduled_encoder_inputs&lt;br/&gt;(何を計算すべきか)"| EC
</pre>

<div class="table-wrapper">
<table>
<thead>
<tr><th>層</th><th>場所</th><th>データ構造</th><th>役割</th></tr>
</thead>
<tbody>
<tr><td>論理管理</td><td>Scheduler</td><td><code>EncoderCacheManager</code></td><td>キャッシュ容量管理、参照カウント、Eviction判定</td></tr>
<tr><td>物理ストレージ</td><td>GPUModelRunner</td><td><code>dict[str, torch.Tensor]</code></td><td>mm_hash → エンコーダ出力テンソルの保持</td></tr>
</tbody>
</table>
</div>
<h2 id="encodercachemanager-詳細"><a class="header" href="#encodercachemanager-詳細">EncoderCacheManager 詳細</a></h2>
<h3 id="主要フィールド"><a class="header" href="#主要フィールド">主要フィールド</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:67-77</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>cache_size</code></td><td><code>int</code></td><td>エンコーダ埋め込み数で測った総容量</td></tr>
<tr><td><code>num_free_slots</code></td><td><code>int</code></td><td>現在利用可能な空きスロット数</td></tr>
<tr><td><code>num_freeable_slots</code></td><td><code>int</code></td><td>参照ゼロエントリの回収で即座に利用可能になるスロット数</td></tr>
<tr><td><code>cached</code></td><td><code>dict[str, set[str]]</code></td><td>mm_hash → 参照中リクエストIDの集合</td></tr>
<tr><td><code>freeable</code></td><td><code>OrderedDict[str, int]</code></td><td>参照ゼロエントリの挿入順リスト（mm_hash → 埋め込み数）</td></tr>
<tr><td><code>freed</code></td><td><code>list[str]</code></td><td>直近のEvictionで物理解放すべきmm_hashリスト</td></tr>
</tbody>
</table>
</div>
<h3 id="eviction方式-fifo参照ゼロエントリの遅延解放"><a class="header" href="#eviction方式-fifo参照ゼロエントリの遅延解放">Eviction方式: FIFO（参照ゼロエントリの遅延解放）</a></h3>
<p>EncoderCacheManagerは<strong>遅延解放FIFO方式</strong>を採用する：</p>
<ol>
<li>リクエスト完了時、参照カウントが0になったエントリは即座には解放されず <code>freeable</code> OrderedDictに追加</li>
<li>新しいエンコーダ出力のキャッシュ確保（<code>can_allocate()</code>）時に空きが不足した場合のみ、古い順にEviction</li>
<li>Evictionされたmm_hashは <code>freed</code> リストに追加され、次の <code>get_freed_mm_hashes()</code> でWorkerに通知</li>
<li>Worker側で <code>encoder_cache.pop(mm_hash)</code> により物理メモリ解放</li>
</ol>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:119-178</code> (can_allocate)</p>
<pre class="mermaid">stateDiagram-v2
    [*] --&gt; Active: allocate()
    Active --&gt; Active: check_and_update_cache()で新リクエスト参照追加
    Active --&gt; Freeable: free_encoder_input()で参照カウント=0
    Freeable --&gt; Active: check_and_update_cache()で再参照
    Freeable --&gt; Evicted: can_allocate()で空き不足時
    Evicted --&gt; [*]: Worker側でpop()
</pre>

<h3 id="共有キャッシュ"><a class="header" href="#共有キャッシュ">共有キャッシュ</a></h3>
<p>同じ画像を含む複数リクエストが同時に処理される場合、同一の<code>mm_hash</code>を持つエンコーダ出力は<strong>共有</strong>される。<code>cached</code> dictのvalue（<code>set[str]</code>）が複数リクエストIDを保持し、全リクエストの完了後にのみ<code>freeable</code>に移行する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:91-117</code> (check_and_update_cache)</p>
<h3 id="主要メソッド"><a class="header" href="#主要メソッド">主要メソッド</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>呼び出し元</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>check_and_update_cache(request, input_id)</code></td><td>Scheduler._schedule_encoder_inputs</td><td>キャッシュヒット判定。ヒット時は参照追加してTrue</td></tr>
<tr><td><code>can_allocate(request, input_id, budget, scheduled)</code></td><td>Scheduler._schedule_encoder_inputs</td><td>空き確認+必要時Eviction。False=予算不足</td></tr>
<tr><td><code>allocate(request, input_id)</code></td><td>Scheduler（RUNNING/WAITING処理）</td><td>論理的にキャッシュ空間を確保</td></tr>
<tr><td><code>free_encoder_input(request, input_id)</code></td><td>Scheduler</td><td>1エントリの参照解放</td></tr>
<tr><td><code>free(request)</code></td><td>Scheduler（リクエスト完了/中断時）</td><td>全エントリの参照解放</td></tr>
<tr><td><code>get_freed_mm_hashes()</code></td><td>Scheduler._build_scheduler_output</td><td>Eviction済みmm_hashリスト取得（Worker通知用）</td></tr>
</tbody>
</table>
</div>
<h3 id="キャッシュ容量の決定"><a class="header" href="#キャッシュ容量の決定">キャッシュ容量の決定</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:269-316</code> (compute_mm_encoder_budget)</p>
<pre><code class="language-python">encoder_compute_budget = max(max_num_encoder_input_tokens, max_tokens_per_mm_item)
encoder_cache_size = max(encoder_cache_size_config, max_tokens_per_mm_item)
</code></pre>
<ul>
<li><code>max_tokens_per_mm_item</code>: モデルがサポートする全モダリティの最大トークン数</li>
<li><code>max_num_encoder_input_tokens</code>: SchedulerConfig設定値</li>
<li>1アイテムは必ずキャッシュできることを保証</li>
</ul>
<h2 id="gpumodelrunnerencoder_cache物理ストレージ"><a class="header" href="#gpumodelrunnerencoder_cache物理ストレージ">GPUModelRunner.encoder_cache（物理ストレージ）</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:439</code></p>
<pre><code class="language-python">self.encoder_cache: dict[str, torch.Tensor] = {}
</code></pre>
<ul>
<li><strong>キー</strong>: <code>mm_hash</code>（マルチモーダルデータのidentifier）</li>
<li><strong>値</strong>: エンコーダ出力テンソル（GPU上）</li>
<li><strong>書き込み</strong>: <code>_execute_mm_encoder()</code> 完了時に <code>encoder_cache[mm_hash] = output</code></li>
<li><strong>読み取り</strong>: <code>_gather_mm_embeddings()</code> でデコーダ入力に合成</li>
<li><strong>削除</strong>: <code>_update_states()</code> で <code>scheduler_output.free_encoder_mm_hashes</code> に従い <code>pop()</code></li>
</ul>
<h3 id="ecconnectorとの連携"><a class="header" href="#ecconnectorとの連携">ECConnectorとの連携</a></h3>
<p>エンコーダ出力をGPUキャッシュに保存した直後に、ECConnector（有効時）にも保存する：</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:2442-2445</code></p>
<pre><code class="language-python">for mm_hash, output in zip(mm_hashes, encoder_outputs):
    self.encoder_cache[mm_hash] = output
    self.maybe_save_ec_to_connector(self.encoder_cache, mm_hash)
</code></pre>
<p>Consumer側では、<code>execute_model()</code> の冒頭で ECConnector から <code>encoder_cache</code> にロードする：</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/ec_connector_model_runner_mixin.py:76-77</code></p>
<pre><code class="language-python">if ec_connector.is_consumer:
    ec_connector.start_load_caches(encoder_cache, **kwargs)
</code></pre>
<h2 id="encoderdecodercachemanager"><a class="header" href="#encoderdecodercachemanager">EncoderDecoderCacheManager</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:323-382</code></p>
<p>Encoder-Decoderモデル（例: Whisper）用の暫定実装。<code>EncoderCacheManager</code>を継承するがキャッシュ共有機能を持たず、毎回エンコーダを実行する。主な違い：</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>特性</th><th>EncoderCacheManager (MM)</th><th>EncoderDecoderCacheManager</th></tr>
</thead>
<tbody>
<tr><td>キャッシュ共有</td><td>あり（mm_hash based）</td><td>なし（常にFalse）</td></tr>
<tr><td>参照カウント</td><td>あり</td><td>なし</td></tr>
<tr><td>Eviction</td><td>FIFO遅延解放</td><td>即時解放（1step遅延バッファあり）</td></tr>
<tr><td>用途</td><td>Vision-Language Model</td><td>Encoder-Decoder Model</td></tr>
</tbody>
</table>
</div>
<h2 id="上流下流依存関係-1"><a class="header" href="#上流下流依存関係-1">上流・下流依存関係</a></h2>
<h3 id="上流-1"><a class="header" href="#上流-1">上流</a></h3>
<ul>
<li><strong>Scheduler</strong>: <code>_schedule_encoder_inputs()</code> でキャッシュヒット判定・割当・解放指示</li>
<li><strong>SchedulerConfig</strong>: <code>encoder_cache_size</code>, <code>max_num_encoder_input_tokens</code> で容量決定</li>
</ul>
<h3 id="下流-1"><a class="header" href="#下流-1">下流</a></h3>
<ul>
<li><strong>GPUModelRunner</strong>: 物理テンソル保持、エンコーダ実行、gather処理</li>
<li><strong>ECConnector</strong>: 外部ストレージへの保存/読み込み（有効時）</li>
</ul>
<h2 id="データフロー"><a class="header" href="#データフロー">データフロー</a></h2>
<pre class="mermaid">sequenceDiagram
    participant S as Scheduler
    participant ECM as EncoderCacheManager
    participant SO as SchedulerOutput
    participant GMR as GPUModelRunner
    participant EC as encoder_cache (GPU)

    Note over S: _schedule_encoder_inputs()
    S-&gt;&gt;ECM: check_and_update_cache(req, i)
    alt キャッシュヒット
        ECM--&gt;&gt;S: True（計算スキップ）
    else キャッシュミス
        ECM--&gt;&gt;S: False
        S-&gt;&gt;ECM: can_allocate(req, i, budget, scheduled)
        alt 空きあり（Eviction含む）
            ECM--&gt;&gt;S: True
            S-&gt;&gt;ECM: allocate(req, i)
        else 空き不足
            ECM--&gt;&gt;S: False（トークン数調整）
        end
    end

    Note over S: _build_scheduler_output()
    S-&gt;&gt;ECM: get_freed_mm_hashes()
    ECM--&gt;&gt;S: freed list
    S-&gt;&gt;SO: free_encoder_mm_hashes, scheduled_encoder_inputs

    Note over GMR: execute_model()
    GMR-&gt;&gt;EC: _execute_mm_encoder() → 保存
    GMR-&gt;&gt;EC: _gather_mm_embeddings() → 読取
    GMR-&gt;&gt;EC: _update_states() → free_encoder_mm_hashes に従い削除
</pre>

<h2 id="注意事項"><a class="header" href="#注意事項">注意事項</a></h2>
<ul>
<li>キャッシュサイズは<strong>エンコーダ埋め込み数</strong>で測定される。画像間のテキストトークン（break tokens等）は含まない</li>
<li>物理メモリ解放はEviction判定（Scheduler側）と実際の<code>pop()</code>（Worker側）の間に1step以上のラグがある</li>
<li><code>num_freeable_slots</code> は <code>num_free_slots</code> 以上の値を常に持つ（freeable + free の合計）</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="enginecore-サマリー"><a class="header" href="#enginecore-サマリー">EngineCore サマリー</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-4"><a class="header" href="#概要-4">概要</a></h2>
<p><code>EngineCore</code>はバックエンドプロセス（<code>EngineCoreProc</code>）内で動作する推論ループの中央制御コンポーネントである。<code>Scheduler</code>、<code>ModelExecutor</code>、<code>KVCacheManager</code>を統括し、<code>step()</code>メソッドで <strong>schedule → execute → update</strong> のサイクルを繰り返す。フロントエンドプロセスとはZMQ IPC経由で通信し、<code>EngineCoreRequest</code>を受信して<code>EngineCoreOutputs</code>を返す。</p>
<h2 id="アーキテクチャ-2"><a class="header" href="#アーキテクチャ-2">アーキテクチャ</a></h2>
<pre class="mermaid">graph TD
    subgraph EngineCoreプロセス
        EC["EngineCore"]
        Sched["Scheduler"]
        KVM["KVCacheManager"]
        Exec["ModelExecutor"]

        EC --&gt;|"1. schedule()"| Sched
        Sched --&gt;|"allocate_slots()"| KVM
        Sched --&gt;|"SchedulerOutput"| EC
        EC --&gt;|"2. execute_model()"| Exec
        Exec --&gt;|"Future&lt;ModelRunnerOutput&gt;"| EC
        EC --&gt;|"3. update_from_output()"| Sched
        Sched --&gt;|"EngineCoreOutputs"| EC
    end

    ZMQ_IN["ZMQ ROUTER&lt;br&gt;(受信)"] --&gt;|"EngineCoreRequest"| EC
    EC --&gt;|"EngineCoreOutputs"| ZMQ_OUT["ZMQ PUSH&lt;br&gt;(送信)"]
</pre>

<h2 id="主要コンポーネント-1"><a class="header" href="#主要コンポーネント-1">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>用途</th><th>ファイル</th></tr>
</thead>
<tbody>
<tr><td><code>EngineCore</code></td><td>推論ループ本体</td><td><code>target/vllm/vllm/v1/engine/core.py:82</code></td></tr>
<tr><td><code>EngineCoreProc</code></td><td>プロセスラッパー。ZMQソケット管理とイベントループ</td><td><code>target/vllm/vllm/v1/engine/core.py</code></td></tr>
</tbody>
</table>
</div>
<h2 id="主要メソッド-1"><a class="header" href="#主要メソッド-1">主要メソッド</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>__init__()</code></td><td>L82</td><td>Scheduler, ModelExecutor, KVキャッシュの初期化</td></tr>
<tr><td><code>step()</code></td><td>L389</td><td>メインループ: schedule → execute → update</td></tr>
<tr><td><code>step_with_batch_queue()</code></td><td>L434</td><td>パイプライン並列化版step（batch_queue使用）</td></tr>
<tr><td><code>add_request()</code></td><td>L288</td><td>リクエストをバリデーション後Schedulerに登録</td></tr>
<tr><td><code>post_step()</code></td><td>L424</td><td>step後処理（Speculative Decodingのドラフトトークン更新）</td></tr>
</tbody>
</table>
</div>
<h2 id="step-サイクル"><a class="header" href="#step-サイクル">step() サイクル</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:389</code></p>
<pre><code>EngineCore.step() -&gt; tuple[dict[int, EngineCoreOutputs], bool]
  │
  ├─ スケジューラ停止チェック                           # L397
  │   if _scheduler_paused: return {}, False
  │
  ├─ リクエスト有無チェック                             # L402
  │   if not scheduler.has_requests(): return {}, False
  │
  ├─ 1. scheduler.schedule()                            # L404
  │   → SchedulerOutput
  │
  ├─ 2. executor.execute_model(scheduler_output,        # L405
  │       non_block=True)
  │   → Future[ModelRunnerOutput | None]
  │
  ├─ 3. scheduler.get_grammar_bitmask(scheduler_output) # L406
  │   → grammar_output（構造化出力用）
  │
  ├─ 4. future.result()                                 # L411
  │   → ModelRunnerOutput（ブロッキング待機）
  │
  ├─ 5. if model_output is None:                        # L413
  │       model_output = executor.sample_tokens(grammar_output)
  │   （非同期スケジューリング時: execute_modelとsamplingが分離）
  │
  ├─ 6. _process_aborts_queue()                         # L417
  │
  └─ 7. scheduler.update_from_output(                   # L418
  │       scheduler_output, model_output)
  │   → dict[int, EngineCoreOutputs]
  │
  └─ return (engine_core_outputs,                       # L422
             total_num_scheduled_tokens &gt; 0)
</code></pre>
<p><strong>戻り値</strong>:</p>
<ul>
<li>第1要素: クライアントインデックス → EngineCoreOutputs のマッピング</li>
<li>第2要素: モデル実行が行われたか（<code>total_num_scheduled_tokens &gt; 0</code>）</li>
</ul>
<h2 id="add_request-フロー"><a class="header" href="#add_request-フロー">add_request() フロー</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:288</code></p>
<pre><code>add_request(request, request_wave=0)                    # L288
  ├─ request_id の型チェック（str必須）                   # L295
  ├─ pooling_params のタスクバリデーション                # L300
  ├─ kv_transfer_params の互換性チェック                  # L311
  └─ scheduler.add_request(request)                      # L319
</code></pre>
<h2 id="batch_queue-パイプライン並列化-shallow"><a class="header" href="#batch_queue-パイプライン並列化-shallow">batch_queue パイプライン並列化 [SHALLOW]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:434</code> (step_with_batch_queue)</p>
<p><code>max_concurrent_batches &gt; 1</code> の場合、<code>step_with_batch_queue()</code> が <code>step_fn</code> として使用される。スケジューリングとモデル実行をパイプライン的にオーバーラップさせ、GPUのアイドル時間を削減する。</p>
<ul>
<li><code>batch_queue</code>: <code>deque[tuple[Future, SchedulerOutput, Future]]</code></li>
<li>新しいスケジュール結果を <code>appendleft()</code> で追加、完了待ちを <code>pop()</code> で取得</li>
<li>前のバッチの実行完了を待たずに次のバッチをスケジュール可能</li>
</ul>
<h2 id="kvキャッシュ初期化フロー-shallow"><a class="header" href="#kvキャッシュ初期化フロー-shallow">KVキャッシュ初期化フロー [SHALLOW]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:82</code> (<strong>init</strong>)</p>
<pre><code>EngineCore.__init__()
  → _initialize_kv_caches()
    → model_executor.get_kv_cache_specs()       # モデルのKVキャッシュ要件取得
    → determine_available_memory()               # GPUメモリプロファイリング
    → get_kv_cache_configs()                     # ブロック数等の設定算出
    → generate_scheduler_kv_cache_config()       # Scheduler用設定生成
    → model_executor.initialize_from_config()    # GPUメモリ確保
</code></pre>
<p>KV Connector（KV Transfer/LMCache連携）が有効な場合:</p>
<ul>
<li><code>scheduler.get_kv_connector()</code> でコネクタ有無を確認 (L159)</li>
<li>各ワーカーのハンドシェイクメタデータを収集・統合 (L164-175)</li>
</ul>
<h2 id="async_scheduling-shallow"><a class="header" href="#async_scheduling-shallow">async_scheduling [SHALLOW]</a></h2>
<p><code>vllm_config.scheduler_config.async_scheduling</code> で有効化。</p>
<ul>
<li>通常: <code>execute_model()</code> がモデル実行 + トークンサンプリングをまとめて実行</li>
<li>async有効時: <code>execute_model()</code> はモデル実行のみ（<code>None</code> を返す）→ <code>sample_tokens()</code> で別途サンプリング</li>
<li><code>post_step()</code> でのSpeculative Decodingドラフトトークン更新タイミングに影響</li>
</ul>
<h2 id="設定"><a class="header" href="#設定">設定</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>max_concurrent_batches</code></td><td>1</td><td>batch_queueサイズ（&gt;1でパイプライン並列化）</td></tr>
<tr><td><code>async_scheduling</code></td><td>False</td><td>非同期スケジューリングモード</td></tr>
</tbody>
</table>
</div>
<h2 id="呼び出しフロー"><a class="header" href="#呼び出しフロー">呼び出しフロー</a></h2>
<pre><code>[EngineCoreProc イベントループ]
  ├─ ZMQ受信 → EngineCore.add_request()
  ├─ EngineCore.step_fn()  (= step() or step_with_batch_queue())
  │   ├─ Scheduler.schedule()
  │   ├─ ModelExecutor.execute_model()
  │   └─ Scheduler.update_from_output()
  ├─ EngineCore.post_step()
  └─ ZMQ送信 ← EngineCoreOutputs
</code></pre>
<h2 id="関連ドキュメント"><a class="header" href="#関連ドキュメント">関連ドキュメント</a></h2>
<ul>
<li><a href="#scheduler-サマリー">Scheduler</a></li>
<li><a href="#kvcachemanager-サマリー">KVCacheManager</a></li>
<li><a href="#enginecoreclient-サマリー">EngineCoreClient</a></li>
<li><a href="#エントリポイント-asyncllm--llm-サマリー">エントリポイント</a></li>
<li><a href="#テキスト推論データフロー">データフロー</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="enginecoreclient-サマリー"><a class="header" href="#enginecoreclient-サマリー">EngineCoreClient サマリー</a></h1>
<blockquote>
<p><strong>深度</strong>: [SHALLOW]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-09</p>
</blockquote>
<h2 id="概要-5"><a class="header" href="#概要-5">概要</a></h2>
<p><code>EngineCoreClient</code>はフロントエンドプロセス（AsyncLLM / LLM）とバックエンドプロセス（EngineCore）間のプロセス間通信を抽象化するコンポーネントである。ZeroMQソケットとmsgpackシリアライゼーションを使用し、<code>EngineCoreRequest</code>の送信と<code>EngineCoreOutputs</code>の受信を効率的に行う。</p>
<h2 id="アーキテクチャ-3"><a class="header" href="#アーキテクチャ-3">アーキテクチャ</a></h2>
<pre><code>フロントエンドプロセス            バックエンドプロセス
┌───────────────────┐            ┌───────────────────┐
│  AsyncMPClient    │            │  EngineCore       │
│                   │            │                   │
│  input_socket     ├──ROUTER──→│  (ZMQ受信)        │
│  (zmq.ROUTER)     │  msgpack   │                   │
│                   │            │                   │
│  output_socket    │←──PULL────┤  (ZMQ送信)        │
│  (zmq.PULL)       │  msgpack   │                   │
│                   │            │                   │
│  outputs_queue    │            │                   │
│  (asyncio.Queue)  │            │                   │
└───────────────────┘            └───────────────────┘
</code></pre>
<h2 id="主要コンポーネント-2"><a class="header" href="#主要コンポーネント-2">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>用途</th><th>ファイル</th></tr>
</thead>
<tbody>
<tr><td><code>EngineCoreClient</code> (ABC)</td><td>抽象インターフェース</td><td><code>target/vllm/vllm/v1/engine/core_client.py:63</code></td></tr>
<tr><td><code>MPClient</code></td><td>マルチプロセスクライアント基底</td><td><code>target/vllm/vllm/v1/engine/core_client.py:442</code></td></tr>
<tr><td><code>AsyncMPClient</code></td><td>非同期マルチプロセスクライアント（AsyncLLM用）</td><td><code>target/vllm/vllm/v1/engine/core_client.py:822</code></td></tr>
<tr><td><code>SyncMPClient</code></td><td>同期マルチプロセスクライアント（LLM用）</td><td><code>target/vllm/vllm/v1/engine/core_client.py</code></td></tr>
<tr><td><code>DPAsyncMPClient</code></td><td>データ並列（外部LB）</td><td><code>target/vllm/vllm/v1/engine/core_client.py</code></td></tr>
<tr><td><code>DPLBAsyncMPClient</code></td><td>データ並列（内部LB）</td><td><code>target/vllm/vllm/v1/engine/core_client.py</code></td></tr>
<tr><td><code>MsgpackEncoder</code></td><td>リクエストのシリアライズ</td><td><code>target/vllm/vllm/v1/serial_utils.py</code></td></tr>
<tr><td><code>MsgpackDecoder</code></td><td>レスポンスのデシリアライズ</td><td><code>target/vllm/vllm/v1/serial_utils.py</code></td></tr>
</tbody>
</table>
</div>
<h2 id="主要メソッド-2"><a class="header" href="#主要メソッド-2">主要メソッド</a></h2>
<h3 id="enginecoreclient-abc"><a class="header" href="#enginecoreclient-abc">EngineCoreClient (ABC)</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>make_client()</code></td><td>ファクトリ。設定に応じた適切なサブクラスを返す</td></tr>
<tr><td><code>make_async_mp_client()</code></td><td>AsyncLLM用ファクトリ。DP構成も考慮</td></tr>
<tr><td><code>add_request()</code></td><td>EngineCoreRequestを送信</td></tr>
<tr><td><code>get_output()</code></td><td>EngineCoreOutputsを受信</td></tr>
<tr><td><code>abort_requests()</code></td><td>リクエストキャンセル</td></tr>
</tbody>
</table>
</div>
<h3 id="asyncmpclient"><a class="header" href="#asyncmpclient">AsyncMPClient</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>_ensure_output_queue_task()</code></td><td>L856</td><td>ZMQ出力受信タスクを起動</td></tr>
<tr><td><code>get_output_async()</code></td><td>L902</td><td>asyncio.Queueから出力を取得</td></tr>
<tr><td><code>_send_input()</code></td><td>L913</td><td>EngineCoreRequestをZMQで送信</td></tr>
<tr><td><code>_send_input_message()</code></td><td>L925</td><td>ZMQ multipart送信（zero-copy対応）</td></tr>
</tbody>
</table>
</div>
<h2 id="ファクトリ選択ロジック"><a class="header" href="#ファクトリ選択ロジック">ファクトリ選択ロジック</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core_client.py:99</code> (make_async_mp_client)</p>
<pre><code>make_async_mp_client(vllm_config, executor_class, ...)
  ├─ data_parallel_size &gt; 1 の場合:
  │   ├─ external_lb → DPAsyncMPClient
  │   └─ internal_lb → DPLBAsyncMPClient
  └─ それ以外 → AsyncMPClient
</code></pre>
<h2 id="設定-1"><a class="header" href="#設定-1">設定</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>parallel_config.data_parallel_size</code></td><td>1</td><td>データ並列数。&gt;1でDP系クライアントを使用</td></tr>
<tr><td><code>parallel_config.data_parallel_external_lb</code></td><td>—</td><td>外部ロードバランサ使用フラグ</td></tr>
</tbody>
</table>
</div>
<h2 id="呼び出しフロー-1"><a class="header" href="#呼び出しフロー-1">呼び出しフロー</a></h2>
<pre><code>[送信パス]
AsyncLLM.add_request()
  → engine_core.add_request_async(request)
    → AsyncMPClient._send_input(REQUEST, request)
      → MsgpackEncoder.encode(request)
      → input_socket.send_multipart(msg, copy=False)
        → ZMQ ROUTER → バックエンドプロセス

[受信パス]
process_outputs_socket() [バックグラウンドタスク]
  → output_socket.recv_multipart()
    → MsgpackDecoder.decode(frames) → EngineCoreOutputs
    → outputs_queue.put_nowait(outputs)

AsyncLLM._run_output_handler()
  → engine_core.get_output_async()
    → outputs_queue.get() → EngineCoreOutputs
</code></pre>
<h2 id="設計上の特徴"><a class="header" href="#設計上の特徴">設計上の特徴</a></h2>
<ul>
<li><strong>プロセス分離</strong>: EngineCoreが別プロセスで動作するため、GILの影響を受けずスケジューリングとGPU実行を並行可能</li>
<li><strong>msgpackシリアライゼーション</strong>: <code>msgspec.Struct</code>の<code>array_like</code>形式でコンパクトなバイナリ表現</li>
<li><strong>zero-copy</strong>: ZMQ <code>copy=False</code> でメモリコピーを最小化。テンソルバッキングバッファの追跡（<code>add_pending_message</code>）</li>
<li><strong>weakref</strong>: 出力タスクがクライアントへの循環参照を持たないよう<code>weakref</code>を使用</li>
</ul>
<h2 id="関連ドキュメント-1"><a class="header" href="#関連ドキュメント-1">関連ドキュメント</a></h2>
<ul>
<li><a href="#エントリポイント-asyncllm--llm-サマリー">エントリポイント</a></li>
<li><a href="#テキスト推論データフロー">データフロー</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="エントリポイント-asyncllm--llm-サマリー"><a class="header" href="#エントリポイント-asyncllm--llm-サマリー">エントリポイント (AsyncLLM / LLM) サマリー</a></h1>
<blockquote>
<p><strong>深度</strong>: [SHALLOW]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-09</p>
</blockquote>
<h2 id="概要-6"><a class="header" href="#概要-6">概要</a></h2>
<p><code>AsyncLLM</code>と<code>LLM</code>はvLLMの2つの主要エントリポイントである。<code>AsyncLLM</code>はAPIサーバー（OpenAI互換API等）が使用する非同期パスで、<code>LLM</code>はオフラインバッチ推論用の同期パスである。どちらも<code>InputProcessor</code>で入力を処理し、<code>EngineCoreClient</code>経由でバックエンド（EngineCore）にリクエストを送信する。</p>
<h2 id="アーキテクチャ-4"><a class="header" href="#アーキテクチャ-4">アーキテクチャ</a></h2>
<pre class="mermaid">graph LR
    subgraph 非同期パス
        OpenAI["OpenAI API Server"] --&gt; AsyncLLM
        AsyncLLM --&gt;|"process_inputs()"| IP["InputProcessor"]
        IP --&gt;|"EngineCoreRequest"| AsyncLLM
        AsyncLLM --&gt;|"add_request_async()"| Client["EngineCoreClient"]
        Client --&gt;|"ZMQ"| EC["EngineCore"]
    end

    subgraph 同期パス
        User["ユーザーコード"] --&gt; LLM
        LLM --&gt;|"process_inputs()"| IP2["InputProcessor"]
        IP2 --&gt;|"EngineCoreRequest"| LLM
        LLM --&gt;|"add_request()"| Client2["EngineCoreClient"]
    end
</pre>

<h2 id="主要コンポーネント-3"><a class="header" href="#主要コンポーネント-3">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>用途</th><th>ファイル</th></tr>
</thead>
<tbody>
<tr><td><code>AsyncLLM</code></td><td>非同期推論エントリポイント。AsyncGeneratorでストリーミング出力</td><td><code>target/vllm/vllm/v1/engine/async_llm.py:71</code></td></tr>
<tr><td><code>LLM</code></td><td>同期バッチ推論エントリポイント。<code>list[RequestOutput]</code>を返す</td><td><code>target/vllm/vllm/entrypoints/llm.py:101</code></td></tr>
<tr><td><code>RequestOutputCollector</code></td><td>非同期パスでの出力キュー管理</td><td><code>target/vllm/vllm/v1/engine/async_llm.py</code></td></tr>
<tr><td><code>ParentRequest</code></td><td>n&gt;1サンプリング時の親リクエスト管理</td><td><code>target/vllm/vllm/v1/engine/async_llm.py</code></td></tr>
</tbody>
</table>
</div>
<h2 id="主要メソッド-3"><a class="header" href="#主要メソッド-3">主要メソッド</a></h2>
<h3 id="asyncllm"><a class="header" href="#asyncllm">AsyncLLM</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>generate()</code></td><td>L537</td><td>メインAPI。AsyncGeneratorでRequestOutputをyield</td></tr>
<tr><td><code>add_request()</code></td><td>L286</td><td>リクエスト追加。InputProcessor→OutputProcessor→EngineCore</td></tr>
<tr><td><code>_add_request()</code></td><td>L414</td><td>内部: OutputProcessorとEngineCoreに登録</td></tr>
<tr><td><code>_run_output_handler()</code></td><td>L647</td><td>バックグラウンドタスク起動。EngineCore出力を受信→キュー</td></tr>
</tbody>
</table>
</div>
<h3 id="llm"><a class="header" href="#llm">LLM</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>generate()</code></td><td>L396</td><td>バッチ推論API。<code>list[RequestOutput]</code>を返す</td></tr>
<tr><td><code>_add_request()</code></td><td>L1850</td><td>InputProcessor→llm_engine.add_request()</td></tr>
<tr><td><code>_run_engine()</code></td><td>L1900</td><td>ポーリングループ。完了まで<code>step()</code>を繰り返す</td></tr>
</tbody>
</table>
</div>
<h2 id="設定-2"><a class="header" href="#設定-2">設定</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>log_requests</code></td><td><code>True</code></td><td>リクエストログ出力</td></tr>
<tr><td><code>log_stats</code></td><td>引数指定</td><td>統計ログ出力</td></tr>
<tr><td><code>start_engine_loop</code></td><td><code>True</code></td><td>エンジンループ自動起動</td></tr>
</tbody>
</table>
</div>
<h2 id="呼び出しフロー-2"><a class="header" href="#呼び出しフロー-2">呼び出しフロー</a></h2>
<pre><code>[APIサーバー or ユーザーコード]
  → AsyncLLM.generate() / LLM.generate()
    → InputProcessor.process_inputs()
      → EngineCoreRequest
    → EngineCoreClient.add_request_async()
      → ZMQ → EngineCore（別プロセス）

[バックグラウンド output_handler タスク]
  → EngineCoreClient.get_output_async()
    → EngineCoreOutputs
  → OutputProcessor.process_outputs()
    → RequestOutput → キューにpush

[generate() AsyncGenerator]
  → キューから取り出してyield
</code></pre>
<h2 id="関連ドキュメント-2"><a class="header" href="#関連ドキュメント-2">関連ドキュメント</a></h2>
<ul>
<li><a href="#inputprocessor-サマリー">入力処理</a></li>
<li><a href="#enginecoreclient-サマリー">EngineCoreClient</a></li>
<li><a href="#テキスト推論データフロー">データフロー</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="executor"><a class="header" href="#executor">Executor</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-14</p>
</blockquote>
<h2 id="概要-7"><a class="header" href="#概要-7">概要</a></h2>
<p>Executorは、EngineCoreとWorker（GPUModelRunner）の間に位置する実行委譲レイヤーである。<code>collective_rpc()</code>パターンで全Workerに対して同一メソッドを呼び出し、出力ランクのWorkerの結果を返す。単一プロセス、マルチプロセス、Ray分散の3つの実装を持つ。</p>
<h2 id="クラス階層"><a class="header" href="#クラス階層">クラス階層</a></h2>
<pre><code>Executor (ABC)                                     abstract.py:36
├── UniProcExecutor                                uniproc_executor.py:26
│   └── ExecutorWithExternalLauncher               uniproc_executor.py:140
├── MultiprocExecutor                              multiproc_executor.py:93
└── RayDistributedExecutor                         ray_executor.py:62
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py:36</code> (Executor)</p>
<h2 id="主要メソッド-4"><a class="header" href="#主要メソッド-4">主要メソッド</a></h2>
<h3 id="collective_rpc"><a class="header" href="#collective_rpc">collective_rpc()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py:180</code> (collective_rpc)</p>
<p>全Workerに対して同一メソッドを実行するRPCメカニズム。</p>
<pre><code class="language-python">def collective_rpc(
    self,
    method: str | Callable[..., _R],  # メソッド名または関数
    timeout: float | None = None,
    args: tuple = (),
    kwargs: dict | None = None,
    non_block: bool = False,          # True: Future返却
) -&gt; list[_R] | Future[list[_R]]
</code></pre>
<h3 id="execute_model"><a class="header" href="#execute_model">execute_model()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py:202</code> (execute_model)</p>
<pre><code class="language-python">def execute_model(
    self,
    scheduler_output: SchedulerOutput,
    non_block: bool = False,
) -&gt; ModelRunnerOutput | None | Future[ModelRunnerOutput | None]:
    output = self.collective_rpc("execute_model",
                                  args=(scheduler_output,),
                                  non_block=non_block)
    return output[0]   # 出力ランクWorkerの結果のみ返す
</code></pre>
<h3 id="sample_tokens"><a class="header" href="#sample_tokens">sample_tokens()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py:222</code> (sample_tokens)</p>
<pre><code class="language-python">def sample_tokens(
    self,
    grammar_output: GrammarOutput | None,
    non_block: bool = False,
) -&gt; ModelRunnerOutput | Future[ModelRunnerOutput]:
    output = self.collective_rpc("sample_tokens",
                                  args=(grammar_output,),
                                  non_block=non_block)
    return output[0]
</code></pre>
<h2 id="実装の使い分け"><a class="header" href="#実装の使い分け">実装の使い分け</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>実装</th><th>用途</th><th>Worker配置</th><th>特徴</th></tr>
</thead>
<tbody>
<tr><td><code>UniProcExecutor</code></td><td>単一GPU</td><td>ドライバプロセス内</td><td>最小オーバーヘッド。<code>max_concurrent_batches &gt; 1</code>時はThreadPoolExecutor使用</td></tr>
<tr><td><code>MultiprocExecutor</code></td><td>複数GPU（TP/PP）</td><td>子プロセス</td><td>MessageQueue（共有メモリ）ベース。Pipeline Parallelism対応</td></tr>
<tr><td><code>RayDistributedExecutor</code></td><td>分散クラスタ</td><td>Rayアクター</td><td>Ray経由のリモートWorker管理</td></tr>
</tbody>
</table>
</div>
<h2 id="multiprocexecutor-のプロセス間通信-medium-verified"><a class="header" href="#multiprocexecutor-のプロセス間通信-medium-verified">MultiprocExecutor のプロセス間通信 [MEDIUM] [VERIFIED]</a></h2>
<p>MultiprocExecutorはSharedMemory MessageQueue（<code>ShmRingBuffer</code>）を使って同一ノード内のWorkerプロセスと通信する。</p>
<h3 id="messagequeue-の仕組み"><a class="header" href="#messagequeue-の仕組み">MessageQueue の仕組み</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/device_communicators/shm_broadcast.py:272</code> (MessageQueue)</p>
<p>2つのチャネルを併用:</p>
<ol>
<li><strong>ShmRingBuffer</strong>（共有メモリ）: 24MiB以下の通常データ。ロックフリー、~20nsメモリフェンスのみ</li>
<li><strong>ZMQ PUB/SUB</strong>（フォールバック）: 24MiBを超えるデータ。ローカルはIPC、リモートはTCP</li>
</ol>
<p>ShmRingBufferのメモリレイアウト:</p>
<pre><code>┌──────────────────────────────────┬──────────────────────────────────────┐
│ data: chunk0 | chunk1 | ... | N  │ metadata: [written|r0|r1|...] × N   │
│ max_chunks × 24MiB               │ max_chunks × (1 + n_reader) bytes    │
└──────────────────────────────────┴──────────────────────────────────────┘
</code></pre>
<p>メタデータフラグで書き込み/読み取り状態を管理。全readerが読み取り完了するとチャンクが再利用される。</p>
<h3 id="キュー構成"><a class="header" href="#キュー構成">キュー構成</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>キュー</th><th>方向</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>rpc_broadcast_mq</code></td><td>Executor → 全Worker</td><td>RPCコマンドのブロードキャスト</td></tr>
<tr><td><code>worker_response_mq</code> × N</td><td>各Worker → Executor</td><td>実行結果の返送</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:131-136</code> (rpc_broadcast_mq生成)</p>
<h3 id="collective_rpc-の動作フロー"><a class="header" href="#collective_rpc-の動作フロー">collective_rpc の動作フロー</a></h3>
<pre><code>MultiprocExecutor.collective_rpc("execute_model", args=(...))
  │
  ├─ rpc_broadcast_mq.enqueue((method, args, kwargs, output_rank))
  │   → pickle → ShmRingBuffer書き込み → メモリフェンス
  │
  ├─ Worker-0: dequeue() → execute → response_mq.enqueue()
  ├─ Worker-1: dequeue() → execute → response_mq.enqueue()
  │
  └─ Executor: response_mqs[output_rank].dequeue() → 結果返却
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:303-375</code> (collective_rpc)
<strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:845-871</code> (worker_busy_loop)</p>
<h3 id="worker-プロセスの起動とビジーループ"><a class="header" href="#worker-プロセスの起動とビジーループ">Worker プロセスの起動とビジーループ</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:696</code> (WorkerProc.worker_main)</p>
<pre><code>WorkerProc.worker_main()
  ├─ Worker.init_device()
  │   └─ torch.distributed.init_process_group(backend="nccl")
  ├─ Worker.load_model()
  ├─ READY送信（Pipe経由）
  └─ worker_busy_loop():
      while True:
        method, args, kwargs, output_rank = rpc_broadcast_mq.dequeue()
        output = getattr(worker, method)(*args, **kwargs)
        worker_response_mq.enqueue((SUCCESS, output))
</code></pre>
<h2 id="worker委譲先"><a class="header" href="#worker委譲先">Worker（委譲先）</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_worker.py:70</code> (Worker)</p>
<p><code>Worker(WorkerBase)</code> はGPUModelRunnerのラッパーで、以下の追加処理を行う:</p>
<ul>
<li><strong>Pipeline Parallelism</strong>: 前段ランクからの<code>IntermediateTensors</code>受信、後段への送信</li>
<li><strong>推論モード管理</strong>: <code>@torch.inference_mode()</code> デコレータ</li>
</ul>
<pre><code>Worker.execute_model(scheduler_output)                    # L604
  ├─ PP: recv_tensor_dict() → IntermediateTensors        # L614-641
  ├─ model_runner.execute_model(scheduler_output, ...)    # L652
  │   → ModelRunnerOutput | None | IntermediateTensors
  ├─ PP: send_tensor_dict(intermediate_tensors)           # L660-671
  └─ return ModelRunnerOutput | None
</code></pre>
<h2 id="enginecore--出力-の委譲フロー"><a class="header" href="#enginecore--出力-の委譲フロー">EngineCore → 出力 の委譲フロー</a></h2>
<pre><code>EngineCore.step()
  └─ executor.execute_model(scheduler_output, non_block=True)
      └─ collective_rpc("execute_model")
          └─ Worker.execute_model()
              └─ GPUModelRunner.execute_model()
                  → ExecuteModelState 保存、None 返却

EngineCore.step()（続き）
  └─ executor.sample_tokens(grammar_output)
      └─ collective_rpc("sample_tokens")
          └─ Worker.sample_tokens()
              └─ GPUModelRunner.sample_tokens()
                  → ModelRunnerOutput 返却
</code></pre>
<h2 id="上流下流の関係"><a class="header" href="#上流下流の関係">上流・下流の関係</a></h2>
<ul>
<li><strong>上流</strong>: EngineCore（<code>step()</code>から呼び出し）</li>
<li><strong>下流</strong>: Worker → GPUModelRunner</li>
</ul>
<h2 id="phase-2-深堀り候補"><a class="header" href="#phase-2-深堀り候補">Phase 2 深堀り候補</a></h2>
<ul>
<li><del>MultiprocExecutorのMessageQueue実装詳細</del> → 調査済み（本ドキュメント）</li>
<li>Pipeline Parallelism時のバッチスケジューリング</li>
<li>Ray分散実行のオーバーヘッドと障害回復</li>
<li>AsyncScheduling時のasync_output_busy_loop動作</li>
</ul>
<h2 id="主要ファイル"><a class="header" href="#主要ファイル">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/v1/executor/abstract.py</code></td><td><code>Executor</code>, <code>collective_rpc()</code> (L180), <code>execute_model()</code> (L202)</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/uniproc_executor.py</code></td><td><code>UniProcExecutor</code> (L26)</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/multiproc_executor.py</code></td><td><code>MultiprocExecutor</code> (L93), <code>WorkerProc</code> (L493), <code>worker_busy_loop</code> (L845)</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/ray_executor.py</code></td><td><code>RayDistributedExecutor</code> (L62)</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_worker.py</code></td><td><code>Worker</code> (L70), <code>execute_model()</code> (L604)</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/worker_base.py</code></td><td><code>WorkerBase</code> (L34), <code>WorkerWrapperBase</code> (L175)</td></tr>
<tr><td><code>target/vllm/vllm/distributed/device_communicators/shm_broadcast.py</code></td><td><code>ShmRingBuffer</code> (L127), <code>MessageQueue</code> (L272)</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="gpumodelrunner"><a class="header" href="#gpumodelrunner">GPUModelRunner</a></h1>
<blockquote>
<p><strong>深度</strong>: [SHALLOW]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-8"><a class="header" href="#概要-8">概要</a></h2>
<p>GPUModelRunnerは、推論パイプラインの実行中核を担う巨大クラス（約6,300行）である。SchedulerOutputを受け取り、入力テンソルの準備、モデルのforward実行、サンプリングを経て、ModelRunnerOutputを返す。<strong>2フェーズ実行パターン</strong>（<code>execute_model()</code> → <code>sample_tokens()</code>）を採用し、モデルフォワードとgrammar bitmask計算の並行実行を可能にしている。</p>
<h2 id="クラス定義"><a class="header" href="#クラス定義">クラス定義</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:329</code> (GPUModelRunner)</p>
<pre><code class="language-python">class GPUModelRunner(
    LoRAModelRunnerMixin,           # LoRAアダプタ管理
    KVConnectorModelRunnerMixin,    # KV Transfer対応
    ECConnectorModelRunnerMixin,    # エンコーダキャッシュ対応
):
</code></pre>
<h2 id="2フェーズ実行パターン"><a class="header" href="#2フェーズ実行パターン">2フェーズ実行パターン</a></h2>
<p>GPUModelRunnerの中核設計。<code>execute_model()</code>でモデルフォワードとlogits計算を行い、結果を<code>ExecuteModelState</code>に保存して<code>None</code>を返す。その後<code>sample_tokens()</code>が状態を復元してサンプリングを実行する。</p>
<pre class="mermaid">sequenceDiagram
    participant EC as EngineCore
    participant MR as GPUModelRunner

    EC-&gt;&gt;MR: execute_model(scheduler_output)
    Note over MR: 入力準備 → モデルフォワード&lt;br&gt;→ logits計算 → 状態保存
    MR--&gt;&gt;EC: None

    Note over EC: grammar bitmask 計算&lt;br&gt;（並行処理）

    EC-&gt;&gt;MR: sample_tokens(grammar_output)
    Note over MR: 状態復元 → grammar適用&lt;br&gt;→ サンプリング → 出力構築
    MR--&gt;&gt;EC: ModelRunnerOutput
</pre>

<h3 id="phase-1-execute_model"><a class="header" href="#phase-1-execute_model">Phase 1: execute_model()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:3312</code> (execute_model)</p>
<pre><code>execute_model(scheduler_output, intermediate_tensors=None)
  │
  ├─ 早期リターン判定                                      # L3335-3367
  │   スケジュールトークン0件 → EMPTY_MODEL_RUNNER_OUTPUT
  │
  ├─ 1. バッチ状態更新                                     # L3376
  │   _update_states(scheduler_output)
  │   → 新規リクエスト登録、キャッシュ済みリクエスト更新
  │
  ├─ 2. 入力準備                                           # L3389
  │   _prepare_inputs(scheduler_output)
  │   → input_ids, positions, logits_indices 計算
  │
  ├─ 3. Attentionメタデータ構築                             # L3400
  │   _build_attention_metadata(scheduler_output, ...)
  │
  ├─ 4. 前処理                                             # L3440-3504
  │   slot_mapping取得、入力トークン/位置/埋め込み準備
  │
  ├─ 5. モデルフォワード                                    # L3521-3603
  │   _model_forward(...)
  │   → hidden_states = model.forward(input_ids, positions, ...)
  │   → logits = compute_logits(hidden_states)
  │
  └─ 6. 状態保存                                            # L3605-3615
      ExecuteModelState(scheduler_output, logits, ...)
      → self.execute_model_state に保存
      → None を返す
</code></pre>
<p><strong>戻り値のパターン</strong>:</p>
<ul>
<li><code>None</code> — 通常ケース（sample_tokens()を後で呼ぶ）</li>
<li><code>ModelRunnerOutput</code> — プーリングモデル等（サンプリング不要）</li>
<li><code>IntermediateTensors</code> — Pipeline Parallelismの中間ランク</li>
<li><code>EMPTY_MODEL_RUNNER_OUTPUT</code> — スケジュールトークン0件</li>
</ul>
<h3 id="phase-2-sample_tokens"><a class="header" href="#phase-2-sample_tokens">Phase 2: sample_tokens()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:3621</code> (sample_tokens)</p>
<pre><code>sample_tokens(grammar_output)
  │
  ├─ 1. 状態復元                                            # L3643-3657
  │   ExecuteModelState から logits, scheduler_output 等を復元
  │   self.execute_model_state = None（クリア）
  │
  ├─ 2. Grammar制約適用                                     # L3659-3663
  │   grammar bitmask を logits に適用（構造化出力時）
  │
  ├─ 3. サンプリング                                        # L3665-3666
  │   _sample(logits, spec_decode_metadata) → SamplerOutput
  │
  ├─ 4. 後処理                                             # L3668-3699
  │   バッチ状態に生成トークンを反映
  │   PP時のトークンブロードキャスト
  │   Speculative Decodingのドラフトトークン提案
  │
  └─ 5. ModelRunnerOutput構築                               # L3775-3787
      ModelRunnerOutput(
        req_ids, req_id_to_index,
        sampled_token_ids,    # list[list[int]]
        logprobs,             # numpy配列
        prompt_logprobs_dict, # torch.Tensor
        kv_connector_output, ...
      )
</code></pre>
<h2 id="executemodelstate"><a class="header" href="#executemodelstate">ExecuteModelState</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:313</code> (ExecuteModelState)</p>
<p>2フェーズ間の一時状態を保持するNamedTuple。GPUテンソルを含むため、シリアライズはされない。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>scheduler_output</code></td><td><code>SchedulerOutput</code></td><td>スケジュール結果</td></tr>
<tr><td><code>logits</code></td><td><code>torch.Tensor</code></td><td>モデル出力logits</td></tr>
<tr><td><code>spec_decode_metadata</code></td><td><code>SpecDecodeMetadata | None</code></td><td>Speculative Decoding情報</td></tr>
<tr><td><code>hidden_states</code></td><td><code>torch.Tensor</code></td><td>隠れ状態</td></tr>
<tr><td><code>sample_hidden_states</code></td><td><code>torch.Tensor</code></td><td>サンプリング用隠れ状態</td></tr>
<tr><td><code>aux_hidden_states</code></td><td><code>list[torch.Tensor] | None</code></td><td>補助隠れ状態</td></tr>
<tr><td><code>ec_connector_output</code></td><td><code>ECConnectorOutput | None</code></td><td>エンコーダ出力</td></tr>
<tr><td><code>cudagraph_stats</code></td><td><code>CUDAGraphStat | None</code></td><td>CUDAGraph統計</td></tr>
<tr><td><code>slot_mappings</code></td><td><code>dict | list | None</code></td><td>KVキャッシュスロットマッピング</td></tr>
</tbody>
</table>
</div>
<h2 id="6300行の内訳-inferred"><a class="header" href="#6300行の内訳-inferred">6,300行の内訳 [INFERRED]</a></h2>
<p>GPUModelRunnerが巨大な理由は、以下の多岐にわたる責務を単一クラスに集約しているため:</p>
<ul>
<li><strong>バッチ状態管理</strong>: リクエストの追加・削除、永続バッチテンソルの管理</li>
<li><strong>入力準備</strong>: トークンID、位置、埋め込みの計算</li>
<li><strong>Attentionメタデータ</strong>: FlashAttention/FlashInfer用メタデータ構築</li>
<li><strong>モデルフォワード</strong>: CUDAGraph対応、torch.compile統合</li>
<li><strong>サンプリング</strong>: トップk/p/温度、logprobs計算</li>
<li><strong>KV Transfer</strong>: KVコネクタとの連携</li>
<li><strong>Speculative Decoding</strong>: ドラフトトークン提案・検証</li>
<li><strong>Pipeline Parallelism</strong>: 中間テンソル管理</li>
<li><strong>LoRA</strong>: アダプタの動的切り替え</li>
<li><strong>マルチモーダル</strong>: エンコーダ入力の処理</li>
</ul>
<h2 id="上流下流の関係-1"><a class="header" href="#上流下流の関係-1">上流・下流の関係</a></h2>
<ul>
<li><strong>上流</strong>: Worker（<code>execute_model()</code> / <code>sample_tokens()</code>経由で呼び出し）</li>
<li><strong>下流</strong>: モデル層（<code>model.forward()</code>）、Sampler</li>
</ul>
<h2 id="phase-2-深堀り候補-1"><a class="header" href="#phase-2-深堀り候補-1">Phase 2 深堀り候補</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>テーマ</th><th>関連メソッド</th><th>ユーザー関心</th></tr>
</thead>
<tbody>
<tr><td>入力準備の詳細</td><td><code>_prepare_inputs()</code>, <code>_update_states()</code></td><td>中</td></tr>
<tr><td>Attentionメタデータ</td><td><code>_build_attention_metadata()</code></td><td>KVキャッシュ関連で高</td></tr>
<tr><td>CUDAGraph統合</td><td><code>_model_forward()</code>, CUDAGraphランナー</td><td>中</td></tr>
<tr><td>サンプリング実装</td><td><code>_sample()</code>, Sampler</td><td>低</td></tr>
<tr><td>KV Transfer連携</td><td><code>KVConnectorModelRunnerMixin</code></td><td>高（ユーザー関心2位）</td></tr>
<tr><td>マルチモーダル入力</td><td>エンコーダ処理、mm_cache</td><td>高（ユーザー関心3位） → <strong>Phase 2bで調査済み</strong></td></tr>
</tbody>
</table>
</div>
<h2 id="マルチモーダル処理"><a class="header" href="#マルチモーダル処理">マルチモーダル処理</a></h2>
<p>マルチモーダル推論時、GPUModelRunnerは <code>execute_model()</code> 内で以下の追加処理を行う:</p>
<ol>
<li><strong><code>_execute_mm_encoder()</code></strong> (L2293): <code>model.embed_multimodal()</code> でビジョンエンコーダ実行。結果を <code>encoder_cache[mm_hash]</code> に格納</li>
<li><strong><code>_gather_mm_embeddings()</code></strong> (L2449): <code>encoder_cache</code> からスケジュール範囲に対応する埋め込みをスライス。チャンクPrefill対応</li>
<li><strong><code>embed_input_ids()</code></strong>: <code>masked_scatter_</code> でテキスト + ビジョン埋め込みをマージ → <code>inputs_embeds</code> として model.forward() に渡す</li>
</ol>
<p><code>encoder_cache: dict[str, torch.Tensor]</code> はGPU上のシンプルなdictキャッシュで、Schedulerの <code>free_encoder_mm_hashes</code> 指示で解放される。</p>
<p>詳細は <a href="#バックエンド-マルチモーダル処理パス-medium-verified">バックエンド MM処理パス</a> を参照。</p>
<h2 id="主要ファイル-1"><a class="header" href="#主要ファイル-1">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_model_runner.py</code></td><td><code>GPUModelRunner</code> (L329), <code>execute_model()</code> (L3312), <code>sample_tokens()</code> (L3621), <code>ExecuteModelState</code> (L313)</td></tr>
<tr><td><code>target/vllm/vllm/v1/outputs.py</code></td><td><code>ModelRunnerOutput</code> (L160), <code>AsyncModelRunnerOutput</code> (L200)</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="inputprocessor-サマリー"><a class="header" href="#inputprocessor-サマリー">InputProcessor サマリー</a></h1>
<blockquote>
<p><strong>深度</strong>: [SHALLOW]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-09</p>
</blockquote>
<h2 id="概要-9"><a class="header" href="#概要-9">概要</a></h2>
<p><code>InputProcessor</code>はユーザー入力（テキストプロンプト、SamplingParams等）を内部表現<code>EngineCoreRequest</code>に変換するコンポーネントである。トークナイズ、パラメータのバリデーションと正規化、マルチモーダル入力の前処理を担当する。AsyncLLMの初期化時に生成され、フロントエンドプロセスで動作する。</p>
<h2 id="アーキテクチャ-5"><a class="header" href="#アーキテクチャ-5">アーキテクチャ</a></h2>
<pre class="mermaid">graph LR
    Prompt["PromptType&lt;br&gt;(str / list[int] / dict)"] --&gt; IP["InputProcessor"]
    Params["SamplingParams"] --&gt; IP
    IP --&gt; IPP["InputPreprocessor&lt;br&gt;tokenizer.encode()"]
    IPP --&gt; PI["ProcessorInputs"]
    PI --&gt; IP
    IP --&gt; ECR["EngineCoreRequest"]
</pre>

<h2 id="主要コンポーネント-4"><a class="header" href="#主要コンポーネント-4">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>用途</th><th>ファイル</th></tr>
</thead>
<tbody>
<tr><td><code>InputProcessor</code></td><td>入力処理のメインクラス</td><td><code>target/vllm/vllm/v1/engine/input_processor.py:56</code></td></tr>
<tr><td><code>InputPreprocessor</code></td><td>トークナイズとマルチモーダル前処理</td><td><code>target/vllm/vllm/v1/engine/input_processor.py</code> (内部利用)</td></tr>
<tr><td><code>ProcessorInputs</code></td><td>前処理結果の中間データ構造</td><td><code>target/vllm/vllm/v1/engine/input_processor.py</code></td></tr>
</tbody>
</table>
</div>
<h2 id="主要メソッド-5"><a class="header" href="#主要メソッド-5">主要メソッド</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>入力</th><th>出力</th></tr>
</thead>
<tbody>
<tr><td><code>process_inputs()</code></td><td>L521</td><td><code>request_id</code>, <code>prompt</code>, <code>params</code></td><td><code>EngineCoreRequest</code></td></tr>
<tr><td><code>assign_request_id()</code></td><td>(別メソッド)</td><td><code>EngineCoreRequest</code></td><td>None (内部IDを付与)</td></tr>
<tr><td><code>_validate_lora()</code></td><td>L535付近</td><td><code>LoRARequest</code></td><td>バリデーション</td></tr>
<tr><td><code>_validate_params()</code></td><td>L536付近</td><td><code>SamplingParams</code></td><td>バリデーション</td></tr>
</tbody>
</table>
</div>
<h2 id="process_inputs-の処理フロー"><a class="header" href="#process_inputs-の処理フロー">process_inputs() の処理フロー</a></h2>
<pre><code>process_inputs(request_id, prompt, params)
  1. バリデーション
     ├─ LoRAリクエスト検証
     ├─ パラメータ検証
     └─ data_parallel_rank 範囲チェック
  2. arrival_time 設定
  3. input_preprocessor.preprocess(prompt)
     → テキストをトークナイズ → ProcessorInputs
  4. split_enc_dec_inputs()
     → エンコーダ/デコーダ入力を分離
  5. SamplingParams 正規化
     ├─ clone() で複製
     ├─ max_tokens 未設定時: max_model_len - seq_len
     ├─ update_from_generation_config()
     └─ update_from_tokenizer()
  6. EngineCoreRequest を構築して返す
</code></pre>
<h2 id="設定-3"><a class="header" href="#設定-3">設定</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>model_config.max_model_len</code></td><td>モデル依存</td><td>max_tokens未指定時の上限計算に使用</td></tr>
<tr><td><code>cache_config.enable_prefix_caching</code></td><td>—</td><td>マルチモーダルUUID生成方式に影響</td></tr>
</tbody>
</table>
</div>
<h2 id="呼び出しフロー-3"><a class="header" href="#呼び出しフロー-3">呼び出しフロー</a></h2>
<pre><code>AsyncLLM.add_request() / LLM._add_request()
  → InputProcessor.process_inputs()
    → InputPreprocessor.preprocess()
      → tokenizer.encode()
    → EngineCoreRequest を返す
  → InputProcessor.assign_request_id()
    → 外部IDを external_req_id に退避
    → 内部ID（外部ID + 8文字ランダム）を request_id に設定
</code></pre>
<h2 id="マルチモーダル処理-1"><a class="header" href="#マルチモーダル処理-1">マルチモーダル処理</a></h2>
<p>テキスト入力に加えて画像等のマルチモーダルデータがある場合、InputProcessorは以下の追加処理を行う:</p>
<ol>
<li><strong>MM Registry から <code>BaseMultiModalProcessor</code> を取得</strong>（<code>_get_mm_processor()</code>）</li>
<li><strong>マルチモーダルデータのパース</strong>: <code>mm_processor.info.parse_mm_data()</code> → <code>MultiModalDataItems</code></li>
<li><strong>HF Processor 実行</strong>: <code>mm_processor.apply()</code> → <code>MultiModalInputs</code>（トークン列 + テンソル + ハッシュ + PlaceholderRange）</li>
<li><strong>ProcessorCache</strong>: <code>mm_processor_cache</code> によるHF処理結果のキャッシュ（4種類: processor_only/lru/shm/none）</li>
<li><strong>MultiModalFeatureSpec 構築</strong>: データ + 位置情報 + ハッシュを <code>EngineCoreRequest.mm_features</code> にセット</li>
</ol>
<p>詳細は <a href="#フロントエンド-マルチモーダル処理パス-medium-verified">マルチモーダル フロントエンド処理</a> を参照。</p>
<h2 id="関連ドキュメント-3"><a class="header" href="#関連ドキュメント-3">関連ドキュメント</a></h2>
<ul>
<li><a href="#エントリポイント-asyncllm--llm-サマリー">エントリポイント</a></li>
<li><a href="#テキスト推論データフロー">データフロー</a></li>
<li><a href="#マルチモーダル処理パイプライン-サマリー">マルチモーダル処理パイプライン</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kvcachemanager-サマリー"><a class="header" href="#kvcachemanager-サマリー">KVCacheManager サマリー</a></h1>
<blockquote>
<p><strong>深度</strong>: [DEEP]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-10"><a class="header" href="#概要-10">概要</a></h2>
<p><code>KVCacheManager</code> は PagedAttention に基づく KV キャッシュブロックの割り当て・解放・プレフィックスキャッシュ検索を管理するクラスである。4層の階層設計（<code>KVCacheManager</code> → <code>KVCacheCoordinator</code> → <code>SingleTypeKVCacheManager</code> → <code>BlockPool</code>）でマルチグループ KV キャッシュを統括する。Scheduler から呼び出され、各リクエストに必要な GPU メモリブロックを確保する。</p>
<h2 id="アーキテクチャ-6"><a class="header" href="#アーキテクチャ-6">アーキテクチャ</a></h2>
<h3 id="クラス階層-1"><a class="header" href="#クラス階層-1">クラス階層</a></h3>
<pre class="mermaid">graph TD
    KVM["KVCacheManager&lt;br&gt;公開 API"]
    Factory["get_kv_cache_coordinator()&lt;br&gt;ファクトリ関数"]

    NPC["NoPrefixCache&lt;br&gt;キャッシュ無効時"]
    UC["UnitaryCoordinator&lt;br&gt;単一グループ"]
    HC["HybridCoordinator&lt;br&gt;複数グループ"]

    FAM["FullAttentionManager"]
    SWM["SlidingWindowManager"]
    CLM["ChunkedLocalManager"]
    MM["MambaManager"]
    CAM["CrossAttentionManager"]
    SFM["SinkFullAttentionManager"]

    BP["BlockPool&lt;br&gt;物理ブロック管理"]
    BH["BlockHashToBlockMap"]
    FQ["FreeKVCacheBlockQueue"]
    KB["KVCacheBlock"]

    KVM --&gt; Factory
    Factory --&gt; NPC
    Factory --&gt; UC
    Factory --&gt; HC

    UC --&gt; FAM
    UC --&gt; SWM
    HC --&gt; FAM
    HC --&gt; SWM
    HC --&gt; CLM
    HC --&gt; MM
    HC --&gt; CAM
    HC --&gt; SFM

    FAM --&gt; BP
    SWM --&gt; BP
    CLM --&gt; BP
    MM --&gt; BP
    CAM --&gt; BP
    SFM --&gt; BP

    BP --&gt; BH
    BP --&gt; FQ
    FQ --&gt; KB
</pre>

<h3 id="coordinator-選択ロジック"><a class="header" href="#coordinator-選択ロジック">Coordinator 選択ロジック</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_coordinator.py:542</code></p>
<pre><code>get_kv_cache_coordinator()
  ├─ enable_caching == False  → KVCacheCoordinatorNoPrefixCache
  ├─ kv_cache_groups == 1     → UnitaryKVCacheCoordinator
  └─ kv_cache_groups &gt; 1      → HybridKVCacheCoordinator
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Coordinator</th><th>用途</th><th>find_longest_cache_hit</th></tr>
</thead>
<tbody>
<tr><td><code>NoPrefixCache</code></td><td>キャッシュ無効</td><td>空リスト、0 トークン</td></tr>
<tr><td><code>Unitary</code></td><td>単一アテンションタイプ</td><td>単一 Manager に委譲</td></tr>
<tr><td><code>Hybrid</code></td><td>複数アテンションタイプ</td><td>反復固定点アルゴリズム</td></tr>
</tbody>
</table>
</div>
<h3 id="kv-キャッシュグループ"><a class="header" href="#kv-キャッシュグループ">KV キャッシュグループ</a></h3>
<p><strong>KV キャッシュグループ</strong>とは、同一の <code>KVCacheSpec</code>（アテンションタイプ・ブロックサイズ）を共有するモデルレイヤーの集合である。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>モデル例</th><th>グループ構成</th></tr>
</thead>
<tbody>
<tr><td>全層 Full Attention</td><td>1 グループ</td></tr>
<tr><td>12 層 Full + 12 層 Sliding Window</td><td>2 グループ</td></tr>
<tr><td>デコーダ + クロスアテンション</td><td>2-3 グループ</td></tr>
</tbody>
</table>
</div>
<p>各グループは独立した <code>SingleTypeKVCacheManager</code> を持ち、異なるキャッシュ検索アルゴリズム・スキップポリシーを適用する。</p>
<h2 id="kvcacheblocks-deep-verified"><a class="header" href="#kvcacheblocks-deep-verified">KVCacheBlocks [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_manager.py:21</code></p>
<p>Scheduler と KVCacheManager のインターフェース。内部データ構造を隠蔽する。</p>
<pre><code class="language-python">@dataclass
class KVCacheBlocks:
    blocks: tuple[Sequence[KVCacheBlock], ...]
    # blocks[i][j] = i 番目の kv_cache_group、j 番目のブロック
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>__add__</code></td><td>2 つの KVCacheBlocks を結合</td></tr>
<tr><td><code>get_block_ids()</code></td><td><code>tuple[list[int], ...]</code> に変換（GPU カーネル用）</td></tr>
<tr><td><code>get_unhashed_block_ids()</code></td><td>未ハッシュブロックの ID リスト（ドラフトトークン用）</td></tr>
<tr><td><code>new_empty()</code></td><td>空の KVCacheBlocks を生成</td></tr>
</tbody>
</table>
</div>
<p><strong>GC 最適化</strong>: <code>KVCacheManager</code> は <code>empty_kv_cache_blocks</code> を事前生成し、空の結果を返す際に再利用する。</p>
<h3 id="ブロック配置図allocate_slots"><a class="header" href="#ブロック配置図allocate_slots">ブロック配置図（allocate_slots）</a></h3>
<p><code>allocate_slots()</code> がリクエストに割り当てるブロックの論理構造:</p>
<pre><code>|  comp  | new_comp | ext_comp |   new   | lookahead |
|&lt;------ 既計算トークン ------&gt;|&lt;-- 新規計算対象 --&gt;|
                               |&lt;- 割り当て対象 -&gt;|
</code></pre>
<ul>
<li><code>comp</code>: <code>request.num_computed_tokens</code> — 前ステップまでに計算済み</li>
<li><code>new_comp</code>: <code>num_new_computed_tokens</code> — プレフィックスキャッシュから新規にヒットしたトークン</li>
<li><code>ext_comp</code>: <code>num_external_computed_tokens</code> — KV コネクタ（LMCache 等）から取得したトークン</li>
<li><code>new</code>: <code>num_new_tokens</code> — 今回計算するトークン</li>
<li><code>lookahead</code>: <code>num_lookahead_tokens</code> — Speculative Decoding 用の先読みトークン</li>
</ul>
<h2 id="主要コンポーネント-5"><a class="header" href="#主要コンポーネント-5">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>用途</th><th>ファイル</th></tr>
</thead>
<tbody>
<tr><td><code>KVCacheManager</code></td><td>Scheduler 向け公開 API</td><td><code>target/vllm/vllm/v1/core/kv_cache_manager.py:94</code></td></tr>
<tr><td><code>KVCacheCoordinator</code></td><td>マルチグループ統括（3 実装）</td><td><code>target/vllm/vllm/v1/core/kv_cache_coordinator.py:28</code></td></tr>
<tr><td><code>SingleTypeKVCacheManager</code></td><td>アテンションタイプ別管理（7 実装）</td><td><code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:24</code></td></tr>
<tr><td><code>BlockPool</code></td><td>物理ブロック割り当て・解放・キャッシュ管理</td><td><code>target/vllm/vllm/v1/core/block_pool.py:128</code></td></tr>
<tr><td><code>KVCacheBlock</code></td><td>ブロックメタデータ（block_id, ref_cnt, block_hash）</td><td><code>target/vllm/vllm/v1/core/kv_cache_utils.py:107</code></td></tr>
<tr><td><code>BlockHashToBlockMap</code></td><td>プレフィックスキャッシュ用ハッシュ→ブロック対応表</td><td><code>target/vllm/vllm/v1/core/block_pool.py:32</code></td></tr>
<tr><td><code>FreeKVCacheBlockQueue</code></td><td>LRU 順序の空きブロック管理（双方向リンクリスト）</td><td><code>target/vllm/vllm/v1/core/kv_cache_utils.py:156</code></td></tr>
</tbody>
</table>
</div>
<h2 id="主要メソッド-6"><a class="header" href="#主要メソッド-6">主要メソッド</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>allocate_slots()</code></td><td>L206</td><td>リクエストに KV キャッシュブロックを割り当て。成功時 <code>KVCacheBlocks</code>、失敗時 <code>None</code></td></tr>
<tr><td><code>get_computed_blocks()</code></td><td>L164</td><td>プレフィックスキャッシュから最長ヒットを検索。<code>(KVCacheBlocks, int)</code></td></tr>
<tr><td><code>free()</code></td><td>L378</td><td>リクエストのブロックをプールに返却</td></tr>
<tr><td><code>usage</code> (property)</td><td>L143</td><td>KV キャッシュ使用率 (0.0-1.0)</td></tr>
<tr><td><code>reset_prefix_cache()</code></td><td>L409</td><td>プレフィックスキャッシュ全体をリセット</td></tr>
<tr><td><code>get_num_common_prefix_blocks()</code></td><td>L425</td><td>全リクエスト共通の先頭ブロック数（Cascade Attention 用）</td></tr>
<tr><td><code>cache_blocks()</code></td><td>L475</td><td>ブロックをプレフィックスキャッシュに登録</td></tr>
</tbody>
</table>
</div>
<h2 id="allocate_slots-の-5-段階フロー-deep-verified"><a class="header" href="#allocate_slots-の-5-段階フロー-deep-verified">allocate_slots() の 5 段階フロー [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_manager.py:206</code></p>
<pre><code>allocate_slots(request, num_new_tokens, ...)
  │
  ├─ Stage 1: スキップブロック解放（Sliding Window 用）
  │  └─ coordinator.remove_skipped_blocks(request_id, total_computed_tokens)
  │
  ├─ Stage 2: 容量チェック
  │  ├─ coordinator.get_num_blocks_to_allocate(...)
  │  └─ 空きブロック不足 → return None（プリエンプション誘発）
  │
  ├─ Stage 3: キャッシュヒットブロック割り当て
  │  └─ new_computed_blocks 非空 or external_computed &gt; 0:
  │     └─ coordinator.allocate_new_computed_blocks(...)
  │
  ├─ Stage 4: 新規ブロック割り当て
  │  └─ coordinator.allocate_new_blocks(request_id, num_tokens_need_slot, ...)
  │
  └─ Stage 5: キャッシュ登録判定
     ├─ NOT enable_caching or delay_cache_blocks → スキップ
     └─ coordinator.cache_blocks(request, num_tokens_to_cache)
        ※ num_tokens_to_cache はドラフトトークンを除外
</code></pre>
<p><strong>delay_cache_blocks</strong>: P/D（Prefill/Decode 分離）構成で KV Transfer 完了前にキャッシュ登録を遅延する。</p>
<h2 id="プレフィックスキャッシュ"><a class="header" href="#プレフィックスキャッシュ">プレフィックスキャッシュ</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_manager.py:164</code> (get_computed_blocks)</p>
<p>プロンプトトークン列をブロックサイズ単位でハッシュ化し、<code>BlockHashToBlockMap</code> で過去に計算済みのブロックを検索する。ハッシュチェーン（各ブロックのハッシュが前ブロックのハッシュに依存）により、プレフィックスの最長一致を効率的に検索する。</p>
<pre><code>get_computed_blocks(request)
  → coordinator.find_longest_cache_hit(request.block_hashes, max_length)
    → アテンションタイプ別の検索アルゴリズム
  → (キャッシュ済みブロック, ヒットトークン数) を返却
</code></pre>
<p><strong>制約</strong>: 全トークンがキャッシュヒットしても、logits 取得のため最後の 1 トークンは再計算が必要（<code>max_cache_hit_length = request.num_tokens - 1</code>）。</p>
<p>→ 詳細は <a href="#プレフィックスキャッシュ詳細">プレフィックスキャッシュ詳細</a> を参照</p>
<h2 id="参照カウントと-eviction"><a class="header" href="#参照カウントと-eviction">参照カウントと Eviction</a></h2>
<p><strong>KVCacheBlock</strong> (L107) の <code>ref_cnt</code> フィールドでブロックの使用状況を管理:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ref_cnt</th><th>状態</th></tr>
</thead>
<tbody>
<tr><td>0</td><td>空きブロックキュー（<code>FreeKVCacheBlockQueue</code>）内。Eviction 候補</td></tr>
<tr><td>≥ 1</td><td>リクエストに使用中。Eviction 対象外</td></tr>
</tbody>
</table>
</div>
<ul>
<li><strong>touch()</strong>: キャッシュヒット時に <code>ref_cnt</code> を増加し、空きキューから除外</li>
<li><strong>free_blocks()</strong>: <code>ref_cnt</code> を減少。0 になったら空きキューに戻す（逆順追加で LRU 効率化）</li>
<li><strong>_maybe_evict_cached_block()</strong>: 新規ブロック要求時に空きキューの先頭（最古）から Evict。ハッシュメタデータをリセット</li>
</ul>
<p>→ 詳細は <a href="#blockpool-詳細">BlockPool 詳細</a> を参照</p>
<h2 id="アテンションタイプ対応"><a class="header" href="#アテンションタイプ対応">アテンションタイプ対応</a></h2>
<p>7 種の <code>SingleTypeKVCacheManager</code> がアテンションタイプごとのブロック管理を担当:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Manager</th><th>KVCacheSpec</th><th>スキップ計算</th><th>キャッシュ検索</th></tr>
</thead>
<tbody>
<tr><td><code>FullAttentionManager</code></td><td>FullAttention / MLA</td><td>0</td><td>左→右</td></tr>
<tr><td><code>SlidingWindowManager</code></td><td>SlidingWindow</td><td><code>max(0, n-w+1)</code></td><td>右→左（連続）</td></tr>
<tr><td><code>ChunkedLocalAttentionManager</code></td><td>ChunkedLocal</td><td><code>(n//c)*c</code></td><td>null_pad + 左→右</td></tr>
<tr><td><code>MambaManager</code></td><td>Mamba</td><td><code>n - 1</code></td><td>右→左（単一）</td></tr>
<tr><td><code>CrossAttentionManager</code></td><td>CrossAttention</td><td>N/A</td><td>非対応</td></tr>
<tr><td><code>SinkFullAttentionManager</code></td><td>SinkFullAttention</td><td>0</td><td>左→右</td></tr>
</tbody>
</table>
</div>
<p>→ 詳細は <a href="#アテンションタイプ別-manager-詳細">アテンションタイプ別 Manager</a> を参照</p>
<h2 id="設定-4"><a class="header" href="#設定-4">設定</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>block_size</code></td><td>モデル依存</td><td>1 ブロックあたりのトークン数</td></tr>
<tr><td><code>enable_caching</code></td><td>設定依存</td><td>プレフィックスキャッシュの有効化</td></tr>
<tr><td><code>num_gpu_blocks</code></td><td>プロファイリングで決定</td><td>GPU メモリから算出される総ブロック数</td></tr>
<tr><td><code>hash_block_size</code></td><td>block_size と同値</td><td>ハッシュ計算に使用するブロックサイズ</td></tr>
<tr><td><code>prefix_caching_hash_algo</code></td><td><code>sha256_cbor</code></td><td>ハッシュ関数（sha256/sha256_cbor/xxhash/xxhash_cbor）</td></tr>
<tr><td><code>enable_kv_cache_events</code></td><td>False</td><td>KV Transfer 用イベント発行</td></tr>
</tbody>
</table>
</div>
<h2 id="呼び出しフロー-4"><a class="header" href="#呼び出しフロー-4">呼び出しフロー</a></h2>
<pre><code>Scheduler.schedule()
  ├─ kv_cache_manager.get_computed_blocks(request)     # プレフィックスキャッシュ検索
  ├─ kv_cache_manager.allocate_slots(request, ...)     # ブロック割り当て
  │   └─ None の場合 → プリエンプション実行
  └─ （完了時）kv_cache_manager.free(request)           # ブロック解放
</code></pre>
<h2 id="ソースファイル一覧"><a class="header" href="#ソースファイル一覧">ソースファイル一覧</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>行数</th><th>内容</th></tr>
</thead>
<tbody>
<tr><td><code>kv_cache_manager.py</code></td><td>490</td><td>KVCacheManager、KVCacheBlocks</td></tr>
<tr><td><code>kv_cache_coordinator.py</code></td><td>586</td><td>Coordinator 3 実装</td></tr>
<tr><td><code>single_type_kv_cache_manager.py</code></td><td>1065</td><td>Manager 7 種</td></tr>
<tr><td><code>block_pool.py</code></td><td>490</td><td>BlockPool、BlockHashToBlockMap</td></tr>
<tr><td><code>kv_cache_utils.py</code></td><td>1644</td><td>KVCacheBlock、Queue、ハッシュ計算</td></tr>
<tr><td><code>kv_cache_metrics.py</code></td><td>96</td><td>メトリクス収集</td></tr>
</tbody>
</table>
</div>
<h2 id="詳細ドキュメント"><a class="header" href="#詳細ドキュメント">詳細ドキュメント</a></h2>
<ul>
<li><strong><a href="#blockpool-詳細">BlockPool 詳細</a></strong> — FreeKVCacheBlockQueue、BlockHashToBlockMap、KVCacheBlock ライフサイクル、Eviction、KV Cache Events</li>
<li><strong><a href="#プレフィックスキャッシュ詳細">プレフィックスキャッシュ詳細</a></strong> — ハッシュチェーン計算、Extra Keys、Lookup アルゴリズム、Hybrid fixed-point</li>
<li><strong><a href="#アテンションタイプ別-manager-詳細">アテンションタイプ別 Manager</a></strong> — 7 種 Manager の詳細、スキップ計算、キャッシュ検索アルゴリズム</li>
</ul>
<h2 id="関連ドキュメント-4"><a class="header" href="#関連ドキュメント-4">関連ドキュメント</a></h2>
<ul>
<li><a href="#scheduler-サマリー">Scheduler</a></li>
<li><a href="#enginecore-サマリー">EngineCore</a></li>
<li><a href="#テキスト推論データフロー">データフロー</a></li>
<li><a href="#用語集">用語集</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="アテンションタイプ別-manager-詳細"><a class="header" href="#アテンションタイプ別-manager-詳細">アテンションタイプ別 Manager 詳細</a></h1>
<blockquote>
<p><strong>深度</strong>: [DEEP]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-11"><a class="header" href="#概要-11">概要</a></h2>
<p><code>SingleTypeKVCacheManager</code> は1種類のアテンションタイプの KV キャッシュ管理ロジックを担当する抽象基底クラスである。アテンションタイプごとにサブクラスが存在し、ブロックの割り当て・解放・プレフィックスキャッシュ検索をそれぞれのセマンティクスに合わせて実装する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py</code></p>
<h2 id="spec_manager_map-deep-verified"><a class="header" href="#spec_manager_map-deep-verified">spec_manager_map [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:1049</code></p>
<pre><code class="language-python">spec_manager_map = {
    FullAttentionSpec:            FullAttentionManager,
    MLAAttentionSpec:             FullAttentionManager,       # MLA も Full 扱い
    SlidingWindowSpec:            SlidingWindowManager,
    ChunkedLocalAttentionSpec:    ChunkedLocalAttentionManager,
    MambaSpec:                    MambaManager,
    CrossAttentionSpec:           CrossAttentionManager,
    SinkFullAttentionSpec:        SinkFullAttentionManager,
}
</code></pre>
<p><code>get_manager_for_kv_cache_spec(spec, **kwargs)</code> ファクトリ関数がこのマップから Manager クラスをディスパッチする。</p>
<h2 id="基底クラス-singletypekvcachemanager-deep-verified"><a class="header" href="#基底クラス-singletypekvcachemanager-deep-verified">基底クラス: SingleTypeKVCacheManager [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:24</code></p>
<h3 id="状態管理"><a class="header" href="#状態管理">状態管理</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>req_to_blocks</code></td><td><code>defaultdict[str, list[KVCacheBlock]]</code></td><td>リクエスト ID → 割り当て済みブロックリスト</td></tr>
<tr><td><code>num_cached_block</code></td><td><code>dict[str, int]</code></td><td>リクエスト ID → キャッシュ登録済みブロック数。RUNNING リクエストのみ追跡</td></tr>
<tr><td><code>block_size</code></td><td><code>int</code></td><td>1 ブロックあたりのトークン数。DCP/PCP &gt; 1 の場合は乗算される</td></tr>
</tbody>
</table>
</div>
<h3 id="コンストラクタ"><a class="header" href="#コンストラクタ">コンストラクタ</a></h3>
<pre><code class="language-python">def __init__(self, kv_cache_spec, block_pool, enable_caching,
             kv_cache_group_id, dcp_world_size=1, pcp_world_size=1):
    self.block_size = kv_cache_spec.block_size
    if dcp_world_size * pcp_world_size &gt; 1:
        self.block_size *= dcp_world_size * pcp_world_size
</code></pre>
<p>DCP（Decode Context Parallelism）/ PCP（Prefill Context Parallelism）ではブロックサイズが並列度倍に拡大される。</p>
<h3 id="get_num_blocks_to_allocate-deep-verified"><a class="header" href="#get_num_blocks_to_allocate-deep-verified">get_num_blocks_to_allocate() [DEEP] [VERIFIED]</a></h3>
<p><strong>参照</strong>: L73</p>
<p>リクエストに必要な新規ブロック数を算出する。2 つのパスが存在:</p>
<pre><code>get_num_blocks_to_allocate(request_id, num_tokens, new_computed_blocks, ...)
  │
  ├─ Fast-path: request_id in num_cached_block（RUNNING リクエスト）
  │  └─ max(num_required_blocks - num_req_blocks, 0)
  │     ※ Speculative Decoding のリジェクトで num_req_blocks &gt; num_required_blocks もあり得る
  │
  └─ Slow-path: 新規リクエスト（プレフィックスキャッシュヒットあり）
     ├─ num_skipped_tokens = get_num_skipped_tokens(total_computed_tokens)
     ├─ num_skipped_blocks = num_skipped_tokens // block_size
     ├─ num_new_blocks = max(required - max(skipped, local_computed), 0)
     ├─ num_evictable_blocks = Σ(ref_cnt==0 かつ非null)
     │  ← touch() 時にキューから除去されるブロック分を加算
     └─ return num_new_blocks + num_evictable_blocks
</code></pre>
<p><strong>Evictable blocks の加算理由</strong>: <code>new_computed_blocks</code> 内のブロックが空きキュー内（<code>ref_cnt == 0</code>）にある場合、<code>touch()</code> で空きキューから除去されるため、実質的に空きブロック数が減る。この分を事前に計上する。</p>
<h3 id="allocate_new_computed_blocks-deep-verified"><a class="header" href="#allocate_new_computed_blocks-deep-verified">allocate_new_computed_blocks() [DEEP] [VERIFIED]</a></h3>
<p><strong>参照</strong>: L137</p>
<p>プレフィックスキャッシュヒットしたブロックをリクエストに追加する。</p>
<pre><code>allocate_new_computed_blocks(request_id, new_computed_blocks, ...)
  │
  ├─ RUNNING → assert len(new_computed_blocks) == 0 → return
  │
  └─ 新規リクエスト:
     ├─ num_skipped_blocks 計算
     ├─ スキップ分を new_computed_blocks から除去
     ├─ enable_caching → block_pool.touch(new_computed_blocks)
     ├─ req_blocks に null_block × num_skipped_blocks を追加
     ├─ req_blocks に new_computed_blocks を追加
     ├─ num_cached_block[request_id] = len(req_blocks)
     └─ external_computed_tokens &gt; 0 → 追加ブロック割り当て
</code></pre>
<h3 id="allocate_new_blocks-verified"><a class="header" href="#allocate_new_blocks-verified">allocate_new_blocks() [VERIFIED]</a></h3>
<p><strong>参照</strong>: L208</p>
<pre><code class="language-python">def allocate_new_blocks(self, request_id, num_tokens, num_tokens_main_model):
    num_required_blocks = cdiv(num_tokens, self.block_size)
    num_new_blocks = num_required_blocks - len(req_blocks)
    if num_new_blocks &lt;= 0:
        return []
    new_blocks = self.block_pool.get_new_blocks(num_new_blocks)
    req_blocks.extend(new_blocks)
    return new_blocks  # 新規分のみ返す
</code></pre>
<h3 id="cache_blocks-verified"><a class="header" href="#cache_blocks-verified">cache_blocks() [VERIFIED]</a></h3>
<p><strong>参照</strong>: L235</p>
<pre><code class="language-python">def cache_blocks(self, request, num_tokens):
    num_full_blocks = num_tokens // self.block_size
    if num_cached_blocks &gt;= num_full_blocks:
        return  # 既に登録済み
    block_pool.cache_full_blocks(request, req_blocks, num_cached_blocks,
                                  num_full_blocks, block_size, group_id)
    num_cached_block[request_id] = num_full_blocks
</code></pre>
<h3 id="free-verified"><a class="header" href="#free-verified">free() [VERIFIED]</a></h3>
<p><strong>参照</strong>: L261</p>
<pre><code class="language-python">def free(self, request_id):
    req_blocks = self.req_to_blocks.pop(request_id, [])
    ordered_blocks = reversed(req_blocks)  # 逆順でLRU最適化
    self.block_pool.free_blocks(ordered_blocks)
    self.num_cached_block.pop(request_id, None)
</code></pre>
<p><strong>逆順の理由</strong>: チェーン末尾（最新トークン）がキューの先頭側に来ることで、次の割り当て時に最初に evict される。先頭ブロック（プレフィックス）は長く残り、共有確率が上がる。</p>
<h3 id="remove_skipped_blocks-verified"><a class="header" href="#remove_skipped_blocks-verified">remove_skipped_blocks() [VERIFIED]</a></h3>
<p><strong>参照</strong>: L343</p>
<pre><code class="language-python">def remove_skipped_blocks(self, request_id, total_computed_tokens):
    num_skipped_tokens = self.get_num_skipped_tokens(total_computed_tokens)
    if num_skipped_tokens &lt;= 0:
        return  # Full Attention: 何もしない
    # 後方から走査して null_block に遭遇したら停止
    for i in range(num_skipped_blocks - 1, -1, -1):
        if blocks[i] == null_block:
            break  # 前回の呼び出しで既に解放済み
        removed_blocks.append(blocks[i])
        blocks[i] = null_block
    block_pool.free_blocks(removed_blocks)
</code></pre>
<h3 id="get_num_skipped_tokens-verified"><a class="header" href="#get_num_skipped_tokens-verified">get_num_skipped_tokens() [VERIFIED]</a></h3>
<p><strong>参照</strong>: L386</p>
<p>デフォルト実装は <code>return 0</code>（全トークンがアテンション対象）。サブクラスでオーバーライド。</p>
<h2 id="fullattentionmanager-deep-verified"><a class="header" href="#fullattentionmanager-deep-verified">FullAttentionManager [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:400</code></p>
<p>標準的な全トークンアテンション。基底クラスの動作をそのまま継承し、2 つのメソッドのみ実装。</p>
<h3 id="find_longest_cache_hit"><a class="header" href="#find_longest_cache_hit">find_longest_cache_hit()</a></h3>
<p><strong>参照</strong>: L401</p>
<pre><code>左→右にブロックハッシュを走査:
  キャッシュヒット → computed_blocks に追加
  キャッシュミス → break（チェーンが途切れたら以降は必ずミス）

EAGLE 使用時:
  最後のブロックを削除（hidden states 再計算が必要）

alignment_tokens でアライメント調整:
  Hybrid モデルで LCM ブロックサイズの倍数に切り詰め
</code></pre>
<p><strong>Downward-closed 性質</strong>: Full Attention では blocks[0..n] がヒットするなら blocks[0..n-1] も必ずヒットする。この性質により、左→右の貪欲スキャンで最適解が得られる。</p>
<h3 id="get_num_common_prefix_blocks"><a class="header" href="#get_num_common_prefix_blocks">get_num_common_prefix_blocks()</a></h3>
<p><strong>参照</strong>: L450</p>
<pre><code class="language-python">def get_num_common_prefix_blocks(self, running_request_id):
    for block in blocks:
        if block.ref_cnt == len(self.req_to_blocks):
            num_common_blocks += 1
        else:
            break
    return num_common_blocks
</code></pre>
<p><strong>原理</strong>: <code>ref_cnt == 全リクエスト数</code> なら、そのブロックは全リクエストで共有されている → 共通プレフィックス。Cascade Attention で共通プレフィックスの再計算をスキップするために使用。</p>
<h2 id="slidingwindowmanager-deep-verified"><a class="header" href="#slidingwindowmanager-deep-verified">SlidingWindowManager [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:461</code></p>
<p>Sliding Window Attention 用。ウィンドウ外のトークンの KV キャッシュを解放してメモリを節約する。</p>
<h3 id="コンストラクタ-1"><a class="header" href="#コンストラクタ-1">コンストラクタ</a></h3>
<pre><code class="language-python">def __init__(self, kv_cache_spec: SlidingWindowSpec, **kwargs):
    super().__init__(kv_cache_spec, **kwargs)
    self.sliding_window = kv_cache_spec.sliding_window
</code></pre>
<h3 id="get_num_skipped_tokens"><a class="header" href="#get_num_skipped_tokens">get_num_skipped_tokens()</a></h3>
<p><strong>参照</strong>: L556</p>
<pre><code class="language-python">def get_num_skipped_tokens(self, num_computed_tokens):
    return max(0, num_computed_tokens - self.sliding_window + 1)
</code></pre>
<pre><code>例: sliding_window=4, num_computed_tokens=7

Tokens: [0 1 2 3 4 5 6 | 7]
                          ↑ 次に計算するトークン
                  [4 5 6 7]  ← sliding window（サイズ4）
        [0 1 2 3]            ← skipped（4トークン）
</code></pre>
<h3 id="find_longest_cache_hit-1"><a class="header" href="#find_longest_cache_hit-1">find_longest_cache_hit()</a></h3>
<p><strong>参照</strong>: L466</p>
<pre><code>右→左にブロックハッシュを走査:
  キャッシュヒット → computed_blocks[i] にセット、連続カウント++
  キャッシュミス → 連続カウント = 0（リセット）

  連続カウント &gt;= sliding_window_contiguous_blocks:
    末尾をトリミングして break

sliding_window_contiguous_blocks = ceil((window - 1) / block_size)
</code></pre>
<p><strong>Right-to-left の理由</strong>: Sliding Window は最新のトークン付近のブロックが重要。右端から連続ヒットを探すことで、ウィンドウ内の有用なキャッシュを効率的に発見する。</p>
<p><strong>初期値</strong>: <code>computed_blocks</code> は <code>null_block</code> で埋められ、ヒットした位置のみ実ブロックで置換される。</p>
<p><strong>制約事項</strong>:</p>
<ul>
<li>DCP/PCP 非対応 (<code>assert dcp_world_size == 1</code>)</li>
<li>EAGLE 使用時は <code>sliding_window_contiguous_blocks += 1</code></li>
</ul>
<h3 id="get_num_common_prefix_blocks-1"><a class="header" href="#get_num_common_prefix_blocks-1">get_num_common_prefix_blocks()</a></h3>
<p><strong>参照</strong>: L584</p>
<p>常に <code>0</code> を返す。プレフィックスブロックは全て null_block に置換されているため、Cascade Attention は使用不可。</p>
<h2 id="chunkedlocalattentionmanager-deep-verified"><a class="header" href="#chunkedlocalattentionmanager-deep-verified">ChunkedLocalAttentionManager [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:594</code></p>
<p>チャンク境界でアテンションが分割されるモデル用。各チャンク内のトークンのみが互いにアテンションする。</p>
<h3 id="コンストラクタ-2"><a class="header" href="#コンストラクタ-2">コンストラクタ</a></h3>
<pre><code class="language-python">def __init__(self, kv_cache_spec: ChunkedLocalAttentionSpec, **kwargs):
    super().__init__(kv_cache_spec, **kwargs)
    self.attention_chunk_size = kv_cache_spec.attention_chunk_size
</code></pre>
<h3 id="get_num_skipped_tokens-1"><a class="header" href="#get_num_skipped_tokens-1">get_num_skipped_tokens()</a></h3>
<p><strong>参照</strong>: L691</p>
<pre><code class="language-python">def get_num_skipped_tokens(self, num_computed_tokens):
    return (num_computed_tokens // self.attention_chunk_size) * self.attention_chunk_size
</code></pre>
<pre><code>例1: chunk_size=8, computed=13 → skipped=8  (チャンク[0,7]全体)
例2: chunk_size=8, computed=8  → skipped=8  (チャンク[0,7]全体)
例3: chunk_size=8, computed=7  → skipped=0  (まだチャンク内)
</code></pre>
<h3 id="find_longest_cache_hit-2"><a class="header" href="#find_longest_cache_hit-2">find_longest_cache_hit()</a></h3>
<p><strong>参照</strong>: L599</p>
<pre><code>1. local_attention_start_idx = (max_length // chunk_size) * chunk_size
2. computed_blocks = [null_block] × (start_idx // block_size)  ← ウィンドウ外
3. start_idx から max_num_blocks まで左→右スキャン:
   ヒット → append、ミス → break
</code></pre>
<p>ウィンドウ外のブロックは null_block でパディングし、ウィンドウ内のみ FullAttention と同様の左→右スキャンを行う。</p>
<p><strong>制約事項</strong>:</p>
<ul>
<li>EAGLE 非対応 (<code>assert use_eagle is False</code>)</li>
<li>DCP/PCP 非対応</li>
<li>異なるブロックサイズの混在非対応</li>
</ul>
<h2 id="mambamanager-deep-verified"><a class="header" href="#mambamanager-deep-verified">MambaManager [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:744</code></p>
<p>Mamba（State Space Model / 線形アテンション）用の Manager。Transformer ベースのアテンションとは根本的に異なり、KV キャッシュではなく「状態」を管理する。</p>
<h3 id="2つのキャッシュモード"><a class="header" href="#2つのキャッシュモード">2つのキャッシュモード</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>モード</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>none</code>（デフォルト）</td><td>基底クラスの動作 + speculative blocks 追加</td></tr>
<tr><td><code>align</code></td><td>最後の状態ブロックのみ追跡、null_block パディング、speculative blocks 再利用</td></tr>
</tbody>
</table>
</div>
<h3 id="追加状態align-モード"><a class="header" href="#追加状態align-モード">追加状態（align モード）</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>last_state_block_idx</code></td><td><code>dict[str, int]</code></td><td>前ステップで割り当てた状態ブロックのインデックス</td></tr>
<tr><td><code>_allocated_block_reqs</code></td><td><code>set[str]</code></td><td>ブロック割り当て済みリクエストの集合</td></tr>
<tr><td><code>num_speculative_blocks</code></td><td><code>int</code></td><td>Speculative Decoding 用の余分なブロック数</td></tr>
</tbody>
</table>
</div>
<h3 id="get_num_skipped_tokens-2"><a class="header" href="#get_num_skipped_tokens-2">get_num_skipped_tokens()</a></h3>
<p><strong>参照</strong>: L967</p>
<pre><code class="language-python">def get_num_skipped_tokens(self, num_computed_tokens):
    return num_computed_tokens - 1  # 最後の状態のみ必要
</code></pre>
<p>Mamba は状態の累積的な更新なので、最後のトークンの状態さえあれば以前のトークンの状態は不要。</p>
<h3 id="find_longest_cache_hit-3"><a class="header" href="#find_longest_cache_hit-3">find_longest_cache_hit()</a></h3>
<p><strong>参照</strong>: L756</p>
<pre><code>右→左に走査:
  最初のヒットで即座に break
  ヒット位置の前を null_block で埋める
</code></pre>
<p>最後の状態のみ必要なため、最右のヒット 1 つで十分。</p>
<h3 id="get_num_blocks_to_allocatealign-モード-deep-verified"><a class="header" href="#get_num_blocks_to_allocatealign-モード-deep-verified">get_num_blocks_to_allocate()（align モード） [DEEP] [VERIFIED]</a></h3>
<p><strong>参照</strong>: L832</p>
<pre><code>align モード:
  ├─ 既存リクエスト（_allocated_block_reqs に存在）:
  │  └─ 最大 1 ブロック追加（speculative blocks 再利用のため）
  │
  └─ 新規リクエスト:
     └─ 1 + num_speculative_blocks ブロック
</code></pre>
<h3 id="allocate_new_blocksalign-モード-deep-verified"><a class="header" href="#allocate_new_blocksalign-モード-deep-verified">allocate_new_blocks()（align モード） [DEEP] [VERIFIED]</a></h3>
<p><strong>参照</strong>: L885</p>
<p>align モードの割り当ては複雑:</p>
<pre><code>1. num_tokens を main model 分に制限（lookahead 除外）
2. last_state_block_idx を記録:
   - 既存: prev_len - 1 - num_speculative_blocks
   - 新規（キャッシュヒット有）: prev_len - 1
3. null_block でスキップ位置をパディング
4. 既存リクエスト: speculative blocks をスキップ位置に移動して再利用
5. 残りの新規ブロックを割り当て
</code></pre>
<h3 id="remove_skipped_blocksalign-モード"><a class="header" href="#remove_skipped_blocksalign-モード">remove_skipped_blocks()（align モード）</a></h3>
<p><strong>参照</strong>: L804</p>
<p>基底クラスの <code>remove_skipped_blocks()</code> に加え、<code>last_state_block_idx</code> のブロックも解放する:</p>
<pre><code class="language-python">if last_state_block_idx &lt; cdiv(num_computed_tokens, block_size) - 1:
    block_pool.free_blocks([blocks[last_state_block_idx]])
    blocks[last_state_block_idx] = null_block
</code></pre>
<p>2 ステップ前のブロックが不要になるタイミングで解放する。</p>
<h3 id="free"><a class="header" href="#free">free()</a></h3>
<p><strong>参照</strong>: L961</p>
<pre><code class="language-python">def free(self, request_id):
    if self.mamba_cache_mode == "align":
        self._allocated_block_reqs.discard(request_id)
        self.last_state_block_idx.pop(request_id, None)
    super().free(request_id)
</code></pre>
<h2 id="crossattentionmanager-deep-verified"><a class="header" href="#crossattentionmanager-deep-verified">CrossAttentionManager [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:976</code></p>
<p>エンコーダ-デコーダモデル（Whisper 等）のクロスアテンション用。エンコーダ出力はリクエスト固有（異なる音声/画像入力）のため、プレフィックスキャッシュの恩恵がない。</p>
<h3 id="制約"><a class="header" href="#制約">制約</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>動作</th></tr>
</thead>
<tbody>
<tr><td><code>allocate_new_computed_blocks()</code></td><td><code>assert len(new_computed_blocks) == 0</code></td></tr>
<tr><td><code>cache_blocks()</code></td><td><code>raise ValueError</code></td></tr>
<tr><td><code>find_longest_cache_hit()</code></td><td><code>raise NotImplementedError</code></td></tr>
<tr><td><code>get_num_common_prefix_blocks()</code></td><td><code>return 0</code></td></tr>
</tbody>
</table>
</div>
<p>エンコーダブロックはリクエスト開始時に <code>num_encoder_tokens</code> に基づいて静的に割り当てられ、デコード中は変化しない。</p>
<h2 id="sinkfullattentionmanager-deep-verified"><a class="header" href="#sinkfullattentionmanager-deep-verified">SinkFullAttentionManager [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:1025</code></p>
<p>StreamingLLM のための Attention Sink 実装。<code>FullAttentionManager</code> を継承し、初期化時に先頭の sink ブロックを事前確保する。</p>
<h3 id="コンストラクタ-3"><a class="header" href="#コンストラクタ-3">コンストラクタ</a></h3>
<pre><code class="language-python">class SinkFullAttentionManager(FullAttentionManager):
    def __init__(self, kv_cache_spec: SinkFullAttentionSpec, ...):
        super().__init__(...)
        sink_len = kv_cache_spec.sink_len
        assert sink_len &gt; 0 and sink_len % self.block_size == 0
        num_sink_block = sink_len // self.block_size
        self.sink_blocks = self.block_pool.free_block_queue.popleft_n(num_sink_block)
</code></pre>
<p><strong>特徴</strong>:</p>
<ul>
<li><code>sink_len</code> は <code>block_size</code> の倍数でなければならない</li>
<li>sink ブロックは初期化時に <code>popleft_n()</code> で確保され、以降解放されない</li>
<li>FullAttentionManager の <code>find_longest_cache_hit()</code> と <code>get_num_common_prefix_blocks()</code> をそのまま使用</li>
</ul>
<h2 id="各-manager-の比較表-verified"><a class="header" href="#各-manager-の比較表-verified">各 Manager の比較表 [VERIFIED]</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Manager</th><th>スキップ計算</th><th>キャッシュ検索</th><th>Cascade</th><th>DCP/PCP</th><th>EAGLE</th></tr>
</thead>
<tbody>
<tr><td><strong>FullAttention</strong></td><td>0（全トークン）</td><td>左→右</td><td>ref_cnt 基準</td><td>対応</td><td>対応</td></tr>
<tr><td><strong>SlidingWindow</strong></td><td><code>max(0, n-w+1)</code></td><td>右→左（連続）</td><td>非対応</td><td>非対応</td><td>対応</td></tr>
<tr><td><strong>ChunkedLocal</strong></td><td><code>(n//c)*c</code></td><td>null_pad + 左→右</td><td>非対応</td><td>非対応</td><td>非対応</td></tr>
<tr><td><strong>Mamba</strong></td><td><code>n - 1</code></td><td>右→左（単一）</td><td>非対応</td><td>非対応</td><td>-</td></tr>
<tr><td><strong>CrossAttention</strong></td><td>0</td><td>非対応</td><td>非対応</td><td>-</td><td>-</td></tr>
<tr><td><strong>SinkFullAttention</strong></td><td>0</td><td>左→右</td><td>ref_cnt 基準</td><td>対応</td><td>対応</td></tr>
</tbody>
</table>
</div>
<h2 id="関連ドキュメント-5"><a class="header" href="#関連ドキュメント-5">関連ドキュメント</a></h2>
<ul>
<li><a href="#kvcachemanager-サマリー">KVCacheManager サマリー</a></li>
<li><a href="#blockpool-詳細">BlockPool 詳細</a></li>
<li><a href="#プレフィックスキャッシュ詳細">プレフィックスキャッシュ詳細</a></li>
<li><a href="#scheduler-サマリー">Scheduler</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="blockpool-詳細"><a class="header" href="#blockpool-詳細">BlockPool 詳細</a></h1>
<blockquote>
<p><strong>深度</strong>: [DEEP]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-12"><a class="header" href="#概要-12">概要</a></h2>
<p><code>BlockPool</code> はKVキャッシュの物理ブロックを管理するクラスである。ブロックの割り当て・解放・プレフィックスキャッシュ索引を一元管理し、LRU Eviction によるメモリ再利用を実現する。3つの内部データ構造（<code>FreeKVCacheBlockQueue</code>、<code>BlockHashToBlockMap</code>、<code>KVCacheBlock</code>）で構成される。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:128</code></p>
<h2 id="kvcacheblock-deep-verified"><a class="header" href="#kvcacheblock-deep-verified">KVCacheBlock [DEEP] [VERIFIED]</a></h2>
<p>KVキャッシュブロック1つのメタデータを保持する dataclass。物理メモリ自体は GPU 上にあり、このオブジェクトは CPU 側のメタデータのみを管理する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:107</code></p>
<h3 id="フィールド"><a class="header" href="#フィールド">フィールド</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>block_id</code></td><td><code>int</code></td><td>0 〜 <code>num_gpu_blocks - 1</code> の一意識別子</td></tr>
<tr><td><code>ref_cnt</code></td><td><code>int</code></td><td>参照カウント。0なら空きキュー内（Eviction候補）</td></tr>
<tr><td><code>_block_hash</code></td><td><code>BlockHashWithGroupId | None</code></td><td>プレフィックスキャッシュ用ハッシュキー。fullブロックでキャッシュ登録済みの場合のみ設定</td></tr>
<tr><td><code>prev_free_block</code></td><td><code>KVCacheBlock | None</code></td><td>空きキューの前ノードポインタ</td></tr>
<tr><td><code>next_free_block</code></td><td><code>KVCacheBlock | None</code></td><td>空きキューの次ノードポインタ</td></tr>
<tr><td><code>is_null</code></td><td><code>bool</code></td><td>null_block フラグ。True の場合は解放・Eviction 対象外</td></tr>
</tbody>
</table>
</div>
<h3 id="block_hash-プロパティ"><a class="header" href="#block_hash-プロパティ">block_hash プロパティ</a></h3>
<pre><code class="language-python">@block_hash.setter
def block_hash(self, block_hash: BlockHashWithGroupId):
    assert self.block_hash is None  # 二重設定を禁止
    self._block_hash = block_hash

def reset_hash(self):
    self._block_hash = None  # Eviction時にリセット
</code></pre>
<p><strong>制約</strong>: setter は <code>assert self.block_hash is None</code> で二重設定を防止する。ハッシュのリセットは <code>reset_hash()</code> のみで行う。これによりブロックのライフサイクルが「未設定 → 設定 → リセット → 再設定」の順序で制御される。</p>
<h3 id="ライフサイクル"><a class="header" href="#ライフサイクル">ライフサイクル</a></h3>
<pre><code>1. 生成: BlockPool.__init__() で全ブロック生成（ref_cnt=0）
2. 割り当て: get_new_blocks() → ref_cnt=1
3. キャッシュ登録: cache_full_blocks() → block_hash 設定
4. 再利用（キャッシュヒット）: touch() → ref_cnt++
5. 解放: free_blocks() → ref_cnt-- → 0なら空きキューへ
6. Eviction: _maybe_evict_cached_block() → hash リセット → 再割り当て
</code></pre>
<h2 id="freekvcacheblockqueue-deep-verified"><a class="header" href="#freekvcacheblockqueue-deep-verified">FreeKVCacheBlockQueue [DEEP] [VERIFIED]</a></h2>
<p>空きブロックを LRU 順序で管理する<strong>双方向リンクリスト</strong>。Python 組み込みの <code>deque</code> ではなく独自実装を採用している。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:156</code></p>
<h3 id="なぜ独自実装か"><a class="header" href="#なぜ独自実装か">なぜ独自実装か</a></h3>
<p><code>deque</code> では中間要素の削除が O(n) であるのに対し、この実装では O(1) で削除できる。<code>touch()</code> でキャッシュヒットしたブロックを空きキューの中間から即座に除去する必要があるため、O(1) の <code>remove()</code> が不可欠である。</p>
<p>また、Python オブジェクトのアロケーションを行わず、<code>KVCacheBlock</code> の <code>prev_free_block</code>/<code>next_free_block</code> ポインタを直接操作するため、GC 負荷が低い。</p>
<h3 id="センチネルノード"><a class="header" href="#センチネルノード">センチネルノード</a></h3>
<pre><code>fake_head ⇄ block_0 ⇄ block_1 ⇄ ... ⇄ block_n ⇄ fake_tail
</code></pre>
<ul>
<li><code>fake_free_list_head</code>: <code>block_id=-1</code> のダミーノード。先頭の前に配置</li>
<li><code>fake_free_list_tail</code>: <code>block_id=-1</code> のダミーノード。末尾の後に配置</li>
<li><strong>目的</strong>: null チェックの分岐を減らし、コードを簡素化</li>
</ul>
<h3 id="操作一覧"><a class="header" href="#操作一覧">操作一覧</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>計算量</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>popleft()</code></td><td>L208</td><td>O(1)</td><td>先頭ブロックを取り出し</td></tr>
<tr><td><code>popleft_n(n)</code></td><td>L245</td><td>O(n)</td><td>先頭から n 個を一括取り出し</td></tr>
<tr><td><code>remove(block)</code></td><td>L278</td><td>O(1)</td><td>中間のブロックを除去（touch 用）</td></tr>
<tr><td><code>append(block)</code></td><td>L298</td><td>O(1)</td><td>末尾にブロックを追加</td></tr>
<tr><td><code>append_n(blocks)</code></td><td>L321</td><td>O(n)</td><td>末尾に複数ブロックを一括追加</td></tr>
<tr><td><code>get_all_free_blocks()</code></td><td>L346</td><td>O(m)</td><td>全空きブロック取得（テスト用）</td></tr>
</tbody>
</table>
</div>
<h3 id="lru-順序の維持"><a class="header" href="#lru-順序の維持">LRU 順序の維持</a></h3>
<ul>
<li><strong>初期状態</strong>: <code>block_id</code> 順（0, 1, 2, …）</li>
<li><strong>再挿入時</strong>: <code>free_blocks()</code> でブロックが返却される際に<strong>逆順</strong>で追加される
<ul>
<li>理由: リクエストのブロックチェーンの末尾（最新トークン）は先に evict されるべき。先頭（古いプレフィックス）は他のリクエストと共有される可能性が高いため後回し</li>
<li>逆順操作は <code>SingleTypeKVCacheManager.free()</code> 側で実行される（BlockPool 外）</li>
</ul>
</li>
</ul>
<h2 id="blockhashtoblockmap-deep-verified"><a class="header" href="#blockhashtoblockmap-deep-verified">BlockHashToBlockMap [DEEP] [VERIFIED]</a></h2>
<p>プレフィックスキャッシュのハッシュ→ブロック対応表。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:32</code></p>
<h3 id="データ構造"><a class="header" href="#データ構造">データ構造</a></h3>
<pre><code class="language-python">_cache: dict[BlockHashWithGroupId, KVCacheBlock | dict[int, KVCacheBlock]]
</code></pre>
<p><strong>Union 型の最適化</strong>: 大半のハッシュキーには 1 ブロックしか対応しないため、単一ブロックは直接格納し、2つ以上の場合のみ内部 dict に昇格する。これにより内部 dict の GC コストを削減。</p>
<h3 id="重複排除なし設計"><a class="header" href="#重複排除なし設計">重複排除なし設計</a></h3>
<p>同一ハッシュのブロックが複数存在しても<strong>重複排除しない</strong>。理由: ブロック ID をリクエストに割り当てた後は追加のみ（append-only）を保証するため。重複排除するとブロック ID が変わり、ブロックテーブルの安定性が崩れる。</p>
<h3 id="操作"><a class="header" href="#操作">操作</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>get_one_block(key)</code></td><td>L60</td><td>ハッシュキーに対応する任意の1ブロックを返す。複数あれば先頭</td></tr>
<tr><td><code>insert(key, block)</code></td><td>L73</td><td>ブロックをキャッシュに追加。1→dict 昇格を自動処理</td></tr>
<tr><td><code>pop(key, block_id)</code></td><td>L91</td><td>特定の block_id を除去。残りがあれば dict を復元</td></tr>
<tr><td><code>__len__()</code></td><td>L121</td><td>ハッシュキー数（ブロック数ではない）</td></tr>
</tbody>
</table>
</div>
<h3 id="insert-の分岐"><a class="header" href="#insert-の分岐">insert の分岐</a></h3>
<pre><code>key なし       → _cache[key] = block          (単一格納)
key に 1 block → _cache[key] = {id: blk, ...}  (dict 昇格)
key に dict    → dict[block.block_id] = block   (dict 追加)
</code></pre>
<h2 id="null_block-deep-verified"><a class="header" href="#null_block-deep-verified">null_block [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:174</code></p>
<h3 id="特性"><a class="header" href="#特性">特性</a></h3>
<ul>
<li><strong>block_id = 0</strong>: 初期化時に空きキューから最初に popleft される</li>
<li><strong>is_null = True</strong>: 解放・Eviction の対象外</li>
<li><strong>ref_cnt 未管理</strong>: <code>touch()</code> や <code>free_blocks()</code> で特別にスキップされる</li>
</ul>
<h3 id="用途"><a class="header" href="#用途">用途</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>用途</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td>Sliding Window Attention</td><td>ウィンドウ外のブロック位置を埋める。物理メモリを消費しない</td></tr>
<tr><td>Mamba (align モード)</td><td>スキップされたブロック位置のパディング</td></tr>
<tr><td>ブロックテーブルの長さ統一</td><td>Attention カーネルにはブロックテーブルの連続性が必要。null_block で長さを揃える</td></tr>
</tbody>
</table>
</div>
<h3 id="ガード条件"><a class="header" href="#ガード条件">ガード条件</a></h3>
<pre><code class="language-python"># touch() (L383)
if block.ref_cnt == 0 and not block.is_null:
    self.free_block_queue.remove(block)

# free_blocks() (L401-402)
[block for block in blocks_list if block.ref_cnt == 0 and not block.is_null]

# cache_full_blocks() (L260-261)
if blk.is_null:
    continue  # null ブロックはキャッシュしない
</code></pre>
<h2 id="ブロック割り当てフロー-deep-verified"><a class="header" href="#ブロック割り当てフロー-deep-verified">ブロック割り当てフロー [DEEP] [VERIFIED]</a></h2>
<h3 id="get_new_blocks"><a class="header" href="#get_new_blocks">get_new_blocks()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:300</code></p>
<pre><code>get_new_blocks(num_blocks)
  │
  ├─ 空きブロック数チェック → 不足なら ValueError
  │
  ├─ free_block_queue.popleft_n(num_blocks)
  │  └─ LRU 先頭（最古）から取り出し
  │
  ├─ enable_caching の場合:
  │  ├─ _maybe_evict_cached_block(block)  ← ハッシュクリア
  │  ├─ assert block.ref_cnt == 0
  │  ├─ block.ref_cnt += 1
  │  └─ metrics_collector.on_block_allocated(block)  ← サンプリング
  │
  └─ enable_caching でない場合:
     ├─ assert block.ref_cnt == 0
     ├─ block.ref_cnt += 1
     └─ metrics_collector.on_block_allocated(block)
</code></pre>
<p><strong>注意</strong>: この関数はキャッシュ検索を行わない。キャッシュヒットの確認は <code>get_cached_block()</code> で別途行う。</p>
<h3 id="get_cached_block"><a class="header" href="#get_cached_block">get_cached_block()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:182</code></p>
<pre><code>get_cached_block(block_hash, kv_cache_group_ids)
  │
  ├─ 各 group_id について:
  │  ├─ make_block_hash_with_group_id(block_hash, group_id)
  │  ├─ cached_block_hash_to_block.get_one_block(hash_with_id)
  │  └─ 1つでも miss → None を返す（全グループ一致が必須）
  │
  └─ 全ヒット → list[KVCacheBlock] を返す
</code></pre>
<p><strong>All-or-nothing セマンティクス</strong>: 複数の KV キャッシュグループがある場合、全グループでヒットしなければキャッシュミスとして扱う。</p>
<h2 id="ブロック解放フロー-deep-verified"><a class="header" href="#ブロック解放フロー-deep-verified">ブロック解放フロー [DEEP] [VERIFIED]</a></h2>
<h3 id="free_blocks"><a class="header" href="#free_blocks">free_blocks()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:389</code></p>
<pre><code class="language-python">def free_blocks(self, ordered_blocks: Iterable[KVCacheBlock]) -&gt; None:
    blocks_list = list(ordered_blocks)           # イテレータを実体化
    for block in blocks_list:
        block.ref_cnt -= 1                       # 参照カウント減少
    self.free_block_queue.append_n(
        [block for block in blocks_list
         if block.ref_cnt == 0 and not block.is_null]  # 0 到達 &amp; 非 null のみ
    )
</code></pre>
<p><strong>逆順解放の理由</strong>: 呼び出し元（<code>SingleTypeKVCacheManager.free()</code>）がブロックを逆順にして渡す。チェーン末尾（最新トークン）がキューの先頭側に来るため、次回の <code>get_new_blocks()</code> で最初に evict される。先頭ブロック（プレフィックス）は末尾側に来るため、プレフィックスキャッシュとして長く生き残る。</p>
<h3 id="touch"><a class="header" href="#touch">touch()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:372</code></p>
<p>プレフィックスキャッシュヒット時に呼ばれ、ブロックの再利用を記録する。</p>
<pre><code>touch(blocks)
  │
  ├─ ref_cnt == 0 かつ非 null の場合:
  │  └─ free_block_queue.remove(block)  ← 空きキューから除去
  │
  ├─ ref_cnt += 1
  │
  └─ metrics_collector.on_block_accessed(block)
</code></pre>
<h2 id="eviction-メカニズム-deep-verified"><a class="header" href="#eviction-メカニズム-deep-verified">Eviction メカニズム [DEEP] [VERIFIED]</a></h2>
<h3 id="_maybe_evict_cached_block"><a class="header" href="#_maybe_evict_cached_block">_maybe_evict_cached_block()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:332</code></p>
<p>新規ブロック割り当て時に、そのブロックがプレフィックスキャッシュに登録されている場合にキャッシュから除去する。</p>
<pre><code>_maybe_evict_cached_block(block)
  │
  ├─ metrics_collector.on_block_evicted(block)  ← 先にメトリクス記録
  │
  ├─ block.block_hash is None → return False（キャッシュ未登録）
  │
  ├─ cached_block_hash_to_block.pop(hash, block_id)
  │  └─ None → return False（マップに不在）
  │
  ├─ block.reset_hash()  ← ハッシュをクリア
  │
  ├─ enable_kv_cache_events の場合:
  │  └─ kv_event_queue.append(BlockRemoved(...))
  │
  └─ return True
</code></pre>
<p><strong>タイミング</strong>: <code>get_new_blocks()</code> 内で <code>ref_cnt</code> をインクリメントする<strong>前</strong>に呼ばれる。</p>
<h3 id="evict_blocks"><a class="header" href="#evict_blocks">evict_blocks()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:405</code></p>
<p>外部（KV コネクタ）から特定の block_id 群を明示的に evict する。<code>_maybe_evict_cached_block()</code> を各ブロックに対して呼ぶ。ブロックは空きキューからは除去しない（ハッシュの除去のみ）。</p>
<h3 id="reset_prefix_cache"><a class="header" href="#reset_prefix_cache">reset_prefix_cache()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:424</code></p>
<p>全プレフィックスキャッシュのリセット。RLHF でモデル重み更新後にキャッシュを無効化する用途。</p>
<p><strong>前提条件</strong>: 使用中のブロックが null_block のみ（<code>num_used_blocks == 1</code>）。条件を満たさない場合は <code>False</code> を返して失敗。</p>
<pre><code>reset_prefix_cache()
  ├─ 使用中ブロック数 != 1 → return False
  ├─ cached_block_hash_to_block を新規インスタンスで置換
  ├─ 全ブロックの hash をリセット
  ├─ metrics_collector.reset()
  ├─ kv_event_queue.append(AllBlocksCleared())
  └─ return True
</code></pre>
<h2 id="kv-cache-events-deep-verified"><a class="header" href="#kv-cache-events-deep-verified">KV Cache Events [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:177-178, 480-490</code></p>
<p>KV Transfer（Disaggregated Prefill）連携のためのイベントシステム。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>イベント型</th><th>発行タイミング</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>BlockStored</code></td><td><code>cache_full_blocks()</code></td><td>新規ブロックがキャッシュ登録された</td></tr>
<tr><td><code>BlockRemoved</code></td><td><code>_maybe_evict_cached_block()</code></td><td>ブロックがキャッシュから除去された</td></tr>
<tr><td><code>AllBlocksCleared</code></td><td><code>reset_prefix_cache()</code></td><td>全キャッシュがリセットされた</td></tr>
</tbody>
</table>
</div>
<pre><code class="language-python">def take_events(self) -&gt; list[KVCacheEvent]:
    """アトミックにイベントキューを排出"""
    events = self.kv_event_queue
    self.kv_event_queue = []  # 新規リストで置換（参照スワップ）
    return events
</code></pre>
<h2 id="キャッシュ使用率-verified"><a class="header" href="#キャッシュ使用率-verified">キャッシュ使用率 [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:467</code></p>
<pre><code class="language-python">def get_usage(self) -&gt; float:
    total_gpu_blocks = self.num_gpu_blocks - 1  # null_block を除外
    return 1.0 - (self.get_num_free_blocks() / total_gpu_blocks)
</code></pre>
<p>null_block は常に「使用中」だが、使用率の計算からは除外される。</p>
<h2 id="メトリクス収集-deep-verified"><a class="header" href="#メトリクス収集-deep-verified">メトリクス収集 [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_metrics.py:46</code></p>
<p><code>KVCacheMetricsCollector</code> はサンプリングベースのブロック滞留メトリクスを収集する。</p>
<h3 id="blockmetricsstate"><a class="header" href="#blockmetricsstate">BlockMetricsState</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_metrics.py:16</code></p>
<p>個別ブロックのライフサイクル指標:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>birth_time_ns</code></td><td>割り当て時刻（<code>time.monotonic_ns()</code>）</td></tr>
<tr><td><code>last_access_ns</code></td><td>最終アクセス時刻</td></tr>
<tr><td><code>access_history</code></td><td>アクセス履歴（最大4件、<code>deque(maxlen=4)</code>）</td></tr>
</tbody>
</table>
</div>
<h3 id="サンプリング"><a class="header" href="#サンプリング">サンプリング</a></h3>
<pre><code class="language-python">sample_rate: float = 0.01  # デフォルト1%
def should_sample_block(self) -&gt; bool:
    return random.random() &lt; self.sample_rate
</code></pre>
<p>全ブロックを追跡するとオーバーヘッドが大きいため、割り当て時に確率的にサンプリングする。サンプリングされたブロックのみ <code>BlockMetricsState</code> が生成される。</p>
<h3 id="イベントフック"><a class="header" href="#イベントフック">イベントフック</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フック</th><th>タイミング</th><th>処理</th></tr>
</thead>
<tbody>
<tr><td><code>on_block_allocated(block)</code></td><td><code>get_new_blocks()</code></td><td>サンプル判定、<code>BlockMetricsState</code> 生成</td></tr>
<tr><td><code>on_block_accessed(block)</code></td><td><code>touch()</code></td><td><code>record_access()</code> 呼び出し</td></tr>
<tr><td><code>on_block_evicted(block)</code></td><td><code>_maybe_evict_cached_block()</code></td><td><code>KVCacheEvictionEvent</code> を生成・蓄積</td></tr>
</tbody>
</table>
</div>
<p>Eviction 時に生成される <code>KVCacheEvictionEvent</code> には <code>lifetime_seconds</code>、<code>idle_seconds</code>、<code>reuse_gaps_seconds</code> が含まれ、<code>drain_events()</code> で一括取得できる。</p>
<h2 id="blockpool-初期化-verified"><a class="header" href="#blockpool-初期化-verified">BlockPool 初期化 [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:147</code></p>
<pre><code class="language-python">def __init__(self, num_gpu_blocks, enable_caching, hash_block_size,
             enable_kv_cache_events=False, metrics_collector=None):
    self.blocks = [KVCacheBlock(idx) for idx in range(num_gpu_blocks)]
    self.free_block_queue = FreeKVCacheBlockQueue(self.blocks)
    self.cached_block_hash_to_block = BlockHashToBlockMap()
    self.null_block = self.free_block_queue.popleft()  # block_id=0
    self.null_block.is_null = True
</code></pre>
<ul>
<li><code>hash_block_size</code>: ハッシュ計算に使うブロックサイズ。通常は実際のブロックサイズと一致するが、Hybrid モデル（異なるブロックサイズの KV キャッシュグループ）では異なる場合がある</li>
<li><code>enable_kv_cache_events</code>: KV Transfer 連携用のイベント発行を有効化</li>
<li><code>metrics_collector</code>: サンプリングベースの滞留メトリクス収集（オプション）</li>
</ul>
<h2 id="関連ドキュメント-6"><a class="header" href="#関連ドキュメント-6">関連ドキュメント</a></h2>
<ul>
<li><a href="#kvcachemanager-サマリー">KVCacheManager サマリー</a></li>
<li><a href="#プレフィックスキャッシュ詳細">プレフィックスキャッシュ詳細</a></li>
<li><a href="#アテンションタイプ別-manager-詳細">アテンションタイプ別 Manager</a></li>
<li><a href="#scheduler-サマリー">Scheduler</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="プレフィックスキャッシュ詳細"><a class="header" href="#プレフィックスキャッシュ詳細">プレフィックスキャッシュ詳細</a></h1>
<blockquote>
<p><strong>深度</strong>: [DEEP]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-13"><a class="header" href="#概要-13">概要</a></h2>
<p>プレフィックスキャッシュは、異なるリクエスト間で共通するプロンプトプレフィックスの KV キャッシュブロックを再利用する機構である。トークン列をブロック単位でハッシュ化し、ハッシュチェーン（各ブロックのハッシュが前のブロックのハッシュに依存）を構築することで、プレフィックスの最長一致を効率的に検索する。</p>
<h2 id="ハッシュチェーン計算-deep-verified"><a class="header" href="#ハッシュチェーン計算-deep-verified">ハッシュチェーン計算 [DEEP] [VERIFIED]</a></h2>
<h3 id="hash_block_tokens"><a class="header" href="#hash_block_tokens">hash_block_tokens()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:525</code></p>
<p>各ブロックのハッシュは 3 要素のタプルから計算される:</p>
<pre><code class="language-python">def hash_block_tokens(hash_function, parent_block_hash, curr_block_token_ids,
                      extra_keys=None) -&gt; BlockHash:
    if not parent_block_hash:
        parent_block_hash = NONE_HASH      # 先頭ブロック用のシード
    curr_block_token_ids_tuple = tuple(curr_block_token_ids)
    return BlockHash(
        hash_function((parent_block_hash, curr_block_token_ids_tuple, extra_keys))
    )
</code></pre>
<p><strong>ハッシュ入力の 3 要素</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>要素</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>parent_block_hash</code></td><td>前ブロックのハッシュ（先頭ブロックは <code>NONE_HASH</code>）</td></tr>
<tr><td><code>curr_block_token_ids_tuple</code></td><td>現ブロックのトークン ID 列（tuple 化）</td></tr>
<tr><td><code>extra_keys</code></td><td>LoRA、マルチモーダル、cache_salt、prompt_embeds（後述）</td></tr>
</tbody>
</table>
</div>
<h3 id="チェーン依存性"><a class="header" href="#チェーン依存性">チェーン依存性</a></h3>
<pre><code>Block 0: hash(NONE_HASH, tokens[0:B], extra)   → H0
Block 1: hash(H0,        tokens[B:2B], extra)  → H1
Block 2: hash(H1,        tokens[2B:3B], extra) → H2
  ...
</code></pre>
<p><strong>なぜチェーンか</strong>: 各ハッシュが全ての先行ブロックに依存するため、プレフィックスが異なれば後続のハッシュも必ず異なる。これにより左から右へのスキャンで「最初のミスで停止」すれば最長プレフィックス一致が得られる。</p>
<h3 id="none_hash"><a class="header" href="#none_hash">NONE_HASH</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:77</code></p>
<p>チェーンの起点となるシード値:</p>
<pre><code class="language-python">def init_none_hash(hash_fn):
    hash_seed = os.getenv("PYTHONHASHSEED")
    if hash_seed is None:
        NONE_HASH = BlockHash(os.urandom(32))    # ランダム 32 バイト
    else:
        NONE_HASH = BlockHash(hash_fn(hash_seed)) # 決定論的
</code></pre>
<ul>
<li><strong>PYTHONHASHSEED 未設定</strong>: ランダムシード → プロセス間でハッシュが一致しない</li>
<li><strong>PYTHONHASHSEED 設定済み</strong>: 決定論的 → プロセス間でハッシュを共有可能（KV Transfer で必要）</li>
<li>CBOR ベースのハッシュ関数で PYTHONHASHSEED 未設定の場合は警告が出る</li>
</ul>
<h2 id="blockhash-型-deep-verified"><a class="header" href="#blockhash-型-deep-verified">BlockHash 型 [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:34</code></p>
<h3 id="型階層"><a class="header" href="#型階層">型階層</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>型</th><th>定義</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>BlockHash</code></td><td><code>NewType("BlockHash", bytes)</code></td><td>ブロック単体のハッシュ値</td></tr>
<tr><td><code>BlockHashWithGroupId</code></td><td><code>NewType("BlockHashWithGroupId", bytes)</code></td><td>ハッシュ + KV キャッシュグループ ID（4 バイト BE）</td></tr>
<tr><td><code>ExternalBlockHash</code></td><td><code>bytes | int</code></td><td>外部向けハッシュ（後方互換性のための Union）</td></tr>
</tbody>
</table>
</div>
<h3 id="blockhashwithgroupid-のパッキング"><a class="header" href="#blockhashwithgroupid-のパッキング">BlockHashWithGroupId のパッキング</a></h3>
<pre><code class="language-python">def make_block_hash_with_group_id(block_hash, group_id):
    return BlockHashWithGroupId(
        block_hash + group_id.to_bytes(4, "big", signed=False)
    )

def get_block_hash(key):     return BlockHash(key[:-4])
def get_group_id(key):       return int.from_bytes(key[-4:], "big")
</code></pre>
<p><strong>設計</strong>: tuple ではなく bytes 結合でパッキングすることで、Python オブジェクト生成を回避し、GC 負荷を低減。</p>
<h2 id="ハッシュ関数-deep-verified"><a class="header" href="#ハッシュ関数-deep-verified">ハッシュ関数 [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/utils/hashing.py</code></p>
<p>4 種類のハッシュ関数が利用可能:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>名前</th><th>シリアライゼーション</th><th>ハッシュ</th><th>出力サイズ</th><th>特徴</th></tr>
</thead>
<tbody>
<tr><td><code>sha256</code></td><td>pickle</td><td>SHA-256</td><td>32 bytes</td><td>Python 依存</td></tr>
<tr><td><code>sha256_cbor</code></td><td>CBOR (canonical)</td><td>SHA-256</td><td>32 bytes</td><td><strong>デフォルト</strong>。言語非依存・再現可能</td></tr>
<tr><td><code>xxhash</code></td><td>pickle</td><td>xxh3_128</td><td>16 bytes</td><td>高速、Python 依存</td></tr>
<tr><td><code>xxhash_cbor</code></td><td>CBOR (canonical)</td><td>xxh3_128</td><td>16 bytes</td><td>高速、言語非依存</td></tr>
</tbody>
</table>
</div>
<p><strong>デフォルト</strong>: <code>sha256_cbor</code> — CBOR の canonical モードにより、PYTHONHASHSEED に依存しないシリアライゼーションが可能。プロセス間でハッシュを共有する KV Transfer に適している。</p>
<p>設定: <code>vllm_config.cache_config.prefix_caching_hash_algo</code></p>
<h2 id="extra-keys-deep-verified"><a class="header" href="#extra-keys-deep-verified">Extra Keys [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:367</code></p>
<p>同一トークン列でも異なる KV キャッシュを持つ場合に、追加情報をハッシュに含める。</p>
<h3 id="必要判定"><a class="header" href="#必要判定">必要判定</a></h3>
<pre><code class="language-python">def need_extra_keys(request):
    return (bool(request.mm_features)           # マルチモーダル
            or (request.lora_request is not None) # LoRA
            or (request.cache_salt is not None))  # キャッシュソルト
</code></pre>
<h3 id="各-extra-key-の生成"><a class="header" href="#各-extra-key-の生成">各 Extra Key の生成</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>生成関数</th><th>行</th><th>内容</th><th>適用範囲</th></tr>
</thead>
<tbody>
<tr><td><code>_gen_mm_extra_hash_keys()</code></td><td>L387</td><td>MM 入力の <code>identifier</code></td><td>ブロックと重なる MM 入力のみ</td></tr>
<tr><td><code>_gen_lora_extra_hash_keys()</code></td><td>L451</td><td>LoRA アダプタの <code>lora_name</code></td><td>全ブロック共通</td></tr>
<tr><td><code>cache_salt</code></td><td>L508-509</td><td>ユーザー指定のキャッシュソルト</td><td><strong>先頭ブロックのみ</strong> (<code>start_token_idx == 0</code>)</td></tr>
<tr><td><code>_gen_prompt_embeds_extra_hash_keys()</code></td><td>L466</td><td>プロンプト埋め込みの生テンソルバイト</td><td>ブロック範囲分のスライス</td></tr>
</tbody>
</table>
</div>
<h3 id="結合順序"><a class="header" href="#結合順序">結合順序</a></h3>
<pre><code class="language-python">extra_keys = lora_extra_keys + mm_extra_keys + cache_salt_keys + prompt_embeds_keys
</code></pre>
<p>空の場合は <code>None</code> を返し、ハッシュ計算の <code>extra_keys</code> 引数として渡される。</p>
<h3 id="マルチモーダル-extra-keys-の詳細"><a class="header" href="#マルチモーダル-extra-keys-の詳細">マルチモーダル Extra Keys の詳細</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:387</code></p>
<p><code>_gen_mm_extra_hash_keys()</code> はブロックの [start, end) トークン範囲と MM 入力の [offset, offset+length) 範囲の重なりを検出する:</p>
<pre><code>MM入力: [offset ─────── offset+length]
Block:         [start ──── end]
               ↑ 重なりあり → identifier を extra_keys に追加
</code></pre>
<ul>
<li><code>mm_features</code> は <code>mm_position.offset</code> でソート済みと仮定</li>
<li><code>start_mm_idx</code> で走査位置を追跡し、毎回先頭から検索しない</li>
<li><code>start_mm_idx = -1</code> は「最後の MM 入力」を示す（生成トークンが増えるデコードフェーズで使用）</li>
</ul>
<h2 id="リクエストブロックハッシャー-deep-verified"><a class="header" href="#リクエストブロックハッシャー-deep-verified">リクエストブロックハッシャー [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:555</code></p>
<h3 id="ファクトリ関数"><a class="header" href="#ファクトリ関数">ファクトリ関数</a></h3>
<pre><code class="language-python">def get_request_block_hasher(block_size, caching_hash_fn):
    def request_block_hasher(request) -&gt; list[BlockHash]:
        start_token_idx = len(request.block_hashes) * block_size
        # full ブロックのみハッシュ（不完全ブロックはスキップ）
        if start_token_idx + block_size &gt; request.num_tokens:
            return []
        # ...ハッシュチェーンを走査して新規 full ブロックのハッシュを計算
    return request_block_hasher
</code></pre>
<h3 id="遅延インクリメンタル計算"><a class="header" href="#遅延インクリメンタル計算">遅延・インクリメンタル計算</a></h3>
<ol>
<li><strong>初期化時</strong>: <code>Request.__init__()</code> で <code>block_hasher</code> が渡された場合、即座に <code>get_hash_new_full_blocks()</code> を呼び、プロンプトの full ブロック分のハッシュを計算</li>
<li><strong>トークン追加時</strong>: <code>Request.append_output_token_ids()</code> で新トークンが追加されるたびに <code>get_hash_new_full_blocks()</code> を呼び、新たに full になったブロックのハッシュをインクリメンタルに追加</li>
</ol>
<pre><code class="language-python"># Request.__init__()
self.block_hashes = self.get_hash_new_full_blocks()  # 初期ハッシュ

# Request.append_output_token_ids()
self.block_hashes.extend(self.get_hash_new_full_blocks())  # 増分追加
</code></pre>
<p><strong>制約</strong>: 不完全ブロック（最後のブロックが block_size 未満）はハッシュされない。これによりプレフィックスキャッシュは常にブロック境界単位で一致する。</p>
<h3 id="チェーンの継続"><a class="header" href="#チェーンの継続">チェーンの継続</a></h3>
<pre><code class="language-python">prev_block_hash_value = request.block_hashes[-1] if request.block_hashes else None
</code></pre>
<p>前回計算済みの最後のハッシュを <code>parent_block_hash</code> として使い、チェーンを継続する。</p>
<h2 id="blockhashlistwithblocksize-deep-verified"><a class="header" href="#blockhashlistwithblocksize-deep-verified">BlockHashListWithBlockSize [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:1571</code></p>
<p>Hybrid モデル（異なるブロックサイズの KV キャッシュグループ）でハッシュ粒度を変換するアダプタ。</p>
<h3 id="動作原理"><a class="header" href="#動作原理">動作原理</a></h3>
<pre><code>hash_block_size = 16, target_block_size = 32 の場合:

元のハッシュ:  [H0, H1, H2, H3]  (各16トークン)
変換後:        [H0+H1, H2+H3]     (各32トークン)
                  ↑ bytes 結合
</code></pre>
<pre><code class="language-python">def _get_value_at(self, idx):
    base = idx * self.scale_factor   # scale_factor = target / hash
    end = base + self.scale_factor
    merged_hash = self.block_hashes[base]
    for i in range(base + 1, end):
        merged_hash += self.block_hashes[i]  # bytes 結合
    return BlockHash(merged_hash)
</code></pre>
<p><strong>遅延評価</strong>: アクセス時にのみ変換を実行。<code>__getitem__</code>、<code>__iter__</code>、<code>__len__</code> をサポートし、通常の <code>list[BlockHash]</code> と同じインターフェースで使える。</p>
<h2 id="lookup-アルゴリズム-deep-verified"><a class="header" href="#lookup-アルゴリズム-deep-verified">Lookup アルゴリズム [DEEP] [VERIFIED]</a></h2>
<p>プレフィックスキャッシュの検索は <code>SingleTypeKVCacheManager</code> のサブクラスごとに異なるアルゴリズムを持つ。</p>
<h3 id="fullattentionmanager-左右スキャン"><a class="header" href="#fullattentionmanager-左右スキャン">FullAttentionManager: 左→右スキャン</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:401</code></p>
<pre><code>for block_hash in block_hashes[:max_num_blocks]:
    if cache hit:
        computed_blocks.append(cached_block)
    else:
        break  ← 最初のミスで停止
</code></pre>
<ul>
<li><strong>Downward-closed 性質</strong>: blocks[0..n] がヒットするなら blocks[0..n-1] も必ずヒットする</li>
<li>EAGLE 使用時は最後のブロックを削除（hidden states が必要なため）</li>
<li><code>alignment_tokens</code> でアライメント調整（Hybrid モデルでの LCM ブロックサイズ）</li>
</ul>
<h3 id="slidingwindowmanager-右左スキャン"><a class="header" href="#slidingwindowmanager-右左スキャン">SlidingWindowManager: 右→左スキャン</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:466</code></p>
<pre><code>sliding_window_contiguous_blocks = ceil((sliding_window - 1) / block_size)

for i in range(max_num_blocks - 1, -1, -1):  # 右→左
    if cache hit:
        computed_blocks[i] = cached_block
        num_contiguous_blocks += 1
        if num_contiguous_blocks &gt;= required:
            break  ← ウィンドウ分の連続ブロック確保
    else:
        num_contiguous_blocks = 0  ← 連続性リセット
</code></pre>
<ul>
<li><strong>連続ブロックが必須</strong>: Sliding Window Attention はウィンドウ内の連続したトークンにのみアテンションするため、不連続なブロックは使えない</li>
<li><code>computed_blocks</code> は初期値として <code>null_block</code> で埋められ、ヒットした位置のみ実ブロックで置換される</li>
<li>アライメントチェック: 右端のブロックがアライメント境界に合わない場合はスキップ</li>
</ul>
<h3 id="chunkedlocalattentionmanager"><a class="header" href="#chunkedlocalattentionmanager">ChunkedLocalAttentionManager</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:594</code></p>
<p>チャンク境界でアテンションが分割されるモデル用。ウィンドウ外のブロックは null_block でパディングし、ウィンドウ内のみ左→右スキャンで検索する。</p>
<h3 id="mambamanager-右左単一一致"><a class="header" href="#mambamanager-右左単一一致">MambaManager: 右→左、単一一致</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:744</code></p>
<p>Mamba（線形アテンション）は最後の状態のみが必要なため、最初のヒットで即座に停止する。</p>
<h3 id="hybridkvcachecoordinator-反復固定点"><a class="header" href="#hybridkvcachecoordinator-反復固定点">HybridKVCacheCoordinator: 反復固定点</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_coordinator.py:448</code></p>
<p>複数のアテンションタイプが混在する Hybrid モデルでは、各グループの最長ヒット長が相互に制約し合う:</p>
<pre><code>hit_length = max_cache_hit_length

while True:
    curr_hit_length = hit_length
    for each attention_group:
        if is_full_attention and cached:
            # Downward-closed: 既存結果を再利用
            curr_hit_length = (curr_hit_length // block_size) * block_size
        else:
            hit = find_longest_cache_hit(curr_hit_length)
            curr_hit_length = len(hit) * block_size

    if curr_hit_length &gt;= hit_length:
        break  ← 収束（もう減らない）
    hit_length = curr_hit_length

    if is_simple_hybrid:  # FullAttn + 1種のみ
        break  ← 1回で十分
</code></pre>
<p><strong>収束保証</strong>: <code>hit_length</code> は単調減少するため、有限回で収束する。
<strong>最適化</strong>: Full Attention は downward-closed なので、他グループの結果に合わせてカットするだけでよい。2グループの simple hybrid ケースでは 1 イテレーションで確定。</p>
<h2 id="キャッシュ登録-deep-verified"><a class="header" href="#キャッシュ登録-deep-verified">キャッシュ登録 [DEEP] [VERIFIED]</a></h2>
<h3 id="cache_full_blocks"><a class="header" href="#cache_full_blocks">cache_full_blocks()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:209</code></p>
<p>計算済みの full ブロックをプレフィックスキャッシュに登録する。</p>
<pre><code>cache_full_blocks(request, blocks, num_cached_blocks, num_full_blocks, ...)
  │
  ├─ num_cached_blocks &gt;= num_full_blocks → return（既に登録済み）
  │
  ├─ new_full_blocks = blocks[num_cached_blocks:num_full_blocks]
  │
  ├─ block_size == hash_block_size の場合:
  │  └─ block_hashes = request.block_hashes（直接使用）
  │
  ├─ block_size != hash_block_size の場合:
  │  └─ block_hashes = BlockHashListWithBlockSize(...)（粒度変換）
  │
  └─ 各 new_full_block について:
     ├─ is_null → skip
     ├─ assert block_hash is None（二重登録防止）
     ├─ block_hash_with_group_id を生成
     ├─ blk.block_hash = block_hash_with_group_id
     ├─ cached_block_hash_to_block.insert(...)
     └─ enable_kv_cache_events → BlockStored イベント発行
</code></pre>
<h3 id="num_cached_block-トラッキング"><a class="header" href="#num_cached_block-トラッキング">num_cached_block トラッキング</a></h3>
<p><code>SingleTypeKVCacheManager</code> が <code>num_cached_block[request_id]</code> で各リクエストの登録済みブロック数を追跡する。<code>cache_blocks()</code> 呼び出し時に <code>num_cached_blocks &gt;= num_full_blocks</code> なら何もせず、新たに full になったブロックのみを登録する。</p>
<h2 id="データフロー全体-verified"><a class="header" href="#データフロー全体-verified">データフロー全体 [VERIFIED]</a></h2>
<pre><code>EngineCore.__init__()
  └─ init_none_hash(hash_fn)        ← グローバル NONE_HASH 初期化
  └─ get_request_block_hasher(...)   ← ハッシャークロージャ生成

Request.__init__(block_hasher=...)
  └─ block_hashes = get_hash_new_full_blocks()  ← プロンプトの full ブロックを即時ハッシュ

Request.append_output_token_ids()
  └─ block_hashes.extend(get_hash_new_full_blocks())  ← 増分ハッシュ

Scheduler.schedule()
  ├─ kv_cache_manager.get_computed_blocks(request)
  │  └─ coordinator.find_longest_cache_hit(request.block_hashes, max_length)
  │     └─ block_pool.get_cached_block(hash, group_ids)
  │        └─ BlockHashToBlockMap.get_one_block(hash_with_group_id)
  │
  └─ kv_cache_manager.allocate_slots(request, ...)
     └─ coordinator.cache_blocks(request, num_tokens_to_cache)
        └─ block_pool.cache_full_blocks(request, blocks, ...)
           └─ BlockHashToBlockMap.insert(hash_with_group_id, block)
</code></pre>
<h2 id="関連ドキュメント-7"><a class="header" href="#関連ドキュメント-7">関連ドキュメント</a></h2>
<ul>
<li><a href="#kvcachemanager-サマリー">KVCacheManager サマリー</a></li>
<li><a href="#blockpool-詳細">BlockPool 詳細</a></li>
<li><a href="#アテンションタイプ別-manager-詳細">アテンションタイプ別 Manager</a></li>
<li><a href="#用語集">用語集</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="マルチモーダル処理パイプライン-サマリー"><a class="header" href="#マルチモーダル処理パイプライン-サマリー">マルチモーダル処理パイプライン サマリー</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-14"><a class="header" href="#概要-14">概要</a></h2>
<p>vLLMのマルチモーダル処理パイプラインは、画像・音声・動画等の非テキストデータをLLMの推論に統合するシステムである。フロントエンド（P0）でのメディア前処理・キャッシュと、バックエンド（P1）でのエンコーダ実行・埋め込みマージの2段構成で動作する。</p>
<h2 id="エンドツーエンド-データフロー"><a class="header" href="#エンドツーエンド-データフロー">エンドツーエンド データフロー</a></h2>
<pre class="mermaid">graph TD
    A["API Request&lt;br&gt;(messages + images)"] --&gt; B["ChatTemplate 適用&lt;br&gt;プレースホルダー挿入"]
    B --&gt; C["InputPreprocessor&lt;br&gt;トークナイズ + HF Processor"]

    C --&gt; D{"ProcessorCache&lt;br&gt;ヒット?"}
    D --&gt;|HIT| E["キャッシュから取得&lt;br&gt;(HF処理スキップ)"]
    D --&gt;|MISS| F["HF Processor 実行&lt;br&gt;pixel_values テンソル生成"]
    E --&gt; G["MultiModalFeatureSpec 構築"]
    F --&gt; G

    G --&gt; H["EngineCoreRequest&lt;br&gt;ZMQ IPC 送信"]

    H --&gt; I["Scheduler"]
    I --&gt; J{"EncoderCacheManager&lt;br&gt;ヒット?"}
    J --&gt;|HIT| K["エンコーダ計算スキップ"]
    J --&gt;|MISS| L["encoder_compute_budget&lt;br&gt;から割り当て"]

    K --&gt; M["GPUModelRunner"]
    L --&gt; M

    M --&gt; N["_execute_mm_encoder()&lt;br&gt;model.embed_multimodal()"]
    N --&gt; O["encoder_cache に格納"]
    O --&gt; P["_gather_mm_embeddings()&lt;br&gt;キャッシュからスライス"]
    P --&gt; Q["embed_input_ids()&lt;br&gt;text + vision マージ&lt;br&gt;(masked_scatter_)"]
    Q --&gt; R["model.forward()&lt;br&gt;統合推論"]
</pre>

<h2 id="主要コンポーネント-6"><a class="header" href="#主要コンポーネント-6">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>場所</th><th>役割</th></tr>
</thead>
<tbody>
<tr><td><code>MULTIMODAL_REGISTRY</code></td><td><code>vllm/multimodal/registry.py</code></td><td>モデルごとのプロセッサ/情報を登録・取得</td></tr>
<tr><td><code>BaseMultiModalProcessor</code></td><td><code>vllm/multimodal/processing/processor.py</code></td><td>HFプロセッサ実行、プロンプト更新管理</td></tr>
<tr><td><code>MultiModalHasher</code></td><td><code>vllm/multimodal/hasher.py</code></td><td>コンテンツベースハッシュ（blake3）</td></tr>
<tr><td><code>ProcessorCache</code> (4種)</td><td><code>vllm/multimodal/cache.py</code></td><td>P0側のHF処理結果キャッシュ</td></tr>
<tr><td><code>EncoderCacheManager</code></td><td><code>vllm/v1/core/encoder_cache_manager.py</code></td><td>P1側のエンコーダ出力の論理管理</td></tr>
<tr><td><code>encoder_cache</code></td><td><code>vllm/v1/worker/gpu_model_runner.py:439</code></td><td>GPU上のエンコーダ出力テンソルキャッシュ</td></tr>
</tbody>
</table>
</div>
<h2 id="キャッシュの3層構造"><a class="header" href="#キャッシュの3層構造">キャッシュの3層構造</a></h2>
<pre><code>P0（フロントエンド）               P1（Scheduler）              P1（GPU）
┌──────────────────┐           ┌─────────────────┐        ┌──────────────┐
│ ProcessorCache   │           │ EncoderCache    │        │ encoder_cache│
│ LRU, サイズベース │           │ Manager         │        │ dict[str,    │
│                  │           │ RefCount + FIFO │        │  Tensor]     │
│ 何をキャッシュ:   │           │                 │        │              │
│ HF処理済みテンソル │           │ 何を管理:       │        │ 何をキャッシュ:│
│ + prompt_updates │           │ 容量・参照カウント│       │ エンコーダ出力│
│                  │           │ Evictionリスト   │        │ (GPUテンソル) │
└──────────────────┘           └─────────────────┘        └──────────────┘
  ヒット時:                      ヒット時:                   ヒット時:
  HF処理スキップ                 エンコーダ計算スキップ        テンソル再利用
  IPC転送量削減                  予算節約                    再計算不要
</code></pre>
<h2 id="テキスト推論との主な差分"><a class="header" href="#テキスト推論との主な差分">テキスト推論との主な差分</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>処理段階</th><th>テキスト推論</th><th>マルチモーダル推論</th></tr>
</thead>
<tbody>
<tr><td>入力前処理</td><td>tokenize のみ</td><td>tokenize + HF Processor + ハッシュ + キャッシュ</td></tr>
<tr><td>プロンプト</td><td>テキストトークンのみ</td><td>テキスト + プレースホルダートークン（<code>&lt;start_of_image&gt;</code> 等）</td></tr>
<tr><td>EngineCoreRequest</td><td><code>mm_features = None</code></td><td><code>mm_features = [MultiModalFeatureSpec, ...]</code></td></tr>
<tr><td>Scheduler</td><td>KVキャッシュ予算のみ</td><td>+ エンコーダ計算予算管理</td></tr>
<tr><td>GPUModelRunner</td><td>input_ids → model.forward()</td><td>encoder実行 → embed_input_ids(masked_scatter_) → inputs_embeds → model.forward()</td></tr>
</tbody>
</table>
</div>
<h2 id="gemma3-固有の特徴"><a class="header" href="#gemma3-固有の特徴">Gemma3 固有の特徴</a></h2>
<ul>
<li><strong>ビジョンエンコーダ</strong>: SiglipVisionModel（SIGLIP ViT、双方向Attention）</li>
<li><strong>プロジェクタ</strong>: AvgPool2d → GemmaRMSNorm → Linear（vision → text空間）</li>
<li><strong>プレースホルダー</strong>: <code>&lt;start_of_image&gt;</code> → <code>image_token × 256</code> に展開</li>
<li><strong>Pan-and-Scan</strong>: アスペクト比が大きい画像を複数クロップ（V1では簡略化されたアテンション）</li>
<li><strong>改行トークン結合</strong>: <code>\n</code> + <code>\n\n</code> → <code>\n\n\n</code> 等の特殊処理</li>
</ul>
<h2 id="詳細ドキュメント-1"><a class="header" href="#詳細ドキュメント-1">詳細ドキュメント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ドキュメント</th><th>内容</th></tr>
</thead>
<tbody>
<tr><td><a href="#フロントエンド-マルチモーダル処理パス-medium-verified">mm-processing.md</a></td><td>フロントエンド: チャットテンプレート、プレースホルダー、MMハッシュ、プロセッサキャッシュ4種、ZMQ送信データ</td></tr>
<tr><td><a href="#バックエンド-マルチモーダル処理パス-medium-verified">mm-engine-gpu.md</a></td><td>バックエンド: EncoderCacheManager、Schedulerエンコーダ予算、GPUModelRunnerエンコーダ実行・キャッシュ・埋め込みマージ</td></tr>
<tr><td><a href="#gemma3-ビジョンエンコーダと画像処理-medium-verified">gemma3-vision.md</a></td><td>Gemma3: SiglipVisionModel、MultiModalProjector、Pan-and-Scan、masked_scatter_マージ</td></tr>
</tbody>
</table>
</div>
<h2 id="上流下流"><a class="header" href="#上流下流">上流・下流</a></h2>
<ul>
<li><strong>上流</strong>: <a href="#エントリポイント-asyncllm--llm-サマリー">エントリポイント</a> → <a href="#inputprocessor-サマリー">InputProcessor</a></li>
<li><strong>下流</strong>: <a href="#scheduler-サマリー">Scheduler</a> → <a href="#gpumodelrunner">GPUModelRunner</a> → モデル層</li>
</ul>
<h2 id="主要ファイル-2"><a class="header" href="#主要ファイル-2">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>概要</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/multimodal/</code></td><td>マルチモーダル処理の基盤（レジストリ、ハッシュ、キャッシュ、パース）</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/input_processor.py</code></td><td>フロントエンドでのMM処理統合</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/encoder_cache_manager.py</code></td><td>バックエンドのエンコーダキャッシュ管理</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_model_runner.py</code></td><td>GPU上でのエンコーダ実行と埋め込みマージ</td></tr>
<tr><td><code>target/vllm/vllm/model_executor/models/gemma3_mm.py</code></td><td>Gemma3のマルチモーダル実装</td></tr>
<tr><td><code>target/vllm/vllm/model_executor/models/siglip.py</code></td><td>SiglipVisionModel（ビジョンエンコーダ）</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="gemma3-ビジョンエンコーダと画像処理-medium-verified"><a class="header" href="#gemma3-ビジョンエンコーダと画像処理-medium-verified">Gemma3 ビジョンエンコーダと画像処理 [MEDIUM] [VERIFIED]</a></h1>
<blockquote>
<p><strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<p>Gemma3 モデルにおけるビジョンエンコーダ（SiglipVisionModel）、プロジェクタ（Gemma3MultiModalProjector）、および text + vision 埋め込みマージの詳細。</p>
<h2 id="モデルアーキテクチャ全体像"><a class="header" href="#モデルアーキテクチャ全体像">モデルアーキテクチャ全体像</a></h2>
<pre class="mermaid">graph TD
    subgraph "Gemma3ForConditionalGeneration"
        subgraph "vision_tower (SiglipVisionModel)"
            PE["SiglipVisionEmbeddings&lt;br&gt;Conv2d patch embedding&lt;br&gt;+ position embedding"]
            ENC["SiglipEncoder&lt;br&gt;N層 Transformer&lt;br&gt;(双方向Attention)"]
            PE --&gt; ENC
        end

        PROJ["Gemma3MultiModalProjector&lt;br&gt;AvgPool2d → RMSNorm → Linear"]
        ENC --&gt; PROJ

        subgraph "language_model (Gemma3ForCausalLM)"
            EMB["embed_tokens()&lt;br&gt;テキスト埋め込み"]
            LM["Transformer Decoder&lt;br&gt;(因果的Attention)"]
            EMB --&gt; LM
        end

        MERGE["masked_scatter_&lt;br&gt;text + vision マージ"]
        PROJ --&gt; MERGE
        EMB --&gt; MERGE
        MERGE --&gt; LM
    end
</pre>

<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:481-694</code></p>
<h2 id="1-siglipvisionmodelビジョンエンコーダ"><a class="header" href="#1-siglipvisionmodelビジョンエンコーダ">1. SiglipVisionModel（ビジョンエンコーダ）</a></h2>
<h3 id="構造"><a class="header" href="#構造">構造</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/siglip.py:848-894</code></p>
<pre><code>SiglipVisionModel
  └─ SiglipVisionTransformer (L681)
      ├─ SiglipVisionEmbeddings (L282)
      │   ├─ patch_embedding: Conv2d(3, hidden_size, kernel=patch_size, stride=patch_size)
      │   └─ position_embedding: Embedding(num_patches, hidden_size)
      ├─ SiglipEncoder (L520)
      │   └─ layers: ModuleList[SiglipEncoderLayer] × N
      │       ├─ layer_norm1 → SiglipAttention (MMEncoderAttention)
      │       └─ layer_norm2 → SiglipMLP
      └─ post_layernorm: LayerNorm（最終層のみ）
</code></pre>
<h3 id="パッチ埋め込み-siglipvisionembeddings"><a class="header" href="#パッチ埋め込み-siglipvisionembeddings">パッチ埋め込み (SiglipVisionEmbeddings)</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/siglip.py:282-352</code></p>
<pre><code class="language-python"># 入力: pixel_values (batch, 3, H, W)
patch_embeds = self.patch_embedding(pixel_values)  # Conv2d: (batch, hidden_size, grid, grid)
embeddings = patch_embeds.flatten(2).transpose(1, 2)  # (batch, num_patches, hidden_size)
embeddings += self.position_embedding(position_ids)    # 位置埋め込みを加算
</code></pre>
<ul>
<li><code>image_size = 256</code> の場合、<code>patch_size = 16</code> → <code>num_patches = (256/16)² = 256</code></li>
<li>各パッチは <code>16×16×3 = 768</code> ピクセルから <code>hidden_size</code> 次元のベクトルに変換</li>
<li>位置埋め込みは学習済みの <code>nn.Embedding</code>（補間対応あり）</li>
</ul>
<h3 id="エンコーダ層-siglipencoder"><a class="header" href="#エンコーダ層-siglipencoder">エンコーダ層 (SiglipEncoder)</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/siglip.py:520-567</code></p>
<p>各エンコーダ層の構造:</p>
<pre><code>SiglipEncoderLayer:
    residual = hidden_states
    hidden_states = layer_norm1(hidden_states)
    hidden_states = self_attn(hidden_states)     ← MMEncoderAttention（双方向）
    hidden_states = residual + hidden_states
    residual = hidden_states
    hidden_states = layer_norm2(hidden_states)
    hidden_states = mlp(hidden_states)
    hidden_states = residual + hidden_states
</code></pre>
<ul>
<li><strong>アテンション型</strong>: <code>MMEncoderAttention</code>（双方向、因果マスクなし）</li>
<li>全パッチが全パッチを参照できる（テキストの因果アテンションとは異なる）</li>
</ul>
<h3 id="forward-フロー"><a class="header" href="#forward-フロー">forward() フロー</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/siglip.py:755-788</code></p>
<pre><code class="language-python">def forward(self, pixel_values, *, select_layers=None, feature_select_strategy=None):
    hidden_states = self.embeddings(pixel_values)      # (batch, num_patches, hidden_size)
    encoder_outputs = self.encoder(inputs_embeds=hidden_states)  # N層 Transformer
    encoder_outputs = resolve_visual_encoder_outputs(   # 特徴選択
        encoder_outputs, select_layers, feature_select_strategy
    )
    return self.last_hs_proc(encoder_outputs)           # post_layernorm + head
</code></pre>
<p>出力: <code>(batch, num_patches, hidden_size)</code> — 典型的に <code>(batch, 256, 1152)</code></p>
<h2 id="2-gemma3multimodalprojector投射層"><a class="header" href="#2-gemma3multimodalprojector投射層">2. Gemma3MultiModalProjector（投射層）</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:432-473</code></p>
<p>ビジョンエンコーダの出力をテキスト埋め込み空間に変換する。</p>
<h3 id="パラメータ"><a class="header" href="#パラメータ">パラメータ</a></h3>
<pre><code class="language-python">mm_input_projection_weight: (vision_hidden_size, text_hidden_size)  # 例: (1152, 2048)
mm_soft_emb_norm: GemmaRMSNorm(vision_hidden_size)
avg_pool: AvgPool2d(kernel_size, stride=kernel_size)
</code></pre>
<h3 id="設定値の計算"><a class="header" href="#設定値の計算">設定値の計算</a></h3>
<pre><code class="language-python">patches_per_image = image_size // patch_size           # 256 // 16 = 16
tokens_per_side = int(mm_tokens_per_image ** 0.5)      # int(256 ** 0.5) = 16
kernel_size = patches_per_image // tokens_per_side     # 16 // 16 = 1
</code></pre>
<p><code>kernel_size = 1</code> の場合、AvgPool2dは実質的にno-op（パッチ数が変わらない）。<code>mm_tokens_per_image</code> が小さい設定ではダウンサンプリングが発生する。</p>
<h3 id="forward-フロー-1"><a class="header" href="#forward-フロー-1">forward() フロー</a></h3>
<pre><code class="language-python">def forward(self, vision_outputs):
    # vision_outputs: (batch, hidden_size, num_patches) — エンコーダ出力のtranspose形式
    batch_size, _, seq_length = vision_outputs.shape

    # 1. 2Dグリッドへリシェイプ
    reshaped = vision_outputs.transpose(1, 2).reshape(
        batch_size, seq_length, patches_per_image, patches_per_image
    )  # (batch, seq_length, 16, 16)

    # 2. 平均プーリング（ダウンサンプリング）
    pooled = self.avg_pool(reshaped)  # kernel=1の場合: (batch, seq_length, 16, 16)
    pooled = pooled.flatten(2).transpose(1, 2)  # (batch, mm_tokens_per_image, seq_length)

    # 3. RMS正規化
    normed = self.mm_soft_emb_norm(pooled)

    # 4. 線形投射: vision_hidden_size → text_hidden_size
    projected = torch.matmul(normed, self.mm_input_projection_weight)
    # (batch, mm_tokens_per_image, text_hidden_size)

    return projected.type_as(vision_outputs)
</code></pre>
<p><strong>重要な注意</strong>: テキスト埋め込みの正規化とは異なり、ビジョン埋め込みには <code>mm_soft_emb_norm</code> のみが適用される。vocab embedding に適用されるスケーリング（<code>embed_tokens * normalizer</code>）はビジョン埋め込みには適用 <strong>されない</strong>。</p>
<h2 id="3-embed_multimodal-と-_process_image_input"><a class="header" href="#3-embed_multimodal-と-_process_image_input">3. embed_multimodal() と _process_image_input()</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:567-594</code></p>
<p>GPUModelRunner の <code>_execute_mm_encoder()</code> から呼ばれるメインのエンコーダ実行パス:</p>
<pre><code class="language-python">def embed_multimodal(self, **kwargs):
    image_input = self._parse_and_validate_image_input(**kwargs)  # pixel_values 取得
    if image_input is None:
        return []
    return self._process_image_input(image_input)

def _process_image_input(self, image_input):
    pixel_values = image_input["pixel_values"]    # (total_patches, 3, image_size, image_size)
    num_patches = image_input["num_patches"]       # (num_images,) — 画像ごとのパッチ数

    # ビジョンエンコーダ実行
    image_features = self._image_pixels_to_features(self.vision_tower, pixel_values)

    # プロジェクタで投射
    image_embeds = self.multi_modal_projector(image_features)

    # 画像ごとに分割 + flatten
    return [e.flatten(0, 1) for e in image_embeds.split(num_patches.tolist())]
    # list[Tensor(mm_tokens_per_image, text_hidden_size)]
</code></pre>
<h3 id="テンソル形状の追跡"><a class="header" href="#テンソル形状の追跡">テンソル形状の追跡</a></h3>
<p>画像1枚、Pan-and-Scan なし（<code>num_patches=1</code>）の場合:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ステージ</th><th>形状</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td>入力 pixel_values</td><td><code>(1, 3, 256, 256)</code></td><td>RGB画像</td></tr>
<tr><td>patch_embedding</td><td><code>(1, 1152, 16, 16)</code></td><td>Conv2d出力</td></tr>
<tr><td>flatten + transpose</td><td><code>(1, 256, 1152)</code></td><td>パッチシーケンス</td></tr>
<tr><td>+ position_embedding</td><td><code>(1, 256, 1152)</code></td><td>位置情報付加</td></tr>
<tr><td>SiglipEncoder (N層)</td><td><code>(1, 256, 1152)</code></td><td>Transformer処理</td></tr>
<tr><td>Projector reshape</td><td><code>(1, 256, 16, 16)</code></td><td>2Dグリッド</td></tr>
<tr><td>AvgPool2d (k=1)</td><td><code>(1, 256, 16, 16)</code></td><td>no-op</td></tr>
<tr><td>flatten + transpose</td><td><code>(1, 256, 256)</code></td><td>?</td></tr>
<tr><td>mm_soft_emb_norm</td><td><code>(1, 256, 1152)</code></td><td>RMS正規化</td></tr>
<tr><td>matmul(projection)</td><td><code>(1, 256, 2048)</code></td><td>テキスト空間</td></tr>
<tr><td>flatten(0,1)</td><td><code>(256, 2048)</code></td><td>最終出力</td></tr>
</tbody>
</table>
</div>
<h2 id="4-テキスト--ビジョン埋め込みのマージ"><a class="header" href="#4-テキスト--ビジョン埋め込みのマージ">4. テキスト + ビジョン埋め込みのマージ</a></h2>
<h3 id="embed_input_ids"><a class="header" href="#embed_input_ids">embed_input_ids()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:596-614</code></p>
<pre><code class="language-python">def embed_input_ids(self, input_ids, multimodal_embeddings=None, *, is_multimodal=None, ...):
    if multimodal_embeddings is None or is_multimodal is None:
        return super().embed_input_ids(input_ids)  # テキストのみ
    return super().embed_input_ids(
        input_ids, multimodal_embeddings=multimodal_embeddings,
        is_multimodal=is_multimodal, handle_oov_mm_token=True,
    )
</code></pre>
<h3 id="_merge_multimodal_embeddings"><a class="header" href="#_merge_multimodal_embeddings">_merge_multimodal_embeddings()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/utils.py:445-487</code></p>
<pre><code class="language-python">def _merge_multimodal_embeddings(inputs_embeds, multimodal_embeddings, is_multimodal):
    mm_embeds_flat = _flatten_embeddings(multimodal_embeddings)
    # in-place 置換: is_multimodal=True の位置を mm_embeds_flat で上書き
    inputs_embeds.masked_scatter_(
        is_multimodal.unsqueeze(-1),
        mm_embeds_flat.to(dtype=inputs_embeds.dtype)
    )
    return inputs_embeds
</code></pre>
<p><strong><code>masked_scatter_</code> の動作</strong>:</p>
<ol>
<li><code>is_multimodal</code>: <code>(seq_len,)</code> のboolテンソル（True = 画像プレースホルダー位置）</li>
<li><code>is_multimodal.unsqueeze(-1)</code>: <code>(seq_len, 1)</code> → ブロードキャストで <code>(seq_len, hidden_size)</code> に展開</li>
<li><code>mm_embeds_flat</code>: True位置の数 × hidden_size の連続テンソル</li>
<li>True位置に順番に mm_embeds_flat の値を書き込む</li>
</ol>
<p><strong>制約</strong>: <code>is_multimodal.sum() == len(mm_embeds_flat)</code> でなければランタイムエラー</p>
<h3 id="handle_oov_mm_token"><a class="header" href="#handle_oov_mm_token">handle_oov_mm_token</a></h3>
<p>Gemma3 は <code>handle_oov_mm_token=True</code> を指定。これは image_token_id が vocab の範囲外の場合でも安全に処理するための仕組み。</p>
<h2 id="5-pan-and-scanパノラマクロップ"><a class="header" href="#5-pan-and-scanパノラマクロップ">5. Pan-and-Scan（パノラマクロップ）</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:109-176</code></p>
<p>アスペクト比が大きい画像に対して、複数のクロップを生成して詳細認識を向上させる仕組み。</p>
<h3 id="クロップ数の計算"><a class="header" href="#クロップ数の計算">クロップ数の計算</a></h3>
<pre><code class="language-python">def get_num_crops(self, *, image_width, image_height, processor):
    # 横長画像の場合
    if image_width &gt;= image_height:
        if width/height &lt; min_ratio:  return 0  # 比率が小さすぎる
        num_crops_w = min(floor(width/min_crop_size), floor(w/h + 0.5))
        num_crops_w = max(2, num_crops_w)
        num_crops_w = min(max_num_crops, num_crops_w)
        num_crops_h = 1

    # 縦長画像の場合は逆
    ...

    # クロップサイズが小さすぎる場合は無効
    if min(crop_size_w, crop_size_h) &lt; min_crop_size:
        return 0

    return num_crops_w * num_crops_h
</code></pre>
<h3 id="pan-and-scan時のプロンプト"><a class="header" href="#pan-and-scan時のプロンプト">Pan-and-Scan時のプロンプト</a></h3>
<pre><code>"Here is the original image &lt;full_image_seq&gt; and here are some crops to help you see better &lt;full_image_seq&gt; &lt;full_image_seq&gt;"
</code></pre>
<p>各 <code>&lt;full_image_seq&gt;</code> は <code>image_seq_length</code>（=256）トークンを消費。</p>
<p><strong>V1での制限</strong>: Pan-and-Scan は簡略化されたアテンションパターンを使用するため、最適ではない結果になる可能性がある。</p>
<h2 id="6-forward--最終推論"><a class="header" href="#6-forward--最終推論">6. forward() — 最終推論</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:616-635</code></p>
<pre><code class="language-python">def forward(self, input_ids, positions, intermediate_tensors=None, inputs_embeds=None, **kwargs):
    if intermediate_tensors is not None:
        inputs_embeds = None  # Pipeline Parallelism の中間ランク

    hidden_states = self.language_model.model(
        input_ids,
        positions,
        intermediate_tensors,
        inputs_embeds=inputs_embeds,  # text + vision マージ済み
        **kwargs,
    )
    return hidden_states
</code></pre>
<p>マルチモーダル時は <code>inputs_embeds</code> が渡され、<code>input_ids</code> は使用されない（embed_input_ids で既に埋め込み済みのため）。</p>
<h2 id="データフロー全体"><a class="header" href="#データフロー全体">データフロー全体</a></h2>
<pre><code>pixel_values: (total_patches, 3, 256, 256)
      │
      ▼
SiglipVisionEmbeddings
  Conv2d(3→1152, k=16, s=16) + position_embedding
      │
      ▼
(total_patches, 256, 1152)
      │
      ▼
SiglipEncoder (N層 Transformer, 双方向Attention)
      │
      ▼
(total_patches, 256, 1152)
      │
      ▼
Gemma3MultiModalProjector
  reshape → AvgPool2d → RMSNorm → matmul(1152→2048)
      │
      ▼
(total_patches, 256, 2048)
      │
      ▼
split by num_patches → list[(mm_tokens, 2048)]
      │
      ▼
encoder_cache[mm_hash] に格納
      │
      ▼
_gather_mm_embeddings() でスライス
      │
      ▼
embed_input_ids():
  text_embeds = embed_tokens(input_ids)     # (seq_len, 2048)
  merged = masked_scatter_(text_embeds, is_multimodal, mm_embeds)
      │
      ▼
language_model.model(inputs_embeds=merged)  # Gemma3 Decoder
</code></pre>
<h2 id="主要ファイル-3"><a class="header" href="#主要ファイル-3">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th><th>行</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/model_executor/models/gemma3_mm.py</code></td><td><code>Gemma3ForConditionalGeneration</code></td><td>L481</td></tr>
<tr><td><code>target/vllm/vllm/model_executor/models/gemma3_mm.py</code></td><td><code>Gemma3MultiModalProjector</code></td><td>L432</td></tr>
<tr><td><code>target/vllm/vllm/model_executor/models/gemma3_mm.py</code></td><td><code>Gemma3ProcessingInfo</code>, <code>Gemma3MultiModalProcessor</code></td><td>L77, L276</td></tr>
<tr><td><code>target/vllm/vllm/model_executor/models/siglip.py</code></td><td><code>SiglipVisionModel</code></td><td>L848</td></tr>
<tr><td><code>target/vllm/vllm/model_executor/models/siglip.py</code></td><td><code>SiglipVisionEmbeddings</code></td><td>L282</td></tr>
<tr><td><code>target/vllm/vllm/model_executor/models/siglip.py</code></td><td><code>SiglipEncoder</code></td><td>L520</td></tr>
<tr><td><code>target/vllm/vllm/model_executor/models/utils.py</code></td><td><code>_merge_multimodal_embeddings()</code></td><td>L445</td></tr>
</tbody>
</table>
</div>
<h2 id="関連ドキュメント-8"><a class="header" href="#関連ドキュメント-8">関連ドキュメント</a></h2>
<ul>
<li><a href="#フロントエンド-マルチモーダル処理パス-medium-verified">フロントエンド MM処理パス</a></li>
<li><a href="#バックエンド-マルチモーダル処理パス-medium-verified">バックエンド MM処理パス</a></li>
<li><a href="#マルチモーダル処理パイプライン-サマリー">マルチモーダル処理パイプライン概要</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="バックエンド-マルチモーダル処理パス-medium-verified"><a class="header" href="#バックエンド-マルチモーダル処理パス-medium-verified">バックエンド マルチモーダル処理パス [MEDIUM] [VERIFIED]</a></h1>
<blockquote>
<p><strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<p>EngineCore（P1）でマルチモーダルリクエストがどのように処理されるかを追跡する。EncoderCacheManager、Schedulerのエンコーダ予算管理、GPUModelRunnerのエンコーダ実行・キャッシュ・埋め込みマージを含む。</p>
<h2 id="全体フロー"><a class="header" href="#全体フロー">全体フロー</a></h2>
<pre class="mermaid">sequenceDiagram
    participant S as Scheduler
    participant ECM as EncoderCacheManager
    participant MR as GPUModelRunner
    participant M as Model

    Note over S: schedule() 実行中
    S-&gt;&gt;ECM: check_and_update_cache(req, i)
    alt キャッシュヒット
        ECM--&gt;&gt;S: True（エンコード不要）
    else キャッシュミス
        ECM--&gt;&gt;S: False
        S-&gt;&gt;ECM: can_allocate(req, i, budget, scheduled)
        alt 空き/回収可能
            ECM--&gt;&gt;S: True（Eviction実行の可能性あり）
            S-&gt;&gt;ECM: allocate(req, i)
            Note over S: scheduled_encoder_inputs[req_id].append(i)
        else 不足
            ECM--&gt;&gt;S: False
            Note over S: num_new_tokens を調整
        end
    end

    Note over S: SchedulerOutput 構築
    S-&gt;&gt;MR: execute_model(scheduler_output)

    MR-&gt;&gt;MR: _batch_mm_inputs_from_scheduler()
    MR-&gt;&gt;M: embed_multimodal(pixel_values)
    M--&gt;&gt;MR: encoder_outputs
    MR-&gt;&gt;MR: encoder_cache[mm_hash] = output

    MR-&gt;&gt;MR: _gather_mm_embeddings()
    Note over MR: encoder_cache からスライス
    MR-&gt;&gt;M: embed_input_ids(ids, mm_embeds, is_multimodal)
    Note over M: masked_scatter_ で text + vision マージ

    MR-&gt;&gt;MR: free_encoder_mm_hashes の処理
    Note over MR: encoder_cache.pop(mm_hash)
</pre>

<h2 id="1-encodercachemanager"><a class="header" href="#1-encodercachemanager">1. EncoderCacheManager</a></h2>
<h3 id="概要-15"><a class="header" href="#概要-15">概要</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:17-267</code></p>
<p>ビジョンエンコーダの出力（埋め込みテンソル）のライフサイクルを管理する。リファレンスカウント方式でリクエスト間のキャッシュ共有を実現し、遅延Evictionでメモリ効率を高める。</p>
<h3 id="データ構造-1"><a class="header" href="#データ構造-1">データ構造</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>cache_size</code></td><td><code>int</code></td><td>キャッシュ容量（エンコーダ埋め込み数単位）</td></tr>
<tr><td><code>num_free_slots</code></td><td><code>int</code></td><td>現在の空き容量</td></tr>
<tr><td><code>num_freeable_slots</code></td><td><code>int</code></td><td>回収可能な容量（参照なしエントリ含む）</td></tr>
<tr><td><code>cached</code></td><td><code>dict[str, set[str]]</code></td><td>mm_hash → 参照中のrequest_id集合</td></tr>
<tr><td><code>freeable</code></td><td><code>OrderedDict[str, int]</code></td><td>mm_hash → 埋め込み数（参照なし、回収可能）</td></tr>
<tr><td><code>freed</code></td><td><code>list[str]</code></td><td>実際にEvictされたmm_hashのリスト</td></tr>
</tbody>
</table>
</div>
<h3 id="主要操作"><a class="header" href="#主要操作">主要操作</a></h3>
<h4 id="check_and_update_cacherequest-input_id--bool"><a class="header" href="#check_and_update_cacherequest-input_id--bool">check_and_update_cache(request, input_id) → bool</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:91-117</code></p>
<pre><code>1. mm_hash が cached にない → False（キャッシュミス）
2. cached[mm_hash] が空集合（参照なし）→ freeable から除去、num_freeable_slots 減算
3. cached[mm_hash] に request_id 追加 → True（キャッシュヒット）
</code></pre>
<p>キャッシュヒット時、エンコーダ計算が <strong>完全にスキップ</strong> される。</p>
<h4 id="can_allocaterequest-input_id-budget-scheduled--bool"><a class="header" href="#can_allocaterequest-input_id-budget-scheduled--bool">can_allocate(request, input_id, budget, scheduled) → bool</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:119-178</code></p>
<pre><code>1. num_embeds &gt; encoder_compute_budget → False（予算不足）
2. total ≤ num_free_slots → True（空きあり）
3. total &gt; num_freeable_slots → False（回収しても不足）
4. total &gt; num_free_slots かつ ≤ num_freeable_slots
   → Eviction 実行: freeable から oldest-first で popitem(last=False)
   → cached から削除、freed に追加
   → num_free_slots 回復 → True
</code></pre>
<p><strong>Eviction ポリシー</strong>: FIFO順（OrderedDict の挿入順）。最も古い unreferenced エントリを先にEvict。</p>
<h4 id="allocaterequest-input_id"><a class="header" href="#allocaterequest-input_id">allocate(request, input_id)</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:180-205</code></p>
<p>キャッシュスペースを予約（論理的な簿記のみ）。物理メモリの割り当てはGPUModelRunnerで行われる。</p>
<h4 id="freerequest"><a class="header" href="#freerequest">free(request)</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:242-253</code></p>
<p>リクエスト完了時に呼ばれる。全エンコーダ入力の参照を解放。参照セットが空になったエントリは <code>freeable</code> に移動する（物理メモリは解放しない）。</p>
<h4 id="get_freed_mm_hashes--liststr"><a class="header" href="#get_freed_mm_hashes--liststr">get_freed_mm_hashes() → list[str]</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:255-266</code></p>
<p>Evictされたmm_hashを返してクリア。SchedulerOutputに含められ、GPUModelRunnerに通知される。</p>
<h3 id="キャッシュ予算の計算"><a class="header" href="#キャッシュ予算の計算">キャッシュ予算の計算</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:269-316</code></p>
<pre><code class="language-python">encoder_compute_budget = max(max_num_encoder_input_tokens, max_tokens_per_mm_item)
encoder_cache_size = max(encoder_cache_size_config, max_tokens_per_mm_item)
</code></pre>
<ul>
<li><code>encoder_compute_budget</code>: 1ステップあたりのエンコーダ計算量上限（埋め込み数）</li>
<li><code>encoder_cache_size</code>: キャッシュ全体の容量</li>
</ul>
<h2 id="2-scheduler-のエンコーダスケジューリング"><a class="header" href="#2-scheduler-のエンコーダスケジューリング">2. Scheduler のエンコーダスケジューリング</a></h2>
<h3 id="_get_encoder_budget"><a class="header" href="#_get_encoder_budget">_get_encoder_budget()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:1060-1215</code></p>
<p>各リクエストのエンコーダ入力をスケジューリングするロジック。<code>mm_features</code> の各アイテムについて:</p>
<pre><code>for i, mm_feature in enumerate(mm_features):
    1. 位置チェック: start_pos + num_encoder_tokens がスケジュール範囲内か
    2. 重複チェック: 同じ mm_hash が既にスケジュール済みか
    3. キャッシュチェック: encoder_cache_manager.check_and_update_cache() → True ならスキップ
    4. チャンクMMチェック: disable_chunked_mm_input の場合、部分スケジュール禁止
    5. 割り当てチェック: can_allocate() → False ならトークン数調整して break
    6. ECConnectorチェック: 外部キャッシュにある場合は external_load_encoder_input に追加
    7. encoder_inputs_to_schedule に追加、予算減算
</code></pre>
<h3 id="scheduleroutput-のmm関連フィールド"><a class="header" href="#scheduleroutput-のmm関連フィールド">SchedulerOutput のMM関連フィールド</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/output.py:207-218</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>scheduled_encoder_inputs</code></td><td><code>dict[str, list[int]]</code></td><td>req_id → エンコーダ入力インデックスのリスト</td></tr>
<tr><td><code>free_encoder_mm_hashes</code></td><td><code>list[str]</code></td><td>解放すべきmm_hashのリスト</td></tr>
</tbody>
</table>
</div>
<h3 id="スケジューリングの統合"><a class="header" href="#スケジューリングの統合">スケジューリングの統合</a></h3>
<p>schedule() のRUNNINGフェーズとWAITINGフェーズの両方で <code>_get_encoder_budget()</code> が呼ばれる:</p>
<pre><code>schedule()
  ├─ encoder_compute_budget = max_num_encoder_input_tokens  # 初期予算
  │
  ├─ RUNNING リクエスト処理
  │   └─ _get_encoder_budget(request, ...)
  │       → encoder_inputs_to_schedule, 調整後の num_new_tokens
  │       → allocate() 実行
  │
  ├─ WAITING リクエスト処理
  │   └─ _get_encoder_budget(request, ...)
  │       → 同上
  │
  └─ SchedulerOutput 構築
      ├─ scheduled_encoder_inputs = {req_id: [input_ids], ...}
      └─ free_encoder_mm_hashes = encoder_cache_manager.get_freed_mm_hashes()
</code></pre>
<h2 id="3-gpumodelrunner-のエンコーダ実行"><a class="header" href="#3-gpumodelrunner-のエンコーダ実行">3. GPUModelRunner のエンコーダ実行</a></h2>
<h3 id="encoder_cache"><a class="header" href="#encoder_cache">encoder_cache</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:439</code></p>
<pre><code class="language-python">self.encoder_cache: dict[str, torch.Tensor] = {}
</code></pre>
<p>mm_hash をキーとして、エンコーダ出力テンソル（GPU上）を保持する単純なdictキャッシュ。</p>
<h3 id="_batch_mm_inputs_from_scheduler"><a class="header" href="#_batch_mm_inputs_from_scheduler">_batch_mm_inputs_from_scheduler()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:2250-2291</code></p>
<p>SchedulerOutput の <code>scheduled_encoder_inputs</code> から、エンコーダに渡すデータをバッチにまとめる。</p>
<pre><code>入力: scheduled_encoder_inputs = {req_id: [input_ids], ...}
出力: (mm_hashes, mm_kwargs, mm_lora_refs)

for req_id, encoder_input_ids in scheduled_encoder_inputs.items():
    for mm_input_id in encoder_input_ids:
        mm_feature = req_state.mm_features[mm_input_id]
        if mm_feature.data is None:  # P0キャッシュヒットで省略された場合
            continue
        mm_hashes.append(mm_feature.identifier)
        mm_kwargs.append((modality, mm_feature.data))
</code></pre>
<h3 id="_execute_mm_encoder"><a class="header" href="#_execute_mm_encoder">_execute_mm_encoder()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:2293-2447</code></p>
<pre><code>1. mm_kwargs をモダリティごとにグループ化（group_mm_kwargs_by_modality）
2. 各グループに対して model.embed_multimodal(**mm_kwargs_group) を実行
3. LoRA tower mapping が必要な場合は事前にセット
4. 出力を encoder_cache[mm_hash] に格納
5. ECConnector があれば maybe_save_ec_to_connector() で外部キャッシュにも保存
</code></pre>
<p><strong>バッチ処理</strong>: 同一モダリティのアイテムはバッチ実行される。異なるモダリティは別グループとして処理。</p>
<h3 id="_gather_mm_embeddings"><a class="header" href="#_gather_mm_embeddings">_gather_mm_embeddings()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:2449-2556</code></p>
<p>スケジュールされたトークン範囲に対応するエンコーダ出力をキャッシュから取得し、マージ用のデータを準備する。</p>
<pre><code>for req_id in input_batch.req_ids:
    for mm_feature in req_state.mm_features:
        1. プレースホルダーの範囲 [start_pos, start_pos + num_encoder_tokens)
        2. スケジュールされた範囲 [num_computed_tokens, num_computed + num_scheduled)
        3. 重複部分を計算 → start_idx, end_idx
        4. encoder_cache[mm_hash] からスライス → mm_embeds_item
        5. is_mm_embed マスクの対応位置を True に設定
</code></pre>
<p><strong>チャンクPrefill対応</strong>: <code>num_computed_tokens</code> と <code>num_scheduled_tokens</code> に基づいて部分的な埋め込み取得が可能。</p>
<p>出力:</p>
<ul>
<li><code>mm_embeds: list[torch.Tensor]</code> — 全リクエストの埋め込みスライスを連結</li>
<li><code>is_mm_embed: torch.Tensor</code> — <code>(total_num_scheduled_tokens,)</code> のboolマスク</li>
</ul>
<h3 id="統合-_model_forward-内の処理"><a class="header" href="#統合-_model_forward-内の処理">統合: _model_forward() 内の処理</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:2756-2777</code></p>
<pre><code class="language-python"># 1. エンコーダ実行 + キャッシュ格納
self._execute_mm_encoder(scheduler_output)

# 2. キャッシュから必要な埋め込みを収集
mm_embeds, is_mm_embed = self._gather_mm_embeddings(scheduler_output)

# 3. テキスト + ビジョン埋め込みのマージ
inputs_embeds_scheduled = self.model.embed_input_ids(
    self.input_ids.gpu[:num_scheduled_tokens],
    multimodal_embeddings=mm_embeds,
    is_multimodal=is_mm_embed,
)
</code></pre>
<p><code>embed_input_ids()</code> は内部で <code>masked_scatter_</code> を使い、<code>is_multimodal=True</code> の位置を <code>mm_embeds</code> で置換する。</p>
<h3 id="エンコーダキャッシュの解放"><a class="header" href="#エンコーダキャッシュの解放">エンコーダキャッシュの解放</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:898-899</code></p>
<pre><code class="language-python">for mm_hash in scheduler_output.free_encoder_mm_hashes:
    self.encoder_cache.pop(mm_hash, None)
</code></pre>
<p>SchedulerOutput に含まれる <code>free_encoder_mm_hashes</code> に基づいて、GPUメモリ上のテンソルを解放。</p>
<h2 id="4-テキスト推論との差分まとめ"><a class="header" href="#4-テキスト推論との差分まとめ">4. テキスト推論との差分まとめ</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>処理ステップ</th><th>テキスト推論</th><th>マルチモーダル推論</th></tr>
</thead>
<tbody>
<tr><td>Scheduler: _get_encoder_budget</td><td>不要（has_encoder_inputs=False）</td><td>エンコーダ入力のスケジューリング</td></tr>
<tr><td>Scheduler: encoder_compute_budget</td><td>消費しない</td><td>ステップごとに予算管理</td></tr>
<tr><td>SchedulerOutput</td><td>scheduled_encoder_inputs={}</td><td>{req_id: [input_ids]}</td></tr>
<tr><td>GPUModelRunner: encoder実行</td><td>なし</td><td>_execute_mm_encoder()</td></tr>
<tr><td>GPUModelRunner: 埋め込み</td><td>input_ids をそのまま使用</td><td>embed_input_ids() で text + vision マージ</td></tr>
<tr><td>GPUModelRunner: キャッシュ</td><td>なし</td><td>encoder_cache dict</td></tr>
<tr><td>モデルforward入力</td><td>input_ids</td><td>inputs_embeds（テンソル）</td></tr>
</tbody>
</table>
</div>
<h2 id="5-キャッシュの3層構造"><a class="header" href="#5-キャッシュの3層構造">5. キャッシュの3層構造</a></h2>
<pre><code>P0（フロントエンド）          P1（バックエンド/Scheduler）      P1（バックエンド/GPU）
┌────────────────────┐     ┌───────────────────────┐      ┌──────────────────┐
│ ProcessorCache     │     │ EncoderCacheManager   │      │ encoder_cache    │
│ (LRU, mm_hash)     │     │ (RefCount, mm_hash)   │      │ (dict, mm_hash)  │
│                    │     │                       │      │                  │
│ キャッシュ対象:     │     │ キャッシュ対象:        │      │ キャッシュ対象:   │
│ HF処理済みテンソル  │     │ 論理的な存在管理      │      │ GPU上のテンソル   │
│ + prompt_updates   │     │ (容量・参照カウント)   │      │ (エンコーダ出力)  │
│                    │     │                       │      │                  │
│ Eviction: LRU      │     │ Eviction: FIFO        │      │ Eviction:        │
│ (サイズベース)      │     │ (OrderedDict oldest)  │      │ Scheduler指示    │
└────────────────────┘     └───────────────────────┘      └──────────────────┘
        ↓                           ↓                            ↓
  HF処理スキップ            エンコーダ計算スキップ          テンソル再利用
</code></pre>
<h2 id="主要ファイル-4"><a class="header" href="#主要ファイル-4">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th><th>行</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/v1/core/encoder_cache_manager.py</code></td><td><code>EncoderCacheManager</code></td><td>L17</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/sched/scheduler.py</code></td><td><code>_get_encoder_budget()</code></td><td>L1060</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/sched/output.py</code></td><td><code>SchedulerOutput</code> (MM fields)</td><td>L207, L218</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_model_runner.py</code></td><td><code>_execute_mm_encoder()</code>, <code>_gather_mm_embeddings()</code></td><td>L2293, L2449</td></tr>
</tbody>
</table>
</div>
<h2 id="関連ドキュメント-9"><a class="header" href="#関連ドキュメント-9">関連ドキュメント</a></h2>
<ul>
<li><a href="#フロントエンド-マルチモーダル処理パス-medium-verified">フロントエンド MM処理パス</a></li>
<li><a href="#gemma3-ビジョンエンコーダと画像処理-medium-verified">Gemma3 ビジョンエンコーダ</a></li>
<li><a href="#gpumodelrunner">GPUModelRunner</a></li>
<li><a href="#scheduler-サマリー">Scheduler</a></li>
<li><a href="#kvcachemanager-サマリー">KVCacheManager</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="フロントエンド-マルチモーダル処理パス-medium-verified"><a class="header" href="#フロントエンド-マルチモーダル処理パス-medium-verified">フロントエンド マルチモーダル処理パス [MEDIUM] [VERIFIED]</a></h1>
<blockquote>
<p><strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<p>APIリクエストに含まれる画像データが、フロントエンドプロセス（P0）でどのように処理され、EngineCoreRequest として ZMQ 経由でバックエンド（P1）へ送信されるかを追跡する。テキスト推論パスとの差分を中心に記述する。</p>
<h2 id="全体フロー-1"><a class="header" href="#全体フロー-1">全体フロー</a></h2>
<pre class="mermaid">graph TD
    A["API Request&lt;br&gt;(messages + images)"] --&gt; B["ChatTemplate適用"]
    B --&gt; C["InputPreprocessor.preprocess()"]
    C --&gt; D["_process_multimodal()"]
    D --&gt; E["mm_processor.info.parse_mm_data()"]
    E --&gt; F["mm_processor.apply()"]
    F --&gt; G{"ProcessorCache&lt;br&gt;ヒット?"}
    G --&gt;|HIT| H["キャッシュからitem+prompt_updates取得&lt;br&gt;(HF処理スキップ)"]
    G --&gt;|MISS| I["HF Processor実行&lt;br&gt;pixel_values等テンソル生成"]
    I --&gt; J["キャッシュに格納"]
    H --&gt; K["MultiModalInputs 構築"]
    J --&gt; K
    K --&gt; L["InputProcessor.process_inputs()"]
    L --&gt; M["MultiModalFeatureSpec 構築"]
    M --&gt; N["EngineCoreRequest&lt;br&gt;(mm_features)"]
    N --&gt; O["ZMQ IPC 送信"]
</pre>

<h2 id="1-チャットテンプレートとプレースホルダー"><a class="header" href="#1-チャットテンプレートとプレースホルダー">1. チャットテンプレートとプレースホルダー</a></h2>
<h3 id="テンプレート適用前後の文字列gemma3の例"><a class="header" href="#テンプレート適用前後の文字列gemma3の例">テンプレート適用前後の文字列（Gemma3の例）</a></h3>
<p><strong>適用前</strong>（OpenAI形式のメッセージ）:</p>
<pre><code class="language-json">{
  "messages": [
    {"role": "user", "content": [
      {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}},
      {"type": "text", "text": "この画像は何ですか？"}
    ]}
  ]
}
</code></pre>
<p><strong>チャットテンプレート適用後</strong>（テキスト）:</p>
<pre><code>&lt;bos&gt;&lt;start_of_turn&gt;user
&lt;start_of_image&gt;この画像は何ですか？&lt;end_of_turn&gt;
&lt;start_of_turn&gt;model
</code></pre>
<p>ここで <code>&lt;start_of_image&gt;</code> がプレースホルダートークンとなる。</p>
<h3 id="プレースホルダーの展開"><a class="header" href="#プレースホルダーの展開">プレースホルダーの展開</a></h3>
<p><code>Gemma3ProcessingInfo.get_image_repl()</code> がプレースホルダーを展開する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:178</code></p>
<pre><code>&lt;start_of_image&gt; → processor.full_image_sequence
</code></pre>
<p><code>processor.full_image_sequence</code> はHuggingFace Gemma3Processorが定義する完全なトークン列で、<code>&lt;start_of_image&gt;</code> + image_token × image_seq_length + <code>&lt;end_of_image&gt;</code> の形式。</p>
<p><strong>Pan-and-Scan有効時</strong>（複数クロップ）:</p>
<pre><code>"Here is the original image &lt;full_image_seq&gt; and here are some crops to help you see better &lt;full_image_seq&gt; &lt;full_image_seq&gt;"
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:196-211</code></p>
<h3 id="画像1枚あたりのトークン数"><a class="header" href="#画像1枚あたりのトークン数">画像1枚あたりのトークン数</a></h3>
<pre><code class="language-python">num_tokens = (num_crops + 1) * image_seq_length
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:213-230</code></p>
<ul>
<li><code>image_seq_length</code>: Gemma3Processorの設定値（典型的に256）</li>
<li><code>num_crops</code>: Pan-and-Scan無効時は0、有効時はアスペクト比に基づいて計算（最大 <code>pan_and_scan_max_num_crops</code>）</li>
<li>よって画像1枚で256〜1280+トークンを消費</li>
</ul>
<h3 id="トークン列の構造"><a class="header" href="#トークン列の構造">トークン列の構造</a></h3>
<p>テキスト推論ではトークン列は純粋なテキストトークンのみ。マルチモーダルでは以下の構造になる:</p>
<pre><code>[BOS] [start_of_turn] [user] [\n]
[start_of_image] [image_token × 256] [end_of_image]    ← 画像プレースホルダー
[テキストトークン列...]                                    ← "この画像は何ですか？"
[end_of_turn] [\n] [start_of_turn] [model] [\n]
</code></pre>
<p><code>image_token</code> の位置がマスクで追跡され（<code>PlaceholderRange</code>）、後にビジョンエンコーダの出力で置換される。</p>
<h3 id="改行トークンの結合処理"><a class="header" href="#改行トークンの結合処理">改行トークンの結合処理</a></h3>
<p>Gemma3固有の問題：<code>\n\n\n</code> と <code>\n\n\n\n</code> が単一トークンとして存在する。画像置換テキストに <code>\n\n</code> が挿入されると、隣接する <code>\n</code> と結合が必要。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:351-385</code></p>
<pre><code class="language-python"># _apply_token_matches() でトークンの結合を実行
replace_token_matches(token_ids, [newline_1, newline_2], [newline_3])  # \n + \n\n → \n\n\n
replace_token_matches(token_ids, [newline_2, newline_1], [newline_3])  # \n\n + \n → \n\n\n
replace_token_matches(token_ids, [newline_2, newline_2], [newline_4])  # \n\n + \n\n → \n\n\n\n
</code></pre>
<h2 id="2-マルチモーダルデータの処理パイプライン"><a class="header" href="#2-マルチモーダルデータの処理パイプライン">2. マルチモーダルデータの処理パイプライン</a></h2>
<h3 id="inputpreprocessor_process_multimodal"><a class="header" href="#inputpreprocessor_process_multimodal">InputPreprocessor._process_multimodal()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/inputs/preprocess.py:193-232</code></p>
<pre><code>_process_multimodal(prompt, mm_data, mm_processor_kwargs, mm_uuids)
  1. mm_processor.info.parse_mm_data(mm_data)
     → MultiModalDataItems（モダリティごとに型付きデータアイテムに変換）
  2. mm_processor.apply(prompt, mm_items, ...)
     → MultiModalInputs（トークン列 + テンソルデータ + プレースホルダー位置 + ハッシュ）
</code></pre>
<h3 id="basemultimodalprocessorapply"><a class="header" href="#basemultimodalprocessorapply">BaseMultiModalProcessor.apply()</a></h3>
<p>HFプロセッサ実行、プロンプト更新（プレースホルダー検出・展開）、キャッシュ管理を統合的に処理する。</p>
<p>主な出力（<code>MultiModalInputs</code>）:</p>
<ul>
<li><code>prompt_token_ids</code>: プレースホルダー展開済みのトークン列</li>
<li><code>mm_kwargs</code>: <code>dict[modality, list[MultiModalKwargsItem]]</code> — 処理済みテンソルデータ</li>
<li><code>mm_hashes</code>: <code>dict[modality, list[str]]</code> — 各アイテムのハッシュ値</li>
<li><code>mm_placeholders</code>: <code>dict[modality, list[PlaceholderRange]]</code> — プレースホルダー位置情報</li>
</ul>
<h3 id="gemma3multimodalprocessor_call_hf_processor"><a class="header" href="#gemma3multimodalprocessor_call_hf_processor">Gemma3MultiModalProcessor._call_hf_processor()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:277-310</code></p>
<p>親クラスのHFプロセッサ呼び出し後、<code>num_patches</code> を追加計算する。HFプロセッサはこの値をpopしてしまうため、vLLM側で再計算が必要。</p>
<pre><code class="language-python">num_crops = [self.info.get_num_crops(...) for size in image_sizes]
processed_outputs["num_patches"] = torch.tensor(num_crops) + 1  # +1 for original
</code></pre>
<p>HFプロセッサの出力:</p>
<ul>
<li><code>pixel_values</code>: <code>(total_patches, 3, image_size, image_size)</code> — 全パッチのピクセルテンソル</li>
<li><code>num_patches</code>: <code>(num_images,)</code> — 画像ごとのパッチ数（= num_crops + 1）</li>
</ul>
<h2 id="3-mmハッシュ"><a class="header" href="#3-mmハッシュ">3. MMハッシュ</a></h2>
<h3 id="multimodalhasher"><a class="header" href="#multimodalhasher">MultiModalHasher</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/hasher.py:50-162</code></p>
<p>マルチモーダルデータのコンテンツベースハッシュを計算する。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>データ型</th><th>シリアライズ方法</th></tr>
</thead>
<tbody>
<tr><td>PIL Image</td><td>ExifTags.ImageID (UUID型の場合) → 16バイト。なければ mode + pixel data (numpy配列)</td></tr>
<tr><td>MediaWithBytes(Image)</td><td>ExifTags.ImageID → 16バイト。なければ original_bytes</td></tr>
<tr><td>torch.Tensor</td><td>numpy変換 → dtype+shape+バイト列。bfloat16は uint8 view 経由</td></tr>
<tr><td>np.ndarray</td><td>dtype.str + shape + contiguous バイト列</td></tr>
<tr><td>その他</td><td>pickle フォールバック（警告あり）</td></tr>
</tbody>
</table>
</div>
<p><strong>ハッシュアルゴリズム</strong>: <code>VLLM_MM_HASHER_ALGORITHM</code> 環境変数で設定</p>
<ul>
<li><code>blake3</code>（デフォルト）: 高速</li>
<li><code>sha256</code> / <code>sha512</code>: FIPS準拠用</li>
</ul>
<p><strong>hash_kwargs()</strong> はキーをソートしてから全データを逐次ハッシュに投入する（決定的）。</p>
<h3 id="uuidオーバーライド"><a class="header" href="#uuidオーバーライド">UUIDオーバーライド</a></h3>
<p>キャッシュとプレフィックスキャッシュの両方が無効な場合、コンテンツハッシュの代わりに <code>{request_id}-{modality}-{index}</code> 形式のUUIDを使用する（ハッシュ計算コストの回避）。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/input_processor.py:551-574</code></p>
<h3 id="lora対応のidentifier"><a class="header" href="#lora対応のidentifier">LoRA対応のidentifier</a></h3>
<p>LoRAのtower_connector_loraが有効な場合、同じ画像でもLoRAによって埋め込みが変わるため、<code>identifier</code> に LoRA名をプレフィックスとして付加する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/input_processor.py:490-506</code></p>
<pre><code class="language-python">def _get_mm_identifier(self, mm_hash, lora_request):
    if lora_request is None or not enable_tower_connector_lora:
        return mm_hash
    return f"{lora_request.lora_name}:{mm_hash}"
</code></pre>
<h2 id="4-プロセッサキャッシュp0側"><a class="header" href="#4-プロセッサキャッシュp0側">4. プロセッサキャッシュ（P0側）</a></h2>
<h3 id="キャッシュタイプの選択"><a class="header" href="#キャッシュタイプの選択">キャッシュタイプの選択</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/registry.py:284-320</code></p>
<pre><code>mm_processor_cache_gb &lt;= 0         → None（キャッシュ無効）
IPC非対応 or API process &gt; 1       → processor_only
mm_processor_cache_type == "lru"   → lru（Sender + Receiver）
mm_processor_cache_type == "shm"   → shm（共有メモリ）
</code></pre>
<h3 id="4種類のキャッシュ実装"><a class="header" href="#4種類のキャッシュ実装">4種類のキャッシュ実装</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/cache.py</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>実装</th><th>場所</th><th>格納内容</th><th>キャッシュヒット時の動作</th></tr>
</thead>
<tbody>
<tr><td><code>MultiModalProcessorOnlyCache</code> (L326)</td><td>P0のみ</td><td>テンソルデータ + prompt_updates</td><td>キャッシュから item + prompt_updates を返す（HF処理スキップ）</td></tr>
<tr><td><code>MultiModalProcessorSenderCache</code> (L379)</td><td>P0</td><td>サイズメタデータ + prompt_updates</td><td>item=None を返す（P1にデータあり、IPC不要）</td></tr>
<tr><td><code>ShmObjectStoreSenderCache</code> (L437)</td><td>P0</td><td>共有メモリ参照 + prompt_updates</td><td>item=None を返す（共有メモリ経由でP1に渡す）</td></tr>
<tr><td><code>MultiModalReceiverCache</code> (L614)</td><td>P1</td><td>テンソルデータ</td><td>lru タイプ時に P1 側で使用</td></tr>
</tbody>
</table>
</div>
<h3 id="p0-p1-キャッシュの整合性"><a class="header" href="#p0-p1-キャッシュの整合性">P0-P1 キャッシュの整合性</a></h3>
<p><strong>設計の核心</strong>: P0とP1のキャッシュは <strong>同一のEviction順序</strong> を維持する。</p>
<pre><code>                 is_cached() × N    get_and_update()
P0: From API ───────────────────&gt; ────────────────&gt; To P1

                get_and_update()
P1: From P0 ───────────────────&gt; To model
</code></pre>
<ul>
<li><code>is_cached()</code> はP0キャッシュのみを参照（Eviction順序を変えない）</li>
<li><code>get_and_update()</code> は P0 と P1 で順番に呼ぶ必要がある（Eviction順序を同期）</li>
<li>これにより、P0のキャッシュ状態を見るだけでP1のキャッシュ状態を推定できる（IPC不要）</li>
</ul>
<h3 id="キャッシュヒット時にスキップされる処理"><a class="header" href="#キャッシュヒット時にスキップされる処理">キャッシュヒット時にスキップされる処理</a></h3>
<ol>
<li><strong>HF Processor実行</strong>（画像のリサイズ、正規化、パッチ分割 → <code>pixel_values</code> テンソル生成）</li>
<li><strong>テンソルデータのIPC送信</strong>（<code>SenderCache</code>/<code>ShmCache</code> 使用時、<code>data=None</code> にしてZMQ転送量削減）</li>
<li><strong>プロンプト更新の再計算は常に必要</strong>（キャッシュにprompt_updatesが保存されているため計算はスキップだが、取得は必要）</li>
</ol>
<h2 id="5-enginecorerequest-への組み立て"><a class="header" href="#5-enginecorerequest-への組み立て">5. EngineCoreRequest への組み立て</a></h2>
<h3 id="multimodalfeaturespec-構築"><a class="header" href="#multimodalfeaturespec-構築">MultiModalFeatureSpec 構築</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/input_processor.py:627-654</code></p>
<pre><code class="language-python">mm_features = []
for modality, idx in sorted_mm_idxs:
    base_mm_hash = decoder_mm_hashes[modality][idx]
    mm_features.append(
        MultiModalFeatureSpec(
            data=decoder_mm_inputs[modality][idx],     # MultiModalKwargsItem | None
            modality=modality,                          # "image"
            identifier=_get_mm_identifier(base_mm_hash, lora_request),
            mm_position=decoder_mm_positions[modality][idx],  # PlaceholderRange
            mm_hash=base_mm_hash,
        )
    )
</code></pre>
<p><code>sorted_mm_idxs</code> は <code>argsort_mm_positions()</code> でプロンプト内の出現順にソートされる。</p>
<h3 id="multimodalfeaturespec-の構造"><a class="header" href="#multimodalfeaturespec-の構造">MultiModalFeatureSpec の構造</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/inputs.py:337-381</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>data</code></td><td><code>MultiModalKwargsItem | None</code></td><td>処理済みテンソルデータ。P0キャッシュヒット時は <code>None</code></td></tr>
<tr><td><code>modality</code></td><td><code>str</code></td><td><code>"image"</code>, <code>"video"</code>, <code>"audio"</code> 等</td></tr>
<tr><td><code>identifier</code></td><td><code>str</code></td><td>エンコーダキャッシュ用ハッシュ（LoRAプレフィックス付きの場合あり）</td></tr>
<tr><td><code>mm_position</code></td><td><code>PlaceholderRange</code></td><td>プロンプト内のプレースホルダー位置</td></tr>
<tr><td><code>mm_hash</code></td><td><code>str | None</code></td><td>プロセッサキャッシュ用ハッシュ（LoRAプレフィックスなし）</td></tr>
</tbody>
</table>
</div>
<h3 id="placeholderrange-の構造"><a class="header" href="#placeholderrange-の構造">PlaceholderRange の構造</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/inputs.py:170-240</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>offset</code></td><td><code>int</code></td><td>プロンプト内の開始位置</td></tr>
<tr><td><code>length</code></td><td><code>int</code></td><td>プレースホルダーの長さ（トークン数）</td></tr>
<tr><td><code>is_embed</code></td><td><code>Tensor[bool] | None</code></td><td>各位置が埋め込みを受け取るかのマスク</td></tr>
</tbody>
</table>
</div>
<p><code>get_num_embeds()</code> は実際のエンコーダ出力の埋め込み数を返す（<code>is_embed</code> のTrue数、またはlength）。</p>
<h3 id="enginecorerequest-1"><a class="header" href="#enginecorerequest-1">EngineCoreRequest</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/__init__.py:55-101</code></p>
<p>テキスト推論との差分:</p>
<ul>
<li><code>mm_features: list[MultiModalFeatureSpec] | None</code> — マルチモーダル時に設定される</li>
<li><code>prompt_token_ids</code> にはプレースホルダー展開済みのトークン列が入る</li>
</ul>
<p>ZMQ送信時は <code>msgspec</code> によるバイナリシリアライゼーション。テンソルデータは <code>MultiModalKwargsItem</code> に含まれ、カスタムエンコーダで処理される。</p>
<h2 id="6-キャッシュタイプ別のデータフロー"><a class="header" href="#6-キャッシュタイプ別のデータフロー">6. キャッシュタイプ別のデータフロー</a></h2>
<h3 id="processor_onlyp0完結"><a class="header" href="#processor_onlyp0完結">processor_only（P0完結）</a></h3>
<pre><code>P0: hash → cache miss → HF処理 → cache store(tensor+prompt) → tensor をリクエストに含めて送信
P0: hash → cache hit  → cache get(tensor+prompt) → tensor をリクエストに含めて送信
</code></pre>
<ul>
<li>テンソルデータは常にZMQ経由で送信される</li>
</ul>
<h3 id="lrup0-sender--p1-receiver"><a class="header" href="#lrup0-sender--p1-receiver">lru（P0 Sender + P1 Receiver）</a></h3>
<pre><code>P0: hash → cache miss → HF処理 → meta store(size+prompt) → tensor をリクエストに含めて送信
P1: hash → cache miss → tensor を受信 → cache store(tensor)

P0: hash → cache hit  → meta get(prompt) → data=None で送信（テンソル省略）
P1: hash → cache hit  → cache get(tensor)
</code></pre>
<ul>
<li>キャッシュヒット時は <strong>テンソルデータのIPC転送がスキップ</strong> される</li>
</ul>
<h3 id="shm共有メモリ"><a class="header" href="#shm共有メモリ">shm（共有メモリ）</a></h3>
<pre><code>P0: hash → cache miss → HF処理 → 共有メモリに書き込み → data=None で送信
P1: hash → cache miss → 共有メモリから読み取り

P0: hash → cache hit  → data=None で送信
P1: hash → cache hit  → 共有メモリから読み取り（ringバッファ）
</code></pre>
<h2 id="主要ファイル-5"><a class="header" href="#主要ファイル-5">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th><th>行</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/v1/engine/input_processor.py</code></td><td><code>InputProcessor</code>, <code>process_inputs()</code>, <code>_get_mm_identifier()</code></td><td>L56, L521, L490</td></tr>
<tr><td><code>target/vllm/vllm/inputs/preprocess.py</code></td><td><code>InputPreprocessor</code>, <code>_process_multimodal()</code>, <code>_get_mm_processor()</code></td><td>L60, L193, L182</td></tr>
<tr><td><code>target/vllm/vllm/multimodal/hasher.py</code></td><td><code>MultiModalHasher</code>, <code>hash_kwargs()</code>, <code>serialize_item()</code></td><td>L50, L154, L52</td></tr>
<tr><td><code>target/vllm/vllm/multimodal/cache.py</code></td><td><code>MultiModalProcessorOnlyCache</code>, <code>SenderCache</code>, <code>ShmCache</code>, <code>ReceiverCache</code></td><td>L326, L379, L437, L614</td></tr>
<tr><td><code>target/vllm/vllm/multimodal/registry.py</code></td><td><code>MULTIMODAL_REGISTRY</code>, <code>processor_cache_from_config()</code></td><td>L305</td></tr>
<tr><td><code>target/vllm/vllm/multimodal/inputs.py</code></td><td><code>MultiModalFeatureSpec</code>, <code>PlaceholderRange</code></td><td>L337, L170</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/__init__.py</code></td><td><code>EngineCoreRequest</code></td><td>L55</td></tr>
<tr><td><code>target/vllm/vllm/model_executor/models/gemma3_mm.py</code></td><td><code>Gemma3MultiModalProcessor</code>, <code>Gemma3ProcessingInfo</code></td><td>L276, L77</td></tr>
</tbody>
</table>
</div>
<h2 id="関連ドキュメント-10"><a class="header" href="#関連ドキュメント-10">関連ドキュメント</a></h2>
<ul>
<li><a href="#マルチモーダル処理パイプライン-サマリー">マルチモーダル処理パイプライン概要</a></li>
<li><a href="#バックエンド-マルチモーダル処理パス-medium-verified">バックエンド MM処理パス</a></li>
<li><a href="#gemma3-ビジョンエンコーダと画像処理-medium-verified">Gemma3 ビジョンエンコーダ</a></li>
<li><a href="#inputprocessor-サマリー">InputProcessor</a></li>
<li><a href="#テキスト推論データフロー">データフロー</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="outputprocessor"><a class="header" href="#outputprocessor">OutputProcessor</a></h1>
<blockquote>
<p><strong>深度</strong>: [SHALLOW]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-16"><a class="header" href="#概要-16">概要</a></h2>
<p>OutputProcessorは<strong>フロントエンドプロセス</strong>で動作し、バックエンド（EngineCore）からZMQ経由で受信した<code>EngineCoreOutput</code>を、ユーザー向けの<code>RequestOutput</code>に変換する。主な処理はインクリメンタルデトークナイズ、停止文字列判定、logprobs処理である。AsyncLLMの<code>output_handler</code>バックグラウンドタスクから呼び出される。</p>
<h2 id="process_outputs-フロー"><a class="header" href="#process_outputs-フロー">process_outputs() フロー</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/output_processor.py:582</code> (process_outputs)</p>
<pre><code>OutputProcessor.process_outputs(engine_core_outputs)       # L582
  │
  for each engine_core_output:
    │
    ├─ req_state = request_states[req_id]                  # RequestState取得
    │   （abortされていればスキップ）
    │
    ├─ 統計情報更新                                         # L620-622
    │
    ├─ デトークナイズ + 停止文字列判定                       # L637-639
    │   stop_string = detokenizer.update(
    │       new_token_ids, stop_terminated)
    │   → トークン→テキスト変換（インクリメンタル）
    │   → 停止文字列検出時は finish_reason = STOP
    │
    ├─ logprobs処理                                         # L646
    │   logprobs_processor.update_from_output(output)
    │
    ├─ RequestOutput構築                                    # L649-656
    │   req_state.make_request_output(
    │       new_token_ids, finish_reason, stop_reason, ...)
    │   → CompletionOutput + RequestOutput
    │
    ├─ 出力配信                                             # L660-665
    │   ├─ AsyncLLM: req_state.queue.put(request_output)
    │   └─ LLM: request_outputs.append(request_output)
    │
    └─ 完了処理                                             # L668-687
        if finish_reason is not None:
          _finish_request(req_state)
          → リクエスト解放、統計記録
</code></pre>
<h2 id="requeststate"><a class="header" href="#requeststate">RequestState</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/output_processor.py:116</code> (RequestState)</p>
<p>各リクエストのフロントエンド側状態を保持する。OutputProcessor.add_request()で作成される。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>external_req_id</code></td><td><code>str</code></td><td>外部リクエストID（クライアント向け）</td></tr>
<tr><td><code>detokenizer</code></td><td><code>IncrementalDetokenizer</code></td><td>デトークナイザインスタンス</td></tr>
<tr><td><code>logprobs_processor</code></td><td><code>LogprobsProcessor</code></td><td>logprobs処理インスタンス</td></tr>
<tr><td><code>output_kind</code></td><td><code>RequestOutputKind</code></td><td>出力モード（CUMULATIVE/DELTA/FINAL_ONLY）</td></tr>
<tr><td><code>queue</code></td><td><code>RequestOutputCollector | None</code></td><td>AsyncLLM用出力キュー</td></tr>
<tr><td><code>prompt_token_ids</code></td><td><code>list[int]</code></td><td>プロンプトトークン（出力に含める用）</td></tr>
</tbody>
</table>
</div>
<h3 id="make_request_output"><a class="header" href="#make_request_output">make_request_output()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/output_processor.py:269</code> (make_request_output)</p>
<pre><code>make_request_output(new_token_ids, finish_reason, ...)
  │
  ├─ FINAL_ONLY モードかつ未完了 → None（出力なし）
  │
  ├─ プーリングモデル → PoolingRequestOutput
  │
  └─ テキスト生成 → RequestOutput
      ├─ _new_completion_output()                          # L377
      │   ├─ detokenizer.get_next_output_text(finished, delta)
      │   │   → DELTAモード: 新規テキストのみ
      │   │   → CUMULATIVEモード: 全テキスト
      │   └─ CompletionOutput(text, token_ids, logprobs, ...)
      └─ RequestOutput(request_id, outputs, finished, ...)
</code></pre>
<h2 id="detokenizerインクリメンタルデトークナイズ-1"><a class="header" href="#detokenizerインクリメンタルデトークナイズ-1">Detokenizer（インクリメンタルデトークナイズ）</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/detokenizer.py:30</code> (IncrementalDetokenizer)</p>
<h3 id="クラス階層-2"><a class="header" href="#クラス階層-2">クラス階層</a></h3>
<pre><code>IncrementalDetokenizer (基底・No-op)               L30
└── BaseIncrementalDetokenizer (ABC)                L65
    ├── FastIncrementalDetokenizer                  L169
    │   → HF tokenizersの DecodeStream 使用
    └── SlowIncrementalDetokenizer                  L258
        → detokenize_incrementally() 使用
</code></pre>
<p>ファクトリメソッド <code>from_new_request()</code> がトークナイザの種類に応じて適切な実装を選択する。</p>
<h3 id="update-メソッド"><a class="header" href="#update-メソッド">update() メソッド</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/detokenizer.py:65</code> (BaseIncrementalDetokenizer.update)</p>
<pre><code>update(new_token_ids, stop_terminated) → stop_string | None
  │
  for each new_token_id:
    ├─ token_ids.append(new_token_id)
    └─ output_text += decode_next(new_token_id)  # 抽象メソッド
  │
  └─ check_stop_strings(output_text, ...)         # L316
      → (stop_string, truncate_offset) | None
</code></pre>
<h3 id="停止文字列判定"><a class="header" href="#停止文字列判定">停止文字列判定</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/detokenizer.py:316</code> (check_stop_strings)</p>
<p><code>check_stop_strings()</code>は累積テキストの末尾付近で停止文字列を検索する。検出時はテキストをトランケートし、停止文字列と切り詰め位置を返す。<code>include_stop_str_in_output</code>フラグで停止文字列を出力に含めるか制御する。</p>
<h2 id="logprobsprocessor"><a class="header" href="#logprobsprocessor">LogprobsProcessor</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/logprobs.py:28</code> (LogprobsProcessor)</p>
<p><code>SamplingParams.logprobs</code> / <code>prompt_logprobs</code> の設定に基づいて初期化される。<code>update_from_output()</code>で<code>EngineCoreOutput</code>からlogprobs情報を抽出し、累積対数確率を更新する。</p>
<h2 id="出力モードrequestoutputkind"><a class="header" href="#出力モードrequestoutputkind">出力モード（RequestOutputKind）</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/sampling_params.py:108</code> (RequestOutputKind)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>モード</th><th>値</th><th>動作</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>CUMULATIVE</code></td><td>0</td><td>毎回全出力テキスト/トークンを返す</td><td>デフォルト</td></tr>
<tr><td><code>DELTA</code></td><td>1</td><td>差分（新規テキスト/トークン）のみ返す</td><td>ストリーミング</td></tr>
<tr><td><code>FINAL_ONLY</code></td><td>2</td><td>完了時のみ出力を返す</td><td>バッチ処理</td></tr>
</tbody>
</table>
</div>
<h2 id="asyncllmとの連携"><a class="header" href="#asyncllmとの連携">AsyncLLMとの連携</a></h2>
<pre><code>AsyncLLM._run_output_handler()                     # async_llm.py:662
  while True:
    outputs = await engine_core.get_output_async()  # ZMQ受信
    for chunk in outputs.outputs:
      output_processor.process_outputs(chunk, ...)  # ← ここで呼ばれる
      → RequestOutputがper-requestキューにpush
      → generate()がキューからyield
</code></pre>
<h2 id="上流下流の関係-2"><a class="header" href="#上流下流の関係-2">上流・下流の関係</a></h2>
<ul>
<li><strong>上流</strong>: AsyncLLM（output_handlerタスクから呼び出し）、EngineCoreOutputs（ZMQ経由受信）</li>
<li><strong>下流</strong>: APIサーバー（RequestOutputをyield）</li>
</ul>
<h2 id="phase-2-深堀り候補-2"><a class="header" href="#phase-2-深堀り候補-2">Phase 2 深堀り候補</a></h2>
<ul>
<li><code>RequestOutputCollector</code>のキューイング実装</li>
<li>ストリーミングモード（DELTA）時のテキスト差分計算詳細</li>
<li>n&gt;1サンプリング時のParentRequest管理</li>
</ul>
<h2 id="主要ファイル-6"><a class="header" href="#主要ファイル-6">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/v1/engine/output_processor.py</code></td><td><code>OutputProcessor</code> (L73), <code>process_outputs()</code> (L582), <code>RequestState</code> (L116), <code>make_request_output()</code> (L269)</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/detokenizer.py</code></td><td><code>IncrementalDetokenizer</code> (L30), <code>FastIncrementalDetokenizer</code> (L169), <code>SlowIncrementalDetokenizer</code> (L258), <code>check_stop_strings()</code> (L316)</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/logprobs.py</code></td><td><code>LogprobsProcessor</code> (L28)</td></tr>
<tr><td><code>target/vllm/vllm/outputs.py</code></td><td><code>RequestOutput</code> (L86), <code>CompletionOutput</code> (L23)</td></tr>
<tr><td><code>target/vllm/vllm/sampling_params.py</code></td><td><code>RequestOutputKind</code> (L108)</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="scheduler-サマリー"><a class="header" href="#scheduler-サマリー">Scheduler サマリー</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-17"><a class="header" href="#概要-17">概要</a></h2>
<p><code>Scheduler</code>はContinuous Batchingの中核コンポーネントであり、各ステップでどのリクエストにどれだけのトークンを計算させるかを決定する。<code>schedule()</code>メソッドは3フェーズ（RUNNING処理 → WAITING処理 → Output構築）で構成され、トークン予算の範囲内で最大限のリクエストをスケジュールする。Unified Compute Modelを採用し、Prefill/Decodeを明示的に区別せず <code>num_computed_tokens</code> の進捗で統一的に管理する。</p>
<h2 id="アーキテクチャ-7"><a class="header" href="#アーキテクチャ-7">アーキテクチャ</a></h2>
<pre class="mermaid">graph TD
    subgraph schedule 3フェーズ
        P1["Phase 1: RUNNING&lt;br&gt;既実行リクエスト処理&lt;br&gt;L350-517"]
        P2["Phase 2: WAITING&lt;br&gt;新規リクエスト受け入れ&lt;br&gt;L532-800"]
        P3["Phase 3: Output構築&lt;br&gt;SchedulerOutput生成&lt;br&gt;L827-896"]
    end

    AR["add_request()"] --&gt;|"WAITINGキューに追加"| P2
    P1 --&gt;|"トークン予算消費"| P2
    P2 --&gt;|"トークン予算消費"| P3
    P1 --&gt;|"プリエンプション"| KVM["KVCacheManager"]
    P2 --&gt;|"allocate_slots()"| KVM
    P2 --&gt;|"get_computed_blocks()"| KVM
    P3 --&gt;|"SchedulerOutput"| EC["EngineCore"]

    UO["update_from_output()"] --&gt;|"ModelRunnerOutput"| OUT["EngineCoreOutputs"]
</pre>

<h2 id="主要コンポーネント-7"><a class="header" href="#主要コンポーネント-7">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>用途</th><th>ファイル</th></tr>
</thead>
<tbody>
<tr><td><code>Scheduler</code></td><td>スケジューリング本体</td><td><code>target/vllm/vllm/v1/core/sched/scheduler.py</code></td></tr>
<tr><td><code>Request</code></td><td>リクエスト内部状態</td><td><code>target/vllm/vllm/v1/request.py</code></td></tr>
<tr><td><code>SchedulerOutput</code></td><td>スケジュール結果（Executor向け）</td><td><code>target/vllm/vllm/v1/core/sched/output.py:184</code></td></tr>
<tr><td><code>NewRequestData</code></td><td>初回スケジュールのフルデータ</td><td><code>target/vllm/vllm/v1/core/sched/output.py:34</code></td></tr>
<tr><td><code>CachedRequestData</code></td><td>既スケジュール済みの差分データ</td><td><code>target/vllm/vllm/v1/core/sched/output.py:114</code></td></tr>
</tbody>
</table>
</div>
<h2 id="主要メソッド-7"><a class="header" href="#主要メソッド-7">主要メソッド</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>schedule()</code></td><td>L321</td><td>メイン: 3フェーズスケジューリング → SchedulerOutput</td></tr>
<tr><td><code>add_request()</code></td><td>L1644</td><td>WAITINGキューにリクエスト登録</td></tr>
<tr><td><code>update_from_output()</code></td><td>L1241</td><td>ModelRunnerOutputから出力生成 → EngineCoreOutputs</td></tr>
<tr><td><code>finish_requests()</code></td><td>L1666</td><td>リクエストを完了/中止状態にする</td></tr>
<tr><td><code>_preempt_request()</code></td><td>L898</td><td>プリエンプション実行（ブロック解放→WAITINGに戻す）</td></tr>
<tr><td><code>_make_cached_request_data()</code></td><td>L999</td><td>CachedRequestData（差分データ）構築</td></tr>
<tr><td><code>_update_request_with_output()</code></td><td>L1538</td><td>生成トークンをリクエストに追加、停止判定</td></tr>
</tbody>
</table>
</div>
<h2 id="schedule-3フェーズ"><a class="header" href="#schedule-3フェーズ">schedule() 3フェーズ</a></h2>
<h3 id="phase-1-running-リクエストのスケジューリングl350-517"><a class="header" href="#phase-1-running-リクエストのスケジューリングl350-517">Phase 1: RUNNING リクエストのスケジューリング（L350-517）</a></h3>
<p>既に実行中のリクエストに対してトークンを割り当てる。</p>
<pre><code>while req_index &lt; len(self.running) and token_budget &gt; 0:
    request = self.running[req_index]

    # 計算すべき新規トークン数
    num_new_tokens = (
        request.num_tokens_with_spec           # 最終目標
        + request.num_output_placeholders      # asyncプレースホルダ
        - request.num_computed_tokens           # 既計算分を差引
    )
    num_new_tokens = min(num_new_tokens, token_budget)

    # KVキャッシュブロック割り当て
    new_blocks = kv_cache_manager.allocate_slots(request, num_new_tokens)

    if new_blocks is None:
        # → プリエンプション: 最低優先度リクエストを解放して再試行
</code></pre>
<p><strong>プリエンプション</strong>: ブロック割り当て失敗時、Priority/FIFOポリシーで最低優先度のリクエストを選びブロック解放。解放後に再試行する。</p>
<h3 id="phase-2-waiting-リクエストのスケジューリングl532-800"><a class="header" href="#phase-2-waiting-リクエストのスケジューリングl532-800">Phase 2: WAITING リクエストのスケジューリング（L532-800）</a></h3>
<p>WAITINGキューから新規リクエストを受け入れる。</p>
<pre><code>for request in self.waiting:
    # スキップ条件チェック
    #   - WAITING_FOR_REMOTE_KVS: 非同期KV受信待ち
    #   - WAITING_FOR_FSM: 構造化出力のFSMコンパイル待ち
    #   - WAITING_FOR_STREAMING_REQ: ストリーミング入力待ち
    #   - LoRA制約超過

    # プレフィックスキャッシュ検索（初回のみ）
    if request.num_computed_tokens == 0:
        computed_blocks, num_hits = kv_cache_manager.get_computed_blocks(request)
        # KVコネクタ（LMCache等）による外部キャッシュも検索

    # 計算対象トークン数
    num_new_tokens = request.num_tokens - num_computed_tokens
    num_new_tokens = min(num_new_tokens, token_budget)

    # KVキャッシュブロック割り当て
    new_blocks = kv_cache_manager.allocate_slots(request, num_new_tokens, ...)
    if new_blocks is None:
        break  # ← RUNNINGと異なりプリエンプションせずループ終了

    # RUNNINGキューに追加
    self.running.append(request)
    request.status = RequestStatus.RUNNING
    token_budget -= num_new_tokens
</code></pre>
<h3 id="phase-3-scheduleroutput-構築l827-896"><a class="header" href="#phase-3-scheduleroutput-構築l827-896">Phase 3: SchedulerOutput 構築（L827-896）</a></h3>
<pre><code># 新規リクエスト → NewRequestData（フルデータ）
new_reqs_data = [NewRequestData.from_request(req, block_ids) for req in scheduled_new_reqs]

# 既実行リクエスト → CachedRequestData（差分のみ）
cached_reqs_data = self._make_cached_request_data(running_reqs, resumed_reqs, ...)

return SchedulerOutput(
    scheduled_new_reqs=new_reqs_data,
    scheduled_cached_reqs=cached_reqs_data,
    num_scheduled_tokens=num_scheduled_tokens,
    total_num_scheduled_tokens=total,
    ...
)
</code></pre>
<h2 id="unified-compute-model"><a class="header" href="#unified-compute-model">Unified Compute Model</a></h2>
<p>vLLM v1のSchedulerはPrefillとDecodeを明示的に区別しない。各リクエストの <code>num_computed_tokens</code> が <code>num_tokens_with_spec</code>（プロンプト長 + 出力長 + スペキュレーショントークン）に追いつくまでトークンを割り当てる。</p>
<p>このアプローチにより以下が統一的に扱える:</p>
<ul>
<li><strong>Chunked Prefill</strong>: 大きなプロンプトを複数ステップに分割</li>
<li><strong>Prefix Caching</strong>: キャッシュヒット分を <code>num_computed_tokens</code> に反映</li>
<li><strong>Speculative Decoding</strong>: ドラフトトークンを <code>num_tokens_with_spec</code> に含める</li>
</ul>
<h2 id="トークン予算"><a class="header" href="#トークン予算">トークン予算</a></h2>
<pre><code class="language-python">token_budget = self.max_num_scheduled_tokens  # ステップあたりの上限
</code></pre>
<ul>
<li>各リクエストのスケジュール時に <code>token_budget -= num_new_tokens</code> で消費</li>
<li>Phase 1（RUNNING）→ Phase 2（WAITING）の順で消費</li>
<li>枯渇時: RUNNING側は continue（次リクエスト試行）、WAITING側は break（ループ終了）</li>
</ul>
<h2 id="プリエンプション"><a class="header" href="#プリエンプション">プリエンプション</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:898</code> (_preempt_request)</p>
<p>KVキャッシュブロック不足時にRUNNINGリクエストに対してのみ発動:</p>
<ol>
<li>ポリシーに基づき最低優先度のリクエストを選択
<ul>
<li><strong>Priority</strong>: <code>(priority, arrival_time)</code> が最大のリクエスト</li>
<li><strong>FIFO</strong>: 最後のリクエスト</li>
</ul>
</li>
<li><code>kv_cache_manager.free(request)</code> でブロック解放</li>
<li><code>request.status = RequestStatus.PREEMPTED</code>、<code>num_computed_tokens = 0</code> にリセット</li>
<li>WAITINGキューの先頭に戻す（LIFO順序で優先再スケジュール）</li>
</ol>
<h2 id="request-ステータス遷移"><a class="header" href="#request-ステータス遷移">Request ステータス遷移</a></h2>
<pre class="mermaid">stateDiagram-v2
    [*] --&gt; WAITING: add_request()
    WAITING --&gt; WAITING_FOR_FSM: FSMコンパイル待ち
    WAITING --&gt; WAITING_FOR_REMOTE_KVS: リモートKV受信待ち
    WAITING --&gt; WAITING_FOR_STREAMING_REQ: ストリーミング入力待ち
    WAITING_FOR_FSM --&gt; WAITING: コンパイル完了
    WAITING_FOR_REMOTE_KVS --&gt; WAITING: 受信完了
    WAITING_FOR_STREAMING_REQ --&gt; WAITING: 入力完了
    WAITING --&gt; RUNNING: schedule()で選択
    RUNNING --&gt; PREEMPTED: KVキャッシュ不足
    PREEMPTED --&gt; WAITING: キュー先頭に戻す
    RUNNING --&gt; FINISHED_STOPPED: EOS/stop_token検出
    RUNNING --&gt; FINISHED_LENGTH_CAPPED: max_tokens到達
    RUNNING --&gt; FINISHED_ABORTED: ユーザーによる中止
    RUNNING --&gt; FINISHED_ERROR: エラー発生
</pre>

<h2 id="update_from_output-フロー"><a class="header" href="#update_from_output-フロー">update_from_output() フロー</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:1241</code></p>
<pre><code>update_from_output(scheduler_output, model_runner_output)
  → dict[int, EngineCoreOutputs]

処理:
  for req_id in scheduler_output.scheduled requests:
    # Speculative Decoding リジェクション処理
    #   → 不採用トークン分 num_computed_tokens を巻き戻し

    # 生成トークンをリクエストに追加
    new_token_ids, stopped = _update_request_with_output(request, tokens)

    # 完了判定
    if stopped:
      finish_reason = request.get_finished_reason()
      _free_request(request)  # ブロック解放

    # EngineCoreOutput を構築
    outputs[client_index].append(EngineCoreOutput(
      request_id, new_token_ids, finish_reason, logprobs, ...
    ))

  return {client_index: EngineCoreOutputs(outputs=outs) for ...}
</code></pre>
<h2 id="設定-5"><a class="header" href="#設定-5">設定</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>max_num_seqs</code></td><td>設定依存</td><td>同時実行リクエスト数上限</td></tr>
<tr><td><code>max_num_batched_tokens</code></td><td>設定依存</td><td>ステップあたりのトークン予算上限</td></tr>
<tr><td><code>enable_chunked_prefill</code></td><td>設定依存</td><td>Chunked Prefillの有効化</td></tr>
<tr><td><code>long_prefill_token_threshold</code></td><td>0</td><td>長プロンプト分割閾値（0=無効）</td></tr>
<tr><td><code>scheduling_policy</code></td><td>Priority</td><td>プリエンプション選択ポリシー</td></tr>
</tbody>
</table>
</div>
<h2 id="呼び出しフロー-5"><a class="header" href="#呼び出しフロー-5">呼び出しフロー</a></h2>
<pre><code>EngineCore.add_request(request)
  → scheduler.add_request(request)       # WAITINGキューに登録

EngineCore.step()
  ├─ scheduler.schedule()                 # → SchedulerOutput
  │   ├─ kv_cache_manager.get_computed_blocks()  # プレフィックスキャッシュ
  │   └─ kv_cache_manager.allocate_slots()       # ブロック割り当て
  ├─ executor.execute_model(scheduler_output)     # GPU実行
  └─ scheduler.update_from_output(output)         # → EngineCoreOutputs
</code></pre>
<h2 id="関連ドキュメント-11"><a class="header" href="#関連ドキュメント-11">関連ドキュメント</a></h2>
<ul>
<li><a href="#kvcachemanager-サマリー">KVCacheManager</a></li>
<li><a href="#enginecore-サマリー">EngineCore</a></li>
<li><a href="#エントリポイント-asyncllm--llm-サマリー">エントリポイント</a></li>
<li><a href="#テキスト推論データフロー">データフロー</a></li>
<li><a href="#用語集">用語集: Continuous Batching, Unified Compute Model</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cacheblend-github-議論調査"><a class="header" href="#cacheblend-github-議論調査">CacheBlend GitHub 議論調査</a></h1>
<p><strong>調査日</strong>: 2026-02-14
<strong>対象リポジトリ</strong>: vllm-project/vllm, LMCache/LMCache, LMCache/LMCache-Ascend</p>
<h2 id="エグゼクティブサマリー"><a class="header" href="#エグゼクティブサマリー">エグゼクティブサマリー</a></h2>
<p>CacheBlendは、プレフィックス一致に限定せず、部分的・非連続的なKVキャッシュの再利用を可能にする技術（<a href="https://arxiv.org/abs/2405.16444">論文: arXiv:2405.16444</a>）。LMCacheプロジェクトがvLLM向けの実装を提供しているが、<strong>vLLM本体への統合はまだ実現していない</strong>。</p>
<p>最大の未解決課題は <strong>オンライン推論（<code>vllm serve</code>）でのCacheBlend利用</strong>であり、2026年2月時点でも安定した動作は実現されていない。オフライン（<code>LLM.generate()</code>直接呼び出し）では動作するが、HTTP APIを通じた利用では複数の技術的障壁がある。</p>
<h2 id="主要な議論論点"><a class="header" href="#主要な議論論点">主要な議論・論点</a></h2>
<h3 id="1-vllm本体への-generalized-kv-cache-reuse-rfc"><a class="header" href="#1-vllm本体への-generalized-kv-cache-reuse-rfc">1. vLLM本体への Generalized KV Cache Reuse RFC</a></h3>
<ul>
<li><strong>Issue</strong>: <a href="https://github.com/vllm-project/vllm/issues/25950">vllm-project/vllm#25950</a> (open)</li>
<li><strong>状態</strong>: 停滞中（staleラベル後にunstaleされたが、実装は未着手）</li>
</ul>
<p>プレフィックス一致に限定しない一般化されたKVキャッシュ再利用をvLLMに組み込む提案。3領域の変更を提案:</p>
<ol>
<li><strong>Attention Kernel</strong>: per-tokenマスキングパラメータの追加（FlashInferは既にサポート）</li>
<li><strong>KV Connector</strong>: 成功ビットマップの返却（現在のsequentialなprefix長ではなく）</li>
<li><strong>Scheduler</strong>: 穴あきKVキャッシュを持つトークンの適切な処理</li>
</ol>
<p><strong>2026年1月12日の重要な進展</strong>: カーネルやグルーロジックを変更せずに実装する方法を発見。<strong>プロンプトを複数のサブリクエストに分割してバッチ内で処理する</strong>アプローチ。コードは「soon」とされたが、まだ公開されていない。</p>
<p>LMCacheチーム（@ApostaC）は「CacheBlendは既にLMCacheに実装済み」と返答。提案者（@iddo10）は「LMCacheの実装は別のforwardパスを書いているが、この提案はvLLM本体に組み込む」と差別化を主張。</p>
<h3 id="2-vllm-serve-でのcacheblendサポートオンライン推論-最重要"><a class="header" href="#2-vllm-serve-でのcacheblendサポートオンライン推論-最重要">2. <code>vllm serve</code> でのCacheBlendサポート（オンライン推論） [最重要]</a></h3>
<ul>
<li><strong>Issue</strong>: <a href="https://github.com/LMCache/LMCache/issues/1936">LMCache#1936</a> (open)</li>
<li><strong>関連</strong>: <a href="https://github.com/LMCache/LMCache/issues/1136">#1136</a>, <a href="https://github.com/LMCache/LMCache/issues/1290">#1290</a>, <a href="https://github.com/LMCache/LMCache/issues/1682">#1682</a> (いずれもclosed/stale)</li>
<li><strong>状態</strong>: 未解決。繰り返し報告されており、最も重要な未解決課題</li>
</ul>
<p>CacheBlendは現在 <code>LLM.generate()</code> によるオフライン推論でのみ動作。<code>vllm serve</code> 経由のHTTP APIでは以下の技術的障壁がある:</p>
<ol>
<li><strong>トークン化の不一致</strong>: <code>blend_special_str</code>（例: <code>" # #"</code>）が前後のコンテキストにより異なるトークンIDに変換される。HTTP API経由ではトークン化を制御不可能</li>
<li><strong><code>/v1/chat/completions</code> は <code>input_ids</code> を受け付けない</strong>: セグメント境界の正確な指定が不可能</li>
<li><strong><code>/v1/completions</code> で <code>input_ids</code> を渡してもキャッシュのロードが発生しない</strong>: ストアは行われるがリユースなし</li>
<li><strong>vLLM GPUワーカーへのパッチが必要</strong>: <code>gpu_worker.py</code> の手動修正が必要で、バージョン間で互換性が壊れる</li>
</ol>
<p><strong>ワークアラウンド</strong>（2026年2月3日 @rick-heig）: Qwen3ファミリーでは <code>&lt;|file_sep|&gt;</code> のような常に同じトークンIDに変換される特殊トークンをblend区切り文字として使うことで、テキストレベルでの操作が可能になる。</p>
<h3 id="3-cacheblend-v1の品質安定性バグ"><a class="header" href="#3-cacheblend-v1の品質安定性バグ">3. CacheBlend V1の品質・安定性バグ</a></h3>
<ul>
<li><strong>ガーブル出力</strong> (<a href="https://github.com/LMCache/LMCache/issues/2496">#2496</a>, open): 複数エントリ処理時、最初以外の出力が文字化け。GPU/CPUメモリのクリア不足の疑い</li>
<li><strong>キャッシュヒット後の保存漏れ</strong> (<a href="https://github.com/LMCache/LMCache/issues/2029">#2029</a>, open/stale): 部分ヒット時、新規計算トークンが保存されない → 後続リクエストのヒット率低下</li>
<li><strong>先頭ミス時の探索打ち切り</strong> (<a href="https://github.com/LMCache/LMCache/issues/2029">#2029</a>): 最初のチャンクがキャッシュミスすると後続チャンクの探索なし → RAGで致命的</li>
<li><strong>layerwiseモードでのKVキャッシュ破損</strong> (PR#2329コメント, 2026-02-03): layerwise有効時、温度0でも異なる応答</li>
</ul>
<h3 id="4-lmcache側の安定化pr"><a class="header" href="#4-lmcache側の安定化pr">4. LMCache側の安定化PR</a></h3>
<ul>
<li><strong><a href="https://github.com/LMCache/LMCache/pull/762">PR#762</a></strong> (2025-06マージ): CacheBlend V1初期実装</li>
<li><strong><a href="https://github.com/LMCache/LMCache/pull/2329">PR#2329</a></strong> (open, コンフリクトあり): layerwise/blendingエッジケース修正 + vLLMパッチヘルパー。テスト中にlayerwiseのKVキャッシュ破損が新たに報告されており、マージ見通し不明</li>
</ul>
<h3 id="5-バージョン互換性"><a class="header" href="#5-バージョン互換性">5. バージョン互換性</a></h3>
<p>LMCache-Ascendチームが整理した互換性マトリクス (<a href="https://github.com/LMCache/LMCache-Ascend/issues/154">#154</a>):</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>vLLM</th><th>LMCache</th><th>CacheBlend</th></tr>
</thead>
<tbody>
<tr><td>0.9.2</td><td>0.3.3 / 0.3.7</td><td>Production Ready</td></tr>
<tr><td>0.10.0</td><td>0.3.7</td><td>Not supported</td></tr>
<tr><td>0.11.0</td><td>0.3.7</td><td>Not supported</td></tr>
<tr><td>0.10.0</td><td>0.3.12</td><td>Production Ready</td></tr>
<tr><td>0.11.0</td><td>0.3.12</td><td>Production Ready</td></tr>
</tbody>
</table>
</div>
<p>※ Ascend NPU版マトリクス。GPU版LMCacheとは異なる可能性あり。</p>
<h2 id="タイムライン"><a class="header" href="#タイムライン">タイムライン</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>日付</th><th>イベント</th></tr>
</thead>
<tbody>
<tr><td>2025-06-07</td><td>CacheBlend V1 初期実装が LMCache にマージ (<a href="https://github.com/LMCache/LMCache/pull/762">PR#762</a>)</td></tr>
<tr><td>2025-07-24</td><td>オンライン推論での動作不良が初報告 (<a href="https://github.com/LMCache/LMCache/issues/1136">#1136</a>)</td></tr>
<tr><td>2025-08-05</td><td>トークン化不一致問題の根本原因が特定</td></tr>
<tr><td>2025-09-30</td><td>vLLM本体に Generalized KV Cache Reuse RFC提出 (<a href="https://github.com/vllm-project/vllm/issues/25950">#25950</a>)</td></tr>
<tr><td>2025-10-31</td><td><code>vllm serve</code> サポートの明確な Feature Request (<a href="https://github.com/LMCache/LMCache/issues/1936">#1936</a>)</td></tr>
<tr><td>2025-12-29</td><td>layerwise/blending修正PR提出 (<a href="https://github.com/LMCache/LMCache/pull/2329">PR#2329</a>)</td></tr>
<tr><td>2026-01-12</td><td>vLLM RFC#25950にてサブリクエスト分割アプローチ発見の報告</td></tr>
<tr><td>2026-01-27</td><td>ガーブル出力バグ報告 (<a href="https://github.com/LMCache/LMCache/issues/2496">#2496</a>)</td></tr>
<tr><td>2026-02-03</td><td>layerwise KVキャッシュ破損の新規報告 + 特殊トークンワークアラウンド提案</td></tr>
</tbody>
</table>
</div>
<h2 id="結論所見"><a class="header" href="#結論所見">結論・所見</a></h2>
<h3 id="オンライン推論vllm-serveの現状"><a class="header" href="#オンライン推論vllm-serveの現状">オンライン推論（<code>vllm serve</code>）の現状</a></h3>
<p><strong>動作しない</strong>。オフライン専用の状態が約8ヶ月続いている。根本的な問題はCacheBlendのセグメント区切りがトークンレベルの精密な制御を要求するのに対し、HTTP APIがテキストレベルの入力しか受け付けない点にある。</p>
<p>2つのアプローチが存在するが、いずれも未完成:</p>
<ol>
<li><strong>LMCache側のアプローチ</strong>: vLLMのworkerにパッチを当てて対応。バージョン間の互換性維持が困難</li>
<li><strong>vLLM本体側のアプローチ</strong>: RFC#25950のサブリクエスト分割方式。コード未公開。実現すればvLLM本体の機能としてCacheBlendが使えるようになる可能性があるが、不透明</li>
</ol>
<h3 id="プラグイン開発への示唆"><a class="header" href="#プラグイン開発への示唆">プラグイン開発への示唆</a></h3>
<p>CacheBlendの現状は、vLLMのKV Connectorインターフェースの限界を示している:</p>
<ul>
<li>現在のKV ConnectorはPrefix Caching前提（連続ブロックの転送）</li>
<li>非連続キャッシュ再利用にはScheduler・Attention Kernelレベルの変更が必要</li>
<li>RFC#25950のサブリクエスト分割アプローチが実現すれば、KV Connector層のみで対応可能になる</li>
</ul>
<p>独自プラグイン作成を検討する場合、CacheBlendの統合方式の行方は重要な参考情報となる。</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ecconnector-github-議論調査レポート"><a class="header" href="#ecconnector-github-議論調査レポート">ECConnector GitHub 議論調査レポート</a></h1>
<blockquote>
<p><strong>ステータス</strong>: 調査完了
<strong>作成日</strong>: 2026-02-14
<strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>関連ドキュメント</strong>: <a href="#encodercache-永続化と階層キャッシュ-調査報告">EncoderCache 永続化と階層キャッシュ</a>、<a href="#kvcachemanager-サマリー">KVCacheManager</a></p>
</blockquote>
<hr>
<h2 id="1-概要"><a class="header" href="#1-概要">1. 概要</a></h2>
<p>ECConnector は、マルチモーダルモデルの <strong>Encode-Prefill-Decode (EPD) 分離推論</strong> を実現するために導入されたインフラストラクチャである。エンコーダ出力（画像埋め込みなど）を別プロセス間で転送・キャッシュするための抽象インタフェース <code>ECConnectorBase</code> を定義し、プラグイン可能な設計になっている。</p>
<p>2025年11月に基盤PR (<a href="https://github.com/vllm-project/vllm/pull/25233">#25233</a>) がマージされた後、encoder-onlyモード、リモートキャッシュチェック最適化、ec_both ロールが順次マージされた。現在は<strong>高性能転送バックエンド</strong>（SHMConnector vs Mooncake ECConnector）の方向性が議論中で、メンテナーは Mooncake ベースの統一的なソリューションを志向している。</p>
<h2 id="2-マージ済み-pr確定した設計"><a class="header" href="#2-マージ済み-pr確定した設計">2. マージ済み PR（確定した設計）</a></h2>
<h3 id="21-基盤-epd分離-25233--2025-11-12-マージ-verified"><a class="header" href="#21-基盤-epd分離-25233--2025-11-12-マージ-verified">2.1 基盤: EPD分離 (#25233) — 2025-11-12 マージ [VERIFIED]</a></h3>
<p><strong>主導</strong>: Chenguang Zheng (fake0fan)</p>
<p>ECConnectorBase はスケジューラ側とワーカー側の責務を明確に分離:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>責務</th><th>メソッド</th><th>実行場所</th></tr>
</thead>
<tbody>
<tr><td>キャッシュ存在確認</td><td><code>check_caches_exist</code></td><td>スケジューラ</td></tr>
<tr><td>割当後状態更新</td><td><code>update_state_after_alloc</code></td><td>スケジューラ</td></tr>
<tr><td>メタデータ構築</td><td><code>build_connector_meta</code></td><td>スケジューラ</td></tr>
<tr><td>リクエスト完了通知</td><td><code>request_finished</code></td><td>スケジューラ</td></tr>
<tr><td>キャッシュ読み込み</td><td><code>start_load_caches</code></td><td>ワーカー</td></tr>
<tr><td>キャッシュ保存</td><td><code>save_caches</code></td><td>ワーカー</td></tr>
</tbody>
</table>
</div>
<p>参照実装として <code>ECExampleConnector</code>（safetensors でディスク保存）を同梱。<code>ec_connector_module_path</code> によるOOTプラグインロードもサポート。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_connector/</code></p>
<h3 id="22-encoder-only-モード-30242--2025-12-22-マージ-verified"><a class="header" href="#22-encoder-only-モード-30242--2025-12-22-マージ-verified">2.2 Encoder-only モード (#30242) — 2025-12-22 マージ [VERIFIED]</a></h3>
<p>EPD分離時にエンコーダ専用インスタンスがLLM重みまでロードしてしまう問題を解決。<code>--convert mm_encoder_only</code> オプションを追加。</p>
<ul>
<li>当初 <code>VisionOnly</code> という名称 → オーディオ等も対象のため <code>MMEncoderOnly</code> に変更</li>
<li>A100でのGPUメモリ使用量: <strong>44GB → 3.7GB</strong> に削減</li>
<li><code>adapters.py</code> の pooling モデルと同様のアプローチ</li>
</ul>
<h3 id="23-リモートキャッシュチェック最適化-32585--2026-01-22-マージ-verified"><a class="header" href="#23-リモートキャッシュチェック最適化-32585--2026-01-22-マージ-verified">2.3 リモートキャッシュチェック最適化 (#32585) — 2026-01-22 マージ [VERIFIED]</a></h3>
<p>スケジューラでのリモートエンコーダキャッシュ存在確認を最適化。EPD追跡RFCのタスクリストの一環。</p>
<h3 id="24-ec_both-ロール-34182--2026-02-10-マージ-verified"><a class="header" href="#24-ec_both-ロール-34182--2026-02-10-マージ-verified">2.4 ec_both ロール (#34182) — 2026-02-10 マージ [VERIFIED]</a></h3>
<p><strong>貢献</strong>: Qi Wang (furionw, NVIDIA)</p>
<p>従来は producer/consumer のどちらか一方のみだったが、集約ノード（EPD一体型）でのエンベディングオフロード（GPU → CPU → GPU）を可能にするため <code>ec_both</code> ロールを追加。KV Connector の both ロールと同様の概念。変更は3ファイル・計10行程度の小規模PR。</p>
<h2 id="3-進行中の議論"><a class="header" href="#3-進行中の議論">3. 進行中の議論</a></h2>
<h3 id="31-高性能転送バックエンド-shmconnector-vs-mooncake-inferred"><a class="header" href="#31-高性能転送バックエンド-shmconnector-vs-mooncake-inferred">3.1 高性能転送バックエンド: SHMConnector vs Mooncake [INFERRED]</a></h3>
<p><strong>関連 PR</strong>: <a href="https://github.com/vllm-project/vllm/pull/33714">#33714</a> (open)</p>
<p>PiratePai が共有メモリ + PyTorch RPC ベースの <code>SHMConnector</code> を提案:</p>
<ul>
<li>RTX 4090 での1E-1PD構成で ExampleConnector より <strong>8-9% の TTFT 改善</strong> を実証</li>
</ul>
<p>しかしメンテナー (NickLucche) は「vllm のツリー内コネクタは少数の高品質ソリューションに絞りたい」と慎重姿勢。fake0fan は <strong>Mooncake ベースの統一 ECConnector</strong> を提案し、RDMA/NVLink/TCP フォールバックの複数バックエンドを1つのコネクタで対応する構想を示している。</p>
<h4 id="技術的課題-エンコーダキャッシュの固定アドレス問題"><a class="header" href="#技術的課題-エンコーダキャッシュの固定アドレス問題">技術的課題: エンコーダキャッシュの固定アドレス問題</a></h4>
<p>エンコーダキャッシュテンソルは KVキャッシュと異なり事前割り当てされた固定アドレスを持たない。NickLucche は2つの選択肢を提示:</p>
<ol>
<li><strong>中間バッファ方式</strong>: 転送前にコピーする</li>
<li><strong>事前割り当て方式</strong>: ECConnector 使用時にはエンコーダキャッシュを固定アドレスバッファに切り替える（現在の dict ベースを置き換え）</li>
</ol>
<h3 id="32-nixlセグメントツリーベースのepd分離-26009-shallow"><a class="header" href="#32-nixlセグメントツリーベースのepd分離-26009-shallow">3.2 NIXL/セグメントツリーベースのEPD分離 (#26009) [SHALLOW]</a></h3>
<p>H. Jhoo (MerHS) が NIXL コミュニケータを使った P2P 直接通信方式を提案。セグメントツリーベースのエンコーダキャッシュマネージャ（ベストフィット割り当て）、動的ロールスイッチング、エンコーダ専用スケジューラなど大規模な変更を含む。</p>
<ul>
<li>90日間の非活動で stale クローズ（2026-02-07）</li>
<li>fake0fan の fork に NIXL ECConnector の作業が継続中</li>
</ul>
<h3 id="33-マルチモーダル前処理の重複排除-27094-inferred"><a class="header" href="#33-マルチモーダル前処理の重複排除-27094-inferred">3.3 マルチモーダル前処理の重複排除 (#27094) [INFERRED]</a></h3>
<p>EPD分離時に全ワーカーで画像前処理が繰り返される問題:</p>
<ul>
<li>プロファイリングで前処理が推論時間の <strong>約50%</strong>（Qwen2.5-VL-3B で 143ms × 3回 ≈ 430ms）を占める</li>
<li><code>image_meta</code> タイプの導入を提案し、エンコーダ処理後は形状メタデータのみを後続ワーカーに転送する案</li>
<li>DarkLight1337 は前処理をエンコーダプロセスに完全移動する方針を示唆</li>
</ul>
<h3 id="34-ecキャッシュの解放メカニズム-32659-shallow"><a class="header" href="#34-ecキャッシュの解放メカニズム-32659-shallow">3.4 ECキャッシュの解放メカニズム (#32659) [SHALLOW]</a></h3>
<p>EPD追跡RFCのタスクの一つ。エンコーダキャッシュの明示的解放メカニズムが未実装で、リソースリークの原因となりうる。fake0fan の fork で作業中。</p>
<h2 id="4-タイムライン"><a class="header" href="#4-タイムライン">4. タイムライン</a></h2>
<pre class="mermaid">gantt
    title ECConnector 開発タイムライン
    dateFormat YYYY-MM-DD
    section 基盤
    初期EPD PR (#21740)          :done, 2025-09-01, 2025-09-19
    EPD基盤 (#25233)             :done, 2025-09-19, 2025-11-12
    section 機能拡張
    NIXL方式提案 (#26009)         :done, 2025-10-01, 2026-02-07
    Encoder-only (#30242)        :done, 2025-12-01, 2025-12-22
    キャッシュチェック最適化 (#32585) :done, 2026-01-15, 2026-01-22
    ec_both ロール (#34182)      :done, 2026-02-03, 2026-02-10
    section 進行中
    EPD追跡RFC (#32659)          :active, 2026-01-20, 2026-03-01
    SHMConnector (#33714)        :active, 2026-02-03, 2026-03-01
    MM前処理重複排除 (#27094)     :active, 2025-10-17, 2026-03-01
</pre>

<h2 id="5-未解決の課題一覧"><a class="header" href="#5-未解決の課題一覧">5. 未解決の課題一覧</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>課題</th><th>Issue/PR</th><th>状態</th><th>備考</th></tr>
</thead>
<tbody>
<tr><td>高性能転送バックエンド</td><td><a href="https://github.com/vllm-project/vllm/pull/33714">#33714</a></td><td>方向性議論中</td><td>Mooncake統一案が有力</td></tr>
<tr><td>ECキャッシュ解放</td><td><a href="https://github.com/vllm-project/vllm/issues/32659">#32659</a></td><td>作業中</td><td>fake0fan fork</td></tr>
<tr><td>EPDプロキシ最適化</td><td>#31017</td><td>未着手</td><td>—</td></tr>
<tr><td>MM前処理重複排除</td><td><a href="https://github.com/vllm-project/vllm/issues/27094">#27094</a></td><td>RFC</td><td>前処理の50%コスト</td></tr>
<tr><td>エンコーダキャッシュ事前割り当て</td><td>#33714 コメント</td><td>提案段階</td><td>dict→固定バッファ</td></tr>
<tr><td>NIXL ECConnector</td><td>#32659 タスク</td><td>fork作業中</td><td>—</td></tr>
</tbody>
</table>
</div>
<h2 id="6-主要コントリビューター"><a class="header" href="#6-主要コントリビューター">6. 主要コントリビューター</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>名前</th><th>GitHub</th><th>役割</th></tr>
</thead>
<tbody>
<tr><td>Chenguang Zheng</td><td>fake0fan</td><td>ECConnector基盤設計者、EPDフォローアップ主導</td></tr>
<tr><td>NickLucche</td><td>NickLucche</td><td>vLLM Collaborator、アーキテクチャ方針決定者</td></tr>
<tr><td>Cyrus Leung</td><td>DarkLight1337</td><td>vLLM Member、マルチモーダルレビューアー</td></tr>
<tr><td>Qi Wang</td><td>furionw (NVIDIA)</td><td>ec_both ロール貢献</td></tr>
<tr><td>H. Jhoo</td><td>MerHS</td><td>NIXL方式提案者</td></tr>
<tr><td>PiratePai</td><td>PiratePai</td><td>SHMConnector 実装者</td></tr>
</tbody>
</table>
</div>
<h2 id="7-プラグイン開発への示唆-inferred"><a class="header" href="#7-プラグイン開発への示唆-inferred">7. プラグイン開発への示唆 [INFERRED]</a></h2>
<p>独自ECConnectorプラグインを開発する場合の留意点:</p>
<ol>
<li><strong>現在の安定インタフェース</strong>: <code>ECConnectorBase</code> の6メソッドは安定しているが、今後のリファクタリング（事前割り当て方式への移行）で変更の可能性あり</li>
<li><strong>OOTプラグイン</strong>: <code>ec_connector_module_path</code> で外部モジュールロード可能。<code>ECConnectorFactory</code> 経由で登録</li>
<li><strong>転送方式の選択</strong>: Mooncake方式が統一バックエンドとして推奨される方向。独自実装よりMooncakeの上に構築する方が将来的に有利</li>
<li><strong>エンコーダキャッシュ管理の変更予定</strong>: dict ベースから事前割り当て型への移行が検討中。プラグインはこの変更に備えるべき</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="encodercache-永続化と階層キャッシュ-調査報告"><a class="header" href="#encodercache-永続化と階層キャッシュ-調査報告">EncoderCache 永続化と階層キャッシュ: 調査報告</a></h1>
<blockquote>
<p><strong>ステータス</strong>: 調査完了 — ECConnector 既存インフラの発見により設計方針が確定
<strong>作成日</strong>: 2026-02-14
<strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>関連ドキュメント</strong>: <a href="#gemma3-ビジョンパイプライン-キャッシュ機構-medium-verified">Gemma3 ビジョンパイプライン: キャッシュ機構</a>、<a href="#バックエンド-マルチモーダル処理パス-medium-verified">マルチモーダル バックエンド MM 処理</a></p>
</blockquote>
<hr>
<h2 id="1-背景と動機"><a class="header" href="#1-背景と動機">1. 背景と動機</a></h2>
<h3 id="現状の-encodercache"><a class="header" href="#現状の-encodercache">現状の EncoderCache</a></h3>
<p>vLLM の EncoderCache は、ビジョンエンコーダ（例: SiglipVisionModel + Projector）の GPU 上の出力テンソルをキャッシュする。Gemma3 27B の場合、出力形状は <code>(N×256, 5376)</code> で、1 画像あたり約 2.6 MB（FP16）。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>項目</th><th>現状</th></tr>
</thead>
<tbody>
<tr><td>格納先</td><td>GPU メモリ（<code>gpu_model_runner.encoder_cache</code>）</td></tr>
<tr><td>キャッシュキー</td><td><code>mm_hash</code> or <code>{lora_name}:{mm_hash}</code></td></tr>
<tr><td>Eviction 方式</td><td><strong>FIFO</strong>（<code>OrderedDict.popitem(last=False)</code>）</td></tr>
<tr><td>容量設定</td><td><code>encoder_cache_size</code>（エンベディング数単位）</td></tr>
<tr><td>永続性</td><td>なし（プロセス終了で消失）</td></tr>
<tr><td>管理</td><td><code>EncoderCacheManager</code>（CPU 側論理管理）+ <code>encoder_cache</code> dict（GPU 側物理格納）</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py</code> (EncoderCacheManager)、<code>target/vllm/vllm/v1/worker/gpu_model_runner.py:439</code> (encoder_cache dict)</p>
<h3 id="rag-ユースケースにおける課題"><a class="header" href="#rag-ユースケースにおける課題">RAG ユースケースにおける課題</a></h3>
<p>RAG では同一ドキュメント画像が異なるクエリから繰り返し参照される。</p>
<ol>
<li><strong>FIFO Eviction との相性の悪さ</strong>: 高頻度アクセス画像でも新しいエントリが来れば古い順に追い出される</li>
<li><strong>GPU メモリの有限性</strong>: <code>encoder_cache_size</code> を大きくしてもデコーダ KV Cache と競合</li>
<li><strong>再起動耐性がない</strong>: vLLM プロセス再起動で全キャッシュが消失</li>
</ol>
<h3 id="エンコーダ処理のコスト"><a class="header" href="#エンコーダ処理のコスト">エンコーダ処理のコスト</a></h3>
<p>EncoderCache がスキップする GPU 上の処理:</p>
<ul>
<li>SiglipVisionModel: Conv2d + position_embedding + <strong>27 層 Transformer Encoder</strong>（双方向 Attention）+ post_layernorm</li>
<li>Gemma3MultiModalProjector: AvgPool2d + GemmaRMSNorm + Linear(1152→5376)</li>
<li>split + flatten</li>
</ul>
<p>RAG コーパスが数千〜数万画像規模の場合、毎回の再計算コストは無視できない。</p>
<hr>
<h2 id="2-重要な発見-ecconnector-既存インフラ"><a class="header" href="#2-重要な発見-ecconnector-既存インフラ">2. 重要な発見: ECConnector 既存インフラ</a></h2>
<h3 id="kv-transfer-ではなく-ecconnector-が正解"><a class="header" href="#kv-transfer-ではなく-ecconnector-が正解">KV Transfer ではなく ECConnector が正解</a></h3>
<p>当初の仮説では「KV Transfer の枠組み（LMCache 等）を活用」する方針だったが、調査の結果、<strong>エンコーダキャッシュの外部ストレージ永続化のために設計された専用インフラ「ECConnector」が既に存在</strong>することが判明した。</p>
<h3 id="ecconnector-の全体像"><a class="header" href="#ecconnector-の全体像">ECConnector の全体像</a></h3>
<pre><code>target/vllm/vllm/distributed/ec_transfer/
  __init__.py                      -- get_ec_transfer(), has_ec_transfer()
  ec_transfer_state.py             -- グローバルシングルトン管理
  ec_connector/
    __init__.py
    base.py                        -- ECConnectorBase (抽象基底クラス)
    factory.py                     -- ECConnectorFactory (プラグイン登録)
    example_connector.py           -- ECExampleConnector (参照実装, 199行)
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_connector/base.py:59</code> (ECConnectorBase)</p>
<h3 id="ecconnectorbase-の抽象メソッド-verified"><a class="header" href="#ecconnectorbase-の抽象メソッド-verified">ECConnectorBase の抽象メソッド [VERIFIED]</a></h3>
<p><code>ECConnectorBase</code> は <code>KVConnectorBase_V1</code> とは<strong>完全に独立</strong>した抽象基底クラス。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>分類</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>start_load_caches(encoder_cache, **kwargs)</code></td><td>Worker</td><td>外部ストレージから <code>encoder_cache</code> dict にテンソルをロード</td></tr>
<tr><td><code>save_caches(encoder_cache, mm_hash, **kwargs)</code></td><td>Worker</td><td><code>encoder_cache</code> から外部ストレージにテンソルを保存</td></tr>
<tr><td><code>has_cache_item(identifier)</code></td><td>Scheduler</td><td>外部ストレージにキャッシュが存在するか確認</td></tr>
<tr><td><code>update_state_after_alloc(request, index)</code></td><td>Scheduler</td><td>アロケーション後の内部状態更新</td></tr>
<tr><td><code>build_connector_meta(scheduler_output)</code></td><td>Scheduler</td><td>Scheduler → Worker 間のメタデータ構築</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_connector/base.py:126-224</code></p>
<h3 id="ecconnector-の統合ポイント-verified"><a class="header" href="#ecconnector-の統合ポイント-verified">ECConnector の統合ポイント [VERIFIED]</a></h3>
<p>既にGPUModelRunnerとSchedulerに統合済み:</p>
<p><strong>Scheduler 側</strong> (<code>target/vllm/vllm/v1/core/sched/scheduler.py:1197-1203</code>):</p>
<pre><code class="language-python">if self.ec_connector is not None and self.ec_connector.has_cache_item(
    item_identifier
):
    mm_hashes_to_schedule.add(item_identifier)
    external_load_encoder_input.append(i)
    num_embeds_to_schedule += num_encoder_embeds
    continue
</code></pre>
<p>→ ECConnector にキャッシュが存在する場合、エンコーダ計算バジェットを消費せず、ロード予約のみ行う。</p>
<p><strong>Worker 側</strong> (<code>target/vllm/vllm/v1/worker/gpu_model_runner.py:2444-2445</code>):</p>
<pre><code class="language-python">self.encoder_cache[mm_hash] = output
self.maybe_save_ec_to_connector(self.encoder_cache, mm_hash)
</code></pre>
<p>→ エンコーダ実行後、結果を GPU dict に格納すると同時に ECConnector にも保存。</p>
<p><strong>Worker 側コンテキストマネージャ</strong> (<code>target/vllm/vllm/v1/worker/ec_connector_model_runner_mixin.py:62-85</code>):</p>
<pre><code class="language-python">ec_connector.bind_connector_metadata(scheduler_output.ec_connector_metadata)
if not ec_connector.is_producer:
    ec_connector.start_load_caches(encoder_cache, **kwargs)
try:
    yield output   # _execute_mm_encoder() + _gather_mm_embeddings() が実行される
finally:
    output.finished_sending, output.finished_recving = (
        ec_connector.get_finished(scheduler_output.finished_req_ids)
    )
    ec_connector.clear_connector_metadata()
</code></pre>
<p>→ <code>start_load_caches()</code> でストレージからロード → エンコーダ実行（ロード済みはスキップ）→ 完了通知。</p>
<h3 id="ectransferconfig-verified"><a class="header" href="#ectransferconfig-verified">ECTransferConfig [VERIFIED]</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/config/ec_transfer.py</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>ec_connector</code></td><td><code>str | None</code></td><td>None</td><td>コネクタ名（例: <code>"ECExampleConnector"</code>）</td></tr>
<tr><td><code>ec_role</code></td><td><code>ECRole | None</code></td><td>None</td><td><code>"ec_producer"</code> or <code>"ec_consumer"</code></td></tr>
<tr><td><code>ec_connector_extra_config</code></td><td><code>dict</code></td><td><code>{}</code></td><td>コネクタ固有の追加設定</td></tr>
<tr><td><code>ec_connector_module_path</code></td><td><code>str | None</code></td><td>None</td><td>動的ロード用モジュールパス</td></tr>
<tr><td><code>engine_id</code></td><td><code>str | None</code></td><td>uuid4 自動生成</td><td>エンジンID</td></tr>
</tbody>
</table>
</div>
<p>起動時パラメータ例: <code>--ec-connector ECExampleConnector --ec-role ec_producer --ec-connector-extra-config '{"shared_storage_path": "/mnt/cache"}'</code></p>
<h3 id="ecconnectorfactory-verified"><a class="header" href="#ecconnectorfactory-verified">ECConnectorFactory [VERIFIED]</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_connector/factory.py</code></p>
<ul>
<li><code>register_connector(name, module_path, class_name)</code> で遅延ロード登録</li>
<li><code>ec_connector_module_path</code> による動的ロード（外部モジュール対応）</li>
<li>現在の登録済みコネクタ: <code>ECExampleConnector</code> のみ</li>
</ul>
<hr>
<h2 id="3-ecexampleconnector-参照実装の分析-verified"><a class="header" href="#3-ecexampleconnector-参照実装の分析-verified">3. ECExampleConnector 参照実装の分析 [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_connector/example_connector.py</code></p>
<h3 id="概要-18"><a class="header" href="#概要-18">概要</a></h3>
<p>safetensors を使ってディスクにエンコーダ出力テンソルを保存/読込するデバッグ用実装。全 199 行。</p>
<h3 id="ストレージ構造-1"><a class="header" href="#ストレージ構造-1">ストレージ構造</a></h3>
<pre><code>{shared_storage_path}/
  {mm_hash}/
    encoder_cache.safetensors
</code></pre>
<h3 id="保存-save_caches-l98-118"><a class="header" href="#保存-save_caches-l98-118">保存 (save_caches, L98-118)</a></h3>
<pre><code class="language-python">def save_caches(self, encoder_cache, mm_hash, **kwargs) -&gt; None:
    if not self.is_producer:
        return
    filename = self._generate_filename_debug(mm_hash)
    ec_cache = encoder_cache[mm_hash]
    tensors = {"ec_cache": ec_cache.detach().cpu()}  # GPU→CPU コピー
    safetensors.torch.save_file(tensors, filename)
</code></pre>
<ul>
<li>Producer ロールの場合のみ保存</li>
<li><code>detach().cpu()</code> で GPU テンソルを CPU に移動してからシリアライズ</li>
<li><strong>テンソル形状に一切依存しない</strong></li>
</ul>
<h3 id="ロード-start_load_caches-l63-96"><a class="header" href="#ロード-start_load_caches-l63-96">ロード (start_load_caches, L63-96)</a></h3>
<pre><code class="language-python">def start_load_caches(self, encoder_cache, **kwargs) -&gt; None:
    metadata = self._get_connector_metadata()
    for mm_data in metadata.mm_datas:
        if mm_data.mm_hash in encoder_cache:
            continue  # 既に GPU dict にあればスキップ
        filename = self._generate_filename_debug(mm_data.mm_hash)
        ec_cache = safetensors.torch.load_file(filename, device=...)["ec_cache"]
        encoder_cache[mm_data.mm_hash] = ec_cache  # dict に直接格納
</code></pre>
<ul>
<li>メタデータ（Scheduler が構築）に基づいてロード対象を決定</li>
<li><code>encoder_cache</code> dict に直接格納 → <code>_gather_mm_embeddings()</code> でそのまま読める</li>
</ul>
<h3 id="存在チェック-has_cache_item-l120-133"><a class="header" href="#存在チェック-has_cache_item-l120-133">存在チェック (has_cache_item, L120-133)</a></h3>
<pre><code class="language-python">def has_cache_item(self, identifier: str) -&gt; bool:
    return self._found_match_for_mm_data(identifier)
    # → os.path.exists(filename)
</code></pre>
<ul>
<li>ファイルの存在確認のみ（同期的）</li>
</ul>
<h3 id="メタデータ管理"><a class="header" href="#メタデータ管理">メタデータ管理</a></h3>
<p><code>ECExampleConnectorMetadata</code> (L35-42): ロードすべき <code>mm_hash</code> と <code>num_token</code> のリスト。</p>
<p><code>update_state_after_alloc()</code> (L135-146): Scheduler が allocate() 後に呼び出し、<code>_mm_datas_need_loads</code> にロード対象を追加。</p>
<p><code>build_connector_meta()</code> (L148-164): <code>_mm_datas_need_loads</code> からメタデータを構築し、Worker に伝達。呼び出し後にクリア。</p>
<hr>
<h2 id="4-kv-transfer-との比較-verified"><a class="header" href="#4-kv-transfer-との比較-verified">4. KV Transfer との比較 [VERIFIED]</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>評価項目</th><th>KV Transfer</th><th>ECConnector</th></tr>
</thead>
<tbody>
<tr><td><strong>設計目的</strong></td><td>デコーダ KV Cache の転送・永続化</td><td>エンコーダ出力テンソルの転送・永続化</td></tr>
<tr><td><strong>テンソル粒度</strong></td><td>レイヤー別、ブロック単位、トークン粒度</td><td><code>mm_hash</code> 単位、任意形状テンソル</td></tr>
<tr><td><strong>テンソル形状依存</strong></td><td>あり (<code>num_layer, 2, chunk_size, num_kv_heads, head_size</code>)</td><td><strong>なし</strong></td></tr>
<tr><td><strong>エンコーダ出力への適合性</strong></td><td>不適合</td><td><strong>最適</strong></td></tr>
<tr><td><strong>既存統合ポイント</strong></td><td>Attention 層デコレータ経由</td><td>GPUModelRunner の <code>_execute_mm_encoder</code> 直後</td></tr>
<tr><td><strong>新規実装量</strong></td><td>大（7 abstract メソッド + KV 概念適合）</td><td><strong>小</strong>（5 abstract メソッド、参照実装 199 行）</td></tr>
<tr><td><strong>ストレージ実装</strong></td><td>LMCache/NIXL/Mooncake（全て KV 前提）</td><td>Example（safetensors/ディスク）、拡張容易</td></tr>
</tbody>
</table>
</div>
<p><strong>結論</strong>: エンコーダ出力テンソルの永続化には ECConnector を使うべき。KV Transfer はデコーダ KV Cache に特化しており、エンコーダ出力の形状・粒度に合わない。</p>
<p>LMCache の KV 形状ハードコード箇所:</p>
<ul>
<li><code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py:477</code>
<pre><code class="language-python">kv_shape = (num_layer, 1 if use_mla else 2, chunk_size, num_kv_heads, head_size)
</code></pre>
</li>
</ul>
<hr>
<h2 id="5-fifo--lru-変更の具体的設計"><a class="header" href="#5-fifo--lru-変更の具体的設計">5. FIFO → LRU 変更の具体的設計</a></h2>
<h3 id="現状の-fifo-実装-verified"><a class="header" href="#現状の-fifo-実装-verified">現状の FIFO 実装 [VERIFIED]</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py</code></p>
<p><strong>データ構造</strong>:</p>
<pre><code class="language-python"># L72-77
self.cached: dict[str, set[str]] = {}           # mm_hash → {request_ids}
self.freeable: OrderedDict[str, int] = OrderedDict()  # mm_hash → num_embeds (挿入順)
self.freed: list[str] = []                       # evict 済みリスト
</code></pre>
<p><strong>FIFO の核心</strong> (L173-177):</p>
<pre><code class="language-python">while num_embeds &gt; self.num_free_slots:
    mm_hash, num_free_embeds = self.freeable.popitem(last=False)  # 最も古いエントリから
    del self.cached[mm_hash]
    self.freed.append(mm_hash)
    self.num_free_slots += num_free_embeds
</code></pre>
<p><code>OrderedDict.popitem(last=False)</code> で<strong>最初に挿入された（＝最も早く参照解放された）エントリ</strong>から Evict。</p>
<h3 id="現状の-fifo-が-lru-と異なる点"><a class="header" href="#現状の-fifo-が-lru-と異なる点">現状の FIFO が LRU と異なる点</a></h3>
<p>FIFO は「最も早く <code>freeable</code> に追加されたものから Evict」する。LRU は「最も長期間アクセスされていないものから Evict」する。</p>
<p>差が出るケース:</p>
<ol>
<li>画像 A が freeable に入る（参照解放）</li>
<li>画像 B が freeable に入る</li>
<li>画像 A が再度参照される → freeable から取り出されて active に戻る</li>
<li>画像 A が再度 freeable に入る → <strong>FIFO: 末尾に追加（最新扱い）、LRU: 末尾に追加（最新扱い）</strong></li>
</ol>
<p>実は、<strong>現状の FIFO は「参照解放順」であり、再参照された画像は freeable の末尾に再挿入される</strong>ため、RAG での繰り返しアクセスパターンでは擬似 LRU として機能する部分もある。</p>
<p>しかし、<strong>active 状態（参照中）のエントリ間でのアクセス頻度は考慮されない</strong>。複数リクエストが同時に異なる画像を参照し、それらが一斉に freeable になった場合、「最後にアクセスされた時刻」ではなく「最後に参照解放された時刻」で順序が決まる。</p>
<h3 id="lru-への変更方法"><a class="header" href="#lru-への変更方法">LRU への変更方法</a></h3>
<p><strong>変更箇所</strong>: <code>encoder_cache_manager.py</code> の 1 ファイルのみ。Scheduler 側・GPUModelRunner 側の変更は不要。</p>
<p><strong>方法 A: アクセス時刻の追跡</strong>（推奨）</p>
<pre><code class="language-python">class EncoderCacheManager:
    def __init__(self, cache_size: int):
        # ... 既存フィールド ...
        self._access_order: dict[str, int] = {}  # mm_hash → monotonic counter
        self._access_counter: int = 0

    def check_and_update_cache(self, request, input_id) -&gt; bool:
        mm_hash = request.mm_features[input_id].identifier
        if mm_hash not in self.cached:
            return False
        if not self.cached[mm_hash]:
            num_encoder_embeds = self.freeable.pop(mm_hash)
            self.num_freeable_slots -= num_encoder_embeds
        self.cached[mm_hash].add(request.request_id)
        # ★ アクセス時刻を更新
        self._access_counter += 1
        self._access_order[mm_hash] = self._access_counter
        return True

    def free_encoder_input(self, request, input_id) -&gt; None:
        req_id = request.request_id
        mm_hash = request.mm_features[input_id].identifier
        if not self.cached.get(mm_hash, None):
            return
        self.cached[mm_hash].discard(req_id)
        if not self.cached[mm_hash]:
            num_encoder_embeds = request.get_num_encoder_embeds(input_id)
            self.freeable[mm_hash] = num_encoder_embeds
            self.num_freeable_slots += num_encoder_embeds
            # ★ アクセス時刻でソートされた位置に挿入
            # OrderedDictを再ソート: 古いアクセスが先頭に来るようにする
            self.freeable.move_to_end(mm_hash)  # 末尾に追加（最新アクセス）

    def allocate(self, request, input_id) -&gt; None:
        mm_hash = request.mm_features[input_id].identifier
        # ... 既存ロジック ...
        # ★ アクセス時刻を記録
        self._access_counter += 1
        self._access_order[mm_hash] = self._access_counter
</code></pre>
<p><strong>方法 B: 簡易 LRU（move_to_end のみ）</strong></p>
<p>現状の実装でも、<code>freeable</code> への再挿入は末尾に行われるため、ほぼ LRU として機能する。唯一の改善点は、<code>check_and_update_cache()</code> で freeable から復活する際のタイムスタンプ更新のみ。実質的に方法 A と同等の効果が得られる。</p>
<h3 id="変更の影響範囲"><a class="header" href="#変更の影響範囲">変更の影響範囲</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>変更</th></tr>
</thead>
<tbody>
<tr><td><code>encoder_cache_manager.py</code></td><td><code>check_and_update_cache()</code> と <code>free_encoder_input()</code> の 2 メソッド修正</td></tr>
<tr><td><code>can_allocate()</code></td><td>変更不要（<code>popitem(last=False)</code> は同じ）</td></tr>
<tr><td>Scheduler</td><td>変更不要（API は同じ）</td></tr>
<tr><td>GPUModelRunner</td><td>変更不要</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="6-階層キャッシュの実装設計"><a class="header" href="#6-階層キャッシュの実装設計">6. 階層キャッシュの実装設計</a></h2>
<h3 id="アーキテクチャ-8"><a class="header" href="#アーキテクチャ-8">アーキテクチャ</a></h3>
<pre><code>リクエスト到着
    │
    ▼
Scheduler: _try_schedule_encoder_inputs()
    │
    ├── check_and_update_cache() → L1 HIT (GPU dict) → スキップ
    │
    ├── L1 MISS → ec_connector.has_cache_item() → L2 HIT (Storage)
    │       │
    │       └── external_load_encoder_input に追加 → Worker でロード予約
    │
    └── L1/L2 MISS → encoder_inputs_to_schedule に追加 → エンコーダ計算
    │
    ▼
Worker: execute_model()
    │
    ├── start_load_caches() → L2 からテンソルを GPU dict にロード
    │
    ├── _execute_mm_encoder() → L1/L2 MISS 分のみエンコーダ実行
    │       └── save_caches() → 新規計算結果を L2 に保存
    │
    └── _gather_mm_embeddings() → GPU dict からテンソル取得
</code></pre>
<h3 id="2-層キャッシュの役割分担"><a class="header" href="#2-層キャッシュの役割分担">2 層キャッシュの役割分担</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th></th><th>L1: GPU dict（ホット）</th><th>L2: ECConnector（コールド）</th></tr>
</thead>
<tbody>
<tr><td>格納先</td><td>GPU メモリ</td><td>Redis / ディスク / NFS 等</td></tr>
<tr><td>容量</td><td>小（<code>encoder_cache_size</code>）</td><td>大（コーパス全体）</td></tr>
<tr><td>レイテンシ</td><td>ナノ秒</td><td>マイクロ〜ミリ秒</td></tr>
<tr><td>Eviction</td><td><strong>LRU</strong>（提案変更後）</td><td>TTL or LRU or なし</td></tr>
<tr><td>永続性</td><td>なし</td><td>あり</td></tr>
<tr><td>管理</td><td><code>EncoderCacheManager</code></td><td>カスタム <code>ECConnectorBase</code> 実装</td></tr>
</tbody>
</table>
</div>
<h3 id="カスタム-ecconnector-の実装ガイド"><a class="header" href="#カスタム-ecconnector-の実装ガイド">カスタム ECConnector の実装ガイド</a></h3>
<p>新しい ECConnector を作成するには、<code>ECConnectorBase</code> を継承して 5 つの abstract メソッドを実装する。</p>
<pre><code class="language-python"># my_ec_connector.py
from vllm.distributed.ec_transfer.ec_connector.base import (
    ECConnectorBase, ECConnectorMetadata, ECConnectorRole
)

class RedisECConnector(ECConnectorBase):
    def __init__(self, vllm_config, role):
        super().__init__(vllm_config=vllm_config, role=role)
        self._redis_url = vllm_config.ec_transfer_config.get_from_extra_config(
            "redis_url", "redis://localhost:6379"
        )
        # Redis クライアント初期化...

    # Worker側: ストレージからGPU dictにロード
    def start_load_caches(self, encoder_cache, **kwargs):
        metadata = self._get_connector_metadata()
        for mm_data in metadata.mm_datas:
            if mm_data.mm_hash in encoder_cache:
                continue
            tensor_bytes = self._redis.get(mm_data.mm_hash)
            if tensor_bytes:
                encoder_cache[mm_data.mm_hash] = deserialize(tensor_bytes)

    # Worker側: GPU dictからストレージに保存
    def save_caches(self, encoder_cache, mm_hash, **kwargs):
        if not self.is_producer:
            return
        tensor = encoder_cache[mm_hash].detach().cpu()
        self._redis.set(mm_hash, serialize(tensor))

    # Scheduler側: ストレージにキャッシュが存在するか
    def has_cache_item(self, identifier):
        return self._redis.exists(identifier)

    # Scheduler側: アロケーション後の状態更新
    def update_state_after_alloc(self, request, index):
        mm_hash = request.mm_features[index].identifier
        num_token = request.get_num_encoder_embeds(index)
        self._need_loads[mm_hash] = num_token

    # Scheduler側: メタデータ構築
    def build_connector_meta(self, scheduler_output):
        meta = MyECConnectorMetadata()
        for mm_hash, num_token in self._need_loads.items():
            meta.add(mm_hash, num_token)
        self._need_loads.clear()
        return meta
</code></pre>
<p><strong>登録方法</strong>:</p>
<ol>
<li>ファクトリ登録: <code>ECConnectorFactory.register_connector("RedisECConnector", "my.module", "RedisECConnector")</code></li>
<li>または動的ロード: <code>--ec-connector RedisECConnector --ec-connector-module-path my.module</code></li>
</ol>
<h3 id="テンソルサイズの見積もり"><a class="header" href="#テンソルサイズの見積もり">テンソルサイズの見積もり</a></h3>
<p>Gemma3 27B、1 画像あたり:</p>
<pre><code>256 tokens × 5376 dim × 2 bytes (FP16) = 2,752,512 bytes ≈ 2.6 MB/画像
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コーパス規模</th><th>L2 ストレージ必要量（FP16）</th></tr>
</thead>
<tbody>
<tr><td>1,000 画像</td><td>≈ 2.6 GB</td></tr>
<tr><td>10,000 画像</td><td>≈ 26 GB</td></tr>
<tr><td>100,000 画像</td><td>≈ 260 GB</td></tr>
</tbody>
</table>
</div>
<h3 id="プリコンピュート運用"><a class="header" href="#プリコンピュート運用">プリコンピュート運用</a></h3>
<p>ECConnector を活用したオフラインプリコンピュートの流れ:</p>
<ol>
<li><strong>Producer モード</strong>で vLLM を起動し、コーパス全画像を含むダミーリクエストを送信</li>
<li><code>save_caches()</code> でエンコーダ出力がストレージに蓄積される</li>
<li><strong>Consumer モード</strong>で本番 vLLM を起動</li>
<li>リクエスト到着時に <code>has_cache_item()</code> → <code>start_load_caches()</code> でストレージからロード</li>
<li>エンコーダ計算をスキップし、ストレージからの読み出し + GPU 転送のみで処理</li>
</ol>
<hr>
<h2 id="7-残る設計上の考慮事項"><a class="header" href="#7-残る設計上の考慮事項">7. 残る設計上の考慮事項</a></h2>
<h3 id="71-ecexampleconnector-の同期-io"><a class="header" href="#71-ecexampleconnector-の同期-io">7.1 ECExampleConnector の同期 I/O</a></h3>
<p>現在の <code>ECExampleConnector</code> の <code>start_load_caches()</code> は同期的な <code>safetensors.torch.load_file()</code> を呼ぶ。ディスク I/O がブロッキングとなり、エンコーダ実行前のレイテンシに直接影響する。</p>
<p><strong>対策案</strong>:</p>
<ul>
<li><code>start_load_caches()</code> を非同期化（別スレッドでロード開始、<code>_gather_mm_embeddings()</code> 前に完了待ち）</li>
<li>Redis 等のインメモリストレージを使い、I/O レイテンシを最小化</li>
<li>EngineCore.step() のスケジューリングとモデル実行の間の時間的ギャップを活用</li>
</ul>
<h3 id="72-lru-とストレージ-eviction-の相互作用"><a class="header" href="#72-lru-とストレージ-eviction-の相互作用">7.2 LRU とストレージ Eviction の相互作用</a></h3>
<p>L1（GPU dict）から LRU で Evict されたテンソルは、L2（ストレージ）には残る。次にアクセスされた時:</p>
<ol>
<li>Scheduler: <code>check_and_update_cache()</code> → L1 MISS</li>
<li>Scheduler: <code>ec_connector.has_cache_item()</code> → L2 HIT</li>
<li>Worker: <code>start_load_caches()</code> → L2 から L1 にロード</li>
</ol>
<p>→ エンコーダ再計算は不要だが、ストレージ→GPU 転送のレイテンシが発生する。</p>
<h3 id="73-producerconsumer-ロールの運用"><a class="header" href="#73-producerconsumer-ロールの運用">7.3 Producer/Consumer ロールの運用</a></h3>
<p>ECConnector は P/D 分離を想定した設計。RAG ユースケースでは:</p>
<ul>
<li><strong>ec_producer</strong>: プリコンピュート用インスタンス（エンコーダ出力をストレージに書き込み）</li>
<li><strong>ec_consumer</strong>: 本番サービング用インスタンス（ストレージからロード）</li>
<li>Producer と Consumer で同じストレージパスを共有する必要がある</li>
</ul>
<h3 id="74-キャッシュ無効化"><a class="header" href="#74-キャッシュ無効化">7.4 キャッシュ無効化</a></h3>
<p>モデル重み更新（LoRA ホットスワップ等）時:</p>
<ul>
<li>L1: <code>EncoderCacheManager.reset()</code> + <code>encoder_cache.clear()</code> で対応済み</li>
<li>L2: ストレージ側のキャッシュクリアが必要（<code>identifier</code> に LoRA プレフィックスが含まれるため、LoRA 別に無効化可能）</li>
</ul>
<hr>
<h2 id="8-次のステップ"><a class="header" href="#8-次のステップ">8. 次のステップ</a></h2>
<ol>
<li><strong>FIFO→LRU の実装</strong>: <code>encoder_cache_manager.py</code> の 2 メソッドを修正（変更量: 数行）</li>
<li><strong>カスタム ECConnector の実装</strong>: Redis バックエンドの ECConnector を作成（参照: ECExampleConnector の 199 行）</li>
<li><strong>ベンチマーク</strong>: RAG ワークロードでの比較
<ul>
<li>ベースライン: FIFO + インメモリのみ</li>
<li>改善 1: LRU + インメモリのみ</li>
<li>改善 2: LRU + Redis ECConnector</li>
</ul>
</li>
<li><strong>コミュニティ調査</strong>: vLLM の Issue/PR で ECConnector 関連の議論を確認</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="gemma3-ビジョンパイプライン-キャッシュ機構-medium-verified"><a class="header" href="#gemma3-ビジョンパイプライン-キャッシュ機構-medium-verified">Gemma3 ビジョンパイプライン: キャッシュ機構 [MEDIUM] [VERIFIED]</a></h1>
<blockquote>
<p><strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<p><a href="#gemma3-27b-ビジョンパイプライン-形状フローと数値まとめ">gemma3-vision-pipeline.md</a> で追跡した Gemma3 27B ビジョンパイプライン上には、3つの独立したキャッシュ層が存在する。各キャッシュは異なるステップの重い処理をスキップし、同一画像の再利用や同一プロンプトの再送時に大幅な計算量削減を実現する。</p>
<blockquote>
<p><strong>関連</strong>: EncoderCache の永続化・階層キャッシュ化については <a href="#encodercache-永続化と階層キャッシュ-調査報告">encoder-cache-persistence.md</a> を参照。</p>
</blockquote>
<hr>
<h2 id="1-パイプラインとキャッシュの位置関係"><a class="header" href="#1-パイプラインとキャッシュの位置関係">1. パイプラインとキャッシュの位置関係</a></h2>
<pre><code>                      Step 1: API Request
                             │
                      Step 2: chat_template 適用
                             │
                ┌────────────┴────────────────┐
                │  Step 3: Gemma3Processor     │
                │  (CPU, P0 フロントエンド)      │
                │                              │
                │  3a. image_processor          │
                │      resize(896×896)          │
                │      rescale(×1/255)          │  ◀── ProcessorCache ヒット時
                │      normalize(0.5, 0.5)      │      Step 3 全体をスキップ
                │      Pan-and-Scan crop        │
                │  3b. num_crops 取得            │
                │  3c. プロンプト書き換え         │
                │  3d. boi→full_image_seq 展開   │
                │  3e. tokenize                 │
                │  3f. token_type_ids 生成       │
                └────────────┬────────────────┘
                             │
                  pixel_values: (N, 3, 896, 896)
                  prompt_token_ids, mm_hashes
                             │
              ═══════════════╪═══════════════ CPU → GPU (ZMQ IPC)
                             │
                ┌────────────┴────────────────┐
                │  Step 4: SiglipVisionModel   │
                │  (GPU, P1 バックエンド)        │
                │  Conv2d → 4096 patches        │
                │  + position_embedding          │  ◀── EncoderCache ヒット時
                │  SiglipEncoder × 27層          │      Step 4+5+6 をスキップ
                │  post_layernorm               │
                ├───────────────────────────────┤
                │  Step 5: Projector            │
                │  AvgPool2d(k=4) → 256 tokens  │
                │  GemmaRMSNorm                  │
                │  Linear(1152→5376)             │
                ├───────────────────────────────┤
                │  Step 6: split + flatten       │
                └────────────┬────────────────┘
                             │
                  encoder output: (N×256, 5376)
                             │
                ┌────────────┴────────────────┐
                │  Step 7: embed_input_ids     │
                │  text_embeds × normalizer     │
                │  masked_scatter_(mm_embeds)    │  ◀── KVプレフィックスキャッシュ ヒット時
                ├───────────────────────────────┤      プレフィックス一致分の
                │  Step 8: Gemma3 Decoder       │      Step 7+8 をスキップ
                │  62層 Transformer              │      (KV再計算不要)
                └──────────────────────────────┘
</code></pre>
<hr>
<h2 id="2-キャッシュ比較テーブル"><a class="header" href="#2-キャッシュ比較テーブル">2. キャッシュ比較テーブル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th></th><th>ProcessorCache</th><th>EncoderCache</th><th>KVプレフィックスキャッシュ</th></tr>
</thead>
<tbody>
<tr><td><strong>場所</strong></td><td>CPU (P0 フロントエンド)</td><td>GPU (P1 バックエンド)</td><td>GPU (P1 バックエンド)</td></tr>
<tr><td><strong>キャッシュキー</strong></td><td>blake3(model_id, 画像ピクセル, processor_kwargs, tokenizer_kwargs)</td><td><code>mm_feature.identifier</code> (= mm_hash or <code>{lora}:{mm_hash}</code>)</td><td>hash(parent_hash, token_ids, extra_keys) — extra_keysにidentifier含む</td></tr>
<tr><td><strong>保存される値</strong></td><td>HF処理済みテンソル (pixel_values, num_patches) + prompt_updates</td><td>エンコーダ出力テンソル (post-Projector, GPU上)</td><td>デコーダ各層のKV状態 (KVCacheブロック)</td></tr>
<tr><td><strong>ヒット時にスキップ</strong></td><td>Step 3全体 (CPU前処理)</td><td>Step 4+5+6 (エンコーダ+プロジェクタ)</td><td>Step 7+8の一部 (プレフィックス分のデコーダ)</td></tr>
<tr><td><strong>Eviction方式</strong></td><td>LRU (サイズベース)</td><td>FIFO (RefCount管理)</td><td>LRU (ブロック単位)</td></tr>
<tr><td><strong>容量設定</strong></td><td><code>mm_processor_cache_gb</code> (default: 4GB)</td><td><code>encoder_cache_size</code> (埋め込み数単位)</td><td>KVCacheの一部 (BlockPool管理)</td></tr>
<tr><td><strong>管理クラス</strong></td><td><code>MultiModalProcessorOnlyCache</code> 等4種</td><td><code>EncoderCacheManager</code> + <code>encoder_cache</code> dict</td><td><code>KVCacheManager</code> (prefix_cache)</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="3-processorcache--cpu側前処理キャッシュ"><a class="header" href="#3-processorcache--cpu側前処理キャッシュ">3. ProcessorCache — CPU側前処理キャッシュ</a></h2>
<h3 id="ハッシュ計算"><a class="header" href="#ハッシュ計算">ハッシュ計算</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/hasher.py:50-162</code>, <code>target/vllm/vllm/multimodal/processing/processor.py:1299-1363</code></p>
<pre><code class="language-python">MultiModalHasher.hash_kwargs(
    model_id=model_id,          # モデル識別子（例: "google/gemma-3-27b-it"）
    image=PIL_Image,            # 画像データ
    **hf_processor_mm_kwargs,   # HF Processorへの追加引数
    **tokenization_kwargs,      # トークナイザ設定
)
</code></pre>
<p>ハッシュに投入されるデータ:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>入力</th><th>シリアライズ方法</th></tr>
</thead>
<tbody>
<tr><td><code>model_id</code> (str)</td><td>UTF-8エンコード</td></tr>
<tr><td><code>image</code> (PIL.Image)</td><td>EXIF ImageID (UUID型) → 16バイト。なければ mode + ピクセルデータ (numpy配列)</td></tr>
<tr><td><code>image</code> (MediaWithBytes)</td><td>EXIF ImageID → 16バイト。なければ original_bytes</td></tr>
<tr><td><code>hf_processor_mm_kwargs</code> (dict)</td><td>キーソート → 再帰的シリアライズ</td></tr>
<tr><td><code>tokenization_kwargs</code> (dict)</td><td>同上</td></tr>
</tbody>
</table>
</div>
<ul>
<li><strong>ハッシュアルゴリズム</strong>: <code>VLLM_MM_HASHER_ALGORITHM</code> 環境変数で設定（<code>blake3</code> デフォルト、<code>sha256</code>/<code>sha512</code> はFIPS準拠用）</li>
<li>キーはアルファベット順にソートされてから逐次ハッシュに投入される（決定的）</li>
</ul>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/hasher.py:154-162</code> (hash_kwargs)</p>
<h3 id="保存される情報"><a class="header" href="#保存される情報">保存される情報</a></h3>
<ul>
<li><strong>テンソルデータ</strong>: <code>pixel_values</code> (形状: <code>(N, 3, 896, 896)</code>), <code>num_patches</code> (形状: <code>(num_images,)</code>)</li>
<li><strong>prompt_updates</strong>: プレースホルダー位置情報、展開パターン</li>
</ul>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/cache.py:326-725</code></p>
<h3 id="cpugpu"><a class="header" href="#cpugpu">CPU/GPU</a></h3>
<p><strong>CPU</strong>。P0フロントエンドプロセスのメモリ上で管理される。</p>
<h3 id="スキップされる処理"><a class="header" href="#スキップされる処理">スキップされる処理</a></h3>
<p><strong>Step 3 全体</strong>（<code>Gemma3Processor.__call__()</code>）:</p>
<ul>
<li>3a: <code>image_processor</code> — resize(896×896), rescale(×1/255), normalize(mean=0.5, std=0.5), Pan-and-Scan時のクロップ生成</li>
<li>3b: <code>num_crops</code> 取得</li>
<li>3c: プロンプト書き換え（Pan-and-Scan時のみ）</li>
<li>3d: <code>boi_token</code> → <code>full_image_sequence</code> 展開</li>
<li>3e: tokenizer による <code>token_ids</code> 変換</li>
<li>3f: <code>token_type_ids</code> 生成</li>
</ul>
<p>さらに、Sender/Shm タイプ使用時は <strong>ZMQ IPC でのテンソルデータ転送もスキップ</strong> される（<code>data=None</code> で送信）。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/processing/processor.py:1513-1596</code> (_cached_apply_hf_processor)</p>
<h3 id="キャッシュフロー詳細"><a class="header" href="#キャッシュフロー詳細">キャッシュフロー詳細</a></h3>
<pre><code>_cached_apply_hf_processor():
  1. _hash_mm_items()          → MultiModalHashes（各画像のblake3ハッシュ）
  2. _get_cache_missing_items() → 各画像がキャッシュにあるか判定
  3. _apply_hf_processor_main() → キャッシュミスの画像だけHF Processor実行
  4. _merge_mm_kwargs()         → キャッシュ済み + 新規処理の結果をマージ
     ※ マージ前に全ハッシュを touch() して LRU Eviction を防止
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/processing/processor.py:1365-1400</code> (_get_cache_missing_items)</p>
<h3 id="4種の実装"><a class="header" href="#4種の実装">4種の実装</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>実装</th><th>用途</th><th>格納先</th><th>ヒット時の動作</th></tr>
</thead>
<tbody>
<tr><td><code>MultiModalProcessorOnlyCache</code></td><td>P0完結（IPC無効時）</td><td>P0メモリ</td><td>テンソル+prompt返却</td></tr>
<tr><td><code>MultiModalProcessorSenderCache</code></td><td>P0→P1（LRUモード）</td><td>P0にメタデータのみ</td><td><code>data=None</code>で送信、IPC転送省略</td></tr>
<tr><td><code>ShmObjectStoreSenderCache</code></td><td>P0→P1（共有メモリ）</td><td>共有メモリ</td><td>共有メモリ参照を返却</td></tr>
<tr><td><code>MultiModalReceiverCache</code></td><td>P1側（LRUモード）</td><td>P1メモリ</td><td>P0と同期したLRUでテンソル取得</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/registry.py:284-320</code> (キャッシュタイプ選択ロジック)</p>
<hr>
<h2 id="4-encodercache--gpuエンコーダ出力キャッシュ"><a class="header" href="#4-encodercache--gpuエンコーダ出力キャッシュ">4. EncoderCache — GPUエンコーダ出力キャッシュ</a></h2>
<h3 id="キャッシュキー"><a class="header" href="#キャッシュキー">キャッシュキー</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/input_processor.py:490-506</code></p>
<pre><code class="language-python">identifier = mm_hash                          # 通常
identifier = f"{lora_name}:{mm_hash}"         # LoRA tower connector有効時
</code></pre>
<p><code>mm_hash</code> は ProcessorCache と同じ blake3 ハッシュ値。LoRA が有効な場合は、同一画像でも LoRA によってエンコーダ出力が変わるため、LoRA名をプレフィックスとして付加する。</p>
<h3 id="保存される情報-1"><a class="header" href="#保存される情報-1">保存される情報</a></h3>
<ul>
<li><strong>GPU上のテンソル</strong>: SiglipVisionModel + Gemma3MultiModalProjector の出力
<ul>
<li>Gemma3の場合: <code>(N×256, 5376)</code> — Projector出力をflattenしたもの</li>
</ul>
</li>
<li><strong>論理管理</strong>: <code>EncoderCacheManager</code> が RefCount + FIFO で管理</li>
<li><strong>物理格納</strong>: <code>gpu_model_runner.encoder_cache: dict[str, torch.Tensor]</code></li>
</ul>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:17-267</code>, <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:439</code></p>
<h3 id="cpugpu-1"><a class="header" href="#cpugpu-1">CPU/GPU</a></h3>
<p><strong>GPU</strong>。エンコーダ出力テンソルはGPUメモリ上に保持される。論理管理（RefCount、Eviction判定）はCPU上の <code>EncoderCacheManager</code> が行う。</p>
<h3 id="スキップされる処理-1"><a class="header" href="#スキップされる処理-1">スキップされる処理</a></h3>
<p><strong>Step 4 + Step 5 + Step 6</strong>:</p>
<ul>
<li>Step 4: <strong>SiglipVisionModel forward</strong> — Conv2d(3→1152) + position_embedding + 27層 Transformer Encoder + post_layernorm</li>
<li>Step 5: <strong>Gemma3MultiModalProjector forward</strong> — reshape + AvgPool2d(k=4, s=4) + GemmaRMSNorm + Linear(1152→5376)</li>
<li>Step 6: <strong>split + flatten</strong> — num_patchesに基づく分割と結合</li>
</ul>
<p>これらはGPU上で最も計算量の大きいビジョン処理であり、特に SiglipEncoder の 27層の双方向 Attention が支配的。</p>
<h3 id="scheduler連携"><a class="header" href="#scheduler連携">Scheduler連携</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:1060-1215</code></p>
<pre><code>Scheduler._get_encoder_budget():
  1. 各 mm_feature について:
  2. encoder_cache_manager.check_and_update_cache(req, i) を呼ぶ
     → True: scheduled_encoder_inputs に含めない（スキップ）
     → False: can_allocate() → allocate() → scheduled_encoder_inputs に追加
  3. SchedulerOutput.scheduled_encoder_inputs = {req_id: [input_ids]}
</code></pre>
<p>GPUModelRunner 側:</p>
<pre><code>_execute_mm_encoder():
  → scheduled_encoder_inputs にあるもののみ model.embed_multimodal() 実行
  → 出力を encoder_cache[mm_hash] に格納

_gather_mm_embeddings():
  → 全ての mm_feature について encoder_cache[mm_hash] からスライス取得
  → キャッシュヒットしたものも、ミスして今回計算したものも、同じキャッシュから取得
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:2293-2447</code> (_execute_mm_encoder), <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:2449-2527</code> (_gather_mm_embeddings)</p>
<hr>
<h2 id="5-kvプレフィックスキャッシュ--デコーダkv状態キャッシュ"><a class="header" href="#5-kvプレフィックスキャッシュ--デコーダkv状態キャッシュ">5. KVプレフィックスキャッシュ — デコーダKV状態キャッシュ</a></h2>
<h3 id="ブロックハッシュ計算"><a class="header" href="#ブロックハッシュ計算">ブロックハッシュ計算</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:525-552</code></p>
<pre><code class="language-python">BlockHash(
    hash_function((parent_block_hash, curr_block_token_ids_tuple, extra_keys))
)
</code></pre>
<p><code>extra_keys</code> は以下の要素の結合:</p>
<pre><code class="language-python">extra_keys = lora_extra_keys + mm_extra_keys + cache_salt_keys + prompt_embeds_keys
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:487-522</code> (generate_block_hash_extra_keys)</p>
<h4 id="mm-extra-keys-の生成"><a class="header" href="#mm-extra-keys-の生成">MM extra keys の生成</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:387-448</code></p>
<p>MMトークン（<code>&lt;image&gt;</code> token_id=262144）を含むブロックでは、そのブロックに重なる <code>mm_feature.identifier</code> が <code>extra_keys</code> に追加される。</p>
<pre><code>ブロック [start_token_idx, end_token_idx) が
mm_feature の [offset, offset+length) と重なる場合:
  → extra_keys.append(mm_feature.identifier)
</code></pre>
<p>これにより:</p>
<ul>
<li><strong>同一テキスト・異なる画像</strong> → 異なるブロックハッシュ → キャッシュミス</li>
<li><strong>同一テキスト・同一画像</strong> → 同一ブロックハッシュ → キャッシュヒット</li>
</ul>
<h3 id="保存される情報-2"><a class="header" href="#保存される情報-2">保存される情報</a></h3>
<ul>
<li><strong>GPUメモリ上のKVCacheブロック</strong>: デコーダ62層分のKey/Value状態</li>
<li>BlockPool が物理ブロックを管理、prefix_cache がハッシュ→ブロック対応を管理</li>
</ul>
<h3 id="cpugpu-2"><a class="header" href="#cpugpu-2">CPU/GPU</a></h3>
<p><strong>GPU</strong>。KV状態はGPUメモリ上のブロックに格納される。ハッシュ計算とブロック対応管理はCPU上の <code>KVCacheManager</code> が行う。</p>
<h3 id="スキップされる処理-2"><a class="header" href="#スキップされる処理-2">スキップされる処理</a></h3>
<p><strong>Step 7 + Step 8 の一部</strong>（プレフィックスが一致するトークン分）:</p>
<ul>
<li>Step 7: <strong>embed_input_ids</strong> — テキスト埋め込み × normalizer + masked_scatter_(mm_embeds)</li>
<li>Step 8: <strong>Gemma3 Decoder forward</strong> — 62層 Transformer の KV 計算</li>
</ul>
<p>プレフィックスキャッシュがヒットすると <code>num_computed_tokens</code> が増加し、新規に forward pass が必要なトークン数が減少する。例えば 1000 トークンのプロンプトで 800 トークン分のプレフィックスがヒットすれば、残り 200 トークンだけ計算すればよい。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_manager.py:164-204</code> (get_computed_blocks)</p>
<hr>
<h2 id="6-キャッシュの独立性と相互作用"><a class="header" href="#6-キャッシュの独立性と相互作用">6. キャッシュの独立性と相互作用</a></h2>
<p>3つのキャッシュは独立に動作する。各キャッシュのヒット/ミスは他のキャッシュの判定に影響しない。</p>
<h3 id="典型シナリオ"><a class="header" href="#典型シナリオ">典型シナリオ</a></h3>
<h4 id="シナリオ1-初回リクエスト全ミス"><a class="header" href="#シナリオ1-初回リクエスト全ミス">シナリオ1: 初回リクエスト（全ミス）</a></h4>
<pre><code>画像A + "この画像は何？"  →  全ステップ実行
  ProcessorCache: MISS → Step 3 実行、結果をキャッシュ
  EncoderCache:   MISS → Step 4+5+6 実行、結果をキャッシュ
  KV Prefix:      MISS → Step 7+8 全トークン実行、KVブロック格納
</code></pre>
<h4 id="シナリオ2-同一画像同一プロンプト再送全ヒット"><a class="header" href="#シナリオ2-同一画像同一プロンプト再送全ヒット">シナリオ2: 同一画像・同一プロンプト再送（全ヒット）</a></h4>
<pre><code>画像A + "この画像は何？"（2回目）
  ProcessorCache: HIT  → Step 3 スキップ（pixel_values をキャッシュから取得）
  EncoderCache:   HIT  → Step 4+5+6 スキップ（エンコーダ出力をGPUキャッシュから取得）
  KV Prefix:      HIT  → Step 7+8 のプレフィックス分スキップ（KV状態再利用）
</code></pre>
<h4 id="シナリオ3-同一画像異なるプロンプト"><a class="header" href="#シナリオ3-同一画像異なるプロンプト">シナリオ3: 同一画像・異なるプロンプト</a></h4>
<pre><code>画像A + "この画像を要約して"
  ProcessorCache: HIT  → Step 3 スキップ（同一画像なのでハッシュ一致）
  EncoderCache:   HIT  → Step 4+5+6 スキップ（同一 identifier）
  KV Prefix:      部分HIT → 画像トークン部分（ブロック単位）はヒットする可能性あり
                           テキスト部分は異なるためミス
</code></pre>
<h4 id="シナリオ4-異なる画像全ミス"><a class="header" href="#シナリオ4-異なる画像全ミス">シナリオ4: 異なる画像（全ミス）</a></h4>
<pre><code>画像B + "この画像は何？"
  ProcessorCache: MISS → ピクセルデータが異なるためハッシュ不一致
  EncoderCache:   MISS → identifier が異なる
  KV Prefix:      MISS → extra_keys の identifier が異なりブロックハッシュ不一致
</code></pre>
<h3 id="キャッシュ間のキー共有"><a class="header" href="#キャッシュ間のキー共有">キャッシュ間のキー共有</a></h3>
<p>3つのキャッシュは同一の <strong>mm_hash</strong>（blake3ハッシュ）を基盤として共有している:</p>
<pre><code>MultiModalHasher.hash_kwargs(model_id, image, kwargs...)
        │
        ▼
    mm_hash (blake3 hex digest)
        │
        ├──▶ ProcessorCache のキー（そのまま使用）
        │
        ├──▶ EncoderCache のキー（= identifier = mm_hash or lora:mm_hash）
        │
        └──▶ KV Prefix Cache の extra_keys の一部（= identifier）
</code></pre>
<hr>
<h2 id="7-主要ファイル参照"><a class="header" href="#7-主要ファイル参照">7. 主要ファイル参照</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th><th>行</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/multimodal/hasher.py</code></td><td><code>MultiModalHasher</code>, <code>hash_kwargs()</code>, <code>serialize_item()</code></td><td>L50, L154, L52</td></tr>
<tr><td><code>target/vllm/vllm/multimodal/cache.py</code></td><td><code>MultiModalProcessorOnlyCache</code>, <code>SenderCache</code>, <code>ShmCache</code>, <code>ReceiverCache</code></td><td>L326, L379, L437, L614</td></tr>
<tr><td><code>target/vllm/vllm/multimodal/processing/processor.py</code></td><td><code>_cached_apply_hf_processor()</code>, <code>_hash_mm_items()</code>, <code>_get_cache_missing_items()</code></td><td>L1513, L1299, L1365</td></tr>
<tr><td><code>target/vllm/vllm/multimodal/registry.py</code></td><td><code>processor_cache_from_config()</code></td><td>L305</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/input_processor.py</code></td><td><code>_get_mm_identifier()</code></td><td>L490</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/encoder_cache_manager.py</code></td><td><code>EncoderCacheManager</code>, <code>check_and_update_cache()</code></td><td>L17, L91</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_model_runner.py</code></td><td><code>encoder_cache</code>, <code>_execute_mm_encoder()</code>, <code>_gather_mm_embeddings()</code></td><td>L439, L2293, L2449</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/kv_cache_utils.py</code></td><td><code>_gen_mm_extra_hash_keys()</code>, <code>generate_block_hash_extra_keys()</code>, <code>hash_block_tokens()</code></td><td>L387, L487, L525</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/kv_cache_manager.py</code></td><td><code>get_computed_blocks()</code></td><td>L164</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/sched/scheduler.py</code></td><td><code>_get_encoder_budget()</code></td><td>L1060</td></tr>
</tbody>
</table>
</div>
<h2 id="関連ドキュメント-12"><a class="header" href="#関連ドキュメント-12">関連ドキュメント</a></h2>
<ul>
<li><a href="#gemma3-27b-ビジョンパイプライン-形状フローと数値まとめ">Gemma3 ビジョンパイプライン: 形状フローと数値まとめ</a></li>
<li><a href="#フロントエンド-マルチモーダル処理パス-medium-verified">フロントエンド MM処理パス</a></li>
<li><a href="#バックエンド-マルチモーダル処理パス-medium-verified">バックエンド MM処理パス</a></li>
<li><a href="#kvcachemanager-サマリー">KVCacheManager</a> — プレフィックスキャッシュの詳細</li>
<li><a href="#プレフィックスキャッシュ詳細">KVCacheManager: プレフィックスキャッシュ</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="gemma3-27b-ビジョンパイプライン-形状フローと数値まとめ"><a class="header" href="#gemma3-27b-ビジョンパイプライン-形状フローと数値まとめ">Gemma3 27B ビジョンパイプライン: 形状フローと数値まとめ</a></h1>
<h2 id="モデルパラメータconfigjson--preprocessor_configjson"><a class="header" href="#モデルパラメータconfigjson--preprocessor_configjson">モデルパラメータ（config.json + preprocessor_config.json）</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>値</th><th>出典</th></tr>
</thead>
<tbody>
<tr><td>image_size</td><td>896</td><td>vision_config</td></tr>
<tr><td>patch_size</td><td>14</td><td>vision_config</td></tr>
<tr><td>vision hidden_size</td><td>1152</td><td>vision_config</td></tr>
<tr><td>vision num_heads</td><td>16</td><td>vision_config</td></tr>
<tr><td>vision num_layers</td><td>27</td><td>vision_config</td></tr>
<tr><td>text hidden_size</td><td>5376</td><td>text_config</td></tr>
<tr><td>text num_heads</td><td>32</td><td>text_config</td></tr>
<tr><td>text num_layers</td><td>62</td><td>text_config</td></tr>
<tr><td>mm_tokens_per_image</td><td>256</td><td>config.json</td></tr>
<tr><td>image_token_index</td><td>262144</td><td>config.json</td></tr>
<tr><td>boi_token_index</td><td>255999</td><td>config.json</td></tr>
<tr><td>eoi_token_index</td><td>256000</td><td>config.json</td></tr>
</tbody>
</table>
</div>
<h3 id="導出値"><a class="header" href="#導出値">導出値</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>導出パラメータ</th><th>計算</th><th>値</th></tr>
</thead>
<tbody>
<tr><td>patches_per_image</td><td>896 / 14</td><td><strong>64</strong></td></tr>
<tr><td>エンコーダ入力パッチ数</td><td>64²</td><td><strong>4096</strong></td></tr>
<tr><td>tokens_per_side</td><td>√256</td><td><strong>16</strong></td></tr>
<tr><td>AvgPool2d kernel_size</td><td>64 / 16</td><td><strong>4</strong></td></tr>
<tr><td>Projector 出力トークン/画像</td><td>16²</td><td><strong>256</strong> (= mm_tokens_per_image ✅)</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="pan-and-scan-設定"><a class="header" href="#pan-and-scan-設定">Pan-and-Scan 設定</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>preprocessor_config.json</th><th>フォールバックデフォルト</th><th>出典</th></tr>
</thead>
<tbody>
<tr><td>do_pan_and_scan</td><td>null</td><td><strong>False</strong></td><td>processing_gemma3.py L44</td></tr>
<tr><td>pan_and_scan_min_crop_size</td><td>null</td><td><strong>256</strong></td><td>processing_gemma3.py L45</td></tr>
<tr><td>pan_and_scan_max_num_crops</td><td>null</td><td><strong>4</strong></td><td>processing_gemma3.py L46</td></tr>
<tr><td>pan_and_scan_min_ratio_to_activate</td><td>null</td><td><strong>1.2</strong></td><td>processing_gemma3.py L47</td></tr>
</tbody>
</table>
</div>
<ul>
<li>Google はモデル配布時にこれらを<strong>すべて null</strong> にしている</li>
<li>デフォルト値は HF transformers の <code>Gemma3ProcessorKwargs._defaults</code> で定義</li>
<li><strong>デフォルトでは Pan-and-Scan は無効</strong></li>
<li>有効化: vLLM では <code>--hf-overrides '{"do_pan_and_scan": true}'</code></li>
</ul>
<hr>
<h2 id="api-リクエストからデコーダ入力までの全体フロー"><a class="header" href="#api-リクエストからデコーダ入力までの全体フロー">API リクエストからデコーダ入力までの全体フロー</a></h2>
<h3 id="step-1-ユーザーの-api-リクエスト"><a class="header" href="#step-1-ユーザーの-api-リクエスト">Step 1: ユーザーの API リクエスト</a></h3>
<pre><code class="language-json">{
  "model": "gemma-3-27b-it",
  "messages": [
    {
      "role": "user",
      "content": [
        {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}},
        {"type": "text", "text": "この文書を要約して"}
      ]
    }
  ]
}
</code></pre>
<p>ユーザーは画像を1枚渡すだけ。クロップの存在を意識する必要はない。</p>
<h3 id="step-2-chat_template-適用"><a class="header" href="#step-2-chat_template-適用">Step 2: chat_template 適用</a></h3>
<p>vLLM が chat_template を適用してプロンプト文字列を生成:</p>
<pre><code>&lt;start_of_turn&gt;user
&lt;start_of_image&gt;この文書を要約して&lt;end_of_turn&gt;
&lt;start_of_turn&gt;model
</code></pre>
<p><code>&lt;start_of_image&gt;</code> は <code>boi_token</code> (token_id=255999)。この時点ではプレースホルダが <strong>1個だけ</strong>。</p>
<h3 id="step-3-gemma3processorcall--cpu-側前処理"><a class="header" href="#step-3-gemma3processorcall--cpu-側前処理">Step 3: Gemma3Processor.<strong>call</strong>() — CPU 側前処理</a></h3>
<h4 id="3a-image_processor-による画像前処理"><a class="header" href="#3a-image_processor-による画像前処理">3a: image_processor による画像前処理</a></h4>
<pre><code class="language-python">image_inputs = self.image_processor(images, **output_kwargs["images_kwargs"])
</code></pre>
<p>画像をリサイズ・正規化し、Pan-and-Scan が有効ならクロップも生成する。</p>
<h4 id="3b-num_crops-の取得"><a class="header" href="#3b-num_crops-の取得">3b: num_crops の取得</a></h4>
<pre><code class="language-python">num_crops = to_py_obj(image_inputs.pop("num_crops"))
</code></pre>
<h4 id="3c-プロンプトの自動書き換えpan-and-scan-時のみ"><a class="header" href="#3c-プロンプトの自動書き換えpan-and-scan-時のみ">3c: プロンプトの自動書き換え（Pan-and-Scan 時のみ）</a></h4>
<pre><code class="language-python">for num, idx in reversed(list(zip(num_crops, image_indexes))):
    if num:  # num=0 なら falsy → この書き換えは発生しない
        formatted_image_text = (
            f"Here is the original image {self.boi_token} "
            f"and here are some crops to help you see better "
            + " ".join([self.boi_token] * num)
        )
        prompt = prompt[:idx] + formatted_image_text + prompt[idx + len(self.boi_token):]
</code></pre>
<p><strong>Pan-and-Scan 無効（num=0）時</strong>: <code>if num:</code> が falsy なので、書き換えは<strong>一切発生しない</strong>。
<code>&lt;start_of_image&gt;</code> は1個のまま次のステップへ。</p>
<p><strong>Pan-and-Scan 有効（num=2）時</strong>: 1個の <code>&lt;start_of_image&gt;</code> が以下に置き換えられる:</p>
<pre><code>Here is the original image &lt;start_of_image&gt; and here are some crops to help you see better &lt;start_of_image&gt; &lt;start_of_image&gt;
</code></pre>
<h4 id="3d-boi_token--full_image_sequence-への展開"><a class="header" href="#3d-boi_token--full_image_sequence-への展開">3d: boi_token → full_image_sequence への展開</a></h4>
<pre><code class="language-python">self.full_image_sequence = f"\n\n{boi_token}{image_token * 256}{eoi_token}\n\n"
# = "\n\n&lt;start_of_image&gt;&lt;image&gt;×256&lt;end_of_image&gt;\n\n"

text = [prompt.replace(self.boi_token, self.full_image_sequence) for prompt in text]
</code></pre>
<p>全ての <code>&lt;start_of_image&gt;</code> がそれぞれ 256個の <code>&lt;image&gt;</code> トークンを含む <code>full_image_sequence</code> に展開される。</p>
<h4 id="3e-tokenizer-で-token_ids-に変換"><a class="header" href="#3e-tokenizer-で-token_ids-に変換">3e: tokenizer で token_ids に変換</a></h4>
<pre><code class="language-python">text_inputs = self.tokenizer(text=text, **output_kwargs["text_kwargs"])
</code></pre>
<p><code>&lt;image&gt;</code> トークン (token_id=262144) が並んだ input_ids が生成される。</p>
<h4 id="3f-token_type_ids-の生成"><a class="header" href="#3f-token_type_ids-の生成">3f: token_type_ids の生成</a></h4>
<pre><code class="language-python">mm_token_type_ids[array_ids == self.image_token_id] = 1
# → &lt;image&gt; トークン位置が 1、それ以外が 0
</code></pre>
<hr>
<h2 id="ケース1-デフォルトpan-and-scan-無効"><a class="header" href="#ケース1-デフォルトpan-and-scan-無効">ケース1: デフォルト（Pan-and-Scan 無効）</a></h2>
<p>入力例: A4 150dpi 画像 (1240 × 1754 pixel)</p>
<h3 id="プロンプト変換の流れ"><a class="header" href="#プロンプト変換の流れ">プロンプト変換の流れ</a></h3>
<pre><code>ユーザー入力:
  画像1枚 + "この文書を要約して"

chat_template 適用後:
  "...&lt;start_of_image&gt;この文書を要約して..."
                ↑
          boi_token 1個

do_pan_and_scan=False → num_crops=0 → プロンプト書き換えなし

boi_token → full_image_sequence 展開後:
  "...\n\n&lt;start_of_image&gt;&lt;image&gt;×256&lt;end_of_image&gt;\n\nこの文書を要約して..."
           ↑              ↑×256  ↑
         255999         262144  256000

tokenize 後の input_ids (概念的):
  [..., 255999, 262144, 262144, ...(×256)..., 262144, 256000, ..., テキスト, ...]
</code></pre>
<h3 id="cpu-側前処理"><a class="header" href="#cpu-側前処理">CPU 側前処理</a></h3>
<pre><code>元画像 (1240×1754)
    │  resize(896×896, bilinear)   ← アスペクト比無視の正方形リサイズ
    │  rescale(× 1/255)            ← [0,255] → [0,1]
    │  normalize(mean=0.5, std=0.5) ← [0,1] → [-1,1]
    ▼
pixel_values: (1, 3, 896, 896)
num_patches:  tensor([1])
</code></pre>
<h3 id="gpu-側-siglipvisionmodel"><a class="header" href="#gpu-側-siglipvisionmodel">GPU 側: SiglipVisionModel</a></h3>
<pre><code>(1, 3, 896, 896)
    │  Conv2d(3 → 1152, kernel=14, stride=14)
    ▼
(1, 1152, 64, 64)              ← 896/14 = 64
    │  flatten + transpose
    ▼
(1, 4096, 1152)                ← 64² = 4096 パッチ
    │  + position_embedding(4096, 1152)
    ▼
(1, 4096, 1152)
    │  SiglipEncoder × 27層
    │  （双方向 Attention, heads=16, 4096トークン間全対全）
    ▼
(1, 4096, 1152)
    │  post_layernorm
    ▼
(1, 4096, 1152)
</code></pre>
<h3 id="gpu-側-gemma3multimodalprojector"><a class="header" href="#gpu-側-gemma3multimodalprojector">GPU 側: Gemma3MultiModalProjector</a></h3>
<pre><code>(1, 4096, 1152)
    │  transpose → (1, 1152, 4096)
    │  reshape  → (1, 1152, 64, 64)     ← 2Dグリッドに復元
    │
    │  AvgPool2d(kernel_size=4, stride=4)
    ▼
(1, 1152, 16, 16)                       ← 64/4 = 16
    │  flatten(2) → (1, 1152, 256)
    │  transpose  → (1, 256, 1152)       ← 16² = 256 トークン
    │
    │  GemmaRMSNorm(1152)
    ▼
(1, 256, 1152)
    │
    │  matmul(mm_input_projection_weight)  ← shape: (1152, 5376)
    ▼
(1, 256, 5376)                           ← text hidden_size 空間
</code></pre>
<h3 id="gpu-側-split--flatten"><a class="header" href="#gpu-側-split--flatten">GPU 側: split + flatten</a></h3>
<pre><code>(1, 256, 5376)
    │  split by num_patches=[1] → [(1, 256, 5376)]
    │  flatten(0, 1)
    ▼
(256, 5376)                              ← 最終出力
</code></pre>
<h3 id="gpu-側-テキスト埋め込みとマージ"><a class="header" href="#gpu-側-テキスト埋め込みとマージ">GPU 側: テキスト埋め込みとマージ</a></h3>
<pre><code class="language-python">text_embeds = embed_tokens(input_ids) * normalizer   # (seq_len, 5376)
# token_id=262144 は vocab 外 → handle_oov_mm_token=True でゼロ埋め
# is_multimodal: (seq_len,) ← 256箇所が True

merged = masked_scatter_(text_embeds, is_multimodal, mm_embeds)  # (256, 5376)
# → 262144 だった256箇所をビジョン埋め込みで上書き
# ※ ビジョン埋め込みには normalizer スケーリングは適用されない

→ (seq_len, 5376) として Gemma3 Decoder (62層) へ
</code></pre>
<h3 id="消費トークン数-256"><a class="header" href="#消費トークン数-256">消費トークン数: <strong>256</strong></a></h3>
<hr>
<h2 id="ケース2-pan-and-scan-有効"><a class="header" href="#ケース2-pan-and-scan-有効">ケース2: Pan-and-Scan 有効</a></h2>
<p>入力例: 同じ A4 150dpi 画像 (1240 × 1754 pixel)</p>
<h3 id="プロンプト変換の流れ-1"><a class="header" href="#プロンプト変換の流れ-1">プロンプト変換の流れ</a></h3>
<pre><code>ユーザー入力:
  画像1枚 + "この文書を要約して"

chat_template 適用後:
  "...&lt;start_of_image&gt;この文書を要約して..."
                ↑
          boi_token 1個

do_pan_and_scan=True → ratio=1754/1240≈1.415 &gt; 1.2 → num_crops=2

Processor がプロンプトを自動書き換え (Step 3c):
  "...Here is the original image &lt;start_of_image&gt; and here are
   some crops to help you see better &lt;start_of_image&gt; &lt;start_of_image&gt;
   この文書を要約して..."
                                      ↑               ↑              ↑
                                 original 用       crop 0 用      crop 1 用

boi_token → full_image_sequence 展開後:
  "...Here is the original image \n\n&lt;boi&gt;&lt;image&gt;×256&lt;eoi&gt;\n\n and here are
   some crops to help you see better \n\n&lt;boi&gt;&lt;image&gt;×256&lt;eoi&gt;\n\n
   \n\n&lt;boi&gt;&lt;image&gt;×256&lt;eoi&gt;\n\nこの文書を要約して..."

tokenize 後:
  [..., "Here", "is", ...,
   255999, 262144×256, 256000,          ← original
   ..., "and", "here", ...,
   255999, 262144×256, 256000,          ← crop 0
   ...,
   255999, 262144×256, 256000,          ← crop 1
   ..., テキスト, ...]
</code></pre>
<h3 id="cpu-側-pan-and-scan-判定"><a class="header" href="#cpu-側-pan-and-scan-判定">CPU 側: Pan-and-Scan 判定</a></h3>
<pre><code class="language-python"># 縦長画像 (height &gt; width)
ratio = 1754 / 1240 ≈ 1.415
min_ratio_to_activate = 1.2
1.415 &gt; 1.2 → ✅ 発動
</code></pre>
<h3 id="cpu-側-クロップ数計算"><a class="header" href="#cpu-側-クロップ数計算">CPU 側: クロップ数計算</a></h3>
<pre><code class="language-python"># 縦長パス (image_height &gt; image_width)
num_crops_h = min(
    floor(1754 / 256),          # = 6  ← min_crop_size 制約
    floor(1754 / 1240 + 0.5),   # = 1  ← アスペクト比近似
)
# → min(6, 1) = 1
num_crops_h = max(2, 1) = 2    # 最低2クロップに強制
num_crops_h = min(4, 2) = 2    # max_num_crops でクリップ
num_crops_w = 1

# クロップサイズ検証
crop_size_w = ceil(1240 / 1) = 1240
crop_size_h = ceil(1754 / 2) = 877
min(1240, 877) = 877 &gt; 256 (min_crop_size) → ✅ 有効

結果: 1 × 2 = 2 クロップ
</code></pre>
<h3 id="cpu-側-クロップ切り出し--リサイズ"><a class="header" href="#cpu-側-クロップ切り出し--リサイズ">CPU 側: クロップ切り出し + リサイズ</a></h3>
<pre><code>元画像 (1240×1754)
  ├── original  (1240×1754) → resize(896×896) → normalize → (3, 896, 896)
  ├── crop 0    (1240×877)  → resize(896×896) → normalize → (3, 896, 896)
  └── crop 1    (1240×877)  → resize(896×896) → normalize → (3, 896, 896)
                                                              │ stack
                                                pixel_values: (3, 3, 896, 896)
                                                num_patches:  tensor([3])
</code></pre>
<h3 id="gpu-側-siglipvisionmodel-1"><a class="header" href="#gpu-側-siglipvisionmodel-1">GPU 側: SiglipVisionModel</a></h3>
<pre><code>(3, 3, 896, 896)
    │  Conv2d(3 → 1152, kernel=14, stride=14)
    ▼
(3, 1152, 64, 64)
    │  flatten + transpose
    ▼
(3, 4096, 1152)                 ← 3枚 × 4096パッチ
    │  + position_embedding
    ▼
(3, 4096, 1152)
    │  SiglipEncoder × 27層（双方向 Attention）
    ▼
(3, 4096, 1152)
</code></pre>
<h3 id="gpu-側-gemma3multimodalprojector-1"><a class="header" href="#gpu-側-gemma3multimodalprojector-1">GPU 側: Gemma3MultiModalProjector</a></h3>
<pre><code>(3, 4096, 1152)
    │  → reshape → (3, 1152, 64, 64)
    │  AvgPool2d(k=4, s=4)
    ▼
(3, 1152, 16, 16)
    │  flatten + transpose
    ▼
(3, 256, 1152)
    │  RMSNorm → matmul(1152 → 5376)
    ▼
(3, 256, 5376)
</code></pre>
<h3 id="gpu-側-split--flatten-1"><a class="header" href="#gpu-側-split--flatten-1">GPU 側: split + flatten</a></h3>
<pre><code>(3, 256, 5376)
    │  split by num_patches=[3] → [(3, 256, 5376)]
    │  flatten(0, 1)
    ▼
(768, 5376)                     ← 3 × 256 = 768 トークン
</code></pre>
<h3 id="gpu-側-テキスト埋め込みとマージ-1"><a class="header" href="#gpu-側-テキスト埋め込みとマージ-1">GPU 側: テキスト埋め込みとマージ</a></h3>
<pre><code>input_ids 中の token_id=262144 が768箇所
↓ masked_scatter_ で (768, 5376) を順番に書き込み
→ (seq_len, 5376) として Gemma3 Decoder へ
</code></pre>
<h3 id="消費トークン数-768--256--3"><a class="header" href="#消費トークン数-768--256--3">消費トークン数: <strong>768</strong> (= 256 × 3)</a></h3>
<hr>
<h2 id="プロンプト比較"><a class="header" href="#プロンプト比較">プロンプト比較</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th></th><th>Pan-and-Scan 無効（デフォルト）</th><th>Pan-and-Scan 有効</th></tr>
</thead>
<tbody>
<tr><td>プロンプト書き換え</td><td>なし</td><td>“Here is the original image … crops …” 挿入</td></tr>
<tr><td>boi_token 数</td><td>1</td><td>1 + num_crops (= 3)</td></tr>
<tr><td><code>&lt;image&gt;</code> トークン数</td><td>256</td><td>256 × (1 + num_crops) = 768</td></tr>
<tr><td>装飾テキスト</td><td>なし</td><td>“Here is the original image”, “and here are some crops to help you see better”</td></tr>
<tr><td>pixel_values shape</td><td>(1, 3, 896, 896)</td><td>(3, 3, 896, 896)</td></tr>
<tr><td>num_patches</td><td>tensor([1])</td><td>tensor([3])</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="全体データフロー図"><a class="header" href="#全体データフロー図">全体データフロー図</a></h2>
<pre><code>                        ┌─────────────────────────────┐
                        │  OpenAI 互換 API リクエスト    │
                        │  画像1枚 + テキスト           │
                        └────────────┬────────────────┘
                                     │
                        ┌────────────┴────────────────┐
                        │  chat_template 適用           │
                        │  → "&lt;start_of_image&gt;テキスト"  │
                        │    boi_token(255999) が1個    │
                        └────────────┬────────────────┘
                                     │
                        ┌────────────┴────────────────┐
                        │  CPU: Gemma3Processor        │
                        │                              │
                        │  image_processor:             │
                        │    resize(896×896)            │
                        │    rescale(×1/255)            │
                        │    normalize(0.5, 0.5)        │
                        │    Pan-and-Scan 時はクロップ生成│
                        │                              │
                        │  do_pan_and_scan?             │
                        │  ├── False:                   │
                        │  │   書き換えなし              │
                        │  │   boi_token 1個のまま       │
                        │  │   pixel_values: (1,3,896,896)│
                        │  │                            │
                        │  └── True &amp; ratio &gt; 1.2:      │
                        │      "Here is the original    │
                        │       image &lt;boi&gt; and here    │
                        │       are some crops ...      │
                        │       &lt;boi&gt; &lt;boi&gt;"            │
                        │      boi_token 3個に増加       │
                        │      pixel_values: (3,3,896,896)│
                        │                              │
                        │  各 boi_token を展開:          │
                        │  "\n\n&lt;boi&gt;&lt;img&gt;×256&lt;eoi&gt;\n\n" │
                        │                              │
                        │  tokenizer → input_ids        │
                        │  token_type_ids 生成           │
                        └────────────┬────────────────┘
                                     │
                        input_ids:     [..., 262144×256, ...(×N)...]
                        pixel_values:  (total_patches, 3, 896, 896)
                        num_patches:   (num_images,)
                                     │
                        ═════════════╪═══════════════ CPU → GPU
                                     │
                        ┌────────────┴────────────────┐
                        │  GPU: SiglipVisionEmbeddings │
                        │  Conv2d(3→1152, k=14, s=14)  │
                        │  + position_embedding         │
                        └────────────┬────────────────┘
                                     │
                        (total_patches, 4096, 1152)
                                     │
                        ┌────────────┴────────────────┐
                        │  GPU: SiglipEncoder           │
                        │  27層 双方向 Transformer       │
                        │  heads=16, hidden=1152        │
                        └────────────┬────────────────┘
                                     │
                        (total_patches, 4096, 1152)
                                     │
                        ┌────────────┴────────────────┐
                        │  GPU: Gemma3MultiModalProjector│
                        │  reshape → (*, 1152, 64, 64) │
                        │  AvgPool2d(k=4, s=4)          │
                        │  → (*, 1152, 16, 16)          │
                        │  flatten + transpose           │
                        │  → (*, 256, 1152)             │
                        │  GemmaRMSNorm(1152)            │
                        │  matmul(1152 → 5376)           │
                        └────────────┬────────────────┘
                                     │
                        (total_patches, 256, 5376)
                                     │
                        ┌────────────┴────────────────┐
                        │  split by num_patches         │
                        │  flatten(0, 1) per image      │
                        │  → list[(N×256, 5376)]        │
                        └────────────┬────────────────┘
                                     │
                        ┌────────────┴────────────────┐
                        │  GPU: embed_input_ids()       │
                        │                              │
                        │  text = embed_tokens(ids)     │
                        │         × normalizer          │
                        │  ※ 262144 は vocab 外         │
                        │    → handle_oov_mm_token=True │
                        │    → ゼロ埋め                  │
                        │                              │
                        │  masked_scatter_(             │
                        │    text, is_multimodal,       │
                        │    mm_embeds)                 │
                        │                              │
                        │  ※ vision embeds には          │
                        │    normalizer 未適用            │
                        └────────────┬────────────────┘
                                     │
                        (seq_len, 5376)
                                     │
                        ┌────────────┴────────────────┐
                        │  GPU: Gemma3 Decoder          │
                        │  62層, heads=32, kv_heads=16  │
                        │  sliding_window=1024          │
                        │  head_dim=128                 │
                        └──────────────────────────────┘
</code></pre>
<hr>
<h2 id="注意事項-1"><a class="header" href="#注意事項-1">注意事項</a></h2>
<ol>
<li>
<p><strong>ビジョン埋め込みの正規化</strong>: テキスト埋め込みには <code>embed_tokens(ids) × normalizer</code> のスケーリングが適用されるが、ビジョン埋め込みには <code>mm_soft_emb_norm</code>（RMSNorm）のみが適用され、<code>normalizer</code> スケーリングは適用されない。</p>
</li>
<li>
<p><strong>V1 での制限</strong>: Pan-and-Scan 有効時、V1 エンジンでは画像トークン間の双方向アテンションが簡略化されたパターンで実装されており、元モデルのアテンションパターンと完全には一致しない。</p>
</li>
<li>
<p><strong>AvgPool2d の役割</strong>: エンコーダは 4096 パッチ（64×64 グリッド）の高解像度で処理しつつ、AvgPool2d(k=4, s=4) で 256 トークン（16×16）に圧縮して LLM に渡す。これにより計算量と情報量のバランスを取っている。</p>
</li>
<li>
<p><strong>Pan-and-Scan のプロンプト</strong>: クロップありの場合のみ、Processor が “Here is the original image … and here are some crops to help you see better …” という装飾テキストを自動挿入する。クロップなしの場合この装飾テキストは存在せず、<code>&lt;image&gt;</code> トークン列のみとなる。ユーザーはクロップの存在を意識する必要はない。</p>
</li>
<li>
<p><strong>token_id=262144 の扱い</strong>: <code>&lt;image&gt;</code> トークンの token_id=262144 は通常の vocab 範囲外（OOV）。<code>handle_oov_mm_token=True</code> により安全にゼロ埋めされ、後続の <code>masked_scatter_</code> でビジョン埋め込みに上書きされる。</p>
</li>
<li>
<p><strong>Pan-and-Scan のデフォルト値の出典</strong>: <code>min_ratio_to_activate=1.2</code> 等の値は Google がモデルと共に配布した設定ではなく（preprocessor_config.json では全て null）、HF transformers の <code>processing_gemma3.py</code> 内の <code>Gemma3ProcessorKwargs._defaults</code> にハードコードされたフォールバック値。</p>
</li>
</ol>
<hr>
<h2 id="主要ファイル参照"><a class="header" href="#主要ファイル参照">主要ファイル参照</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th></tr>
</thead>
<tbody>
<tr><td>vllm/…/gemma3_mm.py</td><td>Gemma3ForConditionalGeneration, Gemma3MultiModalProjector, Gemma3ProcessingInfo</td></tr>
<tr><td>vllm/…/siglip.py</td><td>SiglipVisionModel, SiglipVisionEmbeddings, SiglipEncoder</td></tr>
<tr><td>vllm/…/utils.py</td><td>_merge_multimodal_embeddings()</td></tr>
<tr><td>HF transformers/…/processing_gemma3.py</td><td>Gemma3Processor, Gemma3ProcessorKwargs (デフォルト値定義)</td></tr>
<tr><td>HF transformers/…/image_processing_gemma3.py</td><td>Gemma3ImageProcessor (Pan-and-Scan 実装)</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="プロセスアーキテクチャtp2構成"><a class="header" href="#プロセスアーキテクチャtp2構成">プロセスアーキテクチャ（TP=2構成）</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-14</p>
</blockquote>
<h2 id="概要-19"><a class="header" href="#概要-19">概要</a></h2>
<p>vLLMをGPU2枚・TP=2で起動した場合のプロセス構成、コンポーネント配置、プロセス間通信メカニズムを調査した。</p>
<h2 id="1-プロセス構成合計4プロセス"><a class="header" href="#1-プロセス構成合計4プロセス">1. プロセス構成（合計4プロセス）</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>プロセス名</th><th>生成元</th><th>含まれるコンポーネント</th></tr>
</thead>
<tbody>
<tr><td>Frontend（メインプロセス）</td><td>ユーザー起動</td><td>AsyncLLM, InputProcessor, EngineCoreClient, OutputProcessor</td></tr>
<tr><td>EngineCore (<code>EngineCore_DP0</code>)</td><td>Frontend (<code>mp.Process</code>)</td><td>EngineCore, Scheduler, KVCacheManager, MultiprocExecutor</td></tr>
<tr><td>VllmWorker-0</td><td>EngineCore (<code>mp.Process</code>)</td><td>Worker, GPUModelRunner（GPU 0）</td></tr>
<tr><td>VllmWorker-1</td><td>EngineCore (<code>mp.Process</code>)</td><td>Worker, GPUModelRunner（GPU 1）</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core_client.py:493-507</code> (CoreEngineProcManager)
<strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:147-160</code> (WorkerProc起動)</p>
<h3 id="コンポーネントとプロセスの対応図"><a class="header" href="#コンポーネントとプロセスの対応図">コンポーネントとプロセスの対応図</a></h3>
<pre><code>┌─ Frontend Process ─────────────────────────────────────┐
│  AsyncLLM ─→ InputProcessor                            │
│  EngineCoreClient (ZMQ ROUTER/PULL)                    │
│  OutputProcessor ←─ Detokenizer                        │
└──────────────────────────┬─────────────────────────────┘
                           │ ZMQ (msgpack)
                           ▼
┌─ EngineCore Process ─────────────────────────────────────┐
│  EngineCore.step()                                       │
│  ├─ Scheduler ─→ KVCacheManager                         │
│  └─ MultiprocExecutor                                    │
│       ├─ rpc_broadcast_mq (SharedMemory → 全Worker)      │
│       └─ worker_response_mq × 2 (各Worker → Executor)   │
└──────────┬──────────────────────────────┬────────────────┘
           │ SharedMemory MQ              │ SharedMemory MQ
           ▼                              ▼
┌─ Worker-0 Process ──┐  ┌─ Worker-1 Process ──┐
│  Worker              │  │  Worker              │
│  GPUModelRunner      │  │  GPUModelRunner      │
│  (GPU 0, TP rank 0)  │  │  (GPU 1, TP rank 1)  │
└──────────┬───────────┘  └──────────┬───────────┘
           │         NCCL            │
           └─────────────────────────┘
             (NVLink / PCIe 直接通信)
</code></pre>
<p><strong>注意点</strong>:</p>
<ul>
<li>Scheduler、KVCacheManagerは<strong>EngineCoreプロセス内</strong>で動作し、独立プロセスではない</li>
<li>OutputProcessorは<strong>Frontendプロセス内</strong>で動作する（バックエンドではない）</li>
<li>MultiprocExecutorはEngineCoreプロセス内に存在し、Workerプロセスへの指令管理を行う</li>
</ul>
<h2 id="2-プロセス間通信メカニズム"><a class="header" href="#2-プロセス間通信メカニズム">2. プロセス間通信メカニズム</a></h2>
<h3 id="21-frontend--enginecore-zmq-over-tcp-loopback"><a class="header" href="#21-frontend--enginecore-zmq-over-tcp-loopback">2.1 Frontend ↔ EngineCore: ZMQ over TCP loopback</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>項目</th><th>値</th></tr>
</thead>
<tbody>
<tr><td>プロトコル</td><td>ZMQ over TCP (<code>127.0.0.1:&lt;random_port&gt;</code>)</td></tr>
<tr><td>ソケット型</td><td>Frontend: ROUTER(送信) + PULL(受信), EngineCore: DEALER(受信)</td></tr>
<tr><td>シリアライゼーション</td><td>msgpack（<code>msgspec.Struct(array_like)</code> 対応）</td></tr>
<tr><td>スレッドモデル</td><td>バックグラウンドスレッドでシリアライゼーション/デシリアライゼーション</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core_client.py:510-515</code> (ZMQソケット設定)
<strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:877-950</code> (EngineCoreProc._perform_handshake)</p>
<h3 id="22-enginecore--workers-sharedmemory-messagequeue"><a class="header" href="#22-enginecore--workers-sharedmemory-messagequeue">2.2 EngineCore ↔ Workers: SharedMemory MessageQueue</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>項目</th><th>値</th></tr>
</thead>
<tbody>
<tr><td>プロトコル</td><td>共有メモリ（ShmRingBuffer） + ZMQ PUB/SUB（オーバーフロー時）</td></tr>
<tr><td>キュー</td><td>rpc_broadcast_mq（1対多）+ worker_response_mq（各Worker→Executor）</td></tr>
<tr><td>シリアライゼーション</td><td>pickle（protocol 5, out-of-band buffers対応）</td></tr>
<tr><td>同期方式</td><td>ロックフリー。メモリフェンス（<code>threading.Lock</code> acquire/release, ~20ns）のみ</td></tr>
<tr><td>バッファサイズ</td><td>デフォルト24MiB/チャンク × 10チャンク</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/device_communicators/shm_broadcast.py:127</code> (ShmRingBuffer)
<strong>参照</strong>: <code>target/vllm/vllm/distributed/device_communicators/shm_broadcast.py:272</code> (MessageQueue)
<strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:131-136</code> (rpc_broadcast_mq生成)</p>
<h4 id="shmringbuffer-メモリレイアウト"><a class="header" href="#shmringbuffer-メモリレイアウト">ShmRingBuffer メモリレイアウト</a></h4>
<pre><code>┌─────────────────────────────────┬──────────────────────────────────────┐
│ data: chunk0 | chunk1 | ... | chunkN │ metadata: [written|r0|r1|...|rN] × N │
│ max_chunks × max_chunk_bytes (24MiB) │ max_chunks × (1 + n_reader) bytes    │
└─────────────────────────────────┴──────────────────────────────────────┘
</code></pre>
<p>メタデータの状態遷移:</p>
<ul>
<li><code>0???...???</code>: 未書き込み → 書き込み可</li>
<li><code>1000...000</code>: 書き込み直後 → 全reader読み取り可</li>
<li><code>1???...???</code>: 一部readerが読み取り済み</li>
<li><code>1111...111</code>: 全reader読み取り済み → 書き込み可（再利用）</li>
</ul>
<p><strong>オーバーフロー処理</strong>: データが24MiBを超える場合、ZMQ PUB/SUBソケット（IPC）経由で転送する。ローカルではXPUB/SUBソケット、リモート（マルチノード時）ではTCPソケットを使用。</p>
<h4 id="collective_rpc-の動作フロー-1"><a class="header" href="#collective_rpc-の動作フロー-1">collective_rpc の動作フロー</a></h4>
<pre><code>MultiprocExecutor.collective_rpc("execute_model", args=(scheduler_output,))
  │
  ├─ rpc_broadcast_mq.enqueue((method, args, kwargs, output_rank))
  │   → pickle → ShmRingBuffer書き込み → メモリフェンス
  │
  ├─ Worker-0: rpc_broadcast_mq.dequeue() → Worker.execute_model()
  │   → worker_response_mq.enqueue((SUCCESS, output))
  │
  ├─ Worker-1: rpc_broadcast_mq.dequeue() → Worker.execute_model()
  │   → worker_response_mq.enqueue((SUCCESS, output))
  │
  └─ Executor: response_mqs[0].dequeue() → output[0] を返却
      （output_rank=0 の場合、rank 0 の結果のみ返す）
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:303-375</code> (collective_rpc)
<strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:845-871</code> (worker_busy_loop)</p>
<h3 id="23-worker--worker-torchdistributed--nccl"><a class="header" href="#23-worker--worker-torchdistributed--nccl">2.3 Worker ↔ Worker: torch.distributed + NCCL</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>項目</th><th>値</th></tr>
</thead>
<tbody>
<tr><td>初期化</td><td><code>torch.distributed.init_process_group(backend="nccl")</code></td></tr>
<tr><td>Rendezvous</td><td>TCP（<code>tcp://127.0.0.1:&lt;random_port&gt;</code>）</td></tr>
<tr><td>通信</td><td>NCCL（NVLink / PCIe によるGPU間直接通信）</td></tr>
<tr><td>用途</td><td>Tensor Parallelの<code>all_reduce()</code>, <code>all_gather()</code>, <code>broadcast()</code></td></tr>
<tr><td>タイミング</td><td>モデル forward pass 内部でレイヤーごとに実行</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_worker.py:263-269</code> (init_worker_distributed_environment)</p>
<p>NCCLの初期化は<code>Worker.init_device()</code>内で、メモリプロファイリング<strong>前</strong>に行われる。これによりNCCLバッファが確保された後の利用可能メモリが正確に計測される。</p>
<h2 id="3-起動シーケンス"><a class="header" href="#3-起動シーケンス">3. 起動シーケンス</a></h2>
<pre><code>1. ユーザーが AsyncLLM を生成
2. AsyncLLM → EngineCoreClient.make_async_mp_client()
3.   └─ mp.Process(target=EngineCoreProc.run_engine_core) 起動
4.       └─ EngineCore.__init__() 内で MultiprocExecutor 生成
5.           ├─ distributed_init_method = "tcp://127.0.0.1:&lt;port&gt;" 確保
6.           ├─ rpc_broadcast_mq (ShmRingBuffer) 作成
7.           └─ for rank in [0, 1]:
8.               mp.Process(target=WorkerProc.worker_main) 起動
9.                 ├─ Worker.init_device():
10.                │   └─ torch.distributed.init_process_group(backend="nccl")
11.                ├─ Worker.load_model(): モデルロード
12.                ├─ READY メッセージ送信（Pipe経由）
13.                └─ worker_busy_loop() でRPC待機開始
14.      └─ wait_until_ready(): 全Worker READY 待ち
15. Frontend ↔ EngineCore ZMQハンドシェイク完了
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:696</code> (WorkerProc.worker_main)
<strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:752-770</code> (READY送信)</p>
<h2 id="4-通信方式の設計判断"><a class="header" href="#4-通信方式の設計判断">4. 通信方式の設計判断</a></h2>
<h3 id="なぜ-frontend--enginecore-は-zmq-なのか"><a class="header" href="#なぜ-frontend--enginecore-は-zmq-なのか">なぜ Frontend ↔ EngineCore は ZMQ なのか</a></h3>
<ol>
<li><strong>疎結合</strong>: Data Parallelism構成では別ノードに配置される可能性がある。ZMQはネットワーク透過</li>
<li><strong>asyncio統合</strong>: Frontendはasyncioイベントループ上で動作し、ZMQのasyncioポーラーと相性がよい</li>
<li><strong>バックグラウンドスレッドでの直列化</strong>: msgpackシリアライゼーションをバックグラウンドスレッドで行い、GPU計算とオーバーラップ可能</li>
<li><strong>メッセージ順序保証</strong>: ROUTER/DEALERソケットで確定的なメッセージ順序を保証</li>
</ol>
<h3 id="なぜ-enginecore--workers-は-sharedmemory-mq-なのかzmqではない理由"><a class="header" href="#なぜ-enginecore--workers-は-sharedmemory-mq-なのかzmqではない理由">なぜ EngineCore ↔ Workers は SharedMemory MQ なのか（ZMQではない理由）</a></h3>
<ol>
<li><strong>低レイテンシ</strong>: 同一ノード内通信に特化。ZMQはネットワークソケット抽象であり、カーネル空間でのバッファコピーとシステムコールのオーバーヘッドがある</li>
<li><strong>ゼロコピー可能</strong>: 共有メモリ上でpickleデータを直接読み書きでき、プロセス間のデータコピーが不要</li>
<li><strong>ロックフリー設計</strong>: リングバッファ + メタデータフラグ + メモリフェンス（~20ns）で同期。ロック競合なし</li>
<li><strong>collective_rpc最適化</strong>: 1対多ブロードキャスト（rpc_broadcast_mq）パターンにリングバッファが最適</li>
</ol>
<h3 id="なぜ-worker--worker-は-nccl-なのか"><a class="header" href="#なぜ-worker--worker-は-nccl-なのか">なぜ Worker ↔ Worker は NCCL なのか</a></h3>
<ol>
<li><strong>GPU間テンソル通信専用</strong>: NCCLはGPUメモリ間の集合通信（all-reduce等）に特化した高性能ライブラリ</li>
<li><strong>NVLink活用</strong>: GPU間直接通信でCPU介在なし。NVLink（最大900GB/s）やPCIe（最大64GB/s）を直接利用</li>
<li><strong>PyTorch統合</strong>: モデルコード内の<code>torch.distributed</code>呼び出しと直接統合</li>
<li><strong>Pythonオブジェクト不可</strong>: NCCLはテンソル転送専用であり、Pythonオブジェクト（SchedulerOutput等）の転送には使えない</li>
</ol>
<h3 id="通信方式比較"><a class="header" href="#通信方式比較">通信方式比較</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>通信路</th><th>方式</th><th>レイテンシ</th><th>帯域幅</th><th>転送対象</th><th>ネットワーク透過</th></tr>
</thead>
<tbody>
<tr><td>Frontend ↔ EngineCore</td><td>ZMQ (TCP)</td><td>~µs</td><td>中</td><td>Pythonオブジェクト (msgpack)</td><td>Yes</td></tr>
<tr><td>EngineCore ↔ Workers</td><td>SharedMemory MQ</td><td>~20ns同期</td><td>高</td><td>Pythonオブジェクト (pickle)</td><td>No（同一ノード限定）</td></tr>
<tr><td>Worker ↔ Worker</td><td>NCCL</td><td>~µs</td><td>最高</td><td>GPUテンソルのみ</td><td>Yes（multi-node NCCL対応）</td></tr>
</tbody>
</table>
</div>
<h2 id="5-tp1単一gpuとの比較"><a class="header" href="#5-tp1単一gpuとの比較">5. TP=1（単一GPU）との比較</a></h2>
<p>TP=1の場合、<code>UniProcExecutor</code>が選択される:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>項目</th><th>TP=1</th><th>TP=2</th></tr>
</thead>
<tbody>
<tr><td>Executor</td><td>UniProcExecutor</td><td>MultiprocExecutor</td></tr>
<tr><td>Workerプロセス</td><td>なし（EngineCoreプロセス内）</td><td>2つの子プロセス</td></tr>
<tr><td>Worker通信</td><td>関数呼び出し（同一プロセス）</td><td>SharedMemory MQ</td></tr>
<tr><td>NCCL</td><td>不要</td><td>必要（Worker間）</td></tr>
<tr><td>合計プロセス数</td><td>2（Frontend + EngineCore）</td><td>4（Frontend + EngineCore + Worker×2）</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/uniproc_executor.py:26</code> (UniProcExecutor)</p>
<h2 id="主要ファイル-7"><a class="header" href="#主要ファイル-7">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/v1/engine/async_llm.py</code></td><td><code>AsyncLLM</code> — Frontendプロセスのエントリポイント</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core_client.py</code></td><td><code>EngineCoreClient</code> — ZMQ通信, CoreEngineProcManager</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core.py</code></td><td><code>EngineCore</code>, <code>EngineCoreProc</code> — EngineCoreプロセスのエントリポイント</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/abstract.py</code></td><td><code>Executor</code> — collective_rpc(), execute_model()</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/multiproc_executor.py</code></td><td><code>MultiprocExecutor</code>, <code>WorkerProc</code> — Worker起動, MessageQueue管理, worker_busy_loop</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/uniproc_executor.py</code></td><td><code>UniProcExecutor</code> — 単一GPU用</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_worker.py</code></td><td><code>Worker</code> — init_device(), torch.distributed初期化</td></tr>
<tr><td><code>target/vllm/vllm/distributed/device_communicators/shm_broadcast.py</code></td><td><code>ShmRingBuffer</code>, <code>MessageQueue</code> — 共有メモリ通信基盤</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="用語集"><a class="header" href="#用語集">用語集</a></h1>
<!-- 調査中に発見した対象OSS固有の用語をここに蓄積する -->
<h3 id="pagedattention-1"><a class="header" href="#pagedattention-1">PagedAttention</a></h3>
<p>KVキャッシュをOSの仮想メモリページングに着想を得て、固定サイズのブロック単位で管理する技術。連続したGPUメモリ確保が不要になり、メモリ断片化を大幅に抑制する。SOSP 2023論文で提案。</p>
<p><strong>参照</strong>: <code>target/vllm/csrc/attention/</code> (カーネル実装)</p>
<h3 id="continuous-batching-1"><a class="header" href="#continuous-batching-1">Continuous Batching</a></h3>
<p>リクエストの到着・完了に応じてバッチを動的に更新する手法。固定バッチサイズと異なり、GPU稼働率を最大化できる。vLLMのSchedulerが担う。</p>
<h3 id="prefill"><a class="header" href="#prefill">Prefill</a></h3>
<p>プロンプト入力トークン全体を処理してKVキャッシュに書き込む最初のフェーズ。計算量が多く、GPUの並列性を活かしやすい。</p>
<h3 id="decode"><a class="header" href="#decode">Decode</a></h3>
<p>生成済みコンテキストのKVキャッシュを参照しながら次のトークンを1つずつ逐次生成するフェーズ。メモリバウンドになりやすい。</p>
<h3 id="chunked-prefill"><a class="header" href="#chunked-prefill">Chunked Prefill</a></h3>
<p>Prefillフェーズをチャンクに分割してDecodeフェーズと交互実行する手法。長いプロンプトがDecodeのレイテンシを増加させるのを防ぐ。</p>
<h3 id="enginecore"><a class="header" href="#enginecore">EngineCore</a></h3>
<p>vLLMの推論ループの内側コンポーネント。別プロセス（<code>EngineCoreProc</code>）として動作し、ZeroMQソケットで上位エンジン層と通信する。Scheduler、KVCacheManager、Executorを統括する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:79</code> (<code>EngineCore</code>)</p>
<h3 id="kvcachemanager"><a class="header" href="#kvcachemanager">KVCacheManager</a></h3>
<p>KVキャッシュブロックの割り当て・解放・プレフィックスキャッシュを管理するクラス。BlockPoolを内部で使用する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_manager.py:94</code> (<code>KVCacheManager</code>)</p>
<h3 id="kvcacheblock"><a class="header" href="#kvcacheblock">KVCacheBlock</a></h3>
<p>PagedAttentionで管理するKVキャッシュの最小単位。固定サイズ（block_sizeトークン分）のGPUメモリブロック。</p>
<h3 id="blockpool"><a class="header" href="#blockpool">BlockPool</a></h3>
<p>KVCacheBlockの空きブロックをプール管理するクラス。ブロックの割り当て・返却を効率的に行う。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py</code></p>
<h3 id="vllmconfig"><a class="header" href="#vllmconfig">VllmConfig</a></h3>
<p>全設定を集約するトップレベルクラス。<code>ModelConfig</code>、<code>CacheConfig</code>、<code>SchedulerConfig</code>、<code>ParallelConfig</code>等を内包する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/config/vllm.py</code></p>
<h3 id="gpumodelrunner-1"><a class="header" href="#gpumodelrunner-1">GPUModelRunner</a></h3>
<p>GPU上でモデルのフォワードパスを実際に実行するクラス。LoRA、KVConnector、ECConnectorのMixinを持つ。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:329</code> (<code>GPUModelRunner</code>)</p>
<h3 id="executor-1"><a class="header" href="#executor-1">Executor</a></h3>
<p>Worker群を管理する抽象層。シングルプロセス（<code>UniProcExecutor</code>）、マルチプロセス（<code>MultiprocExecutor</code>）、Ray分散（<code>RayDistributedExecutor</code>）の実装がある。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py</code></p>
<h3 id="worker"><a class="header" href="#worker">Worker</a></h3>
<p>1つのGPU（またはCPU/XPU）デバイスを担当するプロセス。GPUModelRunnerを保持し、Executorから呼び出される。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_worker.py:70</code> (<code>Worker</code>)</p>
<h3 id="speculative-decoding"><a class="header" href="#speculative-decoding">Speculative Decoding</a></h3>
<p>ドラフトモデル（小さいモデル）で複数トークンを仮生成し、メインモデルで一括検証することで推論を高速化する手法。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/spec_decode/</code></p>
<h3 id="lora-low-rank-adaptation"><a class="header" href="#lora-low-rank-adaptation">LoRA (Low-Rank Adaptation)</a></h3>
<p>少量の追加パラメータでLLMをファインチューニングする手法。vLLMは複数LoRAの動的切替（Multi-LoRA）をランタイムでサポートする。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/lora/</code></p>
<h3 id="kv-transfer"><a class="header" href="#kv-transfer">KV Transfer</a></h3>
<p>複数のvLLMインスタンス間でKVキャッシュを転送する機構。Disaggregated Prefill（PrefillとDecodeを異なるインスタンスで実行）等に使用。KVConnector抽象基底クラスとして実装され、LMCache、NIXL、P2P NCCL、Mooncake等の複数バックエンドがある。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/</code> (全体), <code>target/vllm/vllm/config/kv_transfer.py:17</code> (<code>KVTransferConfig</code>)</p>
<h3 id="lmcache"><a class="header" href="#lmcache">LMCache</a></h3>
<p>vLLMと統合可能な外部KVキャッシュストレージ。KV Transferのバックエンドの一つとして動作し、KVキャッシュのCPUオフロード、インスタンス間共有等を提供する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py</code>, <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/</code></p>
<h3 id="multimodal-マルチモーダル"><a class="header" href="#multimodal-マルチモーダル">Multimodal (マルチモーダル)</a></h3>
<p>テキスト以外の入力（画像・動画・音声）を扱うモデル機能。<code>vllm/multimodal/</code> にプロセッサ・レジストリ等が実装されている。Gemma3等のマルチモーダルモデルが対応。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/</code></p>
<h3 id="unified-compute-model-1"><a class="header" href="#unified-compute-model-1">Unified Compute Model</a></h3>
<p>vLLM v1のSchedulerが採用するスケジューリングアプローチ。PrefillフェーズとDecodeフェーズを明示的に区別せず、各リクエストの<code>num_computed_tokens</code>（計算済みトークン数）が目標に追いつくまでトークンを割り当てる。これにより、Chunked Prefill、Prefix Caching、Speculative Decodingを統一的に扱える。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:322</code> (コメント)</p>
<h3 id="collective_rpc-1"><a class="header" href="#collective_rpc-1">collective_rpc</a></h3>
<p>Executor層が全Workerに対して同一メソッドを実行するRPCパターン。メソッド名（文字列）または関数を受け取り、全Workerで並列実行後、出力ランクのWorkerの結果を返す。<code>non_block=True</code>でFuture返却も可能。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py:180</code> (<code>collective_rpc</code>)</p>
<h3 id="executemodelstate-1"><a class="header" href="#executemodelstate-1">ExecuteModelState</a></h3>
<p>GPUModelRunnerの2フェーズ実行パターンで使用される一時状態。execute_model()がlogitsやhidden_statesなどのGPUテンソルを保存し、sample_tokens()が復元してサンプリングを行う。NamedTuple。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:313</code> (<code>ExecuteModelState</code>)</p>
<h3 id="outputprocessor-1"><a class="header" href="#outputprocessor-1">OutputProcessor</a></h3>
<p>フロントエンドプロセスで動作し、EngineCoreOutputをRequestOutputに変換するコンポーネント。インクリメンタルデトークナイズ、停止文字列判定、logprobs処理を行う。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/output_processor.py:73</code> (<code>OutputProcessor</code>)</p>
<h3 id="incrementaldetokenizer"><a class="header" href="#incrementaldetokenizer">IncrementalDetokenizer</a></h3>
<p>トークンIDからテキストへのインクリメンタル変換を行うクラス。FastIncrementalDetokenizer（HF DecodeStream）とSlowIncrementalDetokenizer（Python実装）の2種がある。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/detokenizer.py:30</code> (<code>IncrementalDetokenizer</code>)</p>
<h3 id="requestoutputkind"><a class="header" href="#requestoutputkind">RequestOutputKind</a></h3>
<p>出力モードを定義するEnum。CUMULATIVE（毎回全出力）、DELTA（差分のみ、ストリーミング向け）、FINAL_ONLY（完了時のみ）の3値。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/sampling_params.py:108</code> (<code>RequestOutputKind</code>)</p>
<h3 id="mm_cache-マルチモーダルキャッシュ"><a class="header" href="#mm_cache-マルチモーダルキャッシュ">mm_cache (マルチモーダルキャッシュ)</a></h3>
<p>マルチモーダル入力（画像エンコーダ出力等）のキャッシュ機構。同一画像の繰り返し処理を避けるため、エンコーダ出力をキャッシュする。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu/mm/encoder_runner.py</code></p>
<h3 id="freekvcacheblockqueue"><a class="header" href="#freekvcacheblockqueue">FreeKVCacheBlockQueue</a></h3>
<p>空きKVキャッシュブロックをLRU順序で管理する双方向リンクリスト。Pythonの<code>deque</code>ではなく独自実装を採用し、O(1)の中間要素削除をサポートする。センチネルノード（fake_head/fake_tail）を使用。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:156</code></p>
<h3 id="blockhashwithgroupid"><a class="header" href="#blockhashwithgroupid">BlockHashWithGroupId</a></h3>
<p><code>BlockHash</code>（ブロックのハッシュ値）にKVキャッシュグループID（4バイトBE）を結合したバイト列。Tuple生成を避けてGC負荷を低減する。プレフィックスキャッシュのキーとして使用。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:39</code></p>
<h3 id="null_block"><a class="header" href="#null_block">null_block</a></h3>
<p>BlockPoolが保持する特殊なKVCacheBlock（block_id=0, is_null=True）。Sliding Window Attentionのウィンドウ外位置やMambaのスキップ位置を埋めるプレースホルダ。物理メモリを消費せず、解放・Eviction対象外。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:174</code></p>
<h3 id="kvcachecoordinator"><a class="header" href="#kvcachecoordinator">KVCacheCoordinator</a></h3>
<p>複数のKVキャッシュグループ（異なるアテンションタイプのレイヤー群）を統括する抽象クラス。NoPrefixCache、Unitary（単一グループ）、Hybrid（複数グループ）の3実装がある。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_coordinator.py:28</code></p>
<h3 id="singletypekvcachemanager"><a class="header" href="#singletypekvcachemanager">SingleTypeKVCacheManager</a></h3>
<p>1種類のアテンションタイプのKVキャッシュ管理ロジックを担当する抽象基底クラス。FullAttention、SlidingWindow、ChunkedLocalAttention、Mamba、CrossAttention、SinkFullAttentionの7実装がある。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:24</code></p>
<h3 id="cascade-attention"><a class="header" href="#cascade-attention">Cascade Attention</a></h3>
<p>全リクエストで共有される共通プレフィックスの再計算をスキップする最適化。<code>get_num_common_prefix_blocks()</code>で共通ブロック数を判定し、アテンション計算から除外する。</p>
<h3 id="sliding-window-attention"><a class="header" href="#sliding-window-attention">Sliding Window Attention</a></h3>
<p>各トークンが直近のN個のトークンにのみアテンションするメカニズム。ウィンドウ外のKVキャッシュブロックは<code>null_block</code>で置換されメモリを節約する。</p>
<h3 id="attention-sink-streamingllm"><a class="header" href="#attention-sink-streamingllm">Attention Sink (StreamingLLM)</a></h3>
<p>先頭の少数トークン（sink tokens）のKVキャッシュを常に保持しつつ、中間トークンを捨てて長いシーケンスを処理する手法。<code>SinkFullAttentionManager</code>が実装。</p>
<h3 id="multimodalfeaturespec"><a class="header" href="#multimodalfeaturespec">MultiModalFeatureSpec</a></h3>
<p>マルチモーダル入力1つ分のメタデータとテンソルデータを保持するデータクラス。<code>data</code>（処理済みテンソル、キャッシュヒット時はNone）、<code>identifier</code>（エンコーダキャッシュ用ハッシュ）、<code>mm_position</code>（PlaceholderRange）、<code>modality</code>（“image“等）を含む。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/inputs.py:337</code></p>
<h3 id="placeholderrange"><a class="header" href="#placeholderrange">PlaceholderRange</a></h3>
<p>プロンプト内のマルチモーダルプレースホルダーの位置情報。<code>offset</code>（開始位置）、<code>length</code>（トークン数）、<code>is_embed</code>（埋め込みマスク）を保持。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/inputs.py:170</code></p>
<h3 id="multimodalhasher-1"><a class="header" href="#multimodalhasher-1">MultiModalHasher</a></h3>
<p>マルチモーダルデータのコンテンツベースハッシュを計算するクラス。PIL Image、Tensor、ndarray等を決定的にシリアライズし、blake3（デフォルト）でハッシュ化する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/hasher.py:50</code></p>
<h3 id="processorcache-mm"><a class="header" href="#processorcache-mm">ProcessorCache (MM)</a></h3>
<p>フロントエンド（P0）でHF Processor処理結果をキャッシュする仕組み。4種類の実装（processor_only/lru/shm/none）があり、P0-P1間のキャッシュEviction順序を同期させることでIPCなしにキャッシュ状態を推定できる。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/cache.py</code></p>
<h3 id="encodercachemanager"><a class="header" href="#encodercachemanager">EncoderCacheManager</a></h3>
<p>バックエンド（P1）でビジョンエンコーダ出力のライフサイクルを管理するクラス。リファレンスカウント方式で複数リクエスト間のキャッシュ共有を実現し、FIFO順の遅延Evictionを行う。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:17</code></p>
<h3 id="siglipvisionmodel"><a class="header" href="#siglipvisionmodel">SiglipVisionModel</a></h3>
<p>SIGLIP（Sigmoid Loss for Language Image Pre-training）ベースのViTビジョンエンコーダ。Gemma3のビジョンタワーとして使用される。パッチ埋め込み + 位置埋め込み → Transformer Encoder（双方向Attention）の構造。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/siglip.py:848</code></p>
<h3 id="pan-and-scan"><a class="header" href="#pan-and-scan">Pan-and-Scan</a></h3>
<p>アスペクト比が大きい画像を複数のクロップに分割して詳細認識を向上させるGemma3の仕組み。V1では簡略化されたアテンションパターンのため最適でない結果になりうる。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:109</code></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ファイル索引"><a class="header" href="#ファイル索引">ファイル索引</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->
        <script src="mermaid-eefea253.min.js"></script>
        <script src="mermaid-init-ccf746f1.js"></script>

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
