<!DOCTYPE HTML>
<html lang="ja" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>コードリーディング: vLLM</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-ae938929.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex-9f7dca1f.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc-e9f58a38.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">コードリーディング: vLLM</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="はじめに"><a class="header" href="#はじめに">はじめに</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="テキスト推論データフロー"><a class="header" href="#テキスト推論データフロー">テキスト推論データフロー</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11 (セッション3で下流パス追加)</p>
</blockquote>
<h2 id="概要"><a class="header" href="#概要">概要</a></h2>
<p>テキスト推論リクエストは、APIエントリポイントからエンジン層を経てGPUで実行され、生成されたトークンがデトークナイズされてユーザーに返却される。フロー全体は5つの境界データ構造（EngineCoreRequest → SchedulerOutput → ModelRunnerOutput → EngineCoreOutput → RequestOutput）で区切られ、ZMQ IPCによるプロセス分離とasyncioによる非同期パイプラインで高スループットを実現する。</p>
<h2 id="フロー全体図"><a class="header" href="#フロー全体図">フロー全体図</a></h2>
<pre class="mermaid">graph TD
    subgraph フロントエンドプロセス
        API["API Server / LLM"]
        AsyncLLM["AsyncLLM&lt;br&gt;generate() / add_request()"]
        IP["InputProcessor&lt;br&gt;process_inputs()"]
        OP["OutputProcessor&lt;br&gt;process_outputs()"]
        Client["EngineCoreClient&lt;br&gt;AsyncMPClient"]
    end

    subgraph バックエンドプロセス ["EngineCore プロセス"]
        EC["EngineCore&lt;br&gt;step()"]
        Sched["Scheduler&lt;br&gt;schedule()"]
        KV["KVCacheManager&lt;br&gt;allocate_slots()"]
        Exec["Executor&lt;br&gt;execute_model()"]
        Worker["Worker"]
        MR["GPUModelRunner&lt;br&gt;execute_model()"]
    end

    API --&gt;|"prompt, params"| AsyncLLM
    AsyncLLM --&gt;|"prompt, params"| IP
    IP --&gt;|"EngineCoreRequest"| AsyncLLM
    AsyncLLM --&gt;|"EngineCoreRequest"| Client

    Client --&gt;|"ZMQ ROUTER\nmsgpack"| EC
    EC --&gt; Sched
    Sched --&gt;|"allocate_slots()"| KV
    Sched --&gt;|"SchedulerOutput"| EC
    EC --&gt;|"SchedulerOutput"| Exec
    Exec --&gt;|"MessageQueue\n共有メモリ"| Worker
    Worker --&gt; MR
    MR --&gt;|"ModelRunnerOutput"| Worker
    Worker --&gt;|"ModelRunnerOutput"| Exec
    Exec --&gt;|"ModelRunnerOutput"| EC
    EC --&gt;|"update_from_output()"| Sched
    Sched --&gt;|"EngineCoreOutputs"| EC

    EC --&gt;|"ZMQ PUSH\nmsgpack"| Client
    Client --&gt;|"EngineCoreOutputs"| OP
    OP --&gt;|"RequestOutput"| AsyncLLM
    AsyncLLM --&gt;|"RequestOutput"| API
</pre>

<h2 id="境界データ構造"><a class="header" href="#境界データ構造">境界データ構造</a></h2>
<p>フローは以下の5つのデータ構造で区切られる。各構造はプロセス間またはコンポーネント間の境界を定義する。</p>
<h3 id="enginecorerequest"><a class="header" href="#enginecorerequest">EngineCoreRequest</a></h3>
<p>フロントエンド → バックエンドの境界。ユーザー入力を正規化した内部表現。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/__init__.py:55</code> (EngineCoreRequest)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>request_id</code></td><td><code>str</code></td><td>内部リクエストID（外部IDに8文字ランダムサフィックス付与）</td></tr>
<tr><td><code>prompt_token_ids</code></td><td><code>list[int] | None</code></td><td>トークナイズ済みプロンプト</td></tr>
<tr><td><code>mm_features</code></td><td><code>list[MultiModalFeatureSpec] | None</code></td><td>マルチモーダル入力（テキスト推論ではNone）</td></tr>
<tr><td><code>sampling_params</code></td><td><code>SamplingParams | None</code></td><td>サンプリングパラメータ（clone済み）</td></tr>
<tr><td><code>eos_token_id</code></td><td><code>int | None</code></td><td>終了トークンID</td></tr>
<tr><td><code>arrival_time</code></td><td><code>float</code></td><td>リクエスト到着時刻</td></tr>
<tr><td><code>lora_request</code></td><td><code>LoRARequest | None</code></td><td>LoRAアダプタ情報</td></tr>
<tr><td><code>priority</code></td><td><code>int</code></td><td>優先度（デフォルト0）</td></tr>
<tr><td><code>data_parallel_rank</code></td><td><code>int | None</code></td><td>データ並列ランク指定</td></tr>
</tbody>
</table>
</div>
<p><code>msgspec.Struct</code> を継承し、<code>array_like=True</code> + <code>omit_defaults=True</code> で効率的にmsgpackシリアライズされる。</p>
<h3 id="scheduleroutput"><a class="header" href="#scheduleroutput">SchedulerOutput</a></h3>
<p>Scheduler → Executor の境界。各ステップのスケジュール結果を含む。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/output.py:184</code> (SchedulerOutput)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>scheduled_new_reqs</code></td><td><code>list[NewRequestData]</code></td><td>初回スケジュールされたリクエスト（フルデータ）</td></tr>
<tr><td><code>scheduled_cached_reqs</code></td><td><code>CachedRequestData</code></td><td>既スケジュール済みリクエスト（差分のみ）</td></tr>
<tr><td><code>num_scheduled_tokens</code></td><td><code>dict[str, int]</code></td><td>リクエストごとのスケジュールトークン数</td></tr>
<tr><td><code>total_num_scheduled_tokens</code></td><td><code>int</code></td><td>合計スケジュールトークン数</td></tr>
<tr><td><code>scheduled_spec_decode_tokens</code></td><td><code>dict[str, list[int]]</code></td><td>Speculative Decoding用トークン</td></tr>
<tr><td><code>scheduled_encoder_inputs</code></td><td><code>dict[str, list[int]]</code></td><td>エンコーダ入力インデックス（マルチモーダル）</td></tr>
<tr><td><code>num_common_prefix_blocks</code></td><td><code>list[int]</code></td><td>共通プレフィックスブロック数（Cascade Attention用）</td></tr>
<tr><td><code>finished_req_ids</code></td><td><code>set[str]</code></td><td>このステップで完了したリクエストID</td></tr>
<tr><td><code>free_encoder_mm_hashes</code></td><td><code>list[str]</code></td><td>解放するエンコーダキャッシュのmm_hash</td></tr>
<tr><td><code>preempted_req_ids</code></td><td><code>set[str] | None</code></td><td>プリエンプションされたリクエスト</td></tr>
<tr><td><code>has_structured_output_requests</code></td><td><code>bool</code></td><td>構造化出力リクエストの有無</td></tr>
<tr><td><code>pending_structured_output_tokens</code></td><td><code>bool</code></td><td>Grammar bitmask準備状態</td></tr>
<tr><td><code>num_invalid_spec_tokens</code></td><td><code>dict[str, int] | None</code></td><td>無効スペキュレーショントークン数</td></tr>
<tr><td><code>kv_connector_metadata</code></td><td><code>KVConnectorMetadata | None</code></td><td>KV Transfer メタデータ</td></tr>
<tr><td><code>ec_connector_metadata</code></td><td><code>ECConnectorMetadata | None</code></td><td>EC Transfer メタデータ</td></tr>
</tbody>
</table>
</div>
<p><strong>NewRequestData</strong> は初回スケジュール時のフルデータ（プロンプトトークン、サンプリングパラメータ、ブロックID等）を含む。<strong>CachedRequestData</strong> は既スケジュール済みリクエストの差分（新規ブロックID、新トークンID、計算済みトークン数の更新）のみを含み、プロセス間通信コストを最小化する。</p>
<h3 id="modelrunneroutput"><a class="header" href="#modelrunneroutput">ModelRunnerOutput</a></h3>
<p>GPUModelRunner → EngineCore の境界。モデル推論結果を含む。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/outputs.py:160</code> (ModelRunnerOutput)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>req_ids</code></td><td><code>list[str]</code></td><td>バッチ内のリクエストID一覧</td></tr>
<tr><td><code>req_id_to_index</code></td><td><code>dict[str, int]</code></td><td>リクエストID → バッチインデックス</td></tr>
<tr><td><code>sampled_token_ids</code></td><td><code>list[list[int]]</code></td><td>サンプリング済みトークンID [num_reqs, num_generated]</td></tr>
<tr><td><code>logprobs</code></td><td><code>LogprobsLists | None</code></td><td>生成トークンの対数確率</td></tr>
<tr><td><code>prompt_logprobs_dict</code></td><td><code>dict[str, LogprobsTensors | None]</code></td><td>プロンプトトークンの対数確率</td></tr>
<tr><td><code>pooler_output</code></td><td><code>list[Tensor | None] | None</code></td><td>プーリング出力（埋め込みモデル用）</td></tr>
<tr><td><code>kv_connector_output</code></td><td><code>KVConnectorOutput | None</code></td><td>KV Transfer出力</td></tr>
<tr><td><code>ec_connector_output</code></td><td><code>ECConnectorOutput | None</code></td><td>EC Transfer出力</td></tr>
<tr><td><code>num_nans_in_logits</code></td><td><code>dict[str, int] | None</code></td><td>logits内のNaN数</td></tr>
<tr><td><code>cudagraph_stats</code></td><td><code>CUDAGraphStat | None</code></td><td>CUDAGraph実行統計</td></tr>
</tbody>
</table>
</div>
<p>Worker→Executorへの転送ではPythonリスト形式を使用し、torch.Tensorの高コストなシリアライゼーションを回避する。</p>
<h3 id="enginecoreoutput"><a class="header" href="#enginecoreoutput">EngineCoreOutput</a></h3>
<p>バックエンド → フロントエンドの境界。リクエスト単位の推論結果。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/__init__.py:130</code> (EngineCoreOutput)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>request_id</code></td><td><code>str</code></td><td>対応するリクエストID</td></tr>
<tr><td><code>new_token_ids</code></td><td><code>list[int]</code></td><td>新たに生成されたトークンID</td></tr>
<tr><td><code>finish_reason</code></td><td><code>FinishReason | None</code></td><td>完了理由（stop/length/abort/error）</td></tr>
<tr><td><code>new_logprobs</code></td><td><code>LogprobsLists | None</code></td><td>生成トークンのlogprobs</td></tr>
<tr><td><code>num_cached_tokens</code></td><td><code>int</code></td><td>プレフィックスキャッシュヒット数</td></tr>
</tbody>
</table>
</div>
<p><code>EngineCoreOutputs</code>（複数形）がこれを<code>list[EngineCoreOutput]</code>としてバッチ化し、<code>scheduler_stats</code>やタイムスタンプと共にZMQ経由で送信される。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/__init__.py:176</code> (EngineCoreOutputs)</p>
<h3 id="requestoutput"><a class="header" href="#requestoutput">RequestOutput</a></h3>
<p>OutputProcessor → API の境界。ユーザーに返却される最終出力。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/outputs.py:86</code> (RequestOutput)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>request_id</code></td><td><code>str</code></td><td>外部リクエストID（クライアントが指定したID）</td></tr>
<tr><td><code>prompt</code></td><td><code>str | None</code></td><td>元のプロンプト文字列</td></tr>
<tr><td><code>prompt_token_ids</code></td><td><code>list[int] | None</code></td><td>トークナイズ済みプロンプト</td></tr>
<tr><td><code>prompt_logprobs</code></td><td><code>PromptLogprobs | None</code></td><td>プロンプトトークンの対数確率</td></tr>
<tr><td><code>outputs</code></td><td><code>list[CompletionOutput]</code></td><td>サンプルごとの出力（n&gt;1で複数）</td></tr>
<tr><td><code>finished</code></td><td><code>bool</code></td><td>リクエスト完了フラグ</td></tr>
<tr><td><code>metrics</code></td><td><code>RequestStateStats | None</code></td><td>レイテンシ等の統計情報</td></tr>
<tr><td><code>num_cached_tokens</code></td><td><code>int | None</code></td><td>プレフィックスキャッシュヒット数</td></tr>
<tr><td><code>kv_transfer_params</code></td><td><code>dict[str, Any] | None</code></td><td>KV Transfer情報（完了時）</td></tr>
</tbody>
</table>
</div>
<p><strong>CompletionOutput</strong> (<code>target/vllm/vllm/outputs.py:23</code>) は各サンプルの出力を表す:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>index</code></td><td><code>int</code></td><td>サンプルインデックス</td></tr>
<tr><td><code>text</code></td><td><code>str</code></td><td>デトークナイズ済みテキスト</td></tr>
<tr><td><code>token_ids</code></td><td><code>GenericSequence[int]</code></td><td>生成トークンID列</td></tr>
<tr><td><code>cumulative_logprob</code></td><td><code>float | None</code></td><td>累積対数確率</td></tr>
<tr><td><code>logprobs</code></td><td><code>SampleLogprobs | None</code></td><td>各トークンのlogprobs</td></tr>
<tr><td><code>finish_reason</code></td><td><code>str | None</code></td><td>完了理由（“stop” / “length”）</td></tr>
<tr><td><code>stop_reason</code></td><td><code>int | str | None</code></td><td>停止トークン/文字列</td></tr>
</tbody>
</table>
</div>
<p><strong>出力モード</strong>（<code>RequestOutputKind</code>、<code>target/vllm/vllm/sampling_params.py:108</code>）:</p>
<ul>
<li><code>CUMULATIVE</code>: 毎回全出力を返す（デフォルト）</li>
<li><code>DELTA</code>: 差分のみ返す（ストリーミング向け）</li>
<li><code>FINAL_ONLY</code>: 完了時のみ返す</li>
</ul>
<h2 id="上流パス-リクエスト受信--enginecore"><a class="header" href="#上流パス-リクエスト受信--enginecore">上流パス: リクエスト受信 → EngineCore</a></h2>
<h3 id="エントリポイント-llm--asyncllm"><a class="header" href="#エントリポイント-llm--asyncllm">エントリポイント (LLM / AsyncLLM)</a></h3>
<p>vLLMには同期パス（<code>LLM</code>）と非同期パス（<code>AsyncLLM</code>）の2つのエントリポイントがある。内部的にはどちらも<code>InputProcessor</code>と<code>EngineCoreClient</code>を使用する。</p>
<h4 id="非同期パス主パス-asyncllm"><a class="header" href="#非同期パス主パス-asyncllm">非同期パス（主パス）: AsyncLLM</a></h4>
<p>APIサーバー（OpenAI互換API等）が使用する主要パス。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/async_llm.py:71</code> (AsyncLLM)</p>
<pre><code>AsyncLLM.generate(prompt, sampling_params, request_id)    # L537
  │
  ├─ add_request(request_id, prompt, params)               # L286
  │   ├─ input_processor.process_inputs(prompt, params)    # L364
  │   │   → EngineCoreRequest を生成
  │   ├─ input_processor.assign_request_id(request)        # L378
  │   │   → 内部IDを付与（外部ID + 8文字ランダムサフィックス）
  │   ├─ output_processor.add_request(request, ...)        # L423
  │   │   → フロントエンド側でリクエストを登録
  │   └─ engine_core.add_request_async(request)            # L426
  │       → ZMQ経由でバックエンドへ送信
  │
  └─ while not finished:                                    # L586
      out = q.get_nowait() or await q.get()                # L589
      yield out                                             # L596
</code></pre>
<p><code>generate()</code>はAsyncGeneratorで、バックグラウンドの<code>output_handler</code>タスクがEngineCoreからの出力を<code>RequestOutputCollector</code>キューにpushし、<code>generate()</code>がそれをyieldする。</p>
<p><strong>output_handler（バックグラウンドタスク）</strong>:</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/async_llm.py:647</code> (_run_output_handler)</p>
<pre><code>output_handler():                                           # L662
  while True:
    outputs = await engine_core.get_output_async()          # L666
    for chunk in outputs.outputs:                           # L677
      output_processor.process_outputs(chunk, ...)          # L681
      → RequestOutputをキューにpush
    if reqs_to_abort:
      await engine_core.abort_requests_async(...)           # L693
</code></pre>
<h4 id="同期パス-llm"><a class="header" href="#同期パス-llm">同期パス: LLM</a></h4>
<p>オフライン推論（バッチ処理）で使用される。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/entrypoints/llm.py:396</code> (generate)</p>
<pre><code>LLM.generate(prompts, sampling_params)                      # L396
  → _run_completion(prompts, params)                        # L449
    → _add_request(prompt, params) × N                      # L1850
    │  ├─ input_processor.process_inputs(prompt, params)    # L1879
    │  └─ llm_engine.add_request(request_id, request, ...)  # L1889
    → _run_engine()                                         # L1900
       while has_unfinished_requests():                     # L1918
         step_outputs = llm_engine.step()                   # L1919
</code></pre>
<p>同期パスとの主な違い:</p>
<ul>
<li><code>LLM</code>は<code>_run_engine()</code>でポーリングループを回す（AsyncGeneratorではない）</li>
<li><code>llm_engine</code>（=<code>AsyncLLM</code>のラッパー）の<code>step()</code>を直接呼ぶ</li>
<li>プログレスバー（tqdm）でバッチ処理の進捗を表示</li>
</ul>
<h3 id="入力処理-inputprocessor"><a class="header" href="#入力処理-inputprocessor">入力処理 (InputProcessor)</a></h3>
<p><code>InputProcessor</code>はユーザー入力（テキストプロンプト、パラメータ）を<code>EngineCoreRequest</code>に変換する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/input_processor.py:56</code> (InputProcessor)</p>
<pre><code>InputProcessor.process_inputs(request_id, prompt, params)   # L521
  ├─ _validate_lora(lora_request)                           # L535
  ├─ _validate_params(params)                               # L536
  ├─ data_parallel_rank の範囲チェック                       # L542
  ├─ arrival_time 設定（未指定なら time.time()）             # L548
  │
  ├─ input_preprocessor.preprocess(prompt, ...)             # L581
  │   → テキストをトークナイズ（tokenizer.encode()）
  │   → ProcessorInputs を返す
  │
  ├─ split_enc_dec_inputs(processed_inputs)                 # L597
  │   → エンコーダ/デコーダ入力を分離
  │
  ├─ SamplingParams の正規化                                # L608-623
  │   ├─ params.clone()                                     # L612
  │   ├─ max_tokens 未設定時: max_model_len - seq_len       # L614-618
  │   ├─ update_from_generation_config()                    # L619
  │   └─ update_from_tokenizer()                            # L623
  │
  └─ EngineCoreRequest を構築して返す                        # L656-671
</code></pre>
<p>テキスト推論の場合、マルチモーダル関連処理（L630-654）はスキップされる（<code>mm_features</code>はNone）。</p>
<h3 id="プロセス間通信-enginecoreclient--zmq-ipc"><a class="header" href="#プロセス間通信-enginecoreclient--zmq-ipc">プロセス間通信 (EngineCoreClient / ZMQ IPC)</a></h3>
<p><code>EngineCoreClient</code>はフロントエンドプロセスとバックエンドプロセス（EngineCore）間のZMQ IPC通信を担当する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core_client.py:63</code> (EngineCoreClient)</p>
<h4 id="クライアント階層"><a class="header" href="#クライアント階層">クライアント階層</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>クラス</th><th>用途</th><th>トランスポート</th></tr>
</thead>
<tbody>
<tr><td><code>EngineCoreClient</code> (ABC)</td><td>抽象インターフェース</td><td>—</td></tr>
<tr><td><code>InprocClient</code></td><td>インプロセス（デバッグ用）</td><td>直接呼び出し</td></tr>
<tr><td><code>SyncMPClient</code></td><td>同期マルチプロセス（LLM用）</td><td>ZMQ同期</td></tr>
<tr><td><code>AsyncMPClient</code></td><td>非同期マルチプロセス（AsyncLLM用）</td><td>ZMQ非同期</td></tr>
<tr><td><code>DPAsyncMPClient</code></td><td>データ並列（外部LB）</td><td>複数ZMQ</td></tr>
<tr><td><code>DPLBAsyncMPClient</code></td><td>データ並列（内部LB）</td><td>複数ZMQ</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core_client.py:442</code> (MPClient)</p>
<h4 id="zmqソケット構成"><a class="header" href="#zmqソケット構成">ZMQソケット構成</a></h4>
<pre><code>フロントエンド                 バックエンド
┌──────────────┐              ┌──────────────┐
│ AsyncMPClient│              │ EngineCore   │
│              │              │              │
│ input_socket ├─── ROUTER ──→│ (受信)       │
│ (zmq.ROUTER) │    msgpack   │              │
│              │              │              │
│ output_socket│←── PULL ─────┤ (送信)       │
│ (zmq.PULL)   │    msgpack   │              │
└──────────────┘              └──────────────┘
</code></pre>
<ul>
<li><strong>シリアライゼーション</strong>: <code>MsgpackEncoder</code> / <code>MsgpackDecoder</code>（<code>msgspec</code>ライブラリ）
<ul>
<li><code>EngineCoreRequest</code>のシリアライズ → 入力ソケット経由で送信</li>
<li><code>EngineCoreOutputs</code>のデシリアライズ ← 出力ソケット経由で受信</li>
</ul>
</li>
<li><strong>非同期出力受信</strong>: <code>process_outputs_socket()</code>タスクがZMQソケットをポーリングし、受信したOutputsを<code>asyncio.Queue</code>にpush</li>
</ul>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core_client.py:822</code> (AsyncMPClient)</p>
<h4 id="enginecore側のリクエスト受信"><a class="header" href="#enginecore側のリクエスト受信">EngineCore側のリクエスト受信</a></h4>
<p><code>EngineCore.add_request()</code>はリクエストをバリデーションしてSchedulerに登録する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:288</code> (add_request)</p>
<pre><code>EngineCore.add_request(request)                             # L288
  ├─ request_id の型チェック                                 # L295
  ├─ pooling_params のバリデーション                          # L300
  ├─ kv_transfer_params の互換性チェック                      # L311
  └─ scheduler.add_request(request)                          # L319
</code></pre>
<h4 id="enginecorestep-コアループ概要"><a class="header" href="#enginecorestep-コアループ概要">EngineCore.step() （コアループ概要）</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:389</code> (step)</p>
<pre><code>EngineCore.step()                                            # L389
  ├─ scheduler.schedule()        → SchedulerOutput           # L404
  ├─ executor.execute_model()    → Future[ModelRunnerOutput]  # L405
  ├─ grammar_output 取得                                      # L406
  ├─ future.result()             → ModelRunnerOutput          # L411
  ├─ sample_tokens()（非同期スケジューリング時）              # L413
  └─ scheduler.update_from_output() → EngineCoreOutputs      # L418
</code></pre>
<h2 id="コアループ-enginecorestep"><a class="header" href="#コアループ-enginecorestep">コアループ: EngineCore.step()</a></h2>
<p>EngineCoreの<code>step()</code>メソッドは、各ステップで <strong>schedule → execute → update</strong> のサイクルを実行し、待機中のリクエストから生成トークンを生産する。</p>
<h3 id="step-実行フロー"><a class="header" href="#step-実行フロー">step() 実行フロー</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:389</code> (step)</p>
<pre><code>EngineCore.step()                                            # L389
  │
  ├─ if _scheduler_paused: return {}, False                  # L397
  ├─ if not scheduler.has_requests(): return {}, False       # L402
  │
  ├─ 1. scheduler_output = scheduler.schedule()              # L404
  │      → SchedulerOutput
  │      （RUNNINGリクエストの予算割当 → WAITINGリクエストの受け入れ
  │        → KVキャッシュブロック確保 → SchedulerOutput構築）
  │
  ├─ 2. future = executor.execute_model(                     # L405
  │         scheduler_output, non_block=True)
  │      → Future[ModelRunnerOutput | None]
  │      （非ブロッキング。ワーカープロセスで並行実行）
  │
  ├─ 3. grammar_output = scheduler.get_grammar_bitmask(      # L406
  │         scheduler_output)
  │      （構造化出力有効時のみ使用）
  │
  ├─ 4. model_output = future.result()                       # L411
  │      → ModelRunnerOutput（ブロッキング待機）
  │
  ├─ 5. if model_output is None:                             # L413
  │        model_output = executor.sample_tokens(grammar_output)
  │      （非同期スケジューリング時: execute_modelとsamplingが分離）
  │
  ├─ 6. _process_aborts_queue()                              # L417
  │
  └─ 7. engine_core_outputs = scheduler.update_from_output(  # L418
  │         scheduler_output, model_output)
  │      → dict[int, EngineCoreOutputs]
  │      （生成トークンの追加、完了判定、出力構築）
  │
  └─ return (engine_core_outputs,                            # L422
             total_num_scheduled_tokens &gt; 0)
</code></pre>
<h3 id="scheduler-と-kvcachemanager-の相互作用"><a class="header" href="#scheduler-と-kvcachemanager-の相互作用">Scheduler と KVCacheManager の相互作用</a></h3>
<pre class="mermaid">sequenceDiagram
    participant EC as EngineCore
    participant S as Scheduler
    participant KV as KVCacheManager
    participant Ex as Executor

    EC-&gt;&gt;S: schedule()

    Note over S: Phase 1: RUNNINGリクエスト処理
    loop 各RUNNINGリクエスト
        S-&gt;&gt;KV: allocate_slots(request, num_new_tokens)
        KV--&gt;&gt;S: KVCacheBlocks or None
        alt 割り当て失敗 (None)
            S-&gt;&gt;KV: free(低優先度request)
            Note over S: プリエンプション → 再試行
        end
    end

    Note over S: Phase 2: WAITINGリクエスト受け入れ
    loop 各WAITINGリクエスト
        S-&gt;&gt;KV: get_computed_blocks(request)
        KV--&gt;&gt;S: (cached_blocks, num_hits)
        S-&gt;&gt;KV: allocate_slots(request, num_new_tokens, ...)
        KV--&gt;&gt;S: KVCacheBlocks or None
        alt 割り当て失敗 (None)
            Note over S: break（ループ終了）
        end
    end

    Note over S: Phase 3: SchedulerOutput構築
    S--&gt;&gt;EC: SchedulerOutput

    EC-&gt;&gt;Ex: execute_model(scheduler_output)
    Ex--&gt;&gt;EC: Future[ModelRunnerOutput]
    EC-&gt;&gt;EC: future.result()（待機）

    EC-&gt;&gt;S: update_from_output(scheduler_output, model_output)
    Note over S: トークン追加、完了判定
    S--&gt;&gt;EC: dict[int, EngineCoreOutputs]
</pre>

<h3 id="schedulerschedule-の3フェーズ"><a class="header" href="#schedulerschedule-の3フェーズ">Scheduler.schedule() の3フェーズ</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:321</code> (schedule)</p>
<p><code>schedule()</code> は Unified Compute Model を採用し、Prefill/Decodeを区別せず <code>num_computed_tokens</code> の進捗で統一的にトークンを割り当てる。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フェーズ</th><th>行</th><th>対象</th><th>処理</th></tr>
</thead>
<tbody>
<tr><td>Phase 1</td><td>L350-517</td><td>RUNNINGリクエスト</td><td>トークン予算割当。ブロック不足時はプリエンプション</td></tr>
<tr><td>Phase 2</td><td>L532-800</td><td>WAITINGリクエスト</td><td>新規受け入れ。プレフィックスキャッシュ検索 + ブロック割当</td></tr>
<tr><td>Phase 3</td><td>L827-896</td><td>出力構築</td><td>NewRequestData + CachedRequestData → SchedulerOutput</td></tr>
</tbody>
</table>
</div>
<p><strong>トークン予算</strong>: <code>token_budget = max_num_scheduled_tokens</code>（ステップあたり上限）で、各リクエストのスケジュール時に消費される。</p>
<p>詳細は <a href="#scheduler-サマリー">Scheduler サマリー</a> を参照。</p>
<h3 id="kvcachemanager-のブロック割り当て"><a class="header" href="#kvcachemanager-のブロック割り当て">KVCacheManager のブロック割り当て</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_manager.py:206</code> (allocate_slots)</p>
<p><code>allocate_slots()</code> は以下のブロック配置に基づいてGPUメモリブロックを確保する:</p>
<pre><code>|  comp  | new_comp | ext_comp |   new   | lookahead |
|&lt;------ 既計算トークン ------&gt;|&lt;-- 新規計算対象 --&gt;|
                               |&lt;- 割り当て対象 -&gt;|
</code></pre>
<ul>
<li>成功時: <code>KVCacheBlocks</code>（割り当てたブロック情報）を返す</li>
<li>失敗時: <code>None</code> を返す → Schedulerがプリエンプション（RUNNING）またはスキップ（WAITING）</li>
</ul>
<p>プレフィックスキャッシュ検索は <code>get_computed_blocks()</code> で行い、過去に計算済みのブロックを再利用する。</p>
<p>詳細は <a href="#kvcachemanager-サマリー">KVCacheManager サマリー</a> を参照。</p>
<h3 id="update_from_output--enginecoreoutputs"><a class="header" href="#update_from_output--enginecoreoutputs">update_from_output() → EngineCoreOutputs</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:1241</code> (update_from_output)</p>
<p><code>ModelRunnerOutput</code>を受けてSchedulerの状態を更新し、クライアントに返す<code>EngineCoreOutputs</code>を構築する。</p>
<pre><code>update_from_output(scheduler_output, model_runner_output)
  for each scheduled request:
    ├─ Speculative Decodingリジェクション処理
    │   → 不採用分の num_computed_tokens 巻き戻し
    ├─ 生成トークンをリクエストに追加
    ├─ 完了判定（EOS、max_tokens、stop_token）
    │   → 完了時: kv_cache_manager.free(request) でブロック解放
    └─ EngineCoreOutput 構築（request_id, new_token_ids, finish_reason, ...）
  → dict[int, EngineCoreOutputs]（クライアントインデックス別）
</code></pre>
<h2 id="下流パス-実行--ユーザー応答"><a class="header" href="#下流パス-実行--ユーザー応答">下流パス: 実行 → ユーザー応答</a></h2>
<h3 id="実行層-executor--worker--gpumodelrunner"><a class="header" href="#実行層-executor--worker--gpumodelrunner">実行層: Executor → Worker → GPUModelRunner</a></h3>
<p>EngineCore.step()は<code>executor.execute_model()</code>を<strong>非ブロッキング</strong>で呼び出し、GPUでの推論実行を開始する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py:202</code> (execute_model)</p>
<h4 id="collective_rpc-パターン"><a class="header" href="#collective_rpc-パターン">collective_rpc パターン</a></h4>
<p>Executorは<code>collective_rpc()</code>パターンで全Workerに同一メソッドを実行させ、出力ランクのWorkerの結果のみを返す。</p>
<pre><code>EngineCore.step()
  │
  ├─ executor.execute_model(scheduler_output, non_block=True)
  │   └─ collective_rpc("execute_model", args=(scheduler_output,))
  │       └─ Worker.execute_model(scheduler_output)                # L604
  │           └─ model_runner.execute_model(scheduler_output)      # L652
  │               → ExecuteModelState を内部保存、None を返す
  │
  ├─ grammar_output = scheduler.get_grammar_bitmask(...)           # 並行処理
  │
  ├─ future.result()  → None                                       # 待機
  │
  └─ executor.sample_tokens(grammar_output)                         # L222
      └─ collective_rpc("sample_tokens", args=(grammar_output,))
          └─ Worker.sample_tokens(grammar_output)                  # L598
              └─ model_runner.sample_tokens(grammar_output)        # L3621
                  → ModelRunnerOutput を返す
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_worker.py:604</code> (Worker.execute_model)</p>
<h4 id="gpumodelrunner-の2フェーズ実行"><a class="header" href="#gpumodelrunner-の2フェーズ実行">GPUModelRunner の2フェーズ実行</a></h4>
<p>GPUModelRunnerは <code>execute_model()</code> と <code>sample_tokens()</code> を分離する<strong>2フェーズ実行パターン</strong>を採用する。これにより、モデルフォワード中にgrammar bitmask計算を並行実行できる。</p>
<p><strong>Phase 1: execute_model()</strong> (<code>target/vllm/vllm/v1/worker/gpu_model_runner.py:3312</code>)</p>
<pre><code>execute_model(scheduler_output)
  ├─ _update_states(scheduler_output)          # バッチ状態更新
  ├─ _prepare_inputs(scheduler_output)         # 入力ID・位置計算
  ├─ _build_attention_metadata(...)            # Attention メタデータ構築
  ├─ _model_forward(...)                       # model.forward() 実行
  │   → hidden_states
  ├─ compute_logits(hidden_states)             # logits 計算
  │   → logits
  └─ ExecuteModelState に保存 → None を返す
</code></pre>
<p><strong>Phase 2: sample_tokens()</strong> (<code>target/vllm/vllm/v1/worker/gpu_model_runner.py:3621</code>)</p>
<pre><code>sample_tokens(grammar_output)
  ├─ ExecuteModelState を復元
  ├─ grammar bitmask 適用（構造化出力時）
  ├─ _sample(logits) → SamplerOutput
  ├─ バッチ状態更新（生成トークン反映）
  └─ ModelRunnerOutput を構築して返す
</code></pre>
<p><strong>ExecuteModelState</strong> (<code>target/vllm/vllm/v1/worker/gpu_model_runner.py:313</code>) はGPUテンソル（logits, hidden_states等）を保持するNamedTupleで、2フェーズ間の一時状態転送に使用される。</p>
<h3 id="出力処理-enginecoreoutput--requestoutput"><a class="header" href="#出力処理-enginecoreoutput--requestoutput">出力処理: EngineCoreOutput → RequestOutput</a></h3>
<p>ModelRunnerOutputはバックエンドプロセス（EngineCore）で<code>EngineCoreOutput</code>に変換され、ZMQ経由でフロントエンドプロセスの<code>OutputProcessor</code>に送られてデトークナイズされる。</p>
<pre class="mermaid">sequenceDiagram
    participant MR as GPUModelRunner
    participant S as Scheduler
    participant EC as EngineCore
    participant ZMQ as ZMQ IPC
    participant OP as OutputProcessor
    participant Client as API Client

    MR-&gt;&gt;EC: ModelRunnerOutput
    EC-&gt;&gt;S: update_from_output()
    Note over S: トークン追加、完了判定&lt;br&gt;KVキャッシュ解放
    S-&gt;&gt;EC: dict[int, EngineCoreOutputs]

    EC-&gt;&gt;ZMQ: msgpack シリアライズ
    ZMQ-&gt;&gt;OP: EngineCoreOutputs

    Note over OP: デトークナイズ&lt;br&gt;停止文字列判定&lt;br&gt;logprobs処理
    OP-&gt;&gt;Client: RequestOutput (yield)
</pre>

<h4 id="outputprocessorprocess_outputs"><a class="header" href="#outputprocessorprocess_outputs">OutputProcessor.process_outputs()</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/output_processor.py:582</code> (process_outputs)</p>
<p>OutputProcessorは<strong>フロントエンドプロセス</strong>で動作し、<code>EngineCoreOutput</code>をユーザー向け<code>RequestOutput</code>に変換する。</p>
<pre><code>OutputProcessor.process_outputs(engine_core_outputs)       # L582
  for each engine_core_output:
    ├─ req_state = request_states[req_id]                  # RequestState取得
    │
    ├─ detokenizer.update(new_token_ids, stop_terminated)  # L637
    │   ├─ トークン→テキスト変換（インクリメンタル）
    │   └─ 停止文字列チェック → stop_string or None
    │
    ├─ logprobs_processor.update_from_output(output)       # L646
    │
    ├─ req_state.make_request_output(...)                   # L649
    │   ├─ _new_completion_output(token_ids, finish_reason, ...)
    │   │   ├─ detokenizer.get_next_output_text(finished, delta)
    │   │   └─ CompletionOutput(text, token_ids, logprobs, ...)
    │   └─ RequestOutput(request_id, outputs, finished, ...)
    │
    └─ req_state.queue.put(request_output)                 # L661
        → AsyncLLM.generate() が yield
</code></pre>
<h4 id="detokenizerインクリメンタルデトークナイズ"><a class="header" href="#detokenizerインクリメンタルデトークナイズ">Detokenizer（インクリメンタルデトークナイズ）</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/detokenizer.py:30</code> (IncrementalDetokenizer)</p>
<p>トークンからテキストへの変換はインクリメンタルに行われ、ストリーミング出力を実現する。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>クラス</th><th>条件</th><th>方式</th></tr>
</thead>
<tbody>
<tr><td><code>FastIncrementalDetokenizer</code></td><td><code>PreTrainedTokenizerFast</code> 使用時</td><td>HF tokenizersの<code>DecodeStream</code>で高速変換</td></tr>
<tr><td><code>SlowIncrementalDetokenizer</code></td><td>その他のトークナイザ</td><td><code>detokenize_incrementally()</code>でPython変換</td></tr>
<tr><td><code>IncrementalDetokenizer</code></td><td>トークナイザなし</td><td>No-op（テキスト出力なし）</td></tr>
</tbody>
</table>
</div>
<p><code>update()</code>メソッドで各トークンをインクリメンタルにデコードし、同時に<code>check_stop_strings()</code>で停止文字列を検出する（<code>target/vllm/vllm/v1/engine/detokenizer.py:316</code>）。</p>
<h2 id="prefill-vs-decode"><a class="header" href="#prefill-vs-decode">Prefill vs Decode</a></h2>
<p>vLLM v1は<strong>Unified Compute Model</strong>を採用し、PrefillとDecodeを明示的に区別しない。両者は<code>num_computed_tokens</code>の進捗によって暗黙的に区分される。</p>
<h3 id="統一管理の仕組み"><a class="header" href="#統一管理の仕組み">統一管理の仕組み</a></h3>
<p>各リクエストは<code>num_computed_tokens</code>フィールドで計算済みトークン数を追跡する:</p>
<pre><code>プロンプト: [A, B, C, D, E]    (len=5)
num_computed_tokens: 0 → 5 → 6 → 7 → ...

Prefillフェーズ: num_computed_tokens &lt; len(prompt_token_ids)
  → 複数トークンを一度に計算（チャンクプリフィル可能）

Decodeフェーズ: num_computed_tokens &gt;= len(prompt_token_ids)
  → 1トークンずつ生成
</code></pre>
<h3 id="schedulerでの扱い"><a class="header" href="#schedulerでの扱い">Schedulerでの扱い</a></h3>
<p>Scheduler.schedule()はPrefill/Decodeを区別せず、トークン予算の範囲内で各リクエストに計算トークン数を割り当てる:</p>
<ul>
<li><strong>新規リクエスト（WAITING→RUNNING）</strong>: <code>num_tokens = len(prompt_token_ids) - num_computed_tokens</code>（プレフィックスキャッシュヒット分を差し引き）</li>
<li><strong>継続リクエスト（RUNNING）</strong>: <code>num_tokens = 1</code>（Decode 1トークン）</li>
<li>予算不足時は部分的なPrefill（チャンクプリフィル）も可能</li>
</ul>
<h3 id="gpumodelrunner内での違い"><a class="header" href="#gpumodelrunner内での違い">GPUModelRunner内での違い</a></h3>
<p>GPUModelRunner.execute_model()は入力準備の段階で暗黙的にPrefill/Decodeを処理する:</p>
<ul>
<li><strong>_prepare_inputs()</strong>: <code>num_scheduled_tokens</code>に基づいて入力トークンと位置を計算。Prefillなら複数トークン、Decodeなら1トークン</li>
<li><strong>_build_attention_metadata()</strong>: Prefillはフルattention、Decodeはキャッシュ済みKVに対するattentionのメタデータを構築</li>
<li><strong>モデルフォワード</strong>: 入力テンソルのサイズが異なるだけで、同一のforward()を実行</li>
</ul>
<p>この統一モデルにより、同一バッチ内にPrefillリクエストとDecodeリクエストを混在させるContinuous Batchingが自然に実現される。</p>
<h2 id="コンポーネント優先度確定"><a class="header" href="#コンポーネント優先度確定">コンポーネント優先度（確定）</a></h2>
<p>Phase 2での深堀り順序。ユーザー関心領域とフロー上の重要度に基づく。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>優先度</th><th>コンポーネント</th><th>理由</th><th>現在の深度</th></tr>
</thead>
<tbody>
<tr><td><strong>S</strong></td><td>KVCacheManager</td><td>ユーザー関心1位（メモリ管理/KVキャッシュ）。PagedAttention、ブロック管理、Eviction</td><td>[MEDIUM]</td></tr>
<tr><td><strong>A</strong></td><td>Scheduler</td><td>KVCacheManagerと密連携、推論パイプライン全体を制御。Continuous Batching</td><td>[MEDIUM]</td></tr>
<tr><td><strong>A</strong></td><td>GPUModelRunner</td><td>推論実行の中核。6277行の巨大クラス。将来のプラグイン開発に重要</td><td>[SHALLOW]</td></tr>
<tr><td><strong>B</strong></td><td>EngineCore</td><td>step()サイクル、batch_queueパイプライン。全体の統合ポイント</td><td>[MEDIUM]</td></tr>
<tr><td><strong>B</strong></td><td>OutputProcessor</td><td>デトークナイズ、停止判定。ストリーミング出力の仕組み</td><td>[SHALLOW]</td></tr>
<tr><td><strong>C</strong></td><td>AsyncLLM, InputProcessor</td><td>エントリポイント。薄いレイヤー</td><td>[SHALLOW]</td></tr>
<tr><td><strong>C</strong></td><td>Executor, Worker</td><td>委譲パターン。分散推論時のみ詳細が必要</td><td>[SHALLOW]</td></tr>
<tr><td><strong>C</strong></td><td>EngineCoreClient</td><td>ZMQ IPC通信層。プロトコルは把握済み</td><td>[SHALLOW]</td></tr>
</tbody>
</table>
</div>
<h2 id="参照ファイル一覧"><a class="header" href="#参照ファイル一覧">参照ファイル一覧</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th><th>役割</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/entrypoints/llm.py</code></td><td><code>LLM.generate()</code> (L396), <code>_add_request()</code> (L1850)</td><td>同期エントリポイント</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/async_llm.py</code></td><td><code>AsyncLLM.generate()</code> (L537), <code>add_request()</code> (L286)</td><td>非同期エントリポイント</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/input_processor.py</code></td><td><code>InputProcessor.process_inputs()</code> (L521)</td><td>入力処理</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/__init__.py</code></td><td><code>EngineCoreRequest</code> (L55), <code>EngineCoreOutput</code> (L130), <code>EngineCoreOutputs</code> (L176)</td><td>境界データ構造</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core_client.py</code></td><td><code>EngineCoreClient</code> (L63), <code>MPClient</code> (L442), <code>AsyncMPClient</code> (L822)</td><td>ZMQ IPC通信</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core.py</code></td><td><code>EngineCore.add_request()</code> (L288), <code>step()</code> (L389)</td><td>推論ループ本体</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/sched/scheduler.py</code></td><td><code>Scheduler.schedule()</code> (L321), <code>update_from_output()</code> (L1241)</td><td>スケジューリング</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/sched/output.py</code></td><td><code>SchedulerOutput</code> (L184), <code>NewRequestData</code> (L34), <code>CachedRequestData</code> (L114)</td><td>スケジュール出力データ構造</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/kv_cache_manager.py</code></td><td><code>KVCacheManager.allocate_slots()</code> (L206), <code>get_computed_blocks()</code> (L164)</td><td>KVキャッシュ管理</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/block_pool.py</code></td><td><code>BlockPool</code> (L128)</td><td>物理ブロック管理</td></tr>
<tr><td><code>target/vllm/vllm/v1/request.py</code></td><td><code>Request</code></td><td>リクエスト内部状態</td></tr>
<tr><td><code>target/vllm/vllm/v1/outputs.py</code></td><td><code>ModelRunnerOutput</code> (L160)</td><td>モデル推論出力</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/abstract.py</code></td><td><code>Executor</code> (ABC), <code>execute_model()</code> (L202), <code>collective_rpc()</code> (L180)</td><td>実行層抽象</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/uniproc_executor.py</code></td><td><code>UniProcExecutor</code> (L26)</td><td>単一プロセス実行</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/multiproc_executor.py</code></td><td><code>MultiprocExecutor</code> (L93)</td><td>マルチプロセス実行</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_worker.py</code></td><td><code>Worker.execute_model()</code> (L604), <code>sample_tokens()</code> (L598)</td><td>GPU Worker</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_model_runner.py</code></td><td><code>GPUModelRunner.execute_model()</code> (L3312), <code>sample_tokens()</code> (L3621), <code>ExecuteModelState</code> (L313)</td><td>モデル実行</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/output_processor.py</code></td><td><code>OutputProcessor.process_outputs()</code> (L582), <code>RequestState.make_request_output()</code> (L269)</td><td>出力処理</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/detokenizer.py</code></td><td><code>IncrementalDetokenizer</code> (L30), <code>FastIncrementalDetokenizer</code> (L169), <code>check_stop_strings()</code> (L316)</td><td>デトークナイズ</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/logprobs.py</code></td><td><code>LogprobsProcessor</code> (L28)</td><td>logprobs処理</td></tr>
<tr><td><code>target/vllm/vllm/outputs.py</code></td><td><code>RequestOutput</code> (L86), <code>CompletionOutput</code> (L23)</td><td>最終出力データ構造</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="アーキテクチャ概要"><a class="header" href="#アーキテクチャ概要">アーキテクチャ概要</a></h1>
<blockquote>
<p><strong>深度</strong>: [SHALLOW]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-09</p>
</blockquote>
<h2 id="概要-1"><a class="header" href="#概要-1">概要</a></h2>
<p>vLLMはUC Berkeley Sky Computing Lab発のLLM推論・サービングライブラリである。PagedAttentionによるKVキャッシュの効率的メモリ管理、Continuous Batchingによる動的バッチスケジューリングを中核技術とし、高スループット・低レイテンシのLLM推論を実現する。OpenAI互換APIサーバー、マルチモーダル対応、分散推論（Tensor/Pipeline/Data/Expert並列）を備える。</p>
<h2 id="全体構造"><a class="header" href="#全体構造">全体構造</a></h2>
<pre class="mermaid">graph TD
    subgraph エントリポイント層
        CLI["CLI&lt;br&gt;vllm.entrypoints.cli"]
        LLM["LLM&lt;br&gt;vllm.entrypoints.llm:101"]
        OpenAI["OpenAI互換API&lt;br&gt;vllm.entrypoints.openai"]
    end

    subgraph エンジン層
        AsyncLLM["AsyncLLM&lt;br&gt;vllm.v1.engine.async_llm:71"]
        LLMEngine["LLMEngine&lt;br&gt;vllm.v1.engine.llm_engine"]
        EngineCore["EngineCore&lt;br&gt;vllm.v1.engine.core:79"]
        InputProc["InputProcessor"]
        OutputProc["OutputProcessor"]
    end

    subgraph コア層
        Scheduler["Scheduler&lt;br&gt;vllm.v1.core.sched.scheduler:63"]
        KVCacheMgr["KVCacheManager&lt;br&gt;vllm.v1.core.kv_cache_manager:94"]
        BlockPool["BlockPool&lt;br&gt;vllm.v1.core.block_pool"]
    end

    subgraph 実行層
        Executor["Executor&lt;br&gt;vllm.v1.executor"]
        Worker["Worker&lt;br&gt;vllm.v1.worker.gpu_worker:70"]
        ModelRunner["GPUModelRunner&lt;br&gt;vllm.v1.worker.gpu_model_runner:329"]
    end

    subgraph モデル層
        Models["Models&lt;br&gt;vllm.model_executor.models&lt;br&gt;241ファイル"]
        Attention["Attention&lt;br&gt;2層構造"]
        Layers["Layers&lt;br&gt;vllm.model_executor.layers"]
    end

    CLI --&gt; AsyncLLM
    LLM --&gt; LLMEngine
    OpenAI --&gt; AsyncLLM

    AsyncLLM --&gt;|"ZMQ IPC"| EngineCore
    LLMEngine --&gt; EngineCore
    EngineCore --&gt; InputProc
    EngineCore --&gt; OutputProc

    EngineCore --&gt; Scheduler
    EngineCore --&gt; KVCacheMgr
    KVCacheMgr --&gt; BlockPool

    EngineCore --&gt; Executor
    Executor --&gt; Worker
    Worker --&gt; ModelRunner

    ModelRunner --&gt; Models
    ModelRunner --&gt; Attention
    Models --&gt; Layers
</pre>

<h2 id="アーキテクチャの世代"><a class="header" href="#アーキテクチャの世代">アーキテクチャの世代</a></h2>
<p><code>vllm/engine/</code> は <code>vllm/v1/</code> への薄いラッパーである。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/engine/llm_engine.py:4</code> — <code>LLMEngine = V1LLMEngine</code> の1行エイリアス</p>
<p>v1が現行アーキテクチャの本体であり、コードリーディングでは <code>vllm/v1/</code> を中心に読む。ただし <code>vllm/model_executor/</code>、<code>vllm/distributed/</code>、<code>vllm/multimodal/</code> 等はv1からも直接利用されるため調査対象に含む。</p>
<h2 id="主要コンポーネント"><a class="header" href="#主要コンポーネント">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>クラス</th><th>パス</th><th>役割</th></tr>
</thead>
<tbody>
<tr><td>AsyncLLM</td><td><code>AsyncLLM(EngineClient)</code></td><td><code>target/vllm/vllm/v1/engine/async_llm.py:71</code></td><td>非同期APIトップレベル</td></tr>
<tr><td>EngineCore</td><td><code>EngineCore</code></td><td><code>target/vllm/vllm/v1/engine/core.py:79</code></td><td>推論ループ内側。ZMQで外側と通信</td></tr>
<tr><td>Scheduler</td><td><code>Scheduler(SchedulerInterface)</code></td><td><code>target/vllm/vllm/v1/core/sched/scheduler.py:63</code></td><td>Continuous Batchingスケジューラ</td></tr>
<tr><td>KVCacheManager</td><td><code>KVCacheManager</code></td><td><code>target/vllm/vllm/v1/core/kv_cache_manager.py:94</code></td><td>KVキャッシュブロック管理</td></tr>
<tr><td>Executor</td><td><code>Executor</code>(ABC)</td><td><code>target/vllm/vllm/v1/executor/abstract.py</code></td><td>Worker群を束ねる実行層</td></tr>
<tr><td>Worker</td><td><code>Worker(WorkerBase)</code></td><td><code>target/vllm/vllm/v1/worker/gpu_worker.py:70</code></td><td>1 GPUデバイスを担当</td></tr>
<tr><td>GPUModelRunner</td><td><code>GPUModelRunner</code></td><td><code>target/vllm/vllm/v1/worker/gpu_model_runner.py:329</code></td><td>GPU上のフォワードパス実行</td></tr>
<tr><td>VllmConfig</td><td><code>VllmConfig</code></td><td><code>target/vllm/vllm/config/vllm.py</code></td><td>全設定の集約クラス</td></tr>
</tbody>
</table>
</div>
<h2 id="設計原則"><a class="header" href="#設計原則">設計原則</a></h2>
<h3 id="pagedattention"><a class="header" href="#pagedattention">PagedAttention</a></h3>
<p>KVキャッシュをOSの仮想メモリページングに着想を得てブロック単位で管理する。連続したGPUメモリ確保が不要になり、メモリ断片化を大幅に抑制する。</p>
<h3 id="continuous-batching"><a class="header" href="#continuous-batching">Continuous Batching</a></h3>
<p>リクエストの到着・完了に応じてバッチを動的に更新する。固定バッチサイズと異なり、GPU稼働率を最大化できる。</p>
<h3 id="zmq-ipc-によるプロセス分離"><a class="header" href="#zmq-ipc-によるプロセス分離">ZMQ IPC によるプロセス分離</a></h3>
<p>EngineCoreは別プロセス（<code>EngineCoreProc</code>）として動作し、ZeroMQソケットで上位エンジン層と通信する。これによりスケジューリングと推論処理を並行実行できる。</p>
<h3 id="プラグインシステム"><a class="header" href="#プラグインシステム">プラグインシステム</a></h3>
<p><code>vllm/plugins/</code> によるプラグイン機構を備え、起動時に <code>load_general_plugins()</code> で拡張を読み込む。</p>
<h2 id="ccuda-拡張"><a class="header" href="#ccuda-拡張">C++/CUDA 拡張</a></h2>
<p><code>target/vllm/csrc/</code> にパフォーマンスクリティカルなネイティブコードが配置されている。PagedAttentionカーネル、LayerNorm、量子化カーネル、カスタムAllReduce等が含まれる。Pythonからは <code>vllm._custom_ops</code> 等のバインディング経由で呼び出される。</p>
<h2 id="参照ファイル"><a class="header" href="#参照ファイル">参照ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/engine/llm_engine.py</code></td><td><code>LLMEngine</code>（v1への薄いラッパー）</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/async_llm.py</code></td><td><code>AsyncLLM</code></td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core.py</code></td><td><code>EngineCore</code>, <code>EngineCoreProc</code></td></tr>
<tr><td><code>target/vllm/vllm/v1/core/sched/scheduler.py</code></td><td><code>Scheduler</code></td></tr>
<tr><td><code>target/vllm/vllm/v1/core/kv_cache_manager.py</code></td><td><code>KVCacheManager</code></td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/abstract.py</code></td><td><code>Executor</code>(ABC)</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_worker.py</code></td><td><code>Worker</code></td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_model_runner.py</code></td><td><code>GPUModelRunner</code></td></tr>
<tr><td><code>target/vllm/vllm/config/vllm.py</code></td><td><code>VllmConfig</code></td></tr>
<tr><td><code>target/vllm/vllm/entrypoints/llm.py</code></td><td><code>LLM</code></td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="enginecore-サマリー"><a class="header" href="#enginecore-サマリー">EngineCore サマリー</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-2"><a class="header" href="#概要-2">概要</a></h2>
<p><code>EngineCore</code>はバックエンドプロセス（<code>EngineCoreProc</code>）内で動作する推論ループの中央制御コンポーネントである。<code>Scheduler</code>、<code>ModelExecutor</code>、<code>KVCacheManager</code>を統括し、<code>step()</code>メソッドで <strong>schedule → execute → update</strong> のサイクルを繰り返す。フロントエンドプロセスとはZMQ IPC経由で通信し、<code>EngineCoreRequest</code>を受信して<code>EngineCoreOutputs</code>を返す。</p>
<h2 id="アーキテクチャ"><a class="header" href="#アーキテクチャ">アーキテクチャ</a></h2>
<pre class="mermaid">graph TD
    subgraph EngineCoreプロセス
        EC["EngineCore"]
        Sched["Scheduler"]
        KVM["KVCacheManager"]
        Exec["ModelExecutor"]

        EC --&gt;|"1. schedule()"| Sched
        Sched --&gt;|"allocate_slots()"| KVM
        Sched --&gt;|"SchedulerOutput"| EC
        EC --&gt;|"2. execute_model()"| Exec
        Exec --&gt;|"Future&lt;ModelRunnerOutput&gt;"| EC
        EC --&gt;|"3. update_from_output()"| Sched
        Sched --&gt;|"EngineCoreOutputs"| EC
    end

    ZMQ_IN["ZMQ ROUTER&lt;br&gt;(受信)"] --&gt;|"EngineCoreRequest"| EC
    EC --&gt;|"EngineCoreOutputs"| ZMQ_OUT["ZMQ PUSH&lt;br&gt;(送信)"]
</pre>

<h2 id="主要コンポーネント-1"><a class="header" href="#主要コンポーネント-1">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>用途</th><th>ファイル</th></tr>
</thead>
<tbody>
<tr><td><code>EngineCore</code></td><td>推論ループ本体</td><td><code>target/vllm/vllm/v1/engine/core.py:82</code></td></tr>
<tr><td><code>EngineCoreProc</code></td><td>プロセスラッパー。ZMQソケット管理とイベントループ</td><td><code>target/vllm/vllm/v1/engine/core.py</code></td></tr>
</tbody>
</table>
</div>
<h2 id="主要メソッド"><a class="header" href="#主要メソッド">主要メソッド</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>__init__()</code></td><td>L82</td><td>Scheduler, ModelExecutor, KVキャッシュの初期化</td></tr>
<tr><td><code>step()</code></td><td>L389</td><td>メインループ: schedule → execute → update</td></tr>
<tr><td><code>step_with_batch_queue()</code></td><td>L434</td><td>パイプライン並列化版step（batch_queue使用）</td></tr>
<tr><td><code>add_request()</code></td><td>L288</td><td>リクエストをバリデーション後Schedulerに登録</td></tr>
<tr><td><code>post_step()</code></td><td>L424</td><td>step後処理（Speculative Decodingのドラフトトークン更新）</td></tr>
</tbody>
</table>
</div>
<h2 id="step-サイクル"><a class="header" href="#step-サイクル">step() サイクル</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:389</code></p>
<pre><code>EngineCore.step() -&gt; tuple[dict[int, EngineCoreOutputs], bool]
  │
  ├─ スケジューラ停止チェック                           # L397
  │   if _scheduler_paused: return {}, False
  │
  ├─ リクエスト有無チェック                             # L402
  │   if not scheduler.has_requests(): return {}, False
  │
  ├─ 1. scheduler.schedule()                            # L404
  │   → SchedulerOutput
  │
  ├─ 2. executor.execute_model(scheduler_output,        # L405
  │       non_block=True)
  │   → Future[ModelRunnerOutput | None]
  │
  ├─ 3. scheduler.get_grammar_bitmask(scheduler_output) # L406
  │   → grammar_output（構造化出力用）
  │
  ├─ 4. future.result()                                 # L411
  │   → ModelRunnerOutput（ブロッキング待機）
  │
  ├─ 5. if model_output is None:                        # L413
  │       model_output = executor.sample_tokens(grammar_output)
  │   （非同期スケジューリング時: execute_modelとsamplingが分離）
  │
  ├─ 6. _process_aborts_queue()                         # L417
  │
  └─ 7. scheduler.update_from_output(                   # L418
  │       scheduler_output, model_output)
  │   → dict[int, EngineCoreOutputs]
  │
  └─ return (engine_core_outputs,                       # L422
             total_num_scheduled_tokens &gt; 0)
</code></pre>
<p><strong>戻り値</strong>:</p>
<ul>
<li>第1要素: クライアントインデックス → EngineCoreOutputs のマッピング</li>
<li>第2要素: モデル実行が行われたか（<code>total_num_scheduled_tokens &gt; 0</code>）</li>
</ul>
<h2 id="add_request-フロー"><a class="header" href="#add_request-フロー">add_request() フロー</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:288</code></p>
<pre><code>add_request(request, request_wave=0)                    # L288
  ├─ request_id の型チェック（str必須）                   # L295
  ├─ pooling_params のタスクバリデーション                # L300
  ├─ kv_transfer_params の互換性チェック                  # L311
  └─ scheduler.add_request(request)                      # L319
</code></pre>
<h2 id="batch_queue-パイプライン並列化-shallow"><a class="header" href="#batch_queue-パイプライン並列化-shallow">batch_queue パイプライン並列化 [SHALLOW]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:434</code> (step_with_batch_queue)</p>
<p><code>max_concurrent_batches &gt; 1</code> の場合、<code>step_with_batch_queue()</code> が <code>step_fn</code> として使用される。スケジューリングとモデル実行をパイプライン的にオーバーラップさせ、GPUのアイドル時間を削減する。</p>
<ul>
<li><code>batch_queue</code>: <code>deque[tuple[Future, SchedulerOutput, Future]]</code></li>
<li>新しいスケジュール結果を <code>appendleft()</code> で追加、完了待ちを <code>pop()</code> で取得</li>
<li>前のバッチの実行完了を待たずに次のバッチをスケジュール可能</li>
</ul>
<h2 id="kvキャッシュ初期化フロー-shallow"><a class="header" href="#kvキャッシュ初期化フロー-shallow">KVキャッシュ初期化フロー [SHALLOW]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:82</code> (<strong>init</strong>)</p>
<pre><code>EngineCore.__init__()
  → _initialize_kv_caches()
    → model_executor.get_kv_cache_specs()       # モデルのKVキャッシュ要件取得
    → determine_available_memory()               # GPUメモリプロファイリング
    → get_kv_cache_configs()                     # ブロック数等の設定算出
    → generate_scheduler_kv_cache_config()       # Scheduler用設定生成
    → model_executor.initialize_from_config()    # GPUメモリ確保
</code></pre>
<p>KV Connector（KV Transfer/LMCache連携）が有効な場合:</p>
<ul>
<li><code>scheduler.get_kv_connector()</code> でコネクタ有無を確認 (L159)</li>
<li>各ワーカーのハンドシェイクメタデータを収集・統合 (L164-175)</li>
</ul>
<h2 id="async_scheduling-shallow"><a class="header" href="#async_scheduling-shallow">async_scheduling [SHALLOW]</a></h2>
<p><code>vllm_config.scheduler_config.async_scheduling</code> で有効化。</p>
<ul>
<li>通常: <code>execute_model()</code> がモデル実行 + トークンサンプリングをまとめて実行</li>
<li>async有効時: <code>execute_model()</code> はモデル実行のみ（<code>None</code> を返す）→ <code>sample_tokens()</code> で別途サンプリング</li>
<li><code>post_step()</code> でのSpeculative Decodingドラフトトークン更新タイミングに影響</li>
</ul>
<h2 id="設定"><a class="header" href="#設定">設定</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>max_concurrent_batches</code></td><td>1</td><td>batch_queueサイズ（&gt;1でパイプライン並列化）</td></tr>
<tr><td><code>async_scheduling</code></td><td>False</td><td>非同期スケジューリングモード</td></tr>
</tbody>
</table>
</div>
<h2 id="呼び出しフロー"><a class="header" href="#呼び出しフロー">呼び出しフロー</a></h2>
<pre><code>[EngineCoreProc イベントループ]
  ├─ ZMQ受信 → EngineCore.add_request()
  ├─ EngineCore.step_fn()  (= step() or step_with_batch_queue())
  │   ├─ Scheduler.schedule()
  │   ├─ ModelExecutor.execute_model()
  │   └─ Scheduler.update_from_output()
  ├─ EngineCore.post_step()
  └─ ZMQ送信 ← EngineCoreOutputs
</code></pre>
<h2 id="関連ドキュメント"><a class="header" href="#関連ドキュメント">関連ドキュメント</a></h2>
<ul>
<li><a href="#scheduler-サマリー">Scheduler</a></li>
<li><a href="#kvcachemanager-サマリー">KVCacheManager</a></li>
<li><a href="#enginecoreclient-サマリー">EngineCoreClient</a></li>
<li><a href="#エントリポイント-asyncllm--llm-サマリー">エントリポイント</a></li>
<li><a href="#テキスト推論データフロー">データフロー</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="enginecoreclient-サマリー"><a class="header" href="#enginecoreclient-サマリー">EngineCoreClient サマリー</a></h1>
<blockquote>
<p><strong>深度</strong>: [SHALLOW]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-09</p>
</blockquote>
<h2 id="概要-3"><a class="header" href="#概要-3">概要</a></h2>
<p><code>EngineCoreClient</code>はフロントエンドプロセス（AsyncLLM / LLM）とバックエンドプロセス（EngineCore）間のプロセス間通信を抽象化するコンポーネントである。ZeroMQソケットとmsgpackシリアライゼーションを使用し、<code>EngineCoreRequest</code>の送信と<code>EngineCoreOutputs</code>の受信を効率的に行う。</p>
<h2 id="アーキテクチャ-1"><a class="header" href="#アーキテクチャ-1">アーキテクチャ</a></h2>
<pre><code>フロントエンドプロセス            バックエンドプロセス
┌───────────────────┐            ┌───────────────────┐
│  AsyncMPClient    │            │  EngineCore       │
│                   │            │                   │
│  input_socket     ├──ROUTER──→│  (ZMQ受信)        │
│  (zmq.ROUTER)     │  msgpack   │                   │
│                   │            │                   │
│  output_socket    │←──PULL────┤  (ZMQ送信)        │
│  (zmq.PULL)       │  msgpack   │                   │
│                   │            │                   │
│  outputs_queue    │            │                   │
│  (asyncio.Queue)  │            │                   │
└───────────────────┘            └───────────────────┘
</code></pre>
<h2 id="主要コンポーネント-2"><a class="header" href="#主要コンポーネント-2">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>用途</th><th>ファイル</th></tr>
</thead>
<tbody>
<tr><td><code>EngineCoreClient</code> (ABC)</td><td>抽象インターフェース</td><td><code>target/vllm/vllm/v1/engine/core_client.py:63</code></td></tr>
<tr><td><code>MPClient</code></td><td>マルチプロセスクライアント基底</td><td><code>target/vllm/vllm/v1/engine/core_client.py:442</code></td></tr>
<tr><td><code>AsyncMPClient</code></td><td>非同期マルチプロセスクライアント（AsyncLLM用）</td><td><code>target/vllm/vllm/v1/engine/core_client.py:822</code></td></tr>
<tr><td><code>SyncMPClient</code></td><td>同期マルチプロセスクライアント（LLM用）</td><td><code>target/vllm/vllm/v1/engine/core_client.py</code></td></tr>
<tr><td><code>DPAsyncMPClient</code></td><td>データ並列（外部LB）</td><td><code>target/vllm/vllm/v1/engine/core_client.py</code></td></tr>
<tr><td><code>DPLBAsyncMPClient</code></td><td>データ並列（内部LB）</td><td><code>target/vllm/vllm/v1/engine/core_client.py</code></td></tr>
<tr><td><code>MsgpackEncoder</code></td><td>リクエストのシリアライズ</td><td><code>target/vllm/vllm/v1/serial_utils.py</code></td></tr>
<tr><td><code>MsgpackDecoder</code></td><td>レスポンスのデシリアライズ</td><td><code>target/vllm/vllm/v1/serial_utils.py</code></td></tr>
</tbody>
</table>
</div>
<h2 id="主要メソッド-1"><a class="header" href="#主要メソッド-1">主要メソッド</a></h2>
<h3 id="enginecoreclient-abc"><a class="header" href="#enginecoreclient-abc">EngineCoreClient (ABC)</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>make_client()</code></td><td>ファクトリ。設定に応じた適切なサブクラスを返す</td></tr>
<tr><td><code>make_async_mp_client()</code></td><td>AsyncLLM用ファクトリ。DP構成も考慮</td></tr>
<tr><td><code>add_request()</code></td><td>EngineCoreRequestを送信</td></tr>
<tr><td><code>get_output()</code></td><td>EngineCoreOutputsを受信</td></tr>
<tr><td><code>abort_requests()</code></td><td>リクエストキャンセル</td></tr>
</tbody>
</table>
</div>
<h3 id="asyncmpclient"><a class="header" href="#asyncmpclient">AsyncMPClient</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>_ensure_output_queue_task()</code></td><td>L856</td><td>ZMQ出力受信タスクを起動</td></tr>
<tr><td><code>get_output_async()</code></td><td>L902</td><td>asyncio.Queueから出力を取得</td></tr>
<tr><td><code>_send_input()</code></td><td>L913</td><td>EngineCoreRequestをZMQで送信</td></tr>
<tr><td><code>_send_input_message()</code></td><td>L925</td><td>ZMQ multipart送信（zero-copy対応）</td></tr>
</tbody>
</table>
</div>
<h2 id="ファクトリ選択ロジック"><a class="header" href="#ファクトリ選択ロジック">ファクトリ選択ロジック</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core_client.py:99</code> (make_async_mp_client)</p>
<pre><code>make_async_mp_client(vllm_config, executor_class, ...)
  ├─ data_parallel_size &gt; 1 の場合:
  │   ├─ external_lb → DPAsyncMPClient
  │   └─ internal_lb → DPLBAsyncMPClient
  └─ それ以外 → AsyncMPClient
</code></pre>
<h2 id="設定-1"><a class="header" href="#設定-1">設定</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>parallel_config.data_parallel_size</code></td><td>1</td><td>データ並列数。&gt;1でDP系クライアントを使用</td></tr>
<tr><td><code>parallel_config.data_parallel_external_lb</code></td><td>—</td><td>外部ロードバランサ使用フラグ</td></tr>
</tbody>
</table>
</div>
<h2 id="呼び出しフロー-1"><a class="header" href="#呼び出しフロー-1">呼び出しフロー</a></h2>
<pre><code>[送信パス]
AsyncLLM.add_request()
  → engine_core.add_request_async(request)
    → AsyncMPClient._send_input(REQUEST, request)
      → MsgpackEncoder.encode(request)
      → input_socket.send_multipart(msg, copy=False)
        → ZMQ ROUTER → バックエンドプロセス

[受信パス]
process_outputs_socket() [バックグラウンドタスク]
  → output_socket.recv_multipart()
    → MsgpackDecoder.decode(frames) → EngineCoreOutputs
    → outputs_queue.put_nowait(outputs)

AsyncLLM._run_output_handler()
  → engine_core.get_output_async()
    → outputs_queue.get() → EngineCoreOutputs
</code></pre>
<h2 id="設計上の特徴"><a class="header" href="#設計上の特徴">設計上の特徴</a></h2>
<ul>
<li><strong>プロセス分離</strong>: EngineCoreが別プロセスで動作するため、GILの影響を受けずスケジューリングとGPU実行を並行可能</li>
<li><strong>msgpackシリアライゼーション</strong>: <code>msgspec.Struct</code>の<code>array_like</code>形式でコンパクトなバイナリ表現</li>
<li><strong>zero-copy</strong>: ZMQ <code>copy=False</code> でメモリコピーを最小化。テンソルバッキングバッファの追跡（<code>add_pending_message</code>）</li>
<li><strong>weakref</strong>: 出力タスクがクライアントへの循環参照を持たないよう<code>weakref</code>を使用</li>
</ul>
<h2 id="関連ドキュメント-1"><a class="header" href="#関連ドキュメント-1">関連ドキュメント</a></h2>
<ul>
<li><a href="#エントリポイント-asyncllm--llm-サマリー">エントリポイント</a></li>
<li><a href="#テキスト推論データフロー">データフロー</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="エントリポイント-asyncllm--llm-サマリー"><a class="header" href="#エントリポイント-asyncllm--llm-サマリー">エントリポイント (AsyncLLM / LLM) サマリー</a></h1>
<blockquote>
<p><strong>深度</strong>: [SHALLOW]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-09</p>
</blockquote>
<h2 id="概要-4"><a class="header" href="#概要-4">概要</a></h2>
<p><code>AsyncLLM</code>と<code>LLM</code>はvLLMの2つの主要エントリポイントである。<code>AsyncLLM</code>はAPIサーバー（OpenAI互換API等）が使用する非同期パスで、<code>LLM</code>はオフラインバッチ推論用の同期パスである。どちらも<code>InputProcessor</code>で入力を処理し、<code>EngineCoreClient</code>経由でバックエンド（EngineCore）にリクエストを送信する。</p>
<h2 id="アーキテクチャ-2"><a class="header" href="#アーキテクチャ-2">アーキテクチャ</a></h2>
<pre class="mermaid">graph LR
    subgraph 非同期パス
        OpenAI["OpenAI API Server"] --&gt; AsyncLLM
        AsyncLLM --&gt;|"process_inputs()"| IP["InputProcessor"]
        IP --&gt;|"EngineCoreRequest"| AsyncLLM
        AsyncLLM --&gt;|"add_request_async()"| Client["EngineCoreClient"]
        Client --&gt;|"ZMQ"| EC["EngineCore"]
    end

    subgraph 同期パス
        User["ユーザーコード"] --&gt; LLM
        LLM --&gt;|"process_inputs()"| IP2["InputProcessor"]
        IP2 --&gt;|"EngineCoreRequest"| LLM
        LLM --&gt;|"add_request()"| Client2["EngineCoreClient"]
    end
</pre>

<h2 id="主要コンポーネント-3"><a class="header" href="#主要コンポーネント-3">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>用途</th><th>ファイル</th></tr>
</thead>
<tbody>
<tr><td><code>AsyncLLM</code></td><td>非同期推論エントリポイント。AsyncGeneratorでストリーミング出力</td><td><code>target/vllm/vllm/v1/engine/async_llm.py:71</code></td></tr>
<tr><td><code>LLM</code></td><td>同期バッチ推論エントリポイント。<code>list[RequestOutput]</code>を返す</td><td><code>target/vllm/vllm/entrypoints/llm.py:101</code></td></tr>
<tr><td><code>RequestOutputCollector</code></td><td>非同期パスでの出力キュー管理</td><td><code>target/vllm/vllm/v1/engine/async_llm.py</code></td></tr>
<tr><td><code>ParentRequest</code></td><td>n&gt;1サンプリング時の親リクエスト管理</td><td><code>target/vllm/vllm/v1/engine/async_llm.py</code></td></tr>
</tbody>
</table>
</div>
<h2 id="主要メソッド-2"><a class="header" href="#主要メソッド-2">主要メソッド</a></h2>
<h3 id="asyncllm"><a class="header" href="#asyncllm">AsyncLLM</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>generate()</code></td><td>L537</td><td>メインAPI。AsyncGeneratorでRequestOutputをyield</td></tr>
<tr><td><code>add_request()</code></td><td>L286</td><td>リクエスト追加。InputProcessor→OutputProcessor→EngineCore</td></tr>
<tr><td><code>_add_request()</code></td><td>L414</td><td>内部: OutputProcessorとEngineCoreに登録</td></tr>
<tr><td><code>_run_output_handler()</code></td><td>L647</td><td>バックグラウンドタスク起動。EngineCore出力を受信→キュー</td></tr>
</tbody>
</table>
</div>
<h3 id="llm"><a class="header" href="#llm">LLM</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>generate()</code></td><td>L396</td><td>バッチ推論API。<code>list[RequestOutput]</code>を返す</td></tr>
<tr><td><code>_add_request()</code></td><td>L1850</td><td>InputProcessor→llm_engine.add_request()</td></tr>
<tr><td><code>_run_engine()</code></td><td>L1900</td><td>ポーリングループ。完了まで<code>step()</code>を繰り返す</td></tr>
</tbody>
</table>
</div>
<h2 id="設定-2"><a class="header" href="#設定-2">設定</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>log_requests</code></td><td><code>True</code></td><td>リクエストログ出力</td></tr>
<tr><td><code>log_stats</code></td><td>引数指定</td><td>統計ログ出力</td></tr>
<tr><td><code>start_engine_loop</code></td><td><code>True</code></td><td>エンジンループ自動起動</td></tr>
</tbody>
</table>
</div>
<h2 id="呼び出しフロー-2"><a class="header" href="#呼び出しフロー-2">呼び出しフロー</a></h2>
<pre><code>[APIサーバー or ユーザーコード]
  → AsyncLLM.generate() / LLM.generate()
    → InputProcessor.process_inputs()
      → EngineCoreRequest
    → EngineCoreClient.add_request_async()
      → ZMQ → EngineCore（別プロセス）

[バックグラウンド output_handler タスク]
  → EngineCoreClient.get_output_async()
    → EngineCoreOutputs
  → OutputProcessor.process_outputs()
    → RequestOutput → キューにpush

[generate() AsyncGenerator]
  → キューから取り出してyield
</code></pre>
<h2 id="関連ドキュメント-2"><a class="header" href="#関連ドキュメント-2">関連ドキュメント</a></h2>
<ul>
<li><a href="#inputprocessor-サマリー">入力処理</a></li>
<li><a href="#enginecoreclient-サマリー">EngineCoreClient</a></li>
<li><a href="#テキスト推論データフロー">データフロー</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="executor"><a class="header" href="#executor">Executor</a></h1>
<blockquote>
<p><strong>深度</strong>: [SHALLOW]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-5"><a class="header" href="#概要-5">概要</a></h2>
<p>Executorは、EngineCoreとWorker（GPUModelRunner）の間に位置する実行委譲レイヤーである。<code>collective_rpc()</code>パターンで全Workerに対して同一メソッドを呼び出し、出力ランクのWorkerの結果を返す。単一プロセス、マルチプロセス、Ray分散の3つの実装を持つ。</p>
<h2 id="クラス階層"><a class="header" href="#クラス階層">クラス階層</a></h2>
<pre><code>Executor (ABC)                                     abstract.py:36
├── UniProcExecutor                                uniproc_executor.py:26
│   └── ExecutorWithExternalLauncher               uniproc_executor.py:140
├── MultiprocExecutor                              multiproc_executor.py:93
└── RayDistributedExecutor                         ray_executor.py:62
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py:36</code> (Executor)</p>
<h2 id="主要メソッド-3"><a class="header" href="#主要メソッド-3">主要メソッド</a></h2>
<h3 id="collective_rpc"><a class="header" href="#collective_rpc">collective_rpc()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py:180</code> (collective_rpc)</p>
<p>全Workerに対して同一メソッドを実行するRPCメカニズム。</p>
<pre><code class="language-python">def collective_rpc(
    self,
    method: str | Callable[..., _R],  # メソッド名または関数
    timeout: float | None = None,
    args: tuple = (),
    kwargs: dict | None = None,
    non_block: bool = False,          # True: Future返却
) -&gt; list[_R] | Future[list[_R]]
</code></pre>
<h3 id="execute_model"><a class="header" href="#execute_model">execute_model()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py:202</code> (execute_model)</p>
<pre><code class="language-python">def execute_model(
    self,
    scheduler_output: SchedulerOutput,
    non_block: bool = False,
) -&gt; ModelRunnerOutput | None | Future[ModelRunnerOutput | None]:
    output = self.collective_rpc("execute_model",
                                  args=(scheduler_output,),
                                  non_block=non_block)
    return output[0]   # 出力ランクWorkerの結果のみ返す
</code></pre>
<h3 id="sample_tokens"><a class="header" href="#sample_tokens">sample_tokens()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py:222</code> (sample_tokens)</p>
<pre><code class="language-python">def sample_tokens(
    self,
    grammar_output: GrammarOutput | None,
    non_block: bool = False,
) -&gt; ModelRunnerOutput | Future[ModelRunnerOutput]:
    output = self.collective_rpc("sample_tokens",
                                  args=(grammar_output,),
                                  non_block=non_block)
    return output[0]
</code></pre>
<h2 id="実装の使い分け"><a class="header" href="#実装の使い分け">実装の使い分け</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>実装</th><th>用途</th><th>Worker配置</th><th>特徴</th></tr>
</thead>
<tbody>
<tr><td><code>UniProcExecutor</code></td><td>単一GPU</td><td>ドライバプロセス内</td><td>最小オーバーヘッド。<code>max_concurrent_batches &gt; 1</code>時はThreadPoolExecutor使用</td></tr>
<tr><td><code>MultiprocExecutor</code></td><td>複数GPU（TP/PP）</td><td>子プロセス</td><td>MessageQueue（共有メモリ）ベース。Pipeline Parallelism対応</td></tr>
<tr><td><code>RayDistributedExecutor</code></td><td>分散クラスタ</td><td>Rayアクター</td><td>Ray経由のリモートWorker管理</td></tr>
</tbody>
</table>
</div>
<h2 id="worker委譲先"><a class="header" href="#worker委譲先">Worker（委譲先）</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_worker.py:70</code> (Worker)</p>
<p><code>Worker(WorkerBase)</code> はGPUModelRunnerのラッパーで、以下の追加処理を行う:</p>
<ul>
<li><strong>Pipeline Parallelism</strong>: 前段ランクからの<code>IntermediateTensors</code>受信、後段への送信</li>
<li><strong>推論モード管理</strong>: <code>@torch.inference_mode()</code> デコレータ</li>
</ul>
<pre><code>Worker.execute_model(scheduler_output)                    # L604
  ├─ PP: recv_tensor_dict() → IntermediateTensors        # L614-641
  ├─ model_runner.execute_model(scheduler_output, ...)    # L652
  │   → ModelRunnerOutput | None | IntermediateTensors
  ├─ PP: send_tensor_dict(intermediate_tensors)           # L660-671
  └─ return ModelRunnerOutput | None
</code></pre>
<h2 id="enginecore--出力-の委譲フロー"><a class="header" href="#enginecore--出力-の委譲フロー">EngineCore → 出力 の委譲フロー</a></h2>
<pre><code>EngineCore.step()
  └─ executor.execute_model(scheduler_output, non_block=True)
      └─ collective_rpc("execute_model")
          └─ Worker.execute_model()
              └─ GPUModelRunner.execute_model()
                  → ExecuteModelState 保存、None 返却

EngineCore.step()（続き）
  └─ executor.sample_tokens(grammar_output)
      └─ collective_rpc("sample_tokens")
          └─ Worker.sample_tokens()
              └─ GPUModelRunner.sample_tokens()
                  → ModelRunnerOutput 返却
</code></pre>
<h2 id="上流下流の関係"><a class="header" href="#上流下流の関係">上流・下流の関係</a></h2>
<ul>
<li><strong>上流</strong>: EngineCore（<code>step()</code>から呼び出し）</li>
<li><strong>下流</strong>: Worker → GPUModelRunner</li>
</ul>
<h2 id="phase-2-深堀り候補"><a class="header" href="#phase-2-深堀り候補">Phase 2 深堀り候補</a></h2>
<ul>
<li>MultiprocExecutorのMessageQueue実装詳細</li>
<li>Pipeline Parallelism時のバッチスケジューリング</li>
<li>Ray分散実行のオーバーヘッドと障害回復</li>
</ul>
<h2 id="主要ファイル"><a class="header" href="#主要ファイル">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/v1/executor/abstract.py</code></td><td><code>Executor</code>, <code>collective_rpc()</code> (L180), <code>execute_model()</code> (L202)</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/uniproc_executor.py</code></td><td><code>UniProcExecutor</code> (L26)</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/multiproc_executor.py</code></td><td><code>MultiprocExecutor</code> (L93)</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/ray_executor.py</code></td><td><code>RayDistributedExecutor</code> (L62)</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_worker.py</code></td><td><code>Worker</code> (L70), <code>execute_model()</code> (L604)</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/worker_base.py</code></td><td><code>WorkerBase</code> (L34), <code>WorkerWrapperBase</code> (L175)</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="gpumodelrunner"><a class="header" href="#gpumodelrunner">GPUModelRunner</a></h1>
<blockquote>
<p><strong>深度</strong>: [SHALLOW]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-6"><a class="header" href="#概要-6">概要</a></h2>
<p>GPUModelRunnerは、推論パイプラインの実行中核を担う巨大クラス（約6,300行）である。SchedulerOutputを受け取り、入力テンソルの準備、モデルのforward実行、サンプリングを経て、ModelRunnerOutputを返す。<strong>2フェーズ実行パターン</strong>（<code>execute_model()</code> → <code>sample_tokens()</code>）を採用し、モデルフォワードとgrammar bitmask計算の並行実行を可能にしている。</p>
<h2 id="クラス定義"><a class="header" href="#クラス定義">クラス定義</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:329</code> (GPUModelRunner)</p>
<pre><code class="language-python">class GPUModelRunner(
    LoRAModelRunnerMixin,           # LoRAアダプタ管理
    KVConnectorModelRunnerMixin,    # KV Transfer対応
    ECConnectorModelRunnerMixin,    # エンコーダキャッシュ対応
):
</code></pre>
<h2 id="2フェーズ実行パターン"><a class="header" href="#2フェーズ実行パターン">2フェーズ実行パターン</a></h2>
<p>GPUModelRunnerの中核設計。<code>execute_model()</code>でモデルフォワードとlogits計算を行い、結果を<code>ExecuteModelState</code>に保存して<code>None</code>を返す。その後<code>sample_tokens()</code>が状態を復元してサンプリングを実行する。</p>
<pre class="mermaid">sequenceDiagram
    participant EC as EngineCore
    participant MR as GPUModelRunner

    EC-&gt;&gt;MR: execute_model(scheduler_output)
    Note over MR: 入力準備 → モデルフォワード&lt;br&gt;→ logits計算 → 状態保存
    MR--&gt;&gt;EC: None

    Note over EC: grammar bitmask 計算&lt;br&gt;（並行処理）

    EC-&gt;&gt;MR: sample_tokens(grammar_output)
    Note over MR: 状態復元 → grammar適用&lt;br&gt;→ サンプリング → 出力構築
    MR--&gt;&gt;EC: ModelRunnerOutput
</pre>

<h3 id="phase-1-execute_model"><a class="header" href="#phase-1-execute_model">Phase 1: execute_model()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:3312</code> (execute_model)</p>
<pre><code>execute_model(scheduler_output, intermediate_tensors=None)
  │
  ├─ 早期リターン判定                                      # L3335-3367
  │   スケジュールトークン0件 → EMPTY_MODEL_RUNNER_OUTPUT
  │
  ├─ 1. バッチ状態更新                                     # L3376
  │   _update_states(scheduler_output)
  │   → 新規リクエスト登録、キャッシュ済みリクエスト更新
  │
  ├─ 2. 入力準備                                           # L3389
  │   _prepare_inputs(scheduler_output)
  │   → input_ids, positions, logits_indices 計算
  │
  ├─ 3. Attentionメタデータ構築                             # L3400
  │   _build_attention_metadata(scheduler_output, ...)
  │
  ├─ 4. 前処理                                             # L3440-3504
  │   slot_mapping取得、入力トークン/位置/埋め込み準備
  │
  ├─ 5. モデルフォワード                                    # L3521-3603
  │   _model_forward(...)
  │   → hidden_states = model.forward(input_ids, positions, ...)
  │   → logits = compute_logits(hidden_states)
  │
  └─ 6. 状態保存                                            # L3605-3615
      ExecuteModelState(scheduler_output, logits, ...)
      → self.execute_model_state に保存
      → None を返す
</code></pre>
<p><strong>戻り値のパターン</strong>:</p>
<ul>
<li><code>None</code> — 通常ケース（sample_tokens()を後で呼ぶ）</li>
<li><code>ModelRunnerOutput</code> — プーリングモデル等（サンプリング不要）</li>
<li><code>IntermediateTensors</code> — Pipeline Parallelismの中間ランク</li>
<li><code>EMPTY_MODEL_RUNNER_OUTPUT</code> — スケジュールトークン0件</li>
</ul>
<h3 id="phase-2-sample_tokens"><a class="header" href="#phase-2-sample_tokens">Phase 2: sample_tokens()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:3621</code> (sample_tokens)</p>
<pre><code>sample_tokens(grammar_output)
  │
  ├─ 1. 状態復元                                            # L3643-3657
  │   ExecuteModelState から logits, scheduler_output 等を復元
  │   self.execute_model_state = None（クリア）
  │
  ├─ 2. Grammar制約適用                                     # L3659-3663
  │   grammar bitmask を logits に適用（構造化出力時）
  │
  ├─ 3. サンプリング                                        # L3665-3666
  │   _sample(logits, spec_decode_metadata) → SamplerOutput
  │
  ├─ 4. 後処理                                             # L3668-3699
  │   バッチ状態に生成トークンを反映
  │   PP時のトークンブロードキャスト
  │   Speculative Decodingのドラフトトークン提案
  │
  └─ 5. ModelRunnerOutput構築                               # L3775-3787
      ModelRunnerOutput(
        req_ids, req_id_to_index,
        sampled_token_ids,    # list[list[int]]
        logprobs,             # numpy配列
        prompt_logprobs_dict, # torch.Tensor
        kv_connector_output, ...
      )
</code></pre>
<h2 id="executemodelstate"><a class="header" href="#executemodelstate">ExecuteModelState</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:313</code> (ExecuteModelState)</p>
<p>2フェーズ間の一時状態を保持するNamedTuple。GPUテンソルを含むため、シリアライズはされない。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>scheduler_output</code></td><td><code>SchedulerOutput</code></td><td>スケジュール結果</td></tr>
<tr><td><code>logits</code></td><td><code>torch.Tensor</code></td><td>モデル出力logits</td></tr>
<tr><td><code>spec_decode_metadata</code></td><td><code>SpecDecodeMetadata | None</code></td><td>Speculative Decoding情報</td></tr>
<tr><td><code>hidden_states</code></td><td><code>torch.Tensor</code></td><td>隠れ状態</td></tr>
<tr><td><code>sample_hidden_states</code></td><td><code>torch.Tensor</code></td><td>サンプリング用隠れ状態</td></tr>
<tr><td><code>aux_hidden_states</code></td><td><code>list[torch.Tensor] | None</code></td><td>補助隠れ状態</td></tr>
<tr><td><code>ec_connector_output</code></td><td><code>ECConnectorOutput | None</code></td><td>エンコーダ出力</td></tr>
<tr><td><code>cudagraph_stats</code></td><td><code>CUDAGraphStat | None</code></td><td>CUDAGraph統計</td></tr>
<tr><td><code>slot_mappings</code></td><td><code>dict | list | None</code></td><td>KVキャッシュスロットマッピング</td></tr>
</tbody>
</table>
</div>
<h2 id="6300行の内訳-inferred"><a class="header" href="#6300行の内訳-inferred">6,300行の内訳 [INFERRED]</a></h2>
<p>GPUModelRunnerが巨大な理由は、以下の多岐にわたる責務を単一クラスに集約しているため:</p>
<ul>
<li><strong>バッチ状態管理</strong>: リクエストの追加・削除、永続バッチテンソルの管理</li>
<li><strong>入力準備</strong>: トークンID、位置、埋め込みの計算</li>
<li><strong>Attentionメタデータ</strong>: FlashAttention/FlashInfer用メタデータ構築</li>
<li><strong>モデルフォワード</strong>: CUDAGraph対応、torch.compile統合</li>
<li><strong>サンプリング</strong>: トップk/p/温度、logprobs計算</li>
<li><strong>KV Transfer</strong>: KVコネクタとの連携</li>
<li><strong>Speculative Decoding</strong>: ドラフトトークン提案・検証</li>
<li><strong>Pipeline Parallelism</strong>: 中間テンソル管理</li>
<li><strong>LoRA</strong>: アダプタの動的切り替え</li>
<li><strong>マルチモーダル</strong>: エンコーダ入力の処理</li>
</ul>
<h2 id="上流下流の関係-1"><a class="header" href="#上流下流の関係-1">上流・下流の関係</a></h2>
<ul>
<li><strong>上流</strong>: Worker（<code>execute_model()</code> / <code>sample_tokens()</code>経由で呼び出し）</li>
<li><strong>下流</strong>: モデル層（<code>model.forward()</code>）、Sampler</li>
</ul>
<h2 id="phase-2-深堀り候補-1"><a class="header" href="#phase-2-深堀り候補-1">Phase 2 深堀り候補</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>テーマ</th><th>関連メソッド</th><th>ユーザー関心</th></tr>
</thead>
<tbody>
<tr><td>入力準備の詳細</td><td><code>_prepare_inputs()</code>, <code>_update_states()</code></td><td>中</td></tr>
<tr><td>Attentionメタデータ</td><td><code>_build_attention_metadata()</code></td><td>KVキャッシュ関連で高</td></tr>
<tr><td>CUDAGraph統合</td><td><code>_model_forward()</code>, CUDAGraphランナー</td><td>中</td></tr>
<tr><td>サンプリング実装</td><td><code>_sample()</code>, Sampler</td><td>低</td></tr>
<tr><td>KV Transfer連携</td><td><code>KVConnectorModelRunnerMixin</code></td><td>高（ユーザー関心2位）</td></tr>
<tr><td>マルチモーダル入力</td><td>エンコーダ処理、mm_cache</td><td>高（ユーザー関心3位）</td></tr>
</tbody>
</table>
</div>
<h2 id="主要ファイル-1"><a class="header" href="#主要ファイル-1">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_model_runner.py</code></td><td><code>GPUModelRunner</code> (L329), <code>execute_model()</code> (L3312), <code>sample_tokens()</code> (L3621), <code>ExecuteModelState</code> (L313)</td></tr>
<tr><td><code>target/vllm/vllm/v1/outputs.py</code></td><td><code>ModelRunnerOutput</code> (L160), <code>AsyncModelRunnerOutput</code> (L200)</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="inputprocessor-サマリー"><a class="header" href="#inputprocessor-サマリー">InputProcessor サマリー</a></h1>
<blockquote>
<p><strong>深度</strong>: [SHALLOW]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-09</p>
</blockquote>
<h2 id="概要-7"><a class="header" href="#概要-7">概要</a></h2>
<p><code>InputProcessor</code>はユーザー入力（テキストプロンプト、SamplingParams等）を内部表現<code>EngineCoreRequest</code>に変換するコンポーネントである。トークナイズ、パラメータのバリデーションと正規化、マルチモーダル入力の前処理を担当する。AsyncLLMの初期化時に生成され、フロントエンドプロセスで動作する。</p>
<h2 id="アーキテクチャ-3"><a class="header" href="#アーキテクチャ-3">アーキテクチャ</a></h2>
<pre class="mermaid">graph LR
    Prompt["PromptType&lt;br&gt;(str / list[int] / dict)"] --&gt; IP["InputProcessor"]
    Params["SamplingParams"] --&gt; IP
    IP --&gt; IPP["InputPreprocessor&lt;br&gt;tokenizer.encode()"]
    IPP --&gt; PI["ProcessorInputs"]
    PI --&gt; IP
    IP --&gt; ECR["EngineCoreRequest"]
</pre>

<h2 id="主要コンポーネント-4"><a class="header" href="#主要コンポーネント-4">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>用途</th><th>ファイル</th></tr>
</thead>
<tbody>
<tr><td><code>InputProcessor</code></td><td>入力処理のメインクラス</td><td><code>target/vllm/vllm/v1/engine/input_processor.py:56</code></td></tr>
<tr><td><code>InputPreprocessor</code></td><td>トークナイズとマルチモーダル前処理</td><td><code>target/vllm/vllm/v1/engine/input_processor.py</code> (内部利用)</td></tr>
<tr><td><code>ProcessorInputs</code></td><td>前処理結果の中間データ構造</td><td><code>target/vllm/vllm/v1/engine/input_processor.py</code></td></tr>
</tbody>
</table>
</div>
<h2 id="主要メソッド-4"><a class="header" href="#主要メソッド-4">主要メソッド</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>入力</th><th>出力</th></tr>
</thead>
<tbody>
<tr><td><code>process_inputs()</code></td><td>L521</td><td><code>request_id</code>, <code>prompt</code>, <code>params</code></td><td><code>EngineCoreRequest</code></td></tr>
<tr><td><code>assign_request_id()</code></td><td>(別メソッド)</td><td><code>EngineCoreRequest</code></td><td>None (内部IDを付与)</td></tr>
<tr><td><code>_validate_lora()</code></td><td>L535付近</td><td><code>LoRARequest</code></td><td>バリデーション</td></tr>
<tr><td><code>_validate_params()</code></td><td>L536付近</td><td><code>SamplingParams</code></td><td>バリデーション</td></tr>
</tbody>
</table>
</div>
<h2 id="process_inputs-の処理フロー"><a class="header" href="#process_inputs-の処理フロー">process_inputs() の処理フロー</a></h2>
<pre><code>process_inputs(request_id, prompt, params)
  1. バリデーション
     ├─ LoRAリクエスト検証
     ├─ パラメータ検証
     └─ data_parallel_rank 範囲チェック
  2. arrival_time 設定
  3. input_preprocessor.preprocess(prompt)
     → テキストをトークナイズ → ProcessorInputs
  4. split_enc_dec_inputs()
     → エンコーダ/デコーダ入力を分離
  5. SamplingParams 正規化
     ├─ clone() で複製
     ├─ max_tokens 未設定時: max_model_len - seq_len
     ├─ update_from_generation_config()
     └─ update_from_tokenizer()
  6. EngineCoreRequest を構築して返す
</code></pre>
<h2 id="設定-3"><a class="header" href="#設定-3">設定</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>model_config.max_model_len</code></td><td>モデル依存</td><td>max_tokens未指定時の上限計算に使用</td></tr>
<tr><td><code>cache_config.enable_prefix_caching</code></td><td>—</td><td>マルチモーダルUUID生成方式に影響</td></tr>
</tbody>
</table>
</div>
<h2 id="呼び出しフロー-3"><a class="header" href="#呼び出しフロー-3">呼び出しフロー</a></h2>
<pre><code>AsyncLLM.add_request() / LLM._add_request()
  → InputProcessor.process_inputs()
    → InputPreprocessor.preprocess()
      → tokenizer.encode()
    → EngineCoreRequest を返す
  → InputProcessor.assign_request_id()
    → 外部IDを external_req_id に退避
    → 内部ID（外部ID + 8文字ランダム）を request_id に設定
</code></pre>
<h2 id="関連ドキュメント-3"><a class="header" href="#関連ドキュメント-3">関連ドキュメント</a></h2>
<ul>
<li><a href="#エントリポイント-asyncllm--llm-サマリー">エントリポイント</a></li>
<li><a href="#テキスト推論データフロー">データフロー</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kvcachemanager-サマリー"><a class="header" href="#kvcachemanager-サマリー">KVCacheManager サマリー</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-8"><a class="header" href="#概要-8">概要</a></h2>
<p><code>KVCacheManager</code>はPagedAttentionに基づくKVキャッシュブロックの割り当て・解放・プレフィックスキャッシュ検索を管理するクラスである。階層設計を採用し、<code>KVCacheManager</code> → <code>KVCacheCoordinator</code> → <code>SingleTypeKVCacheManager</code> → <code>BlockPool</code> の4層でマルチグループKVキャッシュを統括する。Schedulerから呼び出され、各リクエストに必要なGPUメモリブロックを確保する。</p>
<h2 id="アーキテクチャ-4"><a class="header" href="#アーキテクチャ-4">アーキテクチャ</a></h2>
<h3 id="クラス階層-1"><a class="header" href="#クラス階層-1">クラス階層</a></h3>
<pre class="mermaid">graph TD
    KVM["KVCacheManager&lt;br&gt;公開API"]
    Coord["KVCacheCoordinator&lt;br&gt;マルチグループ統括"]
    STM["SingleTypeKVCacheManager&lt;br&gt;アテンションタイプ別管理"]
    BP["BlockPool&lt;br&gt;物理ブロック管理"]
    BH["BlockHashToBlockMap&lt;br&gt;ハッシュ→ブロック対応"]
    FQ["FreeKVCacheBlockQueue&lt;br&gt;空きブロックキュー"]
    KB["KVCacheBlock&lt;br&gt;ブロック単位"]

    KVM --&gt; Coord
    Coord --&gt; STM
    STM --&gt; BP
    BP --&gt; BH
    BP --&gt; FQ
    FQ --&gt; KB
</pre>

<h3 id="ブロック配置図allocate_slots"><a class="header" href="#ブロック配置図allocate_slots">ブロック配置図（allocate_slots）</a></h3>
<p><code>allocate_slots()</code> がリクエストに割り当てるブロックの論理構造:</p>
<pre><code>|  comp  | new_comp | ext_comp |   new   | lookahead |
|&lt;------ 既計算トークン ------&gt;|&lt;-- 新規計算対象 --&gt;|
                               |&lt;- 割り当て対象 -&gt;|
</code></pre>
<ul>
<li><code>comp</code>: <code>request.num_computed_tokens</code> — 前ステップまでに計算済み</li>
<li><code>new_comp</code>: <code>num_new_computed_tokens</code> — プレフィックスキャッシュから新規にヒットしたトークン</li>
<li><code>ext_comp</code>: <code>num_external_computed_tokens</code> — KVコネクタ（LMCache等）から取得したトークン</li>
<li><code>new</code>: <code>num_new_tokens</code> — 今回計算するトークン</li>
<li><code>lookahead</code>: <code>num_lookahead_tokens</code> — Speculative Decoding用の先読みトークン</li>
</ul>
<h2 id="主要コンポーネント-5"><a class="header" href="#主要コンポーネント-5">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>用途</th><th>ファイル</th></tr>
</thead>
<tbody>
<tr><td><code>KVCacheManager</code></td><td>Scheduler向け公開API</td><td><code>target/vllm/vllm/v1/core/kv_cache_manager.py:94</code></td></tr>
<tr><td><code>KVCacheCoordinator</code></td><td>複数KVキャッシュグループの統括</td><td><code>target/vllm/vllm/v1/core/kv_cache_manager.py</code> 内部</td></tr>
<tr><td><code>SingleTypeKVCacheManager</code></td><td>アテンションタイプ別（Full/SlidingWindow等）のブロック管理</td><td><code>target/vllm/vllm/v1/core/kv_cache_manager.py</code> 内部</td></tr>
<tr><td><code>BlockPool</code></td><td>物理ブロックの割り当て・解放・キャッシュ管理</td><td><code>target/vllm/vllm/v1/core/block_pool.py:128</code></td></tr>
<tr><td><code>KVCacheBlock</code></td><td>ブロックメタデータ（block_id, ref_cnt, block_hash）</td><td><code>target/vllm/vllm/v1/core/kv_cache_utils.py:107</code></td></tr>
<tr><td><code>BlockHashToBlockMap</code></td><td>プレフィックスキャッシュ用ハッシュ→ブロック対応表</td><td><code>target/vllm/vllm/v1/core/block_pool.py</code></td></tr>
<tr><td><code>FreeKVCacheBlockQueue</code></td><td>LRU順序の空きブロック管理キュー</td><td><code>target/vllm/vllm/v1/core/block_pool.py</code></td></tr>
</tbody>
</table>
</div>
<h2 id="主要メソッド-5"><a class="header" href="#主要メソッド-5">主要メソッド</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>allocate_slots()</code></td><td>L206</td><td>リクエストにKVキャッシュブロックを割り当て。成功時<code>KVCacheBlocks</code>、失敗時<code>None</code></td></tr>
<tr><td><code>get_computed_blocks()</code></td><td>L164</td><td>プレフィックスキャッシュから最長ヒットを検索。<code>(KVCacheBlocks, int)</code></td></tr>
<tr><td><code>free()</code></td><td>L378</td><td>リクエストのブロックをプールに返却</td></tr>
<tr><td><code>usage</code> (property)</td><td>L143</td><td>KVキャッシュ使用率 (0.0-1.0)</td></tr>
<tr><td><code>reset_prefix_cache()</code></td><td>L409</td><td>プレフィックスキャッシュ全体をリセット</td></tr>
<tr><td><code>get_num_common_prefix_blocks()</code></td><td>L425</td><td>全リクエスト共通の先頭ブロック数（Cascade Attention用）</td></tr>
<tr><td><code>cache_blocks()</code></td><td>L475</td><td>ブロックをプレフィックスキャッシュに登録</td></tr>
</tbody>
</table>
</div>
<h2 id="allocate_slots-の詳細"><a class="header" href="#allocate_slots-の詳細">allocate_slots() の詳細</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_manager.py:206</code></p>
<pre><code class="language-python">def allocate_slots(
    self,
    request: Request,
    num_new_tokens: int,
    num_new_computed_tokens: int = 0,
    new_computed_blocks: KVCacheBlocks | None = None,
    num_lookahead_tokens: int = 0,
    num_external_computed_tokens: int = 0,
    delay_cache_blocks: bool = False,
    num_encoder_tokens: int = 0,
) -&gt; KVCacheBlocks | None
</code></pre>
<p>処理フロー:</p>
<ol>
<li>必要ブロック数を計算（<code>coordinator.get_num_blocks_to_allocate()</code>）</li>
<li>空きブロック数と比較 → 不足なら <code>None</code> を返す（プリエンプション誘発）</li>
<li>プレフィックスキャッシュヒットのブロックをリクエストに追加</li>
<li>新規ブロックを割り当て（<code>coordinator.allocate_new_blocks()</code>）</li>
<li>キャッシング有効時、計算済みブロックをプレフィックスキャッシュに登録</li>
</ol>
<p><strong>割り当て失敗時</strong> (L336-338):</p>
<pre><code class="language-python">if num_blocks_to_allocate &gt; self.block_pool.get_num_free_blocks():
    return None  # → Scheduler がプリエンプションを実行
</code></pre>
<h2 id="プレフィックスキャッシュ"><a class="header" href="#プレフィックスキャッシュ">プレフィックスキャッシュ</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_manager.py:164</code> (get_computed_blocks)</p>
<p>プロンプトトークン列をブロックサイズ単位でハッシュ化し、<code>BlockHashToBlockMap</code> で過去に計算済みのブロックを検索する。ハッシュチェーンを前方から走査し、最長一致を見つける。</p>
<pre><code>get_computed_blocks(request)
  → coordinator.find_longest_cache_hit(request.block_hashes, max_length)
    → block_pool.get_cached_block(block_hash) を順次検索
    → キャッシュミスで検索終了
  → (キャッシュ済みブロック, ヒットトークン数) を返却
</code></pre>
<p><strong>制約</strong>: 全トークンがキャッシュヒットしても、logits取得のため最後の1トークンは再計算が必要（<code>max_cache_hit_length = request.num_tokens - 1</code>）。</p>
<h2 id="参照カウントとeviction"><a class="header" href="#参照カウントとeviction">参照カウントとEviction</a></h2>
<p><strong>KVCacheBlock</strong> (L107) の <code>ref_cnt</code> フィールドでブロックの使用状況を管理:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ref_cnt</th><th>状態</th></tr>
</thead>
<tbody>
<tr><td>0</td><td>空きブロックキュー（<code>FreeKVCacheBlockQueue</code>）内。Eviction候補</td></tr>
<tr><td>≥ 1</td><td>リクエストに使用中。Eviction対象外</td></tr>
</tbody>
</table>
</div>
<ul>
<li><strong>touch()</strong>: キャッシュヒット時に<code>ref_cnt</code>を増加し、空きキューから除外</li>
<li><strong>free_blocks()</strong>: <code>ref_cnt</code>を減少。0になったら空きキューに戻す（逆順追加でLRU効率化）</li>
<li><strong>_maybe_evict_cached_block()</strong>: 新規ブロック要求時に空きキューの先頭（最古）からEvict。ハッシュメタデータをリセット</li>
</ul>
<h2 id="スライディングウィンドウ対応"><a class="header" href="#スライディングウィンドウ対応">スライディングウィンドウ対応</a></h2>
<p>アテンションウィンドウ外のブロックを<code>null_block</code>（ダミーブロック）で置換し、物理メモリを解放する:</p>
<pre><code>remove_skipped_blocks(request_id, total_computed_tokens)
  → num_skipped_tokens = get_num_skipped_tokens(total_computed_tokens)
  → 前方のブロックをnull_blockで置換
  → 解放したブロックをBlockPoolに返却
</code></pre>
<h2 id="設定-4"><a class="header" href="#設定-4">設定</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>block_size</code></td><td>モデル依存</td><td>1ブロックあたりのトークン数</td></tr>
<tr><td><code>enable_caching</code></td><td>設定依存</td><td>プレフィックスキャッシュの有効化</td></tr>
<tr><td><code>num_gpu_blocks</code></td><td>プロファイリングで決定</td><td>GPUメモリから算出される総ブロック数</td></tr>
</tbody>
</table>
</div>
<h2 id="呼び出しフロー-4"><a class="header" href="#呼び出しフロー-4">呼び出しフロー</a></h2>
<pre><code>Scheduler.schedule()
  ├─ kv_cache_manager.get_computed_blocks(request)     # プレフィックスキャッシュ検索
  ├─ kv_cache_manager.allocate_slots(request, ...)     # ブロック割り当て
  │   └─ None の場合 → プリエンプション実行
  └─ （完了時）kv_cache_manager.free(request)           # ブロック解放
</code></pre>
<h2 id="関連ドキュメント-4"><a class="header" href="#関連ドキュメント-4">関連ドキュメント</a></h2>
<ul>
<li><a href="#scheduler-サマリー">Scheduler</a></li>
<li><a href="#enginecore-サマリー">EngineCore</a></li>
<li><a href="#テキスト推論データフロー">データフロー</a></li>
<li><a href="#用語集">用語集: PagedAttention, KVCacheBlock, BlockPool</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="outputprocessor"><a class="header" href="#outputprocessor">OutputProcessor</a></h1>
<blockquote>
<p><strong>深度</strong>: [SHALLOW]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-9"><a class="header" href="#概要-9">概要</a></h2>
<p>OutputProcessorは<strong>フロントエンドプロセス</strong>で動作し、バックエンド（EngineCore）からZMQ経由で受信した<code>EngineCoreOutput</code>を、ユーザー向けの<code>RequestOutput</code>に変換する。主な処理はインクリメンタルデトークナイズ、停止文字列判定、logprobs処理である。AsyncLLMの<code>output_handler</code>バックグラウンドタスクから呼び出される。</p>
<h2 id="process_outputs-フロー"><a class="header" href="#process_outputs-フロー">process_outputs() フロー</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/output_processor.py:582</code> (process_outputs)</p>
<pre><code>OutputProcessor.process_outputs(engine_core_outputs)       # L582
  │
  for each engine_core_output:
    │
    ├─ req_state = request_states[req_id]                  # RequestState取得
    │   （abortされていればスキップ）
    │
    ├─ 統計情報更新                                         # L620-622
    │
    ├─ デトークナイズ + 停止文字列判定                       # L637-639
    │   stop_string = detokenizer.update(
    │       new_token_ids, stop_terminated)
    │   → トークン→テキスト変換（インクリメンタル）
    │   → 停止文字列検出時は finish_reason = STOP
    │
    ├─ logprobs処理                                         # L646
    │   logprobs_processor.update_from_output(output)
    │
    ├─ RequestOutput構築                                    # L649-656
    │   req_state.make_request_output(
    │       new_token_ids, finish_reason, stop_reason, ...)
    │   → CompletionOutput + RequestOutput
    │
    ├─ 出力配信                                             # L660-665
    │   ├─ AsyncLLM: req_state.queue.put(request_output)
    │   └─ LLM: request_outputs.append(request_output)
    │
    └─ 完了処理                                             # L668-687
        if finish_reason is not None:
          _finish_request(req_state)
          → リクエスト解放、統計記録
</code></pre>
<h2 id="requeststate"><a class="header" href="#requeststate">RequestState</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/output_processor.py:116</code> (RequestState)</p>
<p>各リクエストのフロントエンド側状態を保持する。OutputProcessor.add_request()で作成される。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>external_req_id</code></td><td><code>str</code></td><td>外部リクエストID（クライアント向け）</td></tr>
<tr><td><code>detokenizer</code></td><td><code>IncrementalDetokenizer</code></td><td>デトークナイザインスタンス</td></tr>
<tr><td><code>logprobs_processor</code></td><td><code>LogprobsProcessor</code></td><td>logprobs処理インスタンス</td></tr>
<tr><td><code>output_kind</code></td><td><code>RequestOutputKind</code></td><td>出力モード（CUMULATIVE/DELTA/FINAL_ONLY）</td></tr>
<tr><td><code>queue</code></td><td><code>RequestOutputCollector | None</code></td><td>AsyncLLM用出力キュー</td></tr>
<tr><td><code>prompt_token_ids</code></td><td><code>list[int]</code></td><td>プロンプトトークン（出力に含める用）</td></tr>
</tbody>
</table>
</div>
<h3 id="make_request_output"><a class="header" href="#make_request_output">make_request_output()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/output_processor.py:269</code> (make_request_output)</p>
<pre><code>make_request_output(new_token_ids, finish_reason, ...)
  │
  ├─ FINAL_ONLY モードかつ未完了 → None（出力なし）
  │
  ├─ プーリングモデル → PoolingRequestOutput
  │
  └─ テキスト生成 → RequestOutput
      ├─ _new_completion_output()                          # L377
      │   ├─ detokenizer.get_next_output_text(finished, delta)
      │   │   → DELTAモード: 新規テキストのみ
      │   │   → CUMULATIVEモード: 全テキスト
      │   └─ CompletionOutput(text, token_ids, logprobs, ...)
      └─ RequestOutput(request_id, outputs, finished, ...)
</code></pre>
<h2 id="detokenizerインクリメンタルデトークナイズ-1"><a class="header" href="#detokenizerインクリメンタルデトークナイズ-1">Detokenizer（インクリメンタルデトークナイズ）</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/detokenizer.py:30</code> (IncrementalDetokenizer)</p>
<h3 id="クラス階層-2"><a class="header" href="#クラス階層-2">クラス階層</a></h3>
<pre><code>IncrementalDetokenizer (基底・No-op)               L30
└── BaseIncrementalDetokenizer (ABC)                L65
    ├── FastIncrementalDetokenizer                  L169
    │   → HF tokenizersの DecodeStream 使用
    └── SlowIncrementalDetokenizer                  L258
        → detokenize_incrementally() 使用
</code></pre>
<p>ファクトリメソッド <code>from_new_request()</code> がトークナイザの種類に応じて適切な実装を選択する。</p>
<h3 id="update-メソッド"><a class="header" href="#update-メソッド">update() メソッド</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/detokenizer.py:65</code> (BaseIncrementalDetokenizer.update)</p>
<pre><code>update(new_token_ids, stop_terminated) → stop_string | None
  │
  for each new_token_id:
    ├─ token_ids.append(new_token_id)
    └─ output_text += decode_next(new_token_id)  # 抽象メソッド
  │
  └─ check_stop_strings(output_text, ...)         # L316
      → (stop_string, truncate_offset) | None
</code></pre>
<h3 id="停止文字列判定"><a class="header" href="#停止文字列判定">停止文字列判定</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/detokenizer.py:316</code> (check_stop_strings)</p>
<p><code>check_stop_strings()</code>は累積テキストの末尾付近で停止文字列を検索する。検出時はテキストをトランケートし、停止文字列と切り詰め位置を返す。<code>include_stop_str_in_output</code>フラグで停止文字列を出力に含めるか制御する。</p>
<h2 id="logprobsprocessor"><a class="header" href="#logprobsprocessor">LogprobsProcessor</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/logprobs.py:28</code> (LogprobsProcessor)</p>
<p><code>SamplingParams.logprobs</code> / <code>prompt_logprobs</code> の設定に基づいて初期化される。<code>update_from_output()</code>で<code>EngineCoreOutput</code>からlogprobs情報を抽出し、累積対数確率を更新する。</p>
<h2 id="出力モードrequestoutputkind"><a class="header" href="#出力モードrequestoutputkind">出力モード（RequestOutputKind）</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/sampling_params.py:108</code> (RequestOutputKind)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>モード</th><th>値</th><th>動作</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>CUMULATIVE</code></td><td>0</td><td>毎回全出力テキスト/トークンを返す</td><td>デフォルト</td></tr>
<tr><td><code>DELTA</code></td><td>1</td><td>差分（新規テキスト/トークン）のみ返す</td><td>ストリーミング</td></tr>
<tr><td><code>FINAL_ONLY</code></td><td>2</td><td>完了時のみ出力を返す</td><td>バッチ処理</td></tr>
</tbody>
</table>
</div>
<h2 id="asyncllmとの連携"><a class="header" href="#asyncllmとの連携">AsyncLLMとの連携</a></h2>
<pre><code>AsyncLLM._run_output_handler()                     # async_llm.py:662
  while True:
    outputs = await engine_core.get_output_async()  # ZMQ受信
    for chunk in outputs.outputs:
      output_processor.process_outputs(chunk, ...)  # ← ここで呼ばれる
      → RequestOutputがper-requestキューにpush
      → generate()がキューからyield
</code></pre>
<h2 id="上流下流の関係-2"><a class="header" href="#上流下流の関係-2">上流・下流の関係</a></h2>
<ul>
<li><strong>上流</strong>: AsyncLLM（output_handlerタスクから呼び出し）、EngineCoreOutputs（ZMQ経由受信）</li>
<li><strong>下流</strong>: APIサーバー（RequestOutputをyield）</li>
</ul>
<h2 id="phase-2-深堀り候補-2"><a class="header" href="#phase-2-深堀り候補-2">Phase 2 深堀り候補</a></h2>
<ul>
<li><code>RequestOutputCollector</code>のキューイング実装</li>
<li>ストリーミングモード（DELTA）時のテキスト差分計算詳細</li>
<li>n&gt;1サンプリング時のParentRequest管理</li>
</ul>
<h2 id="主要ファイル-2"><a class="header" href="#主要ファイル-2">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/v1/engine/output_processor.py</code></td><td><code>OutputProcessor</code> (L73), <code>process_outputs()</code> (L582), <code>RequestState</code> (L116), <code>make_request_output()</code> (L269)</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/detokenizer.py</code></td><td><code>IncrementalDetokenizer</code> (L30), <code>FastIncrementalDetokenizer</code> (L169), <code>SlowIncrementalDetokenizer</code> (L258), <code>check_stop_strings()</code> (L316)</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/logprobs.py</code></td><td><code>LogprobsProcessor</code> (L28)</td></tr>
<tr><td><code>target/vllm/vllm/outputs.py</code></td><td><code>RequestOutput</code> (L86), <code>CompletionOutput</code> (L23)</td></tr>
<tr><td><code>target/vllm/vllm/sampling_params.py</code></td><td><code>RequestOutputKind</code> (L108)</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="scheduler-サマリー"><a class="header" href="#scheduler-サマリー">Scheduler サマリー</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-10"><a class="header" href="#概要-10">概要</a></h2>
<p><code>Scheduler</code>はContinuous Batchingの中核コンポーネントであり、各ステップでどのリクエストにどれだけのトークンを計算させるかを決定する。<code>schedule()</code>メソッドは3フェーズ（RUNNING処理 → WAITING処理 → Output構築）で構成され、トークン予算の範囲内で最大限のリクエストをスケジュールする。Unified Compute Modelを採用し、Prefill/Decodeを明示的に区別せず <code>num_computed_tokens</code> の進捗で統一的に管理する。</p>
<h2 id="アーキテクチャ-5"><a class="header" href="#アーキテクチャ-5">アーキテクチャ</a></h2>
<pre class="mermaid">graph TD
    subgraph schedule 3フェーズ
        P1["Phase 1: RUNNING&lt;br&gt;既実行リクエスト処理&lt;br&gt;L350-517"]
        P2["Phase 2: WAITING&lt;br&gt;新規リクエスト受け入れ&lt;br&gt;L532-800"]
        P3["Phase 3: Output構築&lt;br&gt;SchedulerOutput生成&lt;br&gt;L827-896"]
    end

    AR["add_request()"] --&gt;|"WAITINGキューに追加"| P2
    P1 --&gt;|"トークン予算消費"| P2
    P2 --&gt;|"トークン予算消費"| P3
    P1 --&gt;|"プリエンプション"| KVM["KVCacheManager"]
    P2 --&gt;|"allocate_slots()"| KVM
    P2 --&gt;|"get_computed_blocks()"| KVM
    P3 --&gt;|"SchedulerOutput"| EC["EngineCore"]

    UO["update_from_output()"] --&gt;|"ModelRunnerOutput"| OUT["EngineCoreOutputs"]
</pre>

<h2 id="主要コンポーネント-6"><a class="header" href="#主要コンポーネント-6">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>用途</th><th>ファイル</th></tr>
</thead>
<tbody>
<tr><td><code>Scheduler</code></td><td>スケジューリング本体</td><td><code>target/vllm/vllm/v1/core/sched/scheduler.py</code></td></tr>
<tr><td><code>Request</code></td><td>リクエスト内部状態</td><td><code>target/vllm/vllm/v1/request.py</code></td></tr>
<tr><td><code>SchedulerOutput</code></td><td>スケジュール結果（Executor向け）</td><td><code>target/vllm/vllm/v1/core/sched/output.py:184</code></td></tr>
<tr><td><code>NewRequestData</code></td><td>初回スケジュールのフルデータ</td><td><code>target/vllm/vllm/v1/core/sched/output.py:34</code></td></tr>
<tr><td><code>CachedRequestData</code></td><td>既スケジュール済みの差分データ</td><td><code>target/vllm/vllm/v1/core/sched/output.py:114</code></td></tr>
</tbody>
</table>
</div>
<h2 id="主要メソッド-6"><a class="header" href="#主要メソッド-6">主要メソッド</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>schedule()</code></td><td>L321</td><td>メイン: 3フェーズスケジューリング → SchedulerOutput</td></tr>
<tr><td><code>add_request()</code></td><td>L1644</td><td>WAITINGキューにリクエスト登録</td></tr>
<tr><td><code>update_from_output()</code></td><td>L1241</td><td>ModelRunnerOutputから出力生成 → EngineCoreOutputs</td></tr>
<tr><td><code>finish_requests()</code></td><td>L1666</td><td>リクエストを完了/中止状態にする</td></tr>
<tr><td><code>_preempt_request()</code></td><td>L898</td><td>プリエンプション実行（ブロック解放→WAITINGに戻す）</td></tr>
<tr><td><code>_make_cached_request_data()</code></td><td>L999</td><td>CachedRequestData（差分データ）構築</td></tr>
<tr><td><code>_update_request_with_output()</code></td><td>L1538</td><td>生成トークンをリクエストに追加、停止判定</td></tr>
</tbody>
</table>
</div>
<h2 id="schedule-3フェーズ"><a class="header" href="#schedule-3フェーズ">schedule() 3フェーズ</a></h2>
<h3 id="phase-1-running-リクエストのスケジューリングl350-517"><a class="header" href="#phase-1-running-リクエストのスケジューリングl350-517">Phase 1: RUNNING リクエストのスケジューリング（L350-517）</a></h3>
<p>既に実行中のリクエストに対してトークンを割り当てる。</p>
<pre><code>while req_index &lt; len(self.running) and token_budget &gt; 0:
    request = self.running[req_index]

    # 計算すべき新規トークン数
    num_new_tokens = (
        request.num_tokens_with_spec           # 最終目標
        + request.num_output_placeholders      # asyncプレースホルダ
        - request.num_computed_tokens           # 既計算分を差引
    )
    num_new_tokens = min(num_new_tokens, token_budget)

    # KVキャッシュブロック割り当て
    new_blocks = kv_cache_manager.allocate_slots(request, num_new_tokens)

    if new_blocks is None:
        # → プリエンプション: 最低優先度リクエストを解放して再試行
</code></pre>
<p><strong>プリエンプション</strong>: ブロック割り当て失敗時、Priority/FIFOポリシーで最低優先度のリクエストを選びブロック解放。解放後に再試行する。</p>
<h3 id="phase-2-waiting-リクエストのスケジューリングl532-800"><a class="header" href="#phase-2-waiting-リクエストのスケジューリングl532-800">Phase 2: WAITING リクエストのスケジューリング（L532-800）</a></h3>
<p>WAITINGキューから新規リクエストを受け入れる。</p>
<pre><code>for request in self.waiting:
    # スキップ条件チェック
    #   - WAITING_FOR_REMOTE_KVS: 非同期KV受信待ち
    #   - WAITING_FOR_FSM: 構造化出力のFSMコンパイル待ち
    #   - WAITING_FOR_STREAMING_REQ: ストリーミング入力待ち
    #   - LoRA制約超過

    # プレフィックスキャッシュ検索（初回のみ）
    if request.num_computed_tokens == 0:
        computed_blocks, num_hits = kv_cache_manager.get_computed_blocks(request)
        # KVコネクタ（LMCache等）による外部キャッシュも検索

    # 計算対象トークン数
    num_new_tokens = request.num_tokens - num_computed_tokens
    num_new_tokens = min(num_new_tokens, token_budget)

    # KVキャッシュブロック割り当て
    new_blocks = kv_cache_manager.allocate_slots(request, num_new_tokens, ...)
    if new_blocks is None:
        break  # ← RUNNINGと異なりプリエンプションせずループ終了

    # RUNNINGキューに追加
    self.running.append(request)
    request.status = RequestStatus.RUNNING
    token_budget -= num_new_tokens
</code></pre>
<h3 id="phase-3-scheduleroutput-構築l827-896"><a class="header" href="#phase-3-scheduleroutput-構築l827-896">Phase 3: SchedulerOutput 構築（L827-896）</a></h3>
<pre><code># 新規リクエスト → NewRequestData（フルデータ）
new_reqs_data = [NewRequestData.from_request(req, block_ids) for req in scheduled_new_reqs]

# 既実行リクエスト → CachedRequestData（差分のみ）
cached_reqs_data = self._make_cached_request_data(running_reqs, resumed_reqs, ...)

return SchedulerOutput(
    scheduled_new_reqs=new_reqs_data,
    scheduled_cached_reqs=cached_reqs_data,
    num_scheduled_tokens=num_scheduled_tokens,
    total_num_scheduled_tokens=total,
    ...
)
</code></pre>
<h2 id="unified-compute-model"><a class="header" href="#unified-compute-model">Unified Compute Model</a></h2>
<p>vLLM v1のSchedulerはPrefillとDecodeを明示的に区別しない。各リクエストの <code>num_computed_tokens</code> が <code>num_tokens_with_spec</code>（プロンプト長 + 出力長 + スペキュレーショントークン）に追いつくまでトークンを割り当てる。</p>
<p>このアプローチにより以下が統一的に扱える:</p>
<ul>
<li><strong>Chunked Prefill</strong>: 大きなプロンプトを複数ステップに分割</li>
<li><strong>Prefix Caching</strong>: キャッシュヒット分を <code>num_computed_tokens</code> に反映</li>
<li><strong>Speculative Decoding</strong>: ドラフトトークンを <code>num_tokens_with_spec</code> に含める</li>
</ul>
<h2 id="トークン予算"><a class="header" href="#トークン予算">トークン予算</a></h2>
<pre><code class="language-python">token_budget = self.max_num_scheduled_tokens  # ステップあたりの上限
</code></pre>
<ul>
<li>各リクエストのスケジュール時に <code>token_budget -= num_new_tokens</code> で消費</li>
<li>Phase 1（RUNNING）→ Phase 2（WAITING）の順で消費</li>
<li>枯渇時: RUNNING側は continue（次リクエスト試行）、WAITING側は break（ループ終了）</li>
</ul>
<h2 id="プリエンプション"><a class="header" href="#プリエンプション">プリエンプション</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:898</code> (_preempt_request)</p>
<p>KVキャッシュブロック不足時にRUNNINGリクエストに対してのみ発動:</p>
<ol>
<li>ポリシーに基づき最低優先度のリクエストを選択
<ul>
<li><strong>Priority</strong>: <code>(priority, arrival_time)</code> が最大のリクエスト</li>
<li><strong>FIFO</strong>: 最後のリクエスト</li>
</ul>
</li>
<li><code>kv_cache_manager.free(request)</code> でブロック解放</li>
<li><code>request.status = RequestStatus.PREEMPTED</code>、<code>num_computed_tokens = 0</code> にリセット</li>
<li>WAITINGキューの先頭に戻す（LIFO順序で優先再スケジュール）</li>
</ol>
<h2 id="request-ステータス遷移"><a class="header" href="#request-ステータス遷移">Request ステータス遷移</a></h2>
<pre class="mermaid">stateDiagram-v2
    [*] --&gt; WAITING: add_request()
    WAITING --&gt; WAITING_FOR_FSM: FSMコンパイル待ち
    WAITING --&gt; WAITING_FOR_REMOTE_KVS: リモートKV受信待ち
    WAITING --&gt; WAITING_FOR_STREAMING_REQ: ストリーミング入力待ち
    WAITING_FOR_FSM --&gt; WAITING: コンパイル完了
    WAITING_FOR_REMOTE_KVS --&gt; WAITING: 受信完了
    WAITING_FOR_STREAMING_REQ --&gt; WAITING: 入力完了
    WAITING --&gt; RUNNING: schedule()で選択
    RUNNING --&gt; PREEMPTED: KVキャッシュ不足
    PREEMPTED --&gt; WAITING: キュー先頭に戻す
    RUNNING --&gt; FINISHED_STOPPED: EOS/stop_token検出
    RUNNING --&gt; FINISHED_LENGTH_CAPPED: max_tokens到達
    RUNNING --&gt; FINISHED_ABORTED: ユーザーによる中止
    RUNNING --&gt; FINISHED_ERROR: エラー発生
</pre>

<h2 id="update_from_output-フロー"><a class="header" href="#update_from_output-フロー">update_from_output() フロー</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:1241</code></p>
<pre><code>update_from_output(scheduler_output, model_runner_output)
  → dict[int, EngineCoreOutputs]

処理:
  for req_id in scheduler_output.scheduled requests:
    # Speculative Decoding リジェクション処理
    #   → 不採用トークン分 num_computed_tokens を巻き戻し

    # 生成トークンをリクエストに追加
    new_token_ids, stopped = _update_request_with_output(request, tokens)

    # 完了判定
    if stopped:
      finish_reason = request.get_finished_reason()
      _free_request(request)  # ブロック解放

    # EngineCoreOutput を構築
    outputs[client_index].append(EngineCoreOutput(
      request_id, new_token_ids, finish_reason, logprobs, ...
    ))

  return {client_index: EngineCoreOutputs(outputs=outs) for ...}
</code></pre>
<h2 id="設定-5"><a class="header" href="#設定-5">設定</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>max_num_seqs</code></td><td>設定依存</td><td>同時実行リクエスト数上限</td></tr>
<tr><td><code>max_num_batched_tokens</code></td><td>設定依存</td><td>ステップあたりのトークン予算上限</td></tr>
<tr><td><code>enable_chunked_prefill</code></td><td>設定依存</td><td>Chunked Prefillの有効化</td></tr>
<tr><td><code>long_prefill_token_threshold</code></td><td>0</td><td>長プロンプト分割閾値（0=無効）</td></tr>
<tr><td><code>scheduling_policy</code></td><td>Priority</td><td>プリエンプション選択ポリシー</td></tr>
</tbody>
</table>
</div>
<h2 id="呼び出しフロー-5"><a class="header" href="#呼び出しフロー-5">呼び出しフロー</a></h2>
<pre><code>EngineCore.add_request(request)
  → scheduler.add_request(request)       # WAITINGキューに登録

EngineCore.step()
  ├─ scheduler.schedule()                 # → SchedulerOutput
  │   ├─ kv_cache_manager.get_computed_blocks()  # プレフィックスキャッシュ
  │   └─ kv_cache_manager.allocate_slots()       # ブロック割り当て
  ├─ executor.execute_model(scheduler_output)     # GPU実行
  └─ scheduler.update_from_output(output)         # → EngineCoreOutputs
</code></pre>
<h2 id="関連ドキュメント-5"><a class="header" href="#関連ドキュメント-5">関連ドキュメント</a></h2>
<ul>
<li><a href="#kvcachemanager-サマリー">KVCacheManager</a></li>
<li><a href="#enginecore-サマリー">EngineCore</a></li>
<li><a href="#エントリポイント-asyncllm--llm-サマリー">エントリポイント</a></li>
<li><a href="#テキスト推論データフロー">データフロー</a></li>
<li><a href="#用語集">用語集: Continuous Batching, Unified Compute Model</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="用語集"><a class="header" href="#用語集">用語集</a></h1>
<!-- 調査中に発見した対象OSS固有の用語をここに蓄積する -->
<h3 id="pagedattention-1"><a class="header" href="#pagedattention-1">PagedAttention</a></h3>
<p>KVキャッシュをOSの仮想メモリページングに着想を得て、固定サイズのブロック単位で管理する技術。連続したGPUメモリ確保が不要になり、メモリ断片化を大幅に抑制する。SOSP 2023論文で提案。</p>
<p><strong>参照</strong>: <code>target/vllm/csrc/attention/</code> (カーネル実装)</p>
<h3 id="continuous-batching-1"><a class="header" href="#continuous-batching-1">Continuous Batching</a></h3>
<p>リクエストの到着・完了に応じてバッチを動的に更新する手法。固定バッチサイズと異なり、GPU稼働率を最大化できる。vLLMのSchedulerが担う。</p>
<h3 id="prefill"><a class="header" href="#prefill">Prefill</a></h3>
<p>プロンプト入力トークン全体を処理してKVキャッシュに書き込む最初のフェーズ。計算量が多く、GPUの並列性を活かしやすい。</p>
<h3 id="decode"><a class="header" href="#decode">Decode</a></h3>
<p>生成済みコンテキストのKVキャッシュを参照しながら次のトークンを1つずつ逐次生成するフェーズ。メモリバウンドになりやすい。</p>
<h3 id="chunked-prefill"><a class="header" href="#chunked-prefill">Chunked Prefill</a></h3>
<p>Prefillフェーズをチャンクに分割してDecodeフェーズと交互実行する手法。長いプロンプトがDecodeのレイテンシを増加させるのを防ぐ。</p>
<h3 id="enginecore"><a class="header" href="#enginecore">EngineCore</a></h3>
<p>vLLMの推論ループの内側コンポーネント。別プロセス（<code>EngineCoreProc</code>）として動作し、ZeroMQソケットで上位エンジン層と通信する。Scheduler、KVCacheManager、Executorを統括する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:79</code> (<code>EngineCore</code>)</p>
<h3 id="kvcachemanager"><a class="header" href="#kvcachemanager">KVCacheManager</a></h3>
<p>KVキャッシュブロックの割り当て・解放・プレフィックスキャッシュを管理するクラス。BlockPoolを内部で使用する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_manager.py:94</code> (<code>KVCacheManager</code>)</p>
<h3 id="kvcacheblock"><a class="header" href="#kvcacheblock">KVCacheBlock</a></h3>
<p>PagedAttentionで管理するKVキャッシュの最小単位。固定サイズ（block_sizeトークン分）のGPUメモリブロック。</p>
<h3 id="blockpool"><a class="header" href="#blockpool">BlockPool</a></h3>
<p>KVCacheBlockの空きブロックをプール管理するクラス。ブロックの割り当て・返却を効率的に行う。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py</code></p>
<h3 id="vllmconfig"><a class="header" href="#vllmconfig">VllmConfig</a></h3>
<p>全設定を集約するトップレベルクラス。<code>ModelConfig</code>、<code>CacheConfig</code>、<code>SchedulerConfig</code>、<code>ParallelConfig</code>等を内包する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/config/vllm.py</code></p>
<h3 id="gpumodelrunner-1"><a class="header" href="#gpumodelrunner-1">GPUModelRunner</a></h3>
<p>GPU上でモデルのフォワードパスを実際に実行するクラス。LoRA、KVConnector、ECConnectorのMixinを持つ。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:329</code> (<code>GPUModelRunner</code>)</p>
<h3 id="executor-1"><a class="header" href="#executor-1">Executor</a></h3>
<p>Worker群を管理する抽象層。シングルプロセス（<code>UniProcExecutor</code>）、マルチプロセス（<code>MultiprocExecutor</code>）、Ray分散（<code>RayDistributedExecutor</code>）の実装がある。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py</code></p>
<h3 id="worker"><a class="header" href="#worker">Worker</a></h3>
<p>1つのGPU（またはCPU/XPU）デバイスを担当するプロセス。GPUModelRunnerを保持し、Executorから呼び出される。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_worker.py:70</code> (<code>Worker</code>)</p>
<h3 id="speculative-decoding"><a class="header" href="#speculative-decoding">Speculative Decoding</a></h3>
<p>ドラフトモデル（小さいモデル）で複数トークンを仮生成し、メインモデルで一括検証することで推論を高速化する手法。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/spec_decode/</code></p>
<h3 id="lora-low-rank-adaptation"><a class="header" href="#lora-low-rank-adaptation">LoRA (Low-Rank Adaptation)</a></h3>
<p>少量の追加パラメータでLLMをファインチューニングする手法。vLLMは複数LoRAの動的切替（Multi-LoRA）をランタイムでサポートする。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/lora/</code></p>
<h3 id="kv-transfer"><a class="header" href="#kv-transfer">KV Transfer</a></h3>
<p>複数のvLLMインスタンス間でKVキャッシュを転送する機構。Disaggregated Prefill（PrefillとDecodeを異なるインスタンスで実行）等に使用。KVConnector抽象基底クラスとして実装され、LMCache、NIXL、P2P NCCL、Mooncake等の複数バックエンドがある。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/</code> (全体), <code>target/vllm/vllm/config/kv_transfer.py:17</code> (<code>KVTransferConfig</code>)</p>
<h3 id="lmcache"><a class="header" href="#lmcache">LMCache</a></h3>
<p>vLLMと統合可能な外部KVキャッシュストレージ。KV Transferのバックエンドの一つとして動作し、KVキャッシュのCPUオフロード、インスタンス間共有等を提供する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py</code>, <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/</code></p>
<h3 id="multimodal-マルチモーダル"><a class="header" href="#multimodal-マルチモーダル">Multimodal (マルチモーダル)</a></h3>
<p>テキスト以外の入力（画像・動画・音声）を扱うモデル機能。<code>vllm/multimodal/</code> にプロセッサ・レジストリ等が実装されている。Gemma3等のマルチモーダルモデルが対応。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/</code></p>
<h3 id="unified-compute-model-1"><a class="header" href="#unified-compute-model-1">Unified Compute Model</a></h3>
<p>vLLM v1のSchedulerが採用するスケジューリングアプローチ。PrefillフェーズとDecodeフェーズを明示的に区別せず、各リクエストの<code>num_computed_tokens</code>（計算済みトークン数）が目標に追いつくまでトークンを割り当てる。これにより、Chunked Prefill、Prefix Caching、Speculative Decodingを統一的に扱える。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:322</code> (コメント)</p>
<h3 id="collective_rpc-1"><a class="header" href="#collective_rpc-1">collective_rpc</a></h3>
<p>Executor層が全Workerに対して同一メソッドを実行するRPCパターン。メソッド名（文字列）または関数を受け取り、全Workerで並列実行後、出力ランクのWorkerの結果を返す。<code>non_block=True</code>でFuture返却も可能。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py:180</code> (<code>collective_rpc</code>)</p>
<h3 id="executemodelstate-1"><a class="header" href="#executemodelstate-1">ExecuteModelState</a></h3>
<p>GPUModelRunnerの2フェーズ実行パターンで使用される一時状態。execute_model()がlogitsやhidden_statesなどのGPUテンソルを保存し、sample_tokens()が復元してサンプリングを行う。NamedTuple。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:313</code> (<code>ExecuteModelState</code>)</p>
<h3 id="outputprocessor-1"><a class="header" href="#outputprocessor-1">OutputProcessor</a></h3>
<p>フロントエンドプロセスで動作し、EngineCoreOutputをRequestOutputに変換するコンポーネント。インクリメンタルデトークナイズ、停止文字列判定、logprobs処理を行う。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/output_processor.py:73</code> (<code>OutputProcessor</code>)</p>
<h3 id="incrementaldetokenizer"><a class="header" href="#incrementaldetokenizer">IncrementalDetokenizer</a></h3>
<p>トークンIDからテキストへのインクリメンタル変換を行うクラス。FastIncrementalDetokenizer（HF DecodeStream）とSlowIncrementalDetokenizer（Python実装）の2種がある。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/detokenizer.py:30</code> (<code>IncrementalDetokenizer</code>)</p>
<h3 id="requestoutputkind"><a class="header" href="#requestoutputkind">RequestOutputKind</a></h3>
<p>出力モードを定義するEnum。CUMULATIVE（毎回全出力）、DELTA（差分のみ、ストリーミング向け）、FINAL_ONLY（完了時のみ）の3値。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/sampling_params.py:108</code> (<code>RequestOutputKind</code>)</p>
<h3 id="mm_cache-マルチモーダルキャッシュ"><a class="header" href="#mm_cache-マルチモーダルキャッシュ">mm_cache (マルチモーダルキャッシュ)</a></h3>
<p>マルチモーダル入力（画像エンコーダ出力等）のキャッシュ機構。同一画像の繰り返し処理を避けるため、エンコーダ出力をキャッシュする。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu/mm/encoder_runner.py</code></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ファイル索引"><a class="header" href="#ファイル索引">ファイル索引</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->
        <script src="mermaid-eefea253.min.js"></script>
        <script src="mermaid-init-ccf746f1.js"></script>

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
