<!DOCTYPE HTML>
<html lang="ja" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>コードリーディング: vLLM &amp; LMCache</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-ae938929.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/custom-9cb14b51.css">


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex-baa455d0.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc-d2e92f25.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">コードリーディング: vLLM &amp; LMCache</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="コードリーディング-vllm--lmcache"><a class="header" href="#コードリーディング-vllm--lmcache">コードリーディング: vLLM &amp; LMCache</a></h1>
<p>LLM推論サービングに関連するOSSのコードリーディング結果を構造化して蓄積するプロジェクト。</p>
<h2 id="対象oss"><a class="header" href="#対象oss">対象OSS</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>OSS</th><th>ソースコード</th><th>概要</th><th>Phase</th></tr>
</thead>
<tbody>
<tr><td><a href="vllm/README.html">vLLM</a></td><td><code>target/vllm/</code></td><td>LLM推論サービングエンジン</td><td>Phase 2</td></tr>
<tr><td><a href="lmcache/README.html">LMCache</a></td><td><code>target/LMCache/</code></td><td>KVキャッシュ保存・共有・再利用ライブラリ</td><td>Phase 0a</td></tr>
</tbody>
</table>
</div>
<h2 id="プロジェクト横断"><a class="header" href="#プロジェクト横断">プロジェクト横断</a></h2>
<ul>
<li><a href="cross-project/README.html">横断調査</a> — 複数OSSにまたがる調査報告</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="vllm"><a class="header" href="#vllm">vLLM</a></h1>
<p>LLM推論サービングエンジン <a href="https://github.com/vllm-project/vllm">vLLM</a> のコードリーディング。</p>
<ul>
<li><strong>ソースコード</strong>: <code>target/vllm/</code></li>
<li><strong>現在のPhase</strong>: Phase 2（コンポーネント別深堀り）</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="テキスト推論データフロー"><a class="header" href="#テキスト推論データフロー">テキスト推論データフロー</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11 (Phase 2bでマルチモーダル差分追加)</p>
</blockquote>
<h2 id="概要"><a class="header" href="#概要">概要</a></h2>
<p>テキスト推論リクエストは、APIエントリポイントからエンジン層を経てGPUで実行され、生成されたトークンがデトークナイズされてユーザーに返却される。フロー全体は5つの境界データ構造（EngineCoreRequest → SchedulerOutput → ModelRunnerOutput → EngineCoreOutput → RequestOutput）で区切られ、ZMQ IPCによるプロセス分離とasyncioによる非同期パイプラインで高スループットを実現する。</p>
<h2 id="フロー全体図"><a class="header" href="#フロー全体図">フロー全体図</a></h2>
<pre class="mermaid">graph TD
    subgraph フロントエンドプロセス
        API["API Server / LLM"]
        AsyncLLM["AsyncLLM&lt;br&gt;generate() / add_request()"]
        IP["InputProcessor&lt;br&gt;process_inputs()"]
        OP["OutputProcessor&lt;br&gt;process_outputs()"]
        Client["EngineCoreClient&lt;br&gt;AsyncMPClient"]
    end

    subgraph バックエンドプロセス ["EngineCore プロセス"]
        EC["EngineCore&lt;br&gt;step()"]
        Sched["Scheduler&lt;br&gt;schedule()"]
        KV["KVCacheManager&lt;br&gt;allocate_slots()"]
        Exec["Executor&lt;br&gt;execute_model()"]
        Worker["Worker"]
        MR["GPUModelRunner&lt;br&gt;execute_model()"]
    end

    API --&gt;|"prompt, params"| AsyncLLM
    AsyncLLM --&gt;|"prompt, params"| IP
    IP --&gt;|"EngineCoreRequest"| AsyncLLM
    AsyncLLM --&gt;|"EngineCoreRequest"| Client

    Client --&gt;|"ZMQ ROUTER\nmsgpack"| EC
    EC --&gt; Sched
    Sched --&gt;|"allocate_slots()"| KV
    Sched --&gt;|"SchedulerOutput"| EC
    EC --&gt;|"SchedulerOutput"| Exec
    Exec --&gt;|"MessageQueue\n共有メモリ"| Worker
    Worker --&gt; MR
    MR --&gt;|"ModelRunnerOutput"| Worker
    Worker --&gt;|"ModelRunnerOutput"| Exec
    Exec --&gt;|"ModelRunnerOutput"| EC
    EC --&gt;|"update_from_output()"| Sched
    Sched --&gt;|"EngineCoreOutputs"| EC

    EC --&gt;|"ZMQ PUSH\nmsgpack"| Client
    Client --&gt;|"EngineCoreOutputs"| OP
    OP --&gt;|"RequestOutput"| AsyncLLM
    AsyncLLM --&gt;|"RequestOutput"| API
</pre>

<h2 id="境界データ構造"><a class="header" href="#境界データ構造">境界データ構造</a></h2>
<p>フローは以下の5つのデータ構造で区切られる。各構造はプロセス間またはコンポーネント間の境界を定義する。</p>
<h3 id="enginecorerequest"><a class="header" href="#enginecorerequest">EngineCoreRequest</a></h3>
<p>フロントエンド → バックエンドの境界。ユーザー入力を正規化した内部表現。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/__init__.py:55</code> (EngineCoreRequest)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>request_id</code></td><td><code>str</code></td><td>内部リクエストID（外部IDに8文字ランダムサフィックス付与）</td></tr>
<tr><td><code>prompt_token_ids</code></td><td><code>list[int] | None</code></td><td>トークナイズ済みプロンプト</td></tr>
<tr><td><code>mm_features</code></td><td><code>list[MultiModalFeatureSpec] | None</code></td><td>マルチモーダル入力（テキスト推論ではNone）</td></tr>
<tr><td><code>sampling_params</code></td><td><code>SamplingParams | None</code></td><td>サンプリングパラメータ（clone済み）</td></tr>
<tr><td><code>eos_token_id</code></td><td><code>int | None</code></td><td>終了トークンID</td></tr>
<tr><td><code>arrival_time</code></td><td><code>float</code></td><td>リクエスト到着時刻</td></tr>
<tr><td><code>lora_request</code></td><td><code>LoRARequest | None</code></td><td>LoRAアダプタ情報</td></tr>
<tr><td><code>priority</code></td><td><code>int</code></td><td>優先度（デフォルト0）</td></tr>
<tr><td><code>data_parallel_rank</code></td><td><code>int | None</code></td><td>データ並列ランク指定</td></tr>
</tbody>
</table>
</div>
<p><code>msgspec.Struct</code> を継承し、<code>array_like=True</code> + <code>omit_defaults=True</code> で効率的にmsgpackシリアライズされる。</p>
<h3 id="scheduleroutput"><a class="header" href="#scheduleroutput">SchedulerOutput</a></h3>
<p>Scheduler → Executor の境界。各ステップのスケジュール結果を含む。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/output.py:184</code> (SchedulerOutput)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>scheduled_new_reqs</code></td><td><code>list[NewRequestData]</code></td><td>初回スケジュールされたリクエスト（フルデータ）</td></tr>
<tr><td><code>scheduled_cached_reqs</code></td><td><code>CachedRequestData</code></td><td>既スケジュール済みリクエスト（差分のみ）</td></tr>
<tr><td><code>num_scheduled_tokens</code></td><td><code>dict[str, int]</code></td><td>リクエストごとのスケジュールトークン数</td></tr>
<tr><td><code>total_num_scheduled_tokens</code></td><td><code>int</code></td><td>合計スケジュールトークン数</td></tr>
<tr><td><code>scheduled_spec_decode_tokens</code></td><td><code>dict[str, list[int]]</code></td><td>Speculative Decoding用トークン</td></tr>
<tr><td><code>scheduled_encoder_inputs</code></td><td><code>dict[str, list[int]]</code></td><td>エンコーダ入力インデックス（マルチモーダル）</td></tr>
<tr><td><code>num_common_prefix_blocks</code></td><td><code>list[int]</code></td><td>共通プレフィックスブロック数（Cascade Attention用）</td></tr>
<tr><td><code>finished_req_ids</code></td><td><code>set[str]</code></td><td>このステップで完了したリクエストID</td></tr>
<tr><td><code>free_encoder_mm_hashes</code></td><td><code>list[str]</code></td><td>解放するエンコーダキャッシュのmm_hash</td></tr>
<tr><td><code>preempted_req_ids</code></td><td><code>set[str] | None</code></td><td>プリエンプションされたリクエスト</td></tr>
<tr><td><code>has_structured_output_requests</code></td><td><code>bool</code></td><td>構造化出力リクエストの有無</td></tr>
<tr><td><code>pending_structured_output_tokens</code></td><td><code>bool</code></td><td>Grammar bitmask準備状態</td></tr>
<tr><td><code>num_invalid_spec_tokens</code></td><td><code>dict[str, int] | None</code></td><td>無効スペキュレーショントークン数</td></tr>
<tr><td><code>kv_connector_metadata</code></td><td><code>KVConnectorMetadata | None</code></td><td>KV Transfer メタデータ</td></tr>
<tr><td><code>ec_connector_metadata</code></td><td><code>ECConnectorMetadata | None</code></td><td>EC Transfer メタデータ</td></tr>
</tbody>
</table>
</div>
<p><strong>NewRequestData</strong> は初回スケジュール時のフルデータ（プロンプトトークン、サンプリングパラメータ、ブロックID等）を含む。<strong>CachedRequestData</strong> は既スケジュール済みリクエストの差分（新規ブロックID、新トークンID、計算済みトークン数の更新）のみを含み、プロセス間通信コストを最小化する。</p>
<h3 id="modelrunneroutput"><a class="header" href="#modelrunneroutput">ModelRunnerOutput</a></h3>
<p>GPUModelRunner → EngineCore の境界。モデル推論結果を含む。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/outputs.py:160</code> (ModelRunnerOutput)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>req_ids</code></td><td><code>list[str]</code></td><td>バッチ内のリクエストID一覧</td></tr>
<tr><td><code>req_id_to_index</code></td><td><code>dict[str, int]</code></td><td>リクエストID → バッチインデックス</td></tr>
<tr><td><code>sampled_token_ids</code></td><td><code>list[list[int]]</code></td><td>サンプリング済みトークンID [num_reqs, num_generated]</td></tr>
<tr><td><code>logprobs</code></td><td><code>LogprobsLists | None</code></td><td>生成トークンの対数確率</td></tr>
<tr><td><code>prompt_logprobs_dict</code></td><td><code>dict[str, LogprobsTensors | None]</code></td><td>プロンプトトークンの対数確率</td></tr>
<tr><td><code>pooler_output</code></td><td><code>list[Tensor | None] | None</code></td><td>プーリング出力（埋め込みモデル用）</td></tr>
<tr><td><code>kv_connector_output</code></td><td><code>KVConnectorOutput | None</code></td><td>KV Transfer出力</td></tr>
<tr><td><code>ec_connector_output</code></td><td><code>ECConnectorOutput | None</code></td><td>EC Transfer出力</td></tr>
<tr><td><code>num_nans_in_logits</code></td><td><code>dict[str, int] | None</code></td><td>logits内のNaN数</td></tr>
<tr><td><code>cudagraph_stats</code></td><td><code>CUDAGraphStat | None</code></td><td>CUDAGraph実行統計</td></tr>
</tbody>
</table>
</div>
<p>Worker→Executorへの転送ではPythonリスト形式を使用し、torch.Tensorの高コストなシリアライゼーションを回避する。</p>
<h3 id="enginecoreoutput"><a class="header" href="#enginecoreoutput">EngineCoreOutput</a></h3>
<p>バックエンド → フロントエンドの境界。リクエスト単位の推論結果。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/__init__.py:130</code> (EngineCoreOutput)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>request_id</code></td><td><code>str</code></td><td>対応するリクエストID</td></tr>
<tr><td><code>new_token_ids</code></td><td><code>list[int]</code></td><td>新たに生成されたトークンID</td></tr>
<tr><td><code>finish_reason</code></td><td><code>FinishReason | None</code></td><td>完了理由（stop/length/abort/error）</td></tr>
<tr><td><code>new_logprobs</code></td><td><code>LogprobsLists | None</code></td><td>生成トークンのlogprobs</td></tr>
<tr><td><code>num_cached_tokens</code></td><td><code>int</code></td><td>プレフィックスキャッシュヒット数</td></tr>
</tbody>
</table>
</div>
<p><code>EngineCoreOutputs</code>（複数形）がこれを<code>list[EngineCoreOutput]</code>としてバッチ化し、<code>scheduler_stats</code>やタイムスタンプと共にZMQ経由で送信される。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/__init__.py:176</code> (EngineCoreOutputs)</p>
<h3 id="requestoutput"><a class="header" href="#requestoutput">RequestOutput</a></h3>
<p>OutputProcessor → API の境界。ユーザーに返却される最終出力。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/outputs.py:86</code> (RequestOutput)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>request_id</code></td><td><code>str</code></td><td>外部リクエストID（クライアントが指定したID）</td></tr>
<tr><td><code>prompt</code></td><td><code>str | None</code></td><td>元のプロンプト文字列</td></tr>
<tr><td><code>prompt_token_ids</code></td><td><code>list[int] | None</code></td><td>トークナイズ済みプロンプト</td></tr>
<tr><td><code>prompt_logprobs</code></td><td><code>PromptLogprobs | None</code></td><td>プロンプトトークンの対数確率</td></tr>
<tr><td><code>outputs</code></td><td><code>list[CompletionOutput]</code></td><td>サンプルごとの出力（n&gt;1で複数）</td></tr>
<tr><td><code>finished</code></td><td><code>bool</code></td><td>リクエスト完了フラグ</td></tr>
<tr><td><code>metrics</code></td><td><code>RequestStateStats | None</code></td><td>レイテンシ等の統計情報</td></tr>
<tr><td><code>num_cached_tokens</code></td><td><code>int | None</code></td><td>プレフィックスキャッシュヒット数</td></tr>
<tr><td><code>kv_transfer_params</code></td><td><code>dict[str, Any] | None</code></td><td>KV Transfer情報（完了時）</td></tr>
</tbody>
</table>
</div>
<p><strong>CompletionOutput</strong> (<code>target/vllm/vllm/outputs.py:23</code>) は各サンプルの出力を表す:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>index</code></td><td><code>int</code></td><td>サンプルインデックス</td></tr>
<tr><td><code>text</code></td><td><code>str</code></td><td>デトークナイズ済みテキスト</td></tr>
<tr><td><code>token_ids</code></td><td><code>GenericSequence[int]</code></td><td>生成トークンID列</td></tr>
<tr><td><code>cumulative_logprob</code></td><td><code>float | None</code></td><td>累積対数確率</td></tr>
<tr><td><code>logprobs</code></td><td><code>SampleLogprobs | None</code></td><td>各トークンのlogprobs</td></tr>
<tr><td><code>finish_reason</code></td><td><code>str | None</code></td><td>完了理由（“stop” / “length”）</td></tr>
<tr><td><code>stop_reason</code></td><td><code>int | str | None</code></td><td>停止トークン/文字列</td></tr>
</tbody>
</table>
</div>
<p><strong>出力モード</strong>（<code>RequestOutputKind</code>、<code>target/vllm/vllm/sampling_params.py:108</code>）:</p>
<ul>
<li><code>CUMULATIVE</code>: 毎回全出力を返す（デフォルト）</li>
<li><code>DELTA</code>: 差分のみ返す（ストリーミング向け）</li>
<li><code>FINAL_ONLY</code>: 完了時のみ返す</li>
</ul>
<h2 id="上流パス-リクエスト受信--enginecore"><a class="header" href="#上流パス-リクエスト受信--enginecore">上流パス: リクエスト受信 → EngineCore</a></h2>
<h3 id="エントリポイント-llm--asyncllm"><a class="header" href="#エントリポイント-llm--asyncllm">エントリポイント (LLM / AsyncLLM)</a></h3>
<p>vLLMには同期パス（<code>LLM</code>）と非同期パス（<code>AsyncLLM</code>）の2つのエントリポイントがある。内部的にはどちらも<code>InputProcessor</code>と<code>EngineCoreClient</code>を使用する。</p>
<h4 id="非同期パス主パス-asyncllm"><a class="header" href="#非同期パス主パス-asyncllm">非同期パス（主パス）: AsyncLLM</a></h4>
<p>APIサーバー（OpenAI互換API等）が使用する主要パス。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/async_llm.py:71</code> (AsyncLLM)</p>
<pre><code>AsyncLLM.generate(prompt, sampling_params, request_id)    # L537
  │
  ├─ add_request(request_id, prompt, params)               # L286
  │   ├─ input_processor.process_inputs(prompt, params)    # L364
  │   │   → EngineCoreRequest を生成
  │   ├─ input_processor.assign_request_id(request)        # L378
  │   │   → 内部IDを付与（外部ID + 8文字ランダムサフィックス）
  │   ├─ output_processor.add_request(request, ...)        # L423
  │   │   → フロントエンド側でリクエストを登録
  │   └─ engine_core.add_request_async(request)            # L426
  │       → ZMQ経由でバックエンドへ送信
  │
  └─ while not finished:                                    # L586
      out = q.get_nowait() or await q.get()                # L589
      yield out                                             # L596
</code></pre>
<p><code>generate()</code>はAsyncGeneratorで、バックグラウンドの<code>output_handler</code>タスクがEngineCoreからの出力を<code>RequestOutputCollector</code>キューにpushし、<code>generate()</code>がそれをyieldする。</p>
<p><strong>output_handler（バックグラウンドタスク）</strong>:</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/async_llm.py:647</code> (_run_output_handler)</p>
<pre><code>output_handler():                                           # L662
  while True:
    outputs = await engine_core.get_output_async()          # L666
    for chunk in outputs.outputs:                           # L677
      output_processor.process_outputs(chunk, ...)          # L681
      → RequestOutputをキューにpush
    if reqs_to_abort:
      await engine_core.abort_requests_async(...)           # L693
</code></pre>
<h4 id="同期パス-llm"><a class="header" href="#同期パス-llm">同期パス: LLM</a></h4>
<p>オフライン推論（バッチ処理）で使用される。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/entrypoints/llm.py:396</code> (generate)</p>
<pre><code>LLM.generate(prompts, sampling_params)                      # L396
  → _run_completion(prompts, params)                        # L449
    → _add_request(prompt, params) × N                      # L1850
    │  ├─ input_processor.process_inputs(prompt, params)    # L1879
    │  └─ llm_engine.add_request(request_id, request, ...)  # L1889
    → _run_engine()                                         # L1900
       while has_unfinished_requests():                     # L1918
         step_outputs = llm_engine.step()                   # L1919
</code></pre>
<p>同期パスとの主な違い:</p>
<ul>
<li><code>LLM</code>は<code>_run_engine()</code>でポーリングループを回す（AsyncGeneratorではない）</li>
<li><code>llm_engine</code>（=<code>AsyncLLM</code>のラッパー）の<code>step()</code>を直接呼ぶ</li>
<li>プログレスバー（tqdm）でバッチ処理の進捗を表示</li>
</ul>
<h3 id="入力処理-inputprocessor"><a class="header" href="#入力処理-inputprocessor">入力処理 (InputProcessor)</a></h3>
<p><code>InputProcessor</code>はユーザー入力（テキストプロンプト、パラメータ）を<code>EngineCoreRequest</code>に変換する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/input_processor.py:56</code> (InputProcessor)</p>
<pre><code>InputProcessor.process_inputs(request_id, prompt, params)   # L521
  ├─ _validate_lora(lora_request)                           # L535
  ├─ _validate_params(params)                               # L536
  ├─ data_parallel_rank の範囲チェック                       # L542
  ├─ arrival_time 設定（未指定なら time.time()）             # L548
  │
  ├─ input_preprocessor.preprocess(prompt, ...)             # L581
  │   → テキストをトークナイズ（tokenizer.encode()）
  │   → ProcessorInputs を返す
  │
  ├─ split_enc_dec_inputs(processed_inputs)                 # L597
  │   → エンコーダ/デコーダ入力を分離
  │
  ├─ SamplingParams の正規化                                # L608-623
  │   ├─ params.clone()                                     # L612
  │   ├─ max_tokens 未設定時: max_model_len - seq_len       # L614-618
  │   ├─ update_from_generation_config()                    # L619
  │   └─ update_from_tokenizer()                            # L623
  │
  └─ EngineCoreRequest を構築して返す                        # L656-671
</code></pre>
<p>テキスト推論の場合、マルチモーダル関連処理（L630-654）はスキップされる（<code>mm_features</code>はNone）。</p>
<h3 id="プロセス間通信-enginecoreclient--zmq-ipc"><a class="header" href="#プロセス間通信-enginecoreclient--zmq-ipc">プロセス間通信 (EngineCoreClient / ZMQ IPC)</a></h3>
<p><code>EngineCoreClient</code>はフロントエンドプロセスとバックエンドプロセス（EngineCore）間のZMQ IPC通信を担当する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core_client.py:63</code> (EngineCoreClient)</p>
<h4 id="クライアント階層"><a class="header" href="#クライアント階層">クライアント階層</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>クラス</th><th>用途</th><th>トランスポート</th></tr>
</thead>
<tbody>
<tr><td><code>EngineCoreClient</code> (ABC)</td><td>抽象インターフェース</td><td>—</td></tr>
<tr><td><code>InprocClient</code></td><td>インプロセス（デバッグ用）</td><td>直接呼び出し</td></tr>
<tr><td><code>SyncMPClient</code></td><td>同期マルチプロセス（LLM用）</td><td>ZMQ同期</td></tr>
<tr><td><code>AsyncMPClient</code></td><td>非同期マルチプロセス（AsyncLLM用）</td><td>ZMQ非同期</td></tr>
<tr><td><code>DPAsyncMPClient</code></td><td>データ並列（外部LB）</td><td>複数ZMQ</td></tr>
<tr><td><code>DPLBAsyncMPClient</code></td><td>データ並列（内部LB）</td><td>複数ZMQ</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core_client.py:442</code> (MPClient)</p>
<h4 id="zmqソケット構成"><a class="header" href="#zmqソケット構成">ZMQソケット構成</a></h4>
<pre><code>フロントエンド                 バックエンド
┌──────────────┐              ┌──────────────┐
│ AsyncMPClient│              │ EngineCore   │
│              │              │              │
│ input_socket ├─── ROUTER ──→│ (受信)       │
│ (zmq.ROUTER) │    msgpack   │              │
│              │              │              │
│ output_socket│←── PULL ─────┤ (送信)       │
│ (zmq.PULL)   │    msgpack   │              │
└──────────────┘              └──────────────┘
</code></pre>
<ul>
<li><strong>シリアライゼーション</strong>: <code>MsgpackEncoder</code> / <code>MsgpackDecoder</code>（<code>msgspec</code>ライブラリ）
<ul>
<li><code>EngineCoreRequest</code>のシリアライズ → 入力ソケット経由で送信</li>
<li><code>EngineCoreOutputs</code>のデシリアライズ ← 出力ソケット経由で受信</li>
</ul>
</li>
<li><strong>非同期出力受信</strong>: <code>process_outputs_socket()</code>タスクがZMQソケットをポーリングし、受信したOutputsを<code>asyncio.Queue</code>にpush</li>
</ul>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core_client.py:822</code> (AsyncMPClient)</p>
<h4 id="enginecore側のリクエスト受信"><a class="header" href="#enginecore側のリクエスト受信">EngineCore側のリクエスト受信</a></h4>
<p><code>EngineCore.add_request()</code>はリクエストをバリデーションしてSchedulerに登録する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:288</code> (add_request)</p>
<pre><code>EngineCore.add_request(request)                             # L288
  ├─ request_id の型チェック                                 # L295
  ├─ pooling_params のバリデーション                          # L300
  ├─ kv_transfer_params の互換性チェック                      # L311
  └─ scheduler.add_request(request)                          # L319
</code></pre>
<h4 id="enginecorestep-コアループ概要"><a class="header" href="#enginecorestep-コアループ概要">EngineCore.step() （コアループ概要）</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:389</code> (step)</p>
<pre><code>EngineCore.step()                                            # L389
  ├─ scheduler.schedule()        → SchedulerOutput           # L404
  ├─ executor.execute_model()    → Future[ModelRunnerOutput]  # L405
  ├─ grammar_output 取得                                      # L406
  ├─ future.result()             → ModelRunnerOutput          # L411
  ├─ sample_tokens()（非同期スケジューリング時）              # L413
  └─ scheduler.update_from_output() → EngineCoreOutputs      # L418
</code></pre>
<h2 id="コアループ-enginecorestep"><a class="header" href="#コアループ-enginecorestep">コアループ: EngineCore.step()</a></h2>
<p>EngineCoreの<code>step()</code>メソッドは、各ステップで <strong>schedule → execute → update</strong> のサイクルを実行し、待機中のリクエストから生成トークンを生産する。</p>
<h3 id="step-実行フロー"><a class="header" href="#step-実行フロー">step() 実行フロー</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:389</code> (step)</p>
<pre><code>EngineCore.step()                                            # L389
  │
  ├─ if _scheduler_paused: return {}, False                  # L397
  ├─ if not scheduler.has_requests(): return {}, False       # L402
  │
  ├─ 1. scheduler_output = scheduler.schedule()              # L404
  │      → SchedulerOutput
  │      （RUNNINGリクエストの予算割当 → WAITINGリクエストの受け入れ
  │        → KVキャッシュブロック確保 → SchedulerOutput構築）
  │
  ├─ 2. future = executor.execute_model(                     # L405
  │         scheduler_output, non_block=True)
  │      → Future[ModelRunnerOutput | None]
  │      （非ブロッキング。ワーカープロセスで並行実行）
  │
  ├─ 3. grammar_output = scheduler.get_grammar_bitmask(      # L406
  │         scheduler_output)
  │      （構造化出力有効時のみ使用）
  │
  ├─ 4. model_output = future.result()                       # L411
  │      → ModelRunnerOutput（ブロッキング待機）
  │
  ├─ 5. if model_output is None:                             # L413
  │        model_output = executor.sample_tokens(grammar_output)
  │      （非同期スケジューリング時: execute_modelとsamplingが分離）
  │
  ├─ 6. _process_aborts_queue()                              # L417
  │
  └─ 7. engine_core_outputs = scheduler.update_from_output(  # L418
  │         scheduler_output, model_output)
  │      → dict[int, EngineCoreOutputs]
  │      （生成トークンの追加、完了判定、出力構築）
  │
  └─ return (engine_core_outputs,                            # L422
             total_num_scheduled_tokens &gt; 0)
</code></pre>
<h3 id="scheduler-と-kvcachemanager-の相互作用"><a class="header" href="#scheduler-と-kvcachemanager-の相互作用">Scheduler と KVCacheManager の相互作用</a></h3>
<pre class="mermaid">sequenceDiagram
    participant EC as EngineCore
    participant S as Scheduler
    participant KV as KVCacheManager
    participant Ex as Executor

    EC-&gt;&gt;S: schedule()

    Note over S: Phase 1: RUNNINGリクエスト処理
    loop 各RUNNINGリクエスト
        S-&gt;&gt;KV: allocate_slots(request, num_new_tokens)
        KV--&gt;&gt;S: KVCacheBlocks or None
        alt 割り当て失敗 (None)
            S-&gt;&gt;KV: free(低優先度request)
            Note over S: プリエンプション → 再試行
        end
    end

    Note over S: Phase 2: WAITINGリクエスト受け入れ
    loop 各WAITINGリクエスト
        S-&gt;&gt;KV: get_computed_blocks(request)
        KV--&gt;&gt;S: (cached_blocks, num_hits)
        S-&gt;&gt;KV: allocate_slots(request, num_new_tokens, ...)
        KV--&gt;&gt;S: KVCacheBlocks or None
        alt 割り当て失敗 (None)
            Note over S: break（ループ終了）
        end
    end

    Note over S: Phase 3: SchedulerOutput構築
    S--&gt;&gt;EC: SchedulerOutput

    EC-&gt;&gt;Ex: execute_model(scheduler_output)
    Ex--&gt;&gt;EC: Future[ModelRunnerOutput]
    EC-&gt;&gt;EC: future.result()（待機）

    EC-&gt;&gt;S: update_from_output(scheduler_output, model_output)
    Note over S: トークン追加、完了判定
    S--&gt;&gt;EC: dict[int, EngineCoreOutputs]
</pre>

<h3 id="schedulerschedule-の3フェーズ"><a class="header" href="#schedulerschedule-の3フェーズ">Scheduler.schedule() の3フェーズ</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:321</code> (schedule)</p>
<p><code>schedule()</code> は Unified Compute Model を採用し、Prefill/Decodeを区別せず <code>num_computed_tokens</code> の進捗で統一的にトークンを割り当てる。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フェーズ</th><th>行</th><th>対象</th><th>処理</th></tr>
</thead>
<tbody>
<tr><td>Phase 1</td><td>L350-517</td><td>RUNNINGリクエスト</td><td>トークン予算割当。ブロック不足時はプリエンプション</td></tr>
<tr><td>Phase 2</td><td>L532-800</td><td>WAITINGリクエスト</td><td>新規受け入れ。プレフィックスキャッシュ検索 + ブロック割当</td></tr>
<tr><td>Phase 3</td><td>L827-896</td><td>出力構築</td><td>NewRequestData + CachedRequestData → SchedulerOutput</td></tr>
</tbody>
</table>
</div>
<p><strong>トークン予算</strong>: <code>token_budget = max_num_scheduled_tokens</code>（ステップあたり上限）で、各リクエストのスケジュール時に消費される。</p>
<p>詳細は <a href="#scheduler-サマリー">Scheduler サマリー</a> を参照。</p>
<h3 id="kvcachemanager-のブロック割り当て"><a class="header" href="#kvcachemanager-のブロック割り当て">KVCacheManager のブロック割り当て</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_manager.py:206</code> (allocate_slots)</p>
<p><code>allocate_slots()</code> は以下のブロック配置に基づいてGPUメモリブロックを確保する:</p>
<pre><code>|  comp  | new_comp | ext_comp |   new   | lookahead |
|&lt;------ 既計算トークン ------&gt;|&lt;-- 新規計算対象 --&gt;|
                               |&lt;- 割り当て対象 -&gt;|
</code></pre>
<ul>
<li>成功時: <code>KVCacheBlocks</code>（割り当てたブロック情報）を返す</li>
<li>失敗時: <code>None</code> を返す → Schedulerがプリエンプション（RUNNING）またはスキップ（WAITING）</li>
</ul>
<p>プレフィックスキャッシュ検索は <code>get_computed_blocks()</code> で行い、過去に計算済みのブロックを再利用する。</p>
<p>詳細は <a href="#kvcachemanager-サマリー">KVCacheManager サマリー</a> を参照。</p>
<h3 id="update_from_output--enginecoreoutputs"><a class="header" href="#update_from_output--enginecoreoutputs">update_from_output() → EngineCoreOutputs</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:1241</code> (update_from_output)</p>
<p><code>ModelRunnerOutput</code>を受けてSchedulerの状態を更新し、クライアントに返す<code>EngineCoreOutputs</code>を構築する。</p>
<pre><code>update_from_output(scheduler_output, model_runner_output)
  for each scheduled request:
    ├─ Speculative Decodingリジェクション処理
    │   → 不採用分の num_computed_tokens 巻き戻し
    ├─ 生成トークンをリクエストに追加
    ├─ 完了判定（EOS、max_tokens、stop_token）
    │   → 完了時: kv_cache_manager.free(request) でブロック解放
    └─ EngineCoreOutput 構築（request_id, new_token_ids, finish_reason, ...）
  → dict[int, EngineCoreOutputs]（クライアントインデックス別）
</code></pre>
<h2 id="下流パス-実行--ユーザー応答"><a class="header" href="#下流パス-実行--ユーザー応答">下流パス: 実行 → ユーザー応答</a></h2>
<h3 id="実行層-executor--worker--gpumodelrunner"><a class="header" href="#実行層-executor--worker--gpumodelrunner">実行層: Executor → Worker → GPUModelRunner</a></h3>
<p>EngineCore.step()は<code>executor.execute_model()</code>を<strong>非ブロッキング</strong>で呼び出し、GPUでの推論実行を開始する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py:202</code> (execute_model)</p>
<h4 id="collective_rpc-パターン"><a class="header" href="#collective_rpc-パターン">collective_rpc パターン</a></h4>
<p>Executorは<code>collective_rpc()</code>パターンで全Workerに同一メソッドを実行させ、出力ランクのWorkerの結果のみを返す。</p>
<pre><code>EngineCore.step()
  │
  ├─ executor.execute_model(scheduler_output, non_block=True)
  │   └─ collective_rpc("execute_model", args=(scheduler_output,))
  │       └─ Worker.execute_model(scheduler_output)                # L604
  │           └─ model_runner.execute_model(scheduler_output)      # L652
  │               → ExecuteModelState を内部保存、None を返す
  │
  ├─ grammar_output = scheduler.get_grammar_bitmask(...)           # 並行処理
  │
  ├─ future.result()  → None                                       # 待機
  │
  └─ executor.sample_tokens(grammar_output)                         # L222
      └─ collective_rpc("sample_tokens", args=(grammar_output,))
          └─ Worker.sample_tokens(grammar_output)                  # L598
              └─ model_runner.sample_tokens(grammar_output)        # L3621
                  → ModelRunnerOutput を返す
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_worker.py:604</code> (Worker.execute_model)</p>
<h4 id="gpumodelrunner-の2フェーズ実行"><a class="header" href="#gpumodelrunner-の2フェーズ実行">GPUModelRunner の2フェーズ実行</a></h4>
<p>GPUModelRunnerは <code>execute_model()</code> と <code>sample_tokens()</code> を分離する<strong>2フェーズ実行パターン</strong>を採用する。これにより、モデルフォワード中にgrammar bitmask計算を並行実行できる。</p>
<p><strong>Phase 1: execute_model()</strong> (<code>target/vllm/vllm/v1/worker/gpu_model_runner.py:3312</code>)</p>
<pre><code>execute_model(scheduler_output)
  ├─ _update_states(scheduler_output)          # バッチ状態更新
  ├─ _prepare_inputs(scheduler_output)         # 入力ID・位置計算
  ├─ _build_attention_metadata(...)            # Attention メタデータ構築
  ├─ _model_forward(...)                       # model.forward() 実行
  │   → hidden_states
  ├─ compute_logits(hidden_states)             # logits 計算
  │   → logits
  └─ ExecuteModelState に保存 → None を返す
</code></pre>
<p><strong>Phase 2: sample_tokens()</strong> (<code>target/vllm/vllm/v1/worker/gpu_model_runner.py:3621</code>)</p>
<pre><code>sample_tokens(grammar_output)
  ├─ ExecuteModelState を復元
  ├─ grammar bitmask 適用（構造化出力時）
  ├─ _sample(logits) → SamplerOutput
  ├─ バッチ状態更新（生成トークン反映）
  └─ ModelRunnerOutput を構築して返す
</code></pre>
<p><strong>ExecuteModelState</strong> (<code>target/vllm/vllm/v1/worker/gpu_model_runner.py:313</code>) はGPUテンソル（logits, hidden_states等）を保持するNamedTupleで、2フェーズ間の一時状態転送に使用される。</p>
<h3 id="出力処理-enginecoreoutput--requestoutput"><a class="header" href="#出力処理-enginecoreoutput--requestoutput">出力処理: EngineCoreOutput → RequestOutput</a></h3>
<p>ModelRunnerOutputはバックエンドプロセス（EngineCore）で<code>EngineCoreOutput</code>に変換され、ZMQ経由でフロントエンドプロセスの<code>OutputProcessor</code>に送られてデトークナイズされる。</p>
<pre class="mermaid">sequenceDiagram
    participant MR as GPUModelRunner
    participant S as Scheduler
    participant EC as EngineCore
    participant ZMQ as ZMQ IPC
    participant OP as OutputProcessor
    participant Client as API Client

    MR-&gt;&gt;EC: ModelRunnerOutput
    EC-&gt;&gt;S: update_from_output()
    Note over S: トークン追加、完了判定&lt;br&gt;KVキャッシュ解放
    S-&gt;&gt;EC: dict[int, EngineCoreOutputs]

    EC-&gt;&gt;ZMQ: msgpack シリアライズ
    ZMQ-&gt;&gt;OP: EngineCoreOutputs

    Note over OP: デトークナイズ&lt;br&gt;停止文字列判定&lt;br&gt;logprobs処理
    OP-&gt;&gt;Client: RequestOutput (yield)
</pre>

<h4 id="outputprocessorprocess_outputs"><a class="header" href="#outputprocessorprocess_outputs">OutputProcessor.process_outputs()</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/output_processor.py:582</code> (process_outputs)</p>
<p>OutputProcessorは<strong>フロントエンドプロセス</strong>で動作し、<code>EngineCoreOutput</code>をユーザー向け<code>RequestOutput</code>に変換する。</p>
<pre><code>OutputProcessor.process_outputs(engine_core_outputs)       # L582
  for each engine_core_output:
    ├─ req_state = request_states[req_id]                  # RequestState取得
    │
    ├─ detokenizer.update(new_token_ids, stop_terminated)  # L637
    │   ├─ トークン→テキスト変換（インクリメンタル）
    │   └─ 停止文字列チェック → stop_string or None
    │
    ├─ logprobs_processor.update_from_output(output)       # L646
    │
    ├─ req_state.make_request_output(...)                   # L649
    │   ├─ _new_completion_output(token_ids, finish_reason, ...)
    │   │   ├─ detokenizer.get_next_output_text(finished, delta)
    │   │   └─ CompletionOutput(text, token_ids, logprobs, ...)
    │   └─ RequestOutput(request_id, outputs, finished, ...)
    │
    └─ req_state.queue.put(request_output)                 # L661
        → AsyncLLM.generate() が yield
</code></pre>
<h4 id="detokenizerインクリメンタルデトークナイズ"><a class="header" href="#detokenizerインクリメンタルデトークナイズ">Detokenizer（インクリメンタルデトークナイズ）</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/detokenizer.py:30</code> (IncrementalDetokenizer)</p>
<p>トークンからテキストへの変換はインクリメンタルに行われ、ストリーミング出力を実現する。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>クラス</th><th>条件</th><th>方式</th></tr>
</thead>
<tbody>
<tr><td><code>FastIncrementalDetokenizer</code></td><td><code>PreTrainedTokenizerFast</code> 使用時</td><td>HF tokenizersの<code>DecodeStream</code>で高速変換</td></tr>
<tr><td><code>SlowIncrementalDetokenizer</code></td><td>その他のトークナイザ</td><td><code>detokenize_incrementally()</code>でPython変換</td></tr>
<tr><td><code>IncrementalDetokenizer</code></td><td>トークナイザなし</td><td>No-op（テキスト出力なし）</td></tr>
</tbody>
</table>
</div>
<p><code>update()</code>メソッドで各トークンをインクリメンタルにデコードし、同時に<code>check_stop_strings()</code>で停止文字列を検出する（<code>target/vllm/vllm/v1/engine/detokenizer.py:316</code>）。</p>
<h2 id="prefill-vs-decode"><a class="header" href="#prefill-vs-decode">Prefill vs Decode</a></h2>
<p>vLLM v1は<strong>Unified Compute Model</strong>を採用し、PrefillとDecodeを明示的に区別しない。両者は<code>num_computed_tokens</code>の進捗によって暗黙的に区分される。</p>
<h3 id="統一管理の仕組み"><a class="header" href="#統一管理の仕組み">統一管理の仕組み</a></h3>
<p>各リクエストは<code>num_computed_tokens</code>フィールドで計算済みトークン数を追跡する:</p>
<pre><code>プロンプト: [A, B, C, D, E]    (len=5)
num_computed_tokens: 0 → 5 → 6 → 7 → ...

Prefillフェーズ: num_computed_tokens &lt; len(prompt_token_ids)
  → 複数トークンを一度に計算（チャンクプリフィル可能）

Decodeフェーズ: num_computed_tokens &gt;= len(prompt_token_ids)
  → 1トークンずつ生成
</code></pre>
<h3 id="schedulerでの扱い"><a class="header" href="#schedulerでの扱い">Schedulerでの扱い</a></h3>
<p>Scheduler.schedule()はPrefill/Decodeを区別せず、トークン予算の範囲内で各リクエストに計算トークン数を割り当てる:</p>
<ul>
<li><strong>新規リクエスト（WAITING→RUNNING）</strong>: <code>num_tokens = len(prompt_token_ids) - num_computed_tokens</code>（プレフィックスキャッシュヒット分を差し引き）</li>
<li><strong>継続リクエスト（RUNNING）</strong>: <code>num_tokens = 1</code>（Decode 1トークン）</li>
<li>予算不足時は部分的なPrefill（チャンクプリフィル）も可能</li>
</ul>
<h3 id="gpumodelrunner内での違い"><a class="header" href="#gpumodelrunner内での違い">GPUModelRunner内での違い</a></h3>
<p>GPUModelRunner.execute_model()は入力準備の段階で暗黙的にPrefill/Decodeを処理する:</p>
<ul>
<li><strong>_prepare_inputs()</strong>: <code>num_scheduled_tokens</code>に基づいて入力トークンと位置を計算。Prefillなら複数トークン、Decodeなら1トークン</li>
<li><strong>_build_attention_metadata()</strong>: Prefillはフルattention、Decodeはキャッシュ済みKVに対するattentionのメタデータを構築</li>
<li><strong>モデルフォワード</strong>: 入力テンソルのサイズが異なるだけで、同一のforward()を実行</li>
</ul>
<p>この統一モデルにより、同一バッチ内にPrefillリクエストとDecodeリクエストを混在させるContinuous Batchingが自然に実現される。</p>
<h2 id="コンポーネント優先度確定"><a class="header" href="#コンポーネント優先度確定">コンポーネント優先度（確定）</a></h2>
<p>Phase 2での深堀り順序。ユーザー関心領域とフロー上の重要度に基づく。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>優先度</th><th>コンポーネント</th><th>理由</th><th>現在の深度</th></tr>
</thead>
<tbody>
<tr><td><strong>S</strong></td><td>KVCacheManager</td><td>ユーザー関心1位（メモリ管理/KVキャッシュ）。PagedAttention、ブロック管理、Eviction</td><td>[MEDIUM]</td></tr>
<tr><td><strong>A</strong></td><td>Scheduler</td><td>KVCacheManagerと密連携、推論パイプライン全体を制御。Continuous Batching</td><td>[MEDIUM]</td></tr>
<tr><td><strong>A</strong></td><td>GPUModelRunner</td><td>推論実行の中核。6277行の巨大クラス。将来のプラグイン開発に重要</td><td>[SHALLOW]</td></tr>
<tr><td><strong>B</strong></td><td>EngineCore</td><td>step()サイクル、batch_queueパイプライン。全体の統合ポイント</td><td>[MEDIUM]</td></tr>
<tr><td><strong>B</strong></td><td>OutputProcessor</td><td>デトークナイズ、停止判定。ストリーミング出力の仕組み</td><td>[SHALLOW]</td></tr>
<tr><td><strong>C</strong></td><td>AsyncLLM, InputProcessor</td><td>エントリポイント。薄いレイヤー</td><td>[SHALLOW]</td></tr>
<tr><td><strong>C</strong></td><td>Executor, Worker</td><td>委譲パターン。分散推論時のみ詳細が必要</td><td>[SHALLOW]</td></tr>
<tr><td><strong>C</strong></td><td>EngineCoreClient</td><td>ZMQ IPC通信層。プロトコルは把握済み</td><td>[SHALLOW]</td></tr>
</tbody>
</table>
</div>
<h2 id="参照ファイル一覧"><a class="header" href="#参照ファイル一覧">参照ファイル一覧</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th><th>役割</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/entrypoints/llm.py</code></td><td><code>LLM.generate()</code> (L396), <code>_add_request()</code> (L1850)</td><td>同期エントリポイント</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/async_llm.py</code></td><td><code>AsyncLLM.generate()</code> (L537), <code>add_request()</code> (L286)</td><td>非同期エントリポイント</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/input_processor.py</code></td><td><code>InputProcessor.process_inputs()</code> (L521)</td><td>入力処理</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/__init__.py</code></td><td><code>EngineCoreRequest</code> (L55), <code>EngineCoreOutput</code> (L130), <code>EngineCoreOutputs</code> (L176)</td><td>境界データ構造</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core_client.py</code></td><td><code>EngineCoreClient</code> (L63), <code>MPClient</code> (L442), <code>AsyncMPClient</code> (L822)</td><td>ZMQ IPC通信</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core.py</code></td><td><code>EngineCore.add_request()</code> (L288), <code>step()</code> (L389)</td><td>推論ループ本体</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/sched/scheduler.py</code></td><td><code>Scheduler.schedule()</code> (L321), <code>update_from_output()</code> (L1241)</td><td>スケジューリング</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/sched/output.py</code></td><td><code>SchedulerOutput</code> (L184), <code>NewRequestData</code> (L34), <code>CachedRequestData</code> (L114)</td><td>スケジュール出力データ構造</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/kv_cache_manager.py</code></td><td><code>KVCacheManager.allocate_slots()</code> (L206), <code>get_computed_blocks()</code> (L164)</td><td>KVキャッシュ管理</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/block_pool.py</code></td><td><code>BlockPool</code> (L128)</td><td>物理ブロック管理</td></tr>
<tr><td><code>target/vllm/vllm/v1/request.py</code></td><td><code>Request</code></td><td>リクエスト内部状態</td></tr>
<tr><td><code>target/vllm/vllm/v1/outputs.py</code></td><td><code>ModelRunnerOutput</code> (L160)</td><td>モデル推論出力</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/abstract.py</code></td><td><code>Executor</code> (ABC), <code>execute_model()</code> (L202), <code>collective_rpc()</code> (L180)</td><td>実行層抽象</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/uniproc_executor.py</code></td><td><code>UniProcExecutor</code> (L26)</td><td>単一プロセス実行</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/multiproc_executor.py</code></td><td><code>MultiprocExecutor</code> (L93)</td><td>マルチプロセス実行</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_worker.py</code></td><td><code>Worker.execute_model()</code> (L604), <code>sample_tokens()</code> (L598)</td><td>GPU Worker</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_model_runner.py</code></td><td><code>GPUModelRunner.execute_model()</code> (L3312), <code>sample_tokens()</code> (L3621), <code>ExecuteModelState</code> (L313)</td><td>モデル実行</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/output_processor.py</code></td><td><code>OutputProcessor.process_outputs()</code> (L582), <code>RequestState.make_request_output()</code> (L269)</td><td>出力処理</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/detokenizer.py</code></td><td><code>IncrementalDetokenizer</code> (L30), <code>FastIncrementalDetokenizer</code> (L169), <code>check_stop_strings()</code> (L316)</td><td>デトークナイズ</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/logprobs.py</code></td><td><code>LogprobsProcessor</code> (L28)</td><td>logprobs処理</td></tr>
<tr><td><code>target/vllm/vllm/outputs.py</code></td><td><code>RequestOutput</code> (L86), <code>CompletionOutput</code> (L23)</td><td>最終出力データ構造</td></tr>
</tbody>
</table>
</div>
<h2 id="マルチモーダル推論パスの差分"><a class="header" href="#マルチモーダル推論パスの差分">マルチモーダル推論パスの差分</a></h2>
<p>テキスト推論フローに対し、画像等のマルチモーダル入力がある場合の主要な差分を以下に示す。詳細は <a href="#マルチモーダル処理パイプライン-サマリー">マルチモーダル処理パイプライン</a> を参照。</p>
<h3 id="フロントエンドp0の差分"><a class="header" href="#フロントエンドp0の差分">フロントエンド（P0）の差分</a></h3>
<ol>
<li><strong>チャットテンプレート</strong>: プレースホルダー（<code>&lt;start_of_image&gt;</code> 等）がプロンプトに挿入される</li>
<li><strong>HF Processor実行</strong>: 画像を <code>pixel_values</code> テンソルに変換（リサイズ、正規化、パッチ分割）</li>
<li><strong>MMハッシュ計算</strong>: <code>MultiModalHasher</code> でコンテンツベースのblake3ハッシュを生成</li>
<li><strong>ProcessorCache</strong>: HF処理結果をキャッシュ（4種類の実装: processor_only/lru/shm/none）</li>
<li><strong>EngineCoreRequest</strong>: <code>mm_features: list[MultiModalFeatureSpec]</code> にテンソルデータ・位置情報・ハッシュを格納</li>
</ol>
<h3 id="バックエンドp1の差分"><a class="header" href="#バックエンドp1の差分">バックエンド（P1）の差分</a></h3>
<ol>
<li><strong>EncoderCacheManager</strong>: エンコーダ出力をリファレンスカウント方式で管理。キャッシュヒットでエンコーダ計算スキップ</li>
<li><strong>Scheduler</strong>: <code>encoder_compute_budget</code> でステップあたりのエンコーダ計算量を制御</li>
<li><strong>GPUModelRunner</strong>:
<ul>
<li><code>_execute_mm_encoder()</code>: ビジョンエンコーダ実行（<code>model.embed_multimodal()</code>）</li>
<li><code>_gather_mm_embeddings()</code>: キャッシュからプレースホルダー位置に対応する埋め込みを取得</li>
<li><code>embed_input_ids()</code>: <code>masked_scatter_</code> でテキスト埋め込みとビジョン埋め込みをマージ</li>
</ul>
</li>
<li><strong>モデルforward</strong>: <code>input_ids</code> ではなく <code>inputs_embeds</code>（マージ済み）が渡される</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="アーキテクチャ概要"><a class="header" href="#アーキテクチャ概要">アーキテクチャ概要</a></h1>
<blockquote>
<p><strong>深度</strong>: [SHALLOW]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-09</p>
</blockquote>
<h2 id="概要-1"><a class="header" href="#概要-1">概要</a></h2>
<p>vLLMはUC Berkeley Sky Computing Lab発のLLM推論・サービングライブラリである。PagedAttentionによるKVキャッシュの効率的メモリ管理、Continuous Batchingによる動的バッチスケジューリングを中核技術とし、高スループット・低レイテンシのLLM推論を実現する。OpenAI互換APIサーバー、マルチモーダル対応、分散推論（Tensor/Pipeline/Data/Expert並列）を備える。</p>
<h2 id="全体構造"><a class="header" href="#全体構造">全体構造</a></h2>
<pre class="mermaid">graph TD
    subgraph エントリポイント層
        CLI["CLI&lt;br&gt;vllm.entrypoints.cli"]
        LLM["LLM&lt;br&gt;vllm.entrypoints.llm:101"]
        OpenAI["OpenAI互換API&lt;br&gt;vllm.entrypoints.openai"]
    end

    subgraph エンジン層
        AsyncLLM["AsyncLLM&lt;br&gt;vllm.v1.engine.async_llm:71"]
        LLMEngine["LLMEngine&lt;br&gt;vllm.v1.engine.llm_engine"]
        EngineCore["EngineCore&lt;br&gt;vllm.v1.engine.core:79"]
        InputProc["InputProcessor"]
        OutputProc["OutputProcessor"]
    end

    subgraph コア層
        Scheduler["Scheduler&lt;br&gt;vllm.v1.core.sched.scheduler:63"]
        KVCacheMgr["KVCacheManager&lt;br&gt;vllm.v1.core.kv_cache_manager:94"]
        BlockPool["BlockPool&lt;br&gt;vllm.v1.core.block_pool"]
    end

    subgraph 実行層
        Executor["Executor&lt;br&gt;vllm.v1.executor"]
        Worker["Worker&lt;br&gt;vllm.v1.worker.gpu_worker:70"]
        ModelRunner["GPUModelRunner&lt;br&gt;vllm.v1.worker.gpu_model_runner:329"]
    end

    subgraph モデル層
        Models["Models&lt;br&gt;vllm.model_executor.models&lt;br&gt;241ファイル"]
        Attention["Attention&lt;br&gt;2層構造"]
        Layers["Layers&lt;br&gt;vllm.model_executor.layers"]
    end

    CLI --&gt; AsyncLLM
    LLM --&gt; LLMEngine
    OpenAI --&gt; AsyncLLM

    AsyncLLM --&gt;|"ZMQ IPC"| EngineCore
    LLMEngine --&gt; EngineCore
    EngineCore --&gt; InputProc
    EngineCore --&gt; OutputProc

    EngineCore --&gt; Scheduler
    EngineCore --&gt; KVCacheMgr
    KVCacheMgr --&gt; BlockPool

    EngineCore --&gt; Executor
    Executor --&gt; Worker
    Worker --&gt; ModelRunner

    ModelRunner --&gt; Models
    ModelRunner --&gt; Attention
    Models --&gt; Layers
</pre>

<h2 id="アーキテクチャの世代"><a class="header" href="#アーキテクチャの世代">アーキテクチャの世代</a></h2>
<p><code>vllm/engine/</code> は <code>vllm/v1/</code> への薄いラッパーである。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/engine/llm_engine.py:4</code> — <code>LLMEngine = V1LLMEngine</code> の1行エイリアス</p>
<p>v1が現行アーキテクチャの本体であり、コードリーディングでは <code>vllm/v1/</code> を中心に読む。ただし <code>vllm/model_executor/</code>、<code>vllm/distributed/</code>、<code>vllm/multimodal/</code> 等はv1からも直接利用されるため調査対象に含む。</p>
<h2 id="主要コンポーネント"><a class="header" href="#主要コンポーネント">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>クラス</th><th>パス</th><th>役割</th></tr>
</thead>
<tbody>
<tr><td>AsyncLLM</td><td><code>AsyncLLM(EngineClient)</code></td><td><code>target/vllm/vllm/v1/engine/async_llm.py:71</code></td><td>非同期APIトップレベル</td></tr>
<tr><td>EngineCore</td><td><code>EngineCore</code></td><td><code>target/vllm/vllm/v1/engine/core.py:79</code></td><td>推論ループ内側。ZMQで外側と通信</td></tr>
<tr><td>Scheduler</td><td><code>Scheduler(SchedulerInterface)</code></td><td><code>target/vllm/vllm/v1/core/sched/scheduler.py:63</code></td><td>Continuous Batchingスケジューラ</td></tr>
<tr><td>KVCacheManager</td><td><code>KVCacheManager</code></td><td><code>target/vllm/vllm/v1/core/kv_cache_manager.py:94</code></td><td>KVキャッシュブロック管理</td></tr>
<tr><td>Executor</td><td><code>Executor</code>(ABC)</td><td><code>target/vllm/vllm/v1/executor/abstract.py</code></td><td>Worker群を束ねる実行層</td></tr>
<tr><td>Worker</td><td><code>Worker(WorkerBase)</code></td><td><code>target/vllm/vllm/v1/worker/gpu_worker.py:70</code></td><td>1 GPUデバイスを担当</td></tr>
<tr><td>GPUModelRunner</td><td><code>GPUModelRunner</code></td><td><code>target/vllm/vllm/v1/worker/gpu_model_runner.py:329</code></td><td>GPU上のフォワードパス実行</td></tr>
<tr><td>VllmConfig</td><td><code>VllmConfig</code></td><td><code>target/vllm/vllm/config/vllm.py</code></td><td>全設定の集約クラス</td></tr>
</tbody>
</table>
</div>
<h2 id="設計原則"><a class="header" href="#設計原則">設計原則</a></h2>
<h3 id="pagedattention"><a class="header" href="#pagedattention">PagedAttention</a></h3>
<p>KVキャッシュをOSの仮想メモリページングに着想を得てブロック単位で管理する。連続したGPUメモリ確保が不要になり、メモリ断片化を大幅に抑制する。</p>
<h3 id="continuous-batching"><a class="header" href="#continuous-batching">Continuous Batching</a></h3>
<p>リクエストの到着・完了に応じてバッチを動的に更新する。固定バッチサイズと異なり、GPU稼働率を最大化できる。</p>
<h3 id="zmq-ipc-によるプロセス分離"><a class="header" href="#zmq-ipc-によるプロセス分離">ZMQ IPC によるプロセス分離</a></h3>
<p>EngineCoreは別プロセス（<code>EngineCoreProc</code>）として動作し、ZeroMQソケットで上位エンジン層と通信する。これによりスケジューリングと推論処理を並行実行できる。</p>
<h3 id="プラグインシステム"><a class="header" href="#プラグインシステム">プラグインシステム</a></h3>
<p><code>vllm/plugins/</code> によるプラグイン機構を備え、起動時に <code>load_general_plugins()</code> で拡張を読み込む。</p>
<h2 id="ccuda-拡張"><a class="header" href="#ccuda-拡張">C++/CUDA 拡張</a></h2>
<p><code>target/vllm/csrc/</code> にパフォーマンスクリティカルなネイティブコードが配置されている。PagedAttentionカーネル、LayerNorm、量子化カーネル、カスタムAllReduce等が含まれる。Pythonからは <code>vllm._custom_ops</code> 等のバインディング経由で呼び出される。</p>
<h2 id="参照ファイル"><a class="header" href="#参照ファイル">参照ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/engine/llm_engine.py</code></td><td><code>LLMEngine</code>（v1への薄いラッパー）</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/async_llm.py</code></td><td><code>AsyncLLM</code></td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core.py</code></td><td><code>EngineCore</code>, <code>EngineCoreProc</code></td></tr>
<tr><td><code>target/vllm/vllm/v1/core/sched/scheduler.py</code></td><td><code>Scheduler</code></td></tr>
<tr><td><code>target/vllm/vllm/v1/core/kv_cache_manager.py</code></td><td><code>KVCacheManager</code></td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/abstract.py</code></td><td><code>Executor</code>(ABC)</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_worker.py</code></td><td><code>Worker</code></td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_model_runner.py</code></td><td><code>GPUModelRunner</code></td></tr>
<tr><td><code>target/vllm/vllm/config/vllm.py</code></td><td><code>VllmConfig</code></td></tr>
<tr><td><code>target/vllm/vllm/entrypoints/llm.py</code></td><td><code>LLM</code></td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ecconnectorencoder-cache-connector"><a class="header" href="#ecconnectorencoder-cache-connector">ECConnector（Encoder Cache Connector）</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM] | <strong>確信度</strong>: [VERIFIED] | <strong>最終更新</strong>: 2026-02-14</p>
</blockquote>
<h2 id="概要-2"><a class="header" href="#概要-2">概要</a></h2>
<p>ECConnectorは、マルチモーダルモデルのエンコーダ出力を<strong>vLLMインスタンス間</strong>または<strong>外部ストレージ</strong>と転送するためのプラグインフレームワークである。KV Transfer（デコーダKVキャッシュ用）とは完全に独立した系統で、エンコーダキャッシュに特化している。</p>
<p>主なユースケースは**Encoder-Prefill-Decode分離（EPD）**で、エンコーダ専用インスタンスが画像処理を行い、その結果をデコーダインスタンスに転送する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/</code> (パッケージ全体)</p>
<h2 id="アーキテクチャ"><a class="header" href="#アーキテクチャ">アーキテクチャ</a></h2>
<h3 id="ファイル構成"><a class="header" href="#ファイル構成">ファイル構成</a></h3>
<pre><code>vllm/distributed/ec_transfer/
├── __init__.py                          # get_ec_transfer(), has_ec_transfer() 公開API
├── ec_transfer_state.py                 # グローバルシングルトン管理
└── ec_connector/
    ├── __init__.py
    ├── base.py                          # ECConnectorBase 抽象基底クラス
    ├── factory.py                       # ECConnectorFactory レジストリ + 動的ロード
    └── example_connector.py             # ECExampleConnector 参照実装（safetensors）

vllm/v1/worker/
└── ec_connector_model_runner_mixin.py   # GPUModelRunner統合Mixin

vllm/config/
└── ec_transfer.py                       # ECTransferConfig 設定クラス
</code></pre>
<h3 id="2ロール分離アーキテクチャ"><a class="header" href="#2ロール分離アーキテクチャ">2ロール分離アーキテクチャ</a></h3>
<p>ECConnectorは<strong>Scheduler側</strong>と<strong>Worker側</strong>に分離され、同じクラスが両方のロールを担う：</p>
<pre class="mermaid">graph TB
    subgraph "Scheduler Process (ECConnectorRole.SCHEDULER)"
        SC[ECConnector Scheduler側]
        SCH[Scheduler]
        SCH --&gt;|has_cache_item| SC
        SCH --&gt;|update_state_after_alloc| SC
        SCH --&gt;|build_connector_meta| SC
    end

    subgraph "Worker Process (ECConnectorRole.WORKER)"
        WC[ECConnector Worker側]
        GMR[GPUModelRunner]
        GMR --&gt;|bind_connector_metadata| WC
        GMR --&gt;|start_load_caches| WC
        GMR --&gt;|save_caches| WC
        GMR --&gt;|get_finished| WC
    end

    SC --&gt;|ECConnectorMetadata&lt;br/&gt;SchedulerOutput経由| WC
</pre>

<div class="table-wrapper">
<table>
<thead>
<tr><th>ロール</th><th>生成場所</th><th>主な責務</th></tr>
</thead>
<tbody>
<tr><td>SCHEDULER</td><td><code>Scheduler.__init__()</code> via <code>ECConnectorFactory</code></td><td>キャッシュ存在チェック、メタデータ構築</td></tr>
<tr><td>WORKER</td><td><code>gpu_worker.py</code> の <code>ensure_ec_transfer_initialized()</code></td><td>キャッシュのロード/セーブ</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:135-138</code> (Scheduler側生成)
<strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_transfer_state.py:26-43</code> (Worker側生成)</p>
<h2 id="ecconnectorbase-抽象基底クラス"><a class="header" href="#ecconnectorbase-抽象基底クラス">ECConnectorBase 抽象基底クラス</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_connector/base.py:59-253</code></p>
<h3 id="プロパティ"><a class="header" href="#プロパティ">プロパティ</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>プロパティ</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>role</code></td><td><code>ECConnectorRole</code></td><td>SCHEDULER or WORKER</td></tr>
<tr><td><code>is_producer</code></td><td><code>bool</code></td><td>エンコーダキャッシュを生成する側か</td></tr>
<tr><td><code>is_consumer</code></td><td><code>bool</code></td><td>エンコーダキャッシュを消費する側か</td></tr>
</tbody>
</table>
</div>
<h3 id="抽象メソッド実装必須"><a class="header" href="#抽象メソッド実装必須">抽象メソッド（実装必須）</a></h3>
<h4 id="worker側3メソッド"><a class="header" href="#worker側3メソッド">Worker側（3メソッド）</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>シグネチャ</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>start_load_caches</code></td><td><code>(encoder_cache: dict[str, Tensor], **kwargs) → None</code></td><td>メタデータに基づきキャッシュをロード</td></tr>
<tr><td><code>save_caches</code></td><td><code>(encoder_cache: dict[str, Tensor], mm_hash: str, **kwargs) → None</code></td><td>エンコーダ出力を外部に保存</td></tr>
</tbody>
</table>
</div>
<h4 id="scheduler側3メソッド"><a class="header" href="#scheduler側3メソッド">Scheduler側（3メソッド）</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>シグネチャ</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>has_cache_item</code></td><td><code>(identifier: str) → bool</code></td><td>外部にキャッシュが存在するか判定</td></tr>
<tr><td><code>update_state_after_alloc</code></td><td><code>(request: Request, index: int) → None</code></td><td>割当後の内部状態更新</td></tr>
<tr><td><code>build_connector_meta</code></td><td><code>(scheduler_output: SchedulerOutput) → ECConnectorMetadata</code></td><td>Worker転送用メタデータ構築</td></tr>
</tbody>
</table>
</div>
<h3 id="具象メソッドオーバーライド任意"><a class="header" href="#具象メソッドオーバーライド任意">具象メソッド（オーバーライド任意）</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>デフォルト動作</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>bind_connector_metadata</code></td><td>メタデータ保持</td><td>Worker側: 毎step実行前に呼ばれる</td></tr>
<tr><td><code>clear_connector_metadata</code></td><td>Noneに設定</td><td>Worker側: 毎step実行後に呼ばれる</td></tr>
<tr><td><code>register_caches</code></td><td>no-op</td><td>将来のP2P機能用</td></tr>
<tr><td><code>get_finished</code></td><td><code>(None, None)</code></td><td>非同期転送完了通知</td></tr>
<tr><td><code>update_connector_output</code></td><td>no-op</td><td>Worker出力からScheduler状態を更新</td></tr>
<tr><td><code>request_finished</code></td><td><code>(False, None)</code></td><td>リクエスト完了時のフック</td></tr>
</tbody>
</table>
</div>
<h2 id="ectransferconfig-設定"><a class="header" href="#ectransferconfig-設定">ECTransferConfig 設定</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/config/ec_transfer.py:16-108</code></p>
<h3 id="ecロール"><a class="header" href="#ecロール">ECロール</a></h3>
<pre><code class="language-python">ECRole = Literal["ec_producer", "ec_consumer", "ec_both"]
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ロール</th><th>説明</th><th><code>is_producer</code></th><th><code>is_consumer</code></th></tr>
</thead>
<tbody>
<tr><td><code>ec_producer</code></td><td>エンコーダ計算+キャッシュ保存</td><td>True</td><td>False</td></tr>
<tr><td><code>ec_consumer</code></td><td>キャッシュ読み込み+デコーダ実行</td><td>False</td><td>True</td></tr>
<tr><td><code>ec_both</code></td><td>両方の機能</td><td>True</td><td>True</td></tr>
</tbody>
</table>
</div>
<h3 id="主要設定パラメータ"><a class="header" href="#主要設定パラメータ">主要設定パラメータ</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>ec_connector</code></td><td>None</td><td>コネクタ名（例: “ECExampleConnector”）</td></tr>
<tr><td><code>ec_role</code></td><td>None</td><td>ECロール</td></tr>
<tr><td><code>ec_connector_module_path</code></td><td>None</td><td>カスタムコネクタのPythonモジュールパス</td></tr>
<tr><td><code>ec_connector_extra_config</code></td><td><code>{}</code></td><td>コネクタ固有の追加設定</td></tr>
<tr><td><code>ec_buffer_device</code></td><td>“cuda”</td><td>バッファデバイス</td></tr>
<tr><td><code>ec_buffer_size</code></td><td>1e9</td><td>バッファサイズ（バイト）</td></tr>
<tr><td><code>ec_ip</code> / <code>ec_port</code></td><td>127.0.0.1:14579</td><td>P2P接続用</td></tr>
<tr><td><code>ec_rank</code> / <code>ec_parallel_size</code></td><td>None / 1</td><td>分散接続設定</td></tr>
</tbody>
</table>
</div>
<h2 id="ecconnectorfactory"><a class="header" href="#ecconnectorfactory">ECConnectorFactory</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_connector/factory.py:20-85</code></p>
<h3 id="コネクタ登録方式"><a class="header" href="#コネクタ登録方式">コネクタ登録方式</a></h3>
<p>2つの登録方法がある：</p>
<ol>
<li><strong>静的登録</strong>: <code>ECConnectorFactory.register_connector()</code> でモジュール遅延ロード登録</li>
<li><strong>動的ロード</strong>: <code>ec_connector_module_path</code> で任意のPythonモジュールからロード</li>
</ol>
<pre><code class="language-python"># 静的登録（factory.py末尾）
ECConnectorFactory.register_connector(
    "ECExampleConnector",
    "vllm.distributed.ec_transfer.ec_connector.example_connector",
    "ECExampleConnector",
)

# 動的ロード（ec_connector_module_pathが設定されている場合）
connector_module = importlib.import_module(connector_module_path)
connector_cls = getattr(connector_module, connector_name)
</code></pre>
<h3 id="現在登録済みコネクタ"><a class="header" href="#現在登録済みコネクタ">現在登録済みコネクタ</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>名前</th><th>実装</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>ECExampleConnector</code></td><td><code>example_connector.py</code></td><td>参照実装（safetensorsディスク保存）</td></tr>
</tbody>
</table>
</div>
<h2 id="ecexampleconnector-参照実装"><a class="header" href="#ecexampleconnector-参照実装">ECExampleConnector 参照実装</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_connector/example_connector.py:45-199</code></p>
<p>safetensorsフォーマットでディスクにエンコーダキャッシュを保存/読み込みする参照実装。</p>
<h3 id="メタデータ"><a class="header" href="#メタデータ">メタデータ</a></h3>
<pre><code class="language-python">@dataclass
class MMMeta:
    mm_hash: str      # マルチモーダルデータのハッシュ
    num_token: int     # エンコーダトークン数

@dataclass
class ECExampleConnectorMetadata(ECConnectorMetadata):
    mm_datas: list[MMMeta]  # ロードすべきエントリ一覧
</code></pre>
<h3 id="ストレージ構造"><a class="header" href="#ストレージ構造">ストレージ構造</a></h3>
<pre><code>{shared_storage_path}/
└── {mm_hash}/
    └── encoder_cache.safetensors    # {"ec_cache": Tensor} 形式
</code></pre>
<h3 id="動作フロー"><a class="header" href="#動作フロー">動作フロー</a></h3>
<h4 id="保存producer側"><a class="header" href="#保存producer側">保存（Producer側）</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_connector/example_connector.py:98-118</code></p>
<ol>
<li>GPUModelRunnerが <code>_execute_mm_encoder()</code> 完了後に <code>maybe_save_ec_to_connector()</code> を呼ぶ</li>
<li><code>save_caches()</code>: テンソルを <code>.detach().cpu()</code> してsafetensorsで保存</li>
</ol>
<h4 id="読み込みconsumer側"><a class="header" href="#読み込みconsumer側">読み込み（Consumer側）</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_connector/example_connector.py:63-96</code></p>
<ol>
<li>Scheduler側: <code>has_cache_item()</code> でファイル存在確認 (<code>os.path.exists</code>)</li>
<li>Scheduler側: <code>build_connector_meta()</code> でロード対象リストを構築</li>
<li>Worker側: <code>start_load_caches()</code> でsafetensorsからGPUにロード</li>
</ol>
<h4 id="存在確認"><a class="header" href="#存在確認">存在確認</a></h4>
<p><code>has_cache_item()</code> は <code>os.path.exists()</code> でsafetensorsファイルの存在をチェック。</p>
<h2 id="schedulerとの統合"><a class="header" href="#schedulerとの統合">Schedulerとの統合</a></h2>
<h3 id="_schedule_encoder_inputs-内の分岐"><a class="header" href="#_schedule_encoder_inputs-内の分岐">_schedule_encoder_inputs() 内の分岐</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:1212-1218</code></p>
<pre><code class="language-python">if self.ec_connector is not None and self.ec_connector.has_cache_item(identifier):
    # 外部キャッシュにヒット → エンコーダ計算不要、compute_budget消費なし
    mm_hashes_to_schedule.add(item_identifier)
    external_load_encoder_input.append(i)
    num_embeds_to_schedule += num_encoder_embeds
    continue
</code></pre>
<p>ECConnectorにキャッシュがある場合：</p>
<ul>
<li><code>encoder_compute_budget</code> は<strong>消費しない</strong>（エンコーダ計算不要のため）</li>
<li><code>external_load_encoder_input</code> リストに追加</li>
<li><code>encoder_cache_manager.allocate()</code> は実行される（GPU側に空きが必要）</li>
</ul>
<h3 id="割当後の状態更新"><a class="header" href="#割当後の状態更新">割当後の状態更新</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:523-527</code></p>
<pre><code class="language-python">if external_load_encoder_input:
    for i in external_load_encoder_input:
        self.encoder_cache_manager.allocate(request, i)
        if self.ec_connector is not None:
            self.ec_connector.update_state_after_alloc(request, i)
</code></pre>
<h3 id="メタデータ構築"><a class="header" href="#メタデータ構築">メタデータ構築</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:899-904</code></p>
<p><code>build_connector_meta()</code> がSchedulerOutputに <code>ec_connector_metadata</code> を設定。Worker側はこのメタデータを使ってロード対象を特定する。</p>
<h2 id="gpumodelrunnerとの統合"><a class="header" href="#gpumodelrunnerとの統合">GPUModelRunnerとの統合</a></h2>
<h3 id="ecconnectormodelrunnermixin"><a class="header" href="#ecconnectormodelrunnermixin">ECConnectorModelRunnerMixin</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/ec_connector_model_runner_mixin.py:25-87</code></p>
<p>GPUModelRunnerに3つのstatic methodを提供：</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>maybe_save_ec_to_connector</code></td><td>エンコーダ出力保存（Producer時）</td></tr>
<tr><td><code>get_finished_ec_transfers</code></td><td>非同期転送完了確認</td></tr>
<tr><td><code>maybe_get_ec_connector_output</code></td><td>コンテキストマネージャでライフサイクル管理</td></tr>
</tbody>
</table>
</div>
<h3 id="コンテキストマネージャのライフサイクル"><a class="header" href="#コンテキストマネージャのライフサイクル">コンテキストマネージャのライフサイクル</a></h3>
<pre><code class="language-python">with self.maybe_get_ec_connector_output(scheduler_output, encoder_cache) as output:
    # 1. bind_connector_metadata() → メタデータ設定
    # 2. Consumer時: start_load_caches() → 外部からロード
    # 3. yield → エンコーダ実行、gather処理
    # 4. get_finished() → 非同期完了確認
    # 5. clear_connector_metadata() → クリーンアップ
</code></pre>
<h3 id="producer専用モード"><a class="header" href="#producer専用モード">Producer専用モード</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:3343-3349</code></p>
<p>Producer専用インスタンスは、エンコーダ実行後にデコーダ実行をスキップし、空のModelRunnerOutputを返す：</p>
<pre><code class="language-python">if has_ec_transfer() and get_ec_transfer().is_producer:
    with self.maybe_get_ec_connector_output(...) as ec_connector_output:
        self._execute_mm_encoder(scheduler_output)
        return make_empty_encoder_model_runner_output(scheduler_output)
</code></pre>
<p>また、Producer専用インスタンスはKVキャッシュを確保しない：</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:6160-6161</code></p>
<pre><code class="language-python">if has_ec_transfer() and get_ec_transfer().is_producer:
    return {}  # KVCacheSpec空 → KVキャッシュ確保なし
</code></pre>
<h2 id="ecconnectoroutput"><a class="header" href="#ecconnectoroutput">ECConnectorOutput</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/outputs.py:151-154</code></p>
<pre><code class="language-python">@dataclass
class ECConnectorOutput:
    finished_sending: set[str] | None = None
    finished_recving: set[str] | None = None
</code></pre>
<p>ModelRunnerOutputに含まれてScheduler側に返されるが、<strong>現時点ではScheduler側で未消費</strong>（<code>ec_connector_output</code>を読み取るコードがSchedulerにない）。非同期転送完了フィードバックは将来実装予定。</p>
<h2 id="グローバルシングルトン管理"><a class="header" href="#グローバルシングルトン管理">グローバルシングルトン管理</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_transfer_state.py:14-43</code></p>
<pre><code class="language-python">_EC_CONNECTOR_AGENT: ECConnectorBase | None = None
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>関数</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>has_ec_transfer()</code></td><td>ECConnectorが初期化済みか</td></tr>
<tr><td><code>get_ec_transfer()</code></td><td>シングルトン取得（未初期化ならassert）</td></tr>
<tr><td><code>ensure_ec_transfer_initialized(config)</code></td><td>Workerロールで初期化（冪等）</td></tr>
</tbody>
</table>
</div>
<p>Worker側のシングルトン初期化は <code>gpu_worker.py</code> で <code>ensure_ec_transfer_initialized()</code> を呼ぶことで行われる。Scheduler側は <code>ECConnectorFactory.create_connector()</code> で直接生成し、<code>self.ec_connector</code> に保持する（シングルトンではない）。</p>
<h2 id="カスタムecconnector実装ガイド"><a class="header" href="#カスタムecconnector実装ガイド">カスタムECConnector実装ガイド</a></h2>
<h3 id="最小実装"><a class="header" href="#最小実装">最小実装</a></h3>
<ol>
<li><code>ECConnectorBase</code> を継承</li>
<li>5つの抽象メソッドを実装</li>
<li>ECTransferConfigの<code>ec_connector</code>にクラス名、<code>ec_connector_module_path</code>にモジュールパスを指定</li>
</ol>
<h3 id="実装の要点"><a class="header" href="#実装の要点">実装の要点</a></h3>
<ul>
<li><code>has_cache_item()</code> はSchedulerのホットパスで呼ばれるため<strong>高速</strong>であるべき</li>
<li><code>start_load_caches()</code> は <code>encoder_cache</code> dict に直接テンソルを追加する</li>
<li><code>save_caches()</code> は <code>encoder_cache[mm_hash]</code> からGPUテンソルを取得して保存する</li>
<li><code>build_connector_meta()</code> は内部状態をリセットすること</li>
</ul>
<h3 id="起動コマンド例"><a class="header" href="#起動コマンド例">起動コマンド例</a></h3>
<pre><code class="language-bash"># Producer（エンコーダ専用インスタンス）
vllm serve model_name \
    --ec-connector ECExampleConnector \
    --ec-role ec_producer \
    --ec-connector-extra-config '{"shared_storage_path": "/shared/cache"}'

# Consumer（デコーダインスタンス）
vllm serve model_name \
    --ec-connector ECExampleConnector \
    --ec-role ec_consumer \
    --ec-connector-extra-config '{"shared_storage_path": "/shared/cache"}'
</code></pre>
<h2 id="上流下流依存関係"><a class="header" href="#上流下流依存関係">上流・下流依存関係</a></h2>
<h3 id="上流"><a class="header" href="#上流">上流</a></h3>
<ul>
<li><strong>ECTransferConfig</strong>: <code>ec_connector</code>, <code>ec_role</code> 等の設定</li>
<li><strong>Scheduler</strong>: キャッシュ存在確認、状態更新、メタデータ構築の呼び出し</li>
<li><strong>GPUModelRunner</strong>: エンコーダ実行結果の保存、ロード済みキャッシュの利用</li>
</ul>
<h3 id="下流"><a class="header" href="#下流">下流</a></h3>
<ul>
<li><strong>外部ストレージ</strong>: safetensors（例）、共有メモリ、ネットワーク等（実装依存）</li>
</ul>
<h2 id="開発状況未実装機能"><a class="header" href="#開発状況未実装機能">開発状況・未実装機能</a></h2>
<ol>
<li><strong>ECConnectorOutput未消費</strong>: Worker→Scheduler方向の非同期転送完了フィードバックが未実装</li>
<li><strong>request_finished未統合</strong>: Schedulerから<code>ec_connector.request_finished()</code>が呼ばれていない</li>
<li><strong>register_caches未実装</strong>: P2P直接転送のためのキャッシュ登録（TODO）</li>
<li><strong>エンコーダキャッシュ事前割り当て未対応</strong>: <code>encoder_cache</code> が <code>dict</code> のため、固定バッファへの移行が必要</li>
<li><strong>登録済みコネクタが1つのみ</strong>: ECExampleConnector（デバッグ用）のみ。SHMConnector等は外部PR待ち</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="encodercacheエンコーダキャッシュ"><a class="header" href="#encodercacheエンコーダキャッシュ">EncoderCache（エンコーダキャッシュ）</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM] | <strong>確信度</strong>: [VERIFIED] | <strong>最終更新</strong>: 2026-02-14</p>
</blockquote>
<h2 id="概要-3"><a class="header" href="#概要-3">概要</a></h2>
<p>EncoderCacheは、マルチモーダルモデルにおけるエンコーダ出力（例: ビジョンエンコーダの画像埋め込み）のGPUメモリ上キャッシュを管理するコンポーネントである。2層構造で、Scheduler側の論理管理（<code>EncoderCacheManager</code>）とWorker側の物理ストレージ（<code>GPUModelRunner.encoder_cache</code>）に分離されている。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py</code> (EncoderCacheManager)</p>
<h2 id="アーキテクチャ-1"><a class="header" href="#アーキテクチャ-1">アーキテクチャ</a></h2>
<h3 id="2層構造"><a class="header" href="#2層構造">2層構造</a></h3>
<pre class="mermaid">graph TB
    subgraph Scheduler Process
        ECM[EncoderCacheManager&lt;br/&gt;論理管理]
    end
    subgraph Worker Process
        EC["encoder_cache: dict[str, Tensor]&lt;br/&gt;GPU物理ストレージ"]
    end
    ECM --&gt;|"free_encoder_mm_hashes&lt;br/&gt;(SchedulerOutput経由)"| EC
    ECM --&gt;|"scheduled_encoder_inputs&lt;br/&gt;(何を計算すべきか)"| EC
</pre>

<div class="table-wrapper">
<table>
<thead>
<tr><th>層</th><th>場所</th><th>データ構造</th><th>役割</th></tr>
</thead>
<tbody>
<tr><td>論理管理</td><td>Scheduler</td><td><code>EncoderCacheManager</code></td><td>キャッシュ容量管理、参照カウント、Eviction判定</td></tr>
<tr><td>物理ストレージ</td><td>GPUModelRunner</td><td><code>dict[str, torch.Tensor]</code></td><td>mm_hash → エンコーダ出力テンソルの保持</td></tr>
</tbody>
</table>
</div>
<h2 id="encodercachemanager-詳細"><a class="header" href="#encodercachemanager-詳細">EncoderCacheManager 詳細</a></h2>
<h3 id="主要フィールド"><a class="header" href="#主要フィールド">主要フィールド</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:67-77</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>cache_size</code></td><td><code>int</code></td><td>エンコーダ埋め込み数で測った総容量</td></tr>
<tr><td><code>num_free_slots</code></td><td><code>int</code></td><td>現在利用可能な空きスロット数</td></tr>
<tr><td><code>num_freeable_slots</code></td><td><code>int</code></td><td>参照ゼロエントリの回収で即座に利用可能になるスロット数</td></tr>
<tr><td><code>cached</code></td><td><code>dict[str, set[str]]</code></td><td>mm_hash → 参照中リクエストIDの集合</td></tr>
<tr><td><code>freeable</code></td><td><code>OrderedDict[str, int]</code></td><td>参照ゼロエントリの挿入順リスト（mm_hash → 埋め込み数）</td></tr>
<tr><td><code>freed</code></td><td><code>list[str]</code></td><td>直近のEvictionで物理解放すべきmm_hashリスト</td></tr>
</tbody>
</table>
</div>
<h3 id="eviction方式-fifo参照ゼロエントリの遅延解放"><a class="header" href="#eviction方式-fifo参照ゼロエントリの遅延解放">Eviction方式: FIFO（参照ゼロエントリの遅延解放）</a></h3>
<p>EncoderCacheManagerは<strong>遅延解放FIFO方式</strong>を採用する：</p>
<ol>
<li>リクエスト完了時、参照カウントが0になったエントリは即座には解放されず <code>freeable</code> OrderedDictに追加</li>
<li>新しいエンコーダ出力のキャッシュ確保（<code>can_allocate()</code>）時に空きが不足した場合のみ、古い順にEviction</li>
<li>Evictionされたmm_hashは <code>freed</code> リストに追加され、次の <code>get_freed_mm_hashes()</code> でWorkerに通知</li>
<li>Worker側で <code>encoder_cache.pop(mm_hash)</code> により物理メモリ解放</li>
</ol>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:119-178</code> (can_allocate)</p>
<pre class="mermaid">stateDiagram-v2
    [*] --&gt; Active: allocate()
    Active --&gt; Active: check_and_update_cache()で新リクエスト参照追加
    Active --&gt; Freeable: free_encoder_input()で参照カウント=0
    Freeable --&gt; Active: check_and_update_cache()で再参照
    Freeable --&gt; Evicted: can_allocate()で空き不足時
    Evicted --&gt; [*]: Worker側でpop()
</pre>

<h3 id="共有キャッシュ"><a class="header" href="#共有キャッシュ">共有キャッシュ</a></h3>
<p>同じ画像を含む複数リクエストが同時に処理される場合、同一の<code>mm_hash</code>を持つエンコーダ出力は<strong>共有</strong>される。<code>cached</code> dictのvalue（<code>set[str]</code>）が複数リクエストIDを保持し、全リクエストの完了後にのみ<code>freeable</code>に移行する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:91-117</code> (check_and_update_cache)</p>
<h3 id="主要メソッド"><a class="header" href="#主要メソッド">主要メソッド</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>呼び出し元</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>check_and_update_cache(request, input_id)</code></td><td>Scheduler._schedule_encoder_inputs</td><td>キャッシュヒット判定。ヒット時は参照追加してTrue</td></tr>
<tr><td><code>can_allocate(request, input_id, budget, scheduled)</code></td><td>Scheduler._schedule_encoder_inputs</td><td>空き確認+必要時Eviction。False=予算不足</td></tr>
<tr><td><code>allocate(request, input_id)</code></td><td>Scheduler（RUNNING/WAITING処理）</td><td>論理的にキャッシュ空間を確保</td></tr>
<tr><td><code>free_encoder_input(request, input_id)</code></td><td>Scheduler</td><td>1エントリの参照解放</td></tr>
<tr><td><code>free(request)</code></td><td>Scheduler（リクエスト完了/中断時）</td><td>全エントリの参照解放</td></tr>
<tr><td><code>get_freed_mm_hashes()</code></td><td>Scheduler._build_scheduler_output</td><td>Eviction済みmm_hashリスト取得（Worker通知用）</td></tr>
</tbody>
</table>
</div>
<h3 id="キャッシュ容量の決定"><a class="header" href="#キャッシュ容量の決定">キャッシュ容量の決定</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:269-316</code> (compute_mm_encoder_budget)</p>
<pre><code class="language-python">encoder_compute_budget = max(max_num_encoder_input_tokens, max_tokens_per_mm_item)
encoder_cache_size = max(encoder_cache_size_config, max_tokens_per_mm_item)
</code></pre>
<ul>
<li><code>max_tokens_per_mm_item</code>: モデルがサポートする全モダリティの最大トークン数</li>
<li><code>max_num_encoder_input_tokens</code>: SchedulerConfig設定値</li>
<li>1アイテムは必ずキャッシュできることを保証</li>
</ul>
<h2 id="gpumodelrunnerencoder_cache物理ストレージ"><a class="header" href="#gpumodelrunnerencoder_cache物理ストレージ">GPUModelRunner.encoder_cache（物理ストレージ）</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:439</code></p>
<pre><code class="language-python">self.encoder_cache: dict[str, torch.Tensor] = {}
</code></pre>
<ul>
<li><strong>キー</strong>: <code>mm_hash</code>（マルチモーダルデータのidentifier）</li>
<li><strong>値</strong>: エンコーダ出力テンソル（GPU上）</li>
<li><strong>書き込み</strong>: <code>_execute_mm_encoder()</code> 完了時に <code>encoder_cache[mm_hash] = output</code></li>
<li><strong>読み取り</strong>: <code>_gather_mm_embeddings()</code> でデコーダ入力に合成</li>
<li><strong>削除</strong>: <code>_update_states()</code> で <code>scheduler_output.free_encoder_mm_hashes</code> に従い <code>pop()</code></li>
</ul>
<h3 id="ecconnectorとの連携"><a class="header" href="#ecconnectorとの連携">ECConnectorとの連携</a></h3>
<p>エンコーダ出力をGPUキャッシュに保存した直後に、ECConnector（有効時）にも保存する：</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:2442-2445</code></p>
<pre><code class="language-python">for mm_hash, output in zip(mm_hashes, encoder_outputs):
    self.encoder_cache[mm_hash] = output
    self.maybe_save_ec_to_connector(self.encoder_cache, mm_hash)
</code></pre>
<p>Consumer側では、<code>execute_model()</code> の冒頭で ECConnector から <code>encoder_cache</code> にロードする：</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/ec_connector_model_runner_mixin.py:76-77</code></p>
<pre><code class="language-python">if ec_connector.is_consumer:
    ec_connector.start_load_caches(encoder_cache, **kwargs)
</code></pre>
<h2 id="encoderdecodercachemanager"><a class="header" href="#encoderdecodercachemanager">EncoderDecoderCacheManager</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:323-382</code></p>
<p>Encoder-Decoderモデル（例: Whisper）用の暫定実装。<code>EncoderCacheManager</code>を継承するがキャッシュ共有機能を持たず、毎回エンコーダを実行する。主な違い：</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>特性</th><th>EncoderCacheManager (MM)</th><th>EncoderDecoderCacheManager</th></tr>
</thead>
<tbody>
<tr><td>キャッシュ共有</td><td>あり（mm_hash based）</td><td>なし（常にFalse）</td></tr>
<tr><td>参照カウント</td><td>あり</td><td>なし</td></tr>
<tr><td>Eviction</td><td>FIFO遅延解放</td><td>即時解放（1step遅延バッファあり）</td></tr>
<tr><td>用途</td><td>Vision-Language Model</td><td>Encoder-Decoder Model</td></tr>
</tbody>
</table>
</div>
<h2 id="上流下流依存関係-1"><a class="header" href="#上流下流依存関係-1">上流・下流依存関係</a></h2>
<h3 id="上流-1"><a class="header" href="#上流-1">上流</a></h3>
<ul>
<li><strong>Scheduler</strong>: <code>_schedule_encoder_inputs()</code> でキャッシュヒット判定・割当・解放指示</li>
<li><strong>SchedulerConfig</strong>: <code>encoder_cache_size</code>, <code>max_num_encoder_input_tokens</code> で容量決定</li>
</ul>
<h3 id="下流-1"><a class="header" href="#下流-1">下流</a></h3>
<ul>
<li><strong>GPUModelRunner</strong>: 物理テンソル保持、エンコーダ実行、gather処理</li>
<li><strong>ECConnector</strong>: 外部ストレージへの保存/読み込み（有効時）</li>
</ul>
<h2 id="データフロー"><a class="header" href="#データフロー">データフロー</a></h2>
<pre class="mermaid">sequenceDiagram
    participant S as Scheduler
    participant ECM as EncoderCacheManager
    participant SO as SchedulerOutput
    participant GMR as GPUModelRunner
    participant EC as encoder_cache (GPU)

    Note over S: _schedule_encoder_inputs()
    S-&gt;&gt;ECM: check_and_update_cache(req, i)
    alt キャッシュヒット
        ECM--&gt;&gt;S: True（計算スキップ）
    else キャッシュミス
        ECM--&gt;&gt;S: False
        S-&gt;&gt;ECM: can_allocate(req, i, budget, scheduled)
        alt 空きあり（Eviction含む）
            ECM--&gt;&gt;S: True
            S-&gt;&gt;ECM: allocate(req, i)
        else 空き不足
            ECM--&gt;&gt;S: False（トークン数調整）
        end
    end

    Note over S: _build_scheduler_output()
    S-&gt;&gt;ECM: get_freed_mm_hashes()
    ECM--&gt;&gt;S: freed list
    S-&gt;&gt;SO: free_encoder_mm_hashes, scheduled_encoder_inputs

    Note over GMR: execute_model()
    GMR-&gt;&gt;EC: _execute_mm_encoder() → 保存
    GMR-&gt;&gt;EC: _gather_mm_embeddings() → 読取
    GMR-&gt;&gt;EC: _update_states() → free_encoder_mm_hashes に従い削除
</pre>

<h2 id="注意事項"><a class="header" href="#注意事項">注意事項</a></h2>
<ul>
<li>キャッシュサイズは<strong>エンコーダ埋め込み数</strong>で測定される。画像間のテキストトークン（break tokens等）は含まない</li>
<li>物理メモリ解放はEviction判定（Scheduler側）と実際の<code>pop()</code>（Worker側）の間に1step以上のラグがある</li>
<li><code>num_freeable_slots</code> は <code>num_free_slots</code> 以上の値を常に持つ（freeable + free の合計）</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="enginecore-サマリー"><a class="header" href="#enginecore-サマリー">EngineCore サマリー</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-4"><a class="header" href="#概要-4">概要</a></h2>
<p><code>EngineCore</code>はバックエンドプロセス（<code>EngineCoreProc</code>）内で動作する推論ループの中央制御コンポーネントである。<code>Scheduler</code>、<code>ModelExecutor</code>、<code>KVCacheManager</code>を統括し、<code>step()</code>メソッドで <strong>schedule → execute → update</strong> のサイクルを繰り返す。フロントエンドプロセスとはZMQ IPC経由で通信し、<code>EngineCoreRequest</code>を受信して<code>EngineCoreOutputs</code>を返す。</p>
<h2 id="アーキテクチャ-2"><a class="header" href="#アーキテクチャ-2">アーキテクチャ</a></h2>
<pre class="mermaid">graph TD
    subgraph EngineCoreプロセス
        EC["EngineCore"]
        Sched["Scheduler"]
        KVM["KVCacheManager"]
        Exec["ModelExecutor"]

        EC --&gt;|"1. schedule()"| Sched
        Sched --&gt;|"allocate_slots()"| KVM
        Sched --&gt;|"SchedulerOutput"| EC
        EC --&gt;|"2. execute_model()"| Exec
        Exec --&gt;|"Future&lt;ModelRunnerOutput&gt;"| EC
        EC --&gt;|"3. update_from_output()"| Sched
        Sched --&gt;|"EngineCoreOutputs"| EC
    end

    ZMQ_IN["ZMQ ROUTER&lt;br&gt;(受信)"] --&gt;|"EngineCoreRequest"| EC
    EC --&gt;|"EngineCoreOutputs"| ZMQ_OUT["ZMQ PUSH&lt;br&gt;(送信)"]
</pre>

<h2 id="主要コンポーネント-1"><a class="header" href="#主要コンポーネント-1">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>用途</th><th>ファイル</th></tr>
</thead>
<tbody>
<tr><td><code>EngineCore</code></td><td>推論ループ本体</td><td><code>target/vllm/vllm/v1/engine/core.py:82</code></td></tr>
<tr><td><code>EngineCoreProc</code></td><td>プロセスラッパー。ZMQソケット管理とイベントループ</td><td><code>target/vllm/vllm/v1/engine/core.py</code></td></tr>
</tbody>
</table>
</div>
<h2 id="主要メソッド-1"><a class="header" href="#主要メソッド-1">主要メソッド</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>__init__()</code></td><td>L82</td><td>Scheduler, ModelExecutor, KVキャッシュの初期化</td></tr>
<tr><td><code>step()</code></td><td>L389</td><td>メインループ: schedule → execute → update</td></tr>
<tr><td><code>step_with_batch_queue()</code></td><td>L434</td><td>パイプライン並列化版step（batch_queue使用）</td></tr>
<tr><td><code>add_request()</code></td><td>L288</td><td>リクエストをバリデーション後Schedulerに登録</td></tr>
<tr><td><code>post_step()</code></td><td>L424</td><td>step後処理（Speculative Decodingのドラフトトークン更新）</td></tr>
</tbody>
</table>
</div>
<h2 id="step-サイクル"><a class="header" href="#step-サイクル">step() サイクル</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:389</code></p>
<pre><code>EngineCore.step() -&gt; tuple[dict[int, EngineCoreOutputs], bool]
  │
  ├─ スケジューラ停止チェック                           # L397
  │   if _scheduler_paused: return {}, False
  │
  ├─ リクエスト有無チェック                             # L402
  │   if not scheduler.has_requests(): return {}, False
  │
  ├─ 1. scheduler.schedule()                            # L404
  │   → SchedulerOutput
  │
  ├─ 2. executor.execute_model(scheduler_output,        # L405
  │       non_block=True)
  │   → Future[ModelRunnerOutput | None]
  │
  ├─ 3. scheduler.get_grammar_bitmask(scheduler_output) # L406
  │   → grammar_output（構造化出力用）
  │
  ├─ 4. future.result()                                 # L411
  │   → ModelRunnerOutput（ブロッキング待機）
  │
  ├─ 5. if model_output is None:                        # L413
  │       model_output = executor.sample_tokens(grammar_output)
  │   （非同期スケジューリング時: execute_modelとsamplingが分離）
  │
  ├─ 6. _process_aborts_queue()                         # L417
  │
  └─ 7. scheduler.update_from_output(                   # L418
  │       scheduler_output, model_output)
  │   → dict[int, EngineCoreOutputs]
  │
  └─ return (engine_core_outputs,                       # L422
             total_num_scheduled_tokens &gt; 0)
</code></pre>
<p><strong>戻り値</strong>:</p>
<ul>
<li>第1要素: クライアントインデックス → EngineCoreOutputs のマッピング</li>
<li>第2要素: モデル実行が行われたか（<code>total_num_scheduled_tokens &gt; 0</code>）</li>
</ul>
<h2 id="add_request-フロー"><a class="header" href="#add_request-フロー">add_request() フロー</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:288</code></p>
<pre><code>add_request(request, request_wave=0)                    # L288
  ├─ request_id の型チェック（str必須）                   # L295
  ├─ pooling_params のタスクバリデーション                # L300
  ├─ kv_transfer_params の互換性チェック                  # L311
  └─ scheduler.add_request(request)                      # L319
</code></pre>
<h2 id="batch_queue-パイプライン並列化-shallow"><a class="header" href="#batch_queue-パイプライン並列化-shallow">batch_queue パイプライン並列化 [SHALLOW]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:434</code> (step_with_batch_queue)</p>
<p><code>max_concurrent_batches &gt; 1</code> の場合、<code>step_with_batch_queue()</code> が <code>step_fn</code> として使用される。スケジューリングとモデル実行をパイプライン的にオーバーラップさせ、GPUのアイドル時間を削減する。</p>
<ul>
<li><code>batch_queue</code>: <code>deque[tuple[Future, SchedulerOutput, Future]]</code></li>
<li>新しいスケジュール結果を <code>appendleft()</code> で追加、完了待ちを <code>pop()</code> で取得</li>
<li>前のバッチの実行完了を待たずに次のバッチをスケジュール可能</li>
</ul>
<h2 id="kvキャッシュ初期化フロー-shallow"><a class="header" href="#kvキャッシュ初期化フロー-shallow">KVキャッシュ初期化フロー [SHALLOW]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:82</code> (<strong>init</strong>)</p>
<pre><code>EngineCore.__init__()
  → _initialize_kv_caches()
    → model_executor.get_kv_cache_specs()       # モデルのKVキャッシュ要件取得
    → determine_available_memory()               # GPUメモリプロファイリング
    → get_kv_cache_configs()                     # ブロック数等の設定算出
    → generate_scheduler_kv_cache_config()       # Scheduler用設定生成
    → model_executor.initialize_from_config()    # GPUメモリ確保
</code></pre>
<p>KV Connector（KV Transfer/LMCache連携）が有効な場合:</p>
<ul>
<li><code>scheduler.get_kv_connector()</code> でコネクタ有無を確認 (L159)</li>
<li>各ワーカーのハンドシェイクメタデータを収集・統合 (L164-175)</li>
</ul>
<h2 id="async_scheduling-shallow"><a class="header" href="#async_scheduling-shallow">async_scheduling [SHALLOW]</a></h2>
<p><code>vllm_config.scheduler_config.async_scheduling</code> で有効化。</p>
<ul>
<li>通常: <code>execute_model()</code> がモデル実行 + トークンサンプリングをまとめて実行</li>
<li>async有効時: <code>execute_model()</code> はモデル実行のみ（<code>None</code> を返す）→ <code>sample_tokens()</code> で別途サンプリング</li>
<li><code>post_step()</code> でのSpeculative Decodingドラフトトークン更新タイミングに影響</li>
</ul>
<h2 id="設定"><a class="header" href="#設定">設定</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>max_concurrent_batches</code></td><td>1</td><td>batch_queueサイズ（&gt;1でパイプライン並列化）</td></tr>
<tr><td><code>async_scheduling</code></td><td>False</td><td>非同期スケジューリングモード</td></tr>
</tbody>
</table>
</div>
<h2 id="呼び出しフロー"><a class="header" href="#呼び出しフロー">呼び出しフロー</a></h2>
<pre><code>[EngineCoreProc イベントループ]
  ├─ ZMQ受信 → EngineCore.add_request()
  ├─ EngineCore.step_fn()  (= step() or step_with_batch_queue())
  │   ├─ Scheduler.schedule()
  │   ├─ ModelExecutor.execute_model()
  │   └─ Scheduler.update_from_output()
  ├─ EngineCore.post_step()
  └─ ZMQ送信 ← EngineCoreOutputs
</code></pre>
<h2 id="関連ドキュメント"><a class="header" href="#関連ドキュメント">関連ドキュメント</a></h2>
<ul>
<li><a href="#scheduler-サマリー">Scheduler</a></li>
<li><a href="#kvcachemanager-サマリー">KVCacheManager</a></li>
<li><a href="#enginecoreclient-サマリー">EngineCoreClient</a></li>
<li><a href="#エントリポイント-asyncllm--llm-サマリー">エントリポイント</a></li>
<li><a href="#テキスト推論データフロー">データフロー</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="enginecoreclient-サマリー"><a class="header" href="#enginecoreclient-サマリー">EngineCoreClient サマリー</a></h1>
<blockquote>
<p><strong>深度</strong>: [SHALLOW]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-09</p>
</blockquote>
<h2 id="概要-5"><a class="header" href="#概要-5">概要</a></h2>
<p><code>EngineCoreClient</code>はフロントエンドプロセス（AsyncLLM / LLM）とバックエンドプロセス（EngineCore）間のプロセス間通信を抽象化するコンポーネントである。ZeroMQソケットとmsgpackシリアライゼーションを使用し、<code>EngineCoreRequest</code>の送信と<code>EngineCoreOutputs</code>の受信を効率的に行う。</p>
<h2 id="アーキテクチャ-3"><a class="header" href="#アーキテクチャ-3">アーキテクチャ</a></h2>
<pre><code>フロントエンドプロセス            バックエンドプロセス
┌───────────────────┐            ┌───────────────────┐
│  AsyncMPClient    │            │  EngineCore       │
│                   │            │                   │
│  input_socket     ├──ROUTER──→│  (ZMQ受信)        │
│  (zmq.ROUTER)     │  msgpack   │                   │
│                   │            │                   │
│  output_socket    │←──PULL────┤  (ZMQ送信)        │
│  (zmq.PULL)       │  msgpack   │                   │
│                   │            │                   │
│  outputs_queue    │            │                   │
│  (asyncio.Queue)  │            │                   │
└───────────────────┘            └───────────────────┘
</code></pre>
<h2 id="主要コンポーネント-2"><a class="header" href="#主要コンポーネント-2">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>用途</th><th>ファイル</th></tr>
</thead>
<tbody>
<tr><td><code>EngineCoreClient</code> (ABC)</td><td>抽象インターフェース</td><td><code>target/vllm/vllm/v1/engine/core_client.py:63</code></td></tr>
<tr><td><code>MPClient</code></td><td>マルチプロセスクライアント基底</td><td><code>target/vllm/vllm/v1/engine/core_client.py:442</code></td></tr>
<tr><td><code>AsyncMPClient</code></td><td>非同期マルチプロセスクライアント（AsyncLLM用）</td><td><code>target/vllm/vllm/v1/engine/core_client.py:822</code></td></tr>
<tr><td><code>SyncMPClient</code></td><td>同期マルチプロセスクライアント（LLM用）</td><td><code>target/vllm/vllm/v1/engine/core_client.py</code></td></tr>
<tr><td><code>DPAsyncMPClient</code></td><td>データ並列（外部LB）</td><td><code>target/vllm/vllm/v1/engine/core_client.py</code></td></tr>
<tr><td><code>DPLBAsyncMPClient</code></td><td>データ並列（内部LB）</td><td><code>target/vllm/vllm/v1/engine/core_client.py</code></td></tr>
<tr><td><code>MsgpackEncoder</code></td><td>リクエストのシリアライズ</td><td><code>target/vllm/vllm/v1/serial_utils.py</code></td></tr>
<tr><td><code>MsgpackDecoder</code></td><td>レスポンスのデシリアライズ</td><td><code>target/vllm/vllm/v1/serial_utils.py</code></td></tr>
</tbody>
</table>
</div>
<h2 id="主要メソッド-2"><a class="header" href="#主要メソッド-2">主要メソッド</a></h2>
<h3 id="enginecoreclient-abc"><a class="header" href="#enginecoreclient-abc">EngineCoreClient (ABC)</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>make_client()</code></td><td>ファクトリ。設定に応じた適切なサブクラスを返す</td></tr>
<tr><td><code>make_async_mp_client()</code></td><td>AsyncLLM用ファクトリ。DP構成も考慮</td></tr>
<tr><td><code>add_request()</code></td><td>EngineCoreRequestを送信</td></tr>
<tr><td><code>get_output()</code></td><td>EngineCoreOutputsを受信</td></tr>
<tr><td><code>abort_requests()</code></td><td>リクエストキャンセル</td></tr>
</tbody>
</table>
</div>
<h3 id="asyncmpclient"><a class="header" href="#asyncmpclient">AsyncMPClient</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>_ensure_output_queue_task()</code></td><td>L856</td><td>ZMQ出力受信タスクを起動</td></tr>
<tr><td><code>get_output_async()</code></td><td>L902</td><td>asyncio.Queueから出力を取得</td></tr>
<tr><td><code>_send_input()</code></td><td>L913</td><td>EngineCoreRequestをZMQで送信</td></tr>
<tr><td><code>_send_input_message()</code></td><td>L925</td><td>ZMQ multipart送信（zero-copy対応）</td></tr>
</tbody>
</table>
</div>
<h2 id="ファクトリ選択ロジック"><a class="header" href="#ファクトリ選択ロジック">ファクトリ選択ロジック</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core_client.py:99</code> (make_async_mp_client)</p>
<pre><code>make_async_mp_client(vllm_config, executor_class, ...)
  ├─ data_parallel_size &gt; 1 の場合:
  │   ├─ external_lb → DPAsyncMPClient
  │   └─ internal_lb → DPLBAsyncMPClient
  └─ それ以外 → AsyncMPClient
</code></pre>
<h2 id="設定-1"><a class="header" href="#設定-1">設定</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>parallel_config.data_parallel_size</code></td><td>1</td><td>データ並列数。&gt;1でDP系クライアントを使用</td></tr>
<tr><td><code>parallel_config.data_parallel_external_lb</code></td><td>—</td><td>外部ロードバランサ使用フラグ</td></tr>
</tbody>
</table>
</div>
<h2 id="呼び出しフロー-1"><a class="header" href="#呼び出しフロー-1">呼び出しフロー</a></h2>
<pre><code>[送信パス]
AsyncLLM.add_request()
  → engine_core.add_request_async(request)
    → AsyncMPClient._send_input(REQUEST, request)
      → MsgpackEncoder.encode(request)
      → input_socket.send_multipart(msg, copy=False)
        → ZMQ ROUTER → バックエンドプロセス

[受信パス]
process_outputs_socket() [バックグラウンドタスク]
  → output_socket.recv_multipart()
    → MsgpackDecoder.decode(frames) → EngineCoreOutputs
    → outputs_queue.put_nowait(outputs)

AsyncLLM._run_output_handler()
  → engine_core.get_output_async()
    → outputs_queue.get() → EngineCoreOutputs
</code></pre>
<h2 id="設計上の特徴"><a class="header" href="#設計上の特徴">設計上の特徴</a></h2>
<ul>
<li><strong>プロセス分離</strong>: EngineCoreが別プロセスで動作するため、GILの影響を受けずスケジューリングとGPU実行を並行可能</li>
<li><strong>msgpackシリアライゼーション</strong>: <code>msgspec.Struct</code>の<code>array_like</code>形式でコンパクトなバイナリ表現</li>
<li><strong>zero-copy</strong>: ZMQ <code>copy=False</code> でメモリコピーを最小化。テンソルバッキングバッファの追跡（<code>add_pending_message</code>）</li>
<li><strong>weakref</strong>: 出力タスクがクライアントへの循環参照を持たないよう<code>weakref</code>を使用</li>
</ul>
<h2 id="関連ドキュメント-1"><a class="header" href="#関連ドキュメント-1">関連ドキュメント</a></h2>
<ul>
<li><a href="#エントリポイント-asyncllm--llm-サマリー">エントリポイント</a></li>
<li><a href="#テキスト推論データフロー">データフロー</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="エントリポイント-asyncllm--llm-サマリー"><a class="header" href="#エントリポイント-asyncllm--llm-サマリー">エントリポイント (AsyncLLM / LLM) サマリー</a></h1>
<blockquote>
<p><strong>深度</strong>: [SHALLOW]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-09</p>
</blockquote>
<h2 id="概要-6"><a class="header" href="#概要-6">概要</a></h2>
<p><code>AsyncLLM</code>と<code>LLM</code>はvLLMの2つの主要エントリポイントである。<code>AsyncLLM</code>はAPIサーバー（OpenAI互換API等）が使用する非同期パスで、<code>LLM</code>はオフラインバッチ推論用の同期パスである。どちらも<code>InputProcessor</code>で入力を処理し、<code>EngineCoreClient</code>経由でバックエンド（EngineCore）にリクエストを送信する。</p>
<h2 id="アーキテクチャ-4"><a class="header" href="#アーキテクチャ-4">アーキテクチャ</a></h2>
<pre class="mermaid">graph LR
    subgraph 非同期パス
        OpenAI["OpenAI API Server"] --&gt; AsyncLLM
        AsyncLLM --&gt;|"process_inputs()"| IP["InputProcessor"]
        IP --&gt;|"EngineCoreRequest"| AsyncLLM
        AsyncLLM --&gt;|"add_request_async()"| Client["EngineCoreClient"]
        Client --&gt;|"ZMQ"| EC["EngineCore"]
    end

    subgraph 同期パス
        User["ユーザーコード"] --&gt; LLM
        LLM --&gt;|"process_inputs()"| IP2["InputProcessor"]
        IP2 --&gt;|"EngineCoreRequest"| LLM
        LLM --&gt;|"add_request()"| Client2["EngineCoreClient"]
    end
</pre>

<h2 id="主要コンポーネント-3"><a class="header" href="#主要コンポーネント-3">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>用途</th><th>ファイル</th></tr>
</thead>
<tbody>
<tr><td><code>AsyncLLM</code></td><td>非同期推論エントリポイント。AsyncGeneratorでストリーミング出力</td><td><code>target/vllm/vllm/v1/engine/async_llm.py:71</code></td></tr>
<tr><td><code>LLM</code></td><td>同期バッチ推論エントリポイント。<code>list[RequestOutput]</code>を返す</td><td><code>target/vllm/vllm/entrypoints/llm.py:101</code></td></tr>
<tr><td><code>RequestOutputCollector</code></td><td>非同期パスでの出力キュー管理</td><td><code>target/vllm/vllm/v1/engine/async_llm.py</code></td></tr>
<tr><td><code>ParentRequest</code></td><td>n&gt;1サンプリング時の親リクエスト管理</td><td><code>target/vllm/vllm/v1/engine/async_llm.py</code></td></tr>
</tbody>
</table>
</div>
<h2 id="主要メソッド-3"><a class="header" href="#主要メソッド-3">主要メソッド</a></h2>
<h3 id="asyncllm"><a class="header" href="#asyncllm">AsyncLLM</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>generate()</code></td><td>L537</td><td>メインAPI。AsyncGeneratorでRequestOutputをyield</td></tr>
<tr><td><code>add_request()</code></td><td>L286</td><td>リクエスト追加。InputProcessor→OutputProcessor→EngineCore</td></tr>
<tr><td><code>_add_request()</code></td><td>L414</td><td>内部: OutputProcessorとEngineCoreに登録</td></tr>
<tr><td><code>_run_output_handler()</code></td><td>L647</td><td>バックグラウンドタスク起動。EngineCore出力を受信→キュー</td></tr>
</tbody>
</table>
</div>
<h3 id="llm"><a class="header" href="#llm">LLM</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>generate()</code></td><td>L396</td><td>バッチ推論API。<code>list[RequestOutput]</code>を返す</td></tr>
<tr><td><code>_add_request()</code></td><td>L1850</td><td>InputProcessor→llm_engine.add_request()</td></tr>
<tr><td><code>_run_engine()</code></td><td>L1900</td><td>ポーリングループ。完了まで<code>step()</code>を繰り返す</td></tr>
</tbody>
</table>
</div>
<h2 id="設定-2"><a class="header" href="#設定-2">設定</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>log_requests</code></td><td><code>True</code></td><td>リクエストログ出力</td></tr>
<tr><td><code>log_stats</code></td><td>引数指定</td><td>統計ログ出力</td></tr>
<tr><td><code>start_engine_loop</code></td><td><code>True</code></td><td>エンジンループ自動起動</td></tr>
</tbody>
</table>
</div>
<h2 id="呼び出しフロー-2"><a class="header" href="#呼び出しフロー-2">呼び出しフロー</a></h2>
<pre><code>[APIサーバー or ユーザーコード]
  → AsyncLLM.generate() / LLM.generate()
    → InputProcessor.process_inputs()
      → EngineCoreRequest
    → EngineCoreClient.add_request_async()
      → ZMQ → EngineCore（別プロセス）

[バックグラウンド output_handler タスク]
  → EngineCoreClient.get_output_async()
    → EngineCoreOutputs
  → OutputProcessor.process_outputs()
    → RequestOutput → キューにpush

[generate() AsyncGenerator]
  → キューから取り出してyield
</code></pre>
<h2 id="関連ドキュメント-2"><a class="header" href="#関連ドキュメント-2">関連ドキュメント</a></h2>
<ul>
<li><a href="#inputprocessor-サマリー">入力処理</a></li>
<li><a href="#enginecoreclient-サマリー">EngineCoreClient</a></li>
<li><a href="#テキスト推論データフロー">データフロー</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="executor"><a class="header" href="#executor">Executor</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-14</p>
</blockquote>
<h2 id="概要-7"><a class="header" href="#概要-7">概要</a></h2>
<p>Executorは、EngineCoreとWorker（GPUModelRunner）の間に位置する実行委譲レイヤーである。<code>collective_rpc()</code>パターンで全Workerに対して同一メソッドを呼び出し、出力ランクのWorkerの結果を返す。単一プロセス、マルチプロセス、Ray分散の3つの実装を持つ。</p>
<h2 id="クラス階層"><a class="header" href="#クラス階層">クラス階層</a></h2>
<pre><code>Executor (ABC)                                     abstract.py:36
├── UniProcExecutor                                uniproc_executor.py:26
│   └── ExecutorWithExternalLauncher               uniproc_executor.py:140
├── MultiprocExecutor                              multiproc_executor.py:93
└── RayDistributedExecutor                         ray_executor.py:62
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py:36</code> (Executor)</p>
<h2 id="主要メソッド-4"><a class="header" href="#主要メソッド-4">主要メソッド</a></h2>
<h3 id="collective_rpc"><a class="header" href="#collective_rpc">collective_rpc()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py:180</code> (collective_rpc)</p>
<p>全Workerに対して同一メソッドを実行するRPCメカニズム。</p>
<pre><code class="language-python">def collective_rpc(
    self,
    method: str | Callable[..., _R],  # メソッド名または関数
    timeout: float | None = None,
    args: tuple = (),
    kwargs: dict | None = None,
    non_block: bool = False,          # True: Future返却
) -&gt; list[_R] | Future[list[_R]]
</code></pre>
<h3 id="execute_model"><a class="header" href="#execute_model">execute_model()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py:202</code> (execute_model)</p>
<pre><code class="language-python">def execute_model(
    self,
    scheduler_output: SchedulerOutput,
    non_block: bool = False,
) -&gt; ModelRunnerOutput | None | Future[ModelRunnerOutput | None]:
    output = self.collective_rpc("execute_model",
                                  args=(scheduler_output,),
                                  non_block=non_block)
    return output[0]   # 出力ランクWorkerの結果のみ返す
</code></pre>
<h3 id="sample_tokens"><a class="header" href="#sample_tokens">sample_tokens()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py:222</code> (sample_tokens)</p>
<pre><code class="language-python">def sample_tokens(
    self,
    grammar_output: GrammarOutput | None,
    non_block: bool = False,
) -&gt; ModelRunnerOutput | Future[ModelRunnerOutput]:
    output = self.collective_rpc("sample_tokens",
                                  args=(grammar_output,),
                                  non_block=non_block)
    return output[0]
</code></pre>
<h2 id="実装の使い分け"><a class="header" href="#実装の使い分け">実装の使い分け</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>実装</th><th>用途</th><th>Worker配置</th><th>特徴</th></tr>
</thead>
<tbody>
<tr><td><code>UniProcExecutor</code></td><td>単一GPU</td><td>ドライバプロセス内</td><td>最小オーバーヘッド。<code>max_concurrent_batches &gt; 1</code>時はThreadPoolExecutor使用</td></tr>
<tr><td><code>MultiprocExecutor</code></td><td>複数GPU（TP/PP）</td><td>子プロセス</td><td>MessageQueue（共有メモリ）ベース。Pipeline Parallelism対応</td></tr>
<tr><td><code>RayDistributedExecutor</code></td><td>分散クラスタ</td><td>Rayアクター</td><td>Ray経由のリモートWorker管理</td></tr>
</tbody>
</table>
</div>
<h2 id="multiprocexecutor-のプロセス間通信-medium-verified"><a class="header" href="#multiprocexecutor-のプロセス間通信-medium-verified">MultiprocExecutor のプロセス間通信 [MEDIUM] [VERIFIED]</a></h2>
<p>MultiprocExecutorはSharedMemory MessageQueue（<code>ShmRingBuffer</code>）を使って同一ノード内のWorkerプロセスと通信する。</p>
<h3 id="messagequeue-の仕組み"><a class="header" href="#messagequeue-の仕組み">MessageQueue の仕組み</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/device_communicators/shm_broadcast.py:272</code> (MessageQueue)</p>
<p>2つのチャネルを併用:</p>
<ol>
<li><strong>ShmRingBuffer</strong>（共有メモリ）: 24MiB以下の通常データ。ロックフリー、~20nsメモリフェンスのみ</li>
<li><strong>ZMQ PUB/SUB</strong>（フォールバック）: 24MiBを超えるデータ。ローカルはIPC、リモートはTCP</li>
</ol>
<p>ShmRingBufferのメモリレイアウト:</p>
<pre><code>┌──────────────────────────────────┬──────────────────────────────────────┐
│ data: chunk0 | chunk1 | ... | N  │ metadata: [written|r0|r1|...] × N   │
│ max_chunks × 24MiB               │ max_chunks × (1 + n_reader) bytes    │
└──────────────────────────────────┴──────────────────────────────────────┘
</code></pre>
<p>メタデータフラグで書き込み/読み取り状態を管理。全readerが読み取り完了するとチャンクが再利用される。</p>
<h3 id="キュー構成"><a class="header" href="#キュー構成">キュー構成</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>キュー</th><th>方向</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>rpc_broadcast_mq</code></td><td>Executor → 全Worker</td><td>RPCコマンドのブロードキャスト</td></tr>
<tr><td><code>worker_response_mq</code> × N</td><td>各Worker → Executor</td><td>実行結果の返送</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:131-136</code> (rpc_broadcast_mq生成)</p>
<h3 id="collective_rpc-の動作フロー"><a class="header" href="#collective_rpc-の動作フロー">collective_rpc の動作フロー</a></h3>
<pre><code>MultiprocExecutor.collective_rpc("execute_model", args=(...))
  │
  ├─ rpc_broadcast_mq.enqueue((method, args, kwargs, output_rank))
  │   → pickle → ShmRingBuffer書き込み → メモリフェンス
  │
  ├─ Worker-0: dequeue() → execute → response_mq.enqueue()
  ├─ Worker-1: dequeue() → execute → response_mq.enqueue()
  │
  └─ Executor: response_mqs[output_rank].dequeue() → 結果返却
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:303-375</code> (collective_rpc)
<strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:845-871</code> (worker_busy_loop)</p>
<h3 id="worker-プロセスの起動とビジーループ"><a class="header" href="#worker-プロセスの起動とビジーループ">Worker プロセスの起動とビジーループ</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:696</code> (WorkerProc.worker_main)</p>
<pre><code>WorkerProc.worker_main()
  ├─ Worker.init_device()
  │   └─ torch.distributed.init_process_group(backend="nccl")
  ├─ Worker.load_model()
  ├─ READY送信（Pipe経由）
  └─ worker_busy_loop():
      while True:
        method, args, kwargs, output_rank = rpc_broadcast_mq.dequeue()
        output = getattr(worker, method)(*args, **kwargs)
        worker_response_mq.enqueue((SUCCESS, output))
</code></pre>
<h2 id="worker委譲先"><a class="header" href="#worker委譲先">Worker（委譲先）</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_worker.py:70</code> (Worker)</p>
<p><code>Worker(WorkerBase)</code> はGPUModelRunnerのラッパーで、以下の追加処理を行う:</p>
<ul>
<li><strong>Pipeline Parallelism</strong>: 前段ランクからの<code>IntermediateTensors</code>受信、後段への送信</li>
<li><strong>推論モード管理</strong>: <code>@torch.inference_mode()</code> デコレータ</li>
</ul>
<pre><code>Worker.execute_model(scheduler_output)                    # L604
  ├─ PP: recv_tensor_dict() → IntermediateTensors        # L614-641
  ├─ model_runner.execute_model(scheduler_output, ...)    # L652
  │   → ModelRunnerOutput | None | IntermediateTensors
  ├─ PP: send_tensor_dict(intermediate_tensors)           # L660-671
  └─ return ModelRunnerOutput | None
</code></pre>
<h2 id="enginecore--出力-の委譲フロー"><a class="header" href="#enginecore--出力-の委譲フロー">EngineCore → 出力 の委譲フロー</a></h2>
<pre><code>EngineCore.step()
  └─ executor.execute_model(scheduler_output, non_block=True)
      └─ collective_rpc("execute_model")
          └─ Worker.execute_model()
              └─ GPUModelRunner.execute_model()
                  → ExecuteModelState 保存、None 返却

EngineCore.step()（続き）
  └─ executor.sample_tokens(grammar_output)
      └─ collective_rpc("sample_tokens")
          └─ Worker.sample_tokens()
              └─ GPUModelRunner.sample_tokens()
                  → ModelRunnerOutput 返却
</code></pre>
<h2 id="上流下流の関係"><a class="header" href="#上流下流の関係">上流・下流の関係</a></h2>
<ul>
<li><strong>上流</strong>: EngineCore（<code>step()</code>から呼び出し）</li>
<li><strong>下流</strong>: Worker → GPUModelRunner</li>
</ul>
<h2 id="phase-2-深堀り候補"><a class="header" href="#phase-2-深堀り候補">Phase 2 深堀り候補</a></h2>
<ul>
<li><del>MultiprocExecutorのMessageQueue実装詳細</del> → 調査済み（本ドキュメント）</li>
<li>Pipeline Parallelism時のバッチスケジューリング</li>
<li>Ray分散実行のオーバーヘッドと障害回復</li>
<li>AsyncScheduling時のasync_output_busy_loop動作</li>
</ul>
<h2 id="主要ファイル"><a class="header" href="#主要ファイル">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/v1/executor/abstract.py</code></td><td><code>Executor</code>, <code>collective_rpc()</code> (L180), <code>execute_model()</code> (L202)</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/uniproc_executor.py</code></td><td><code>UniProcExecutor</code> (L26)</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/multiproc_executor.py</code></td><td><code>MultiprocExecutor</code> (L93), <code>WorkerProc</code> (L493), <code>worker_busy_loop</code> (L845)</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/ray_executor.py</code></td><td><code>RayDistributedExecutor</code> (L62)</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_worker.py</code></td><td><code>Worker</code> (L70), <code>execute_model()</code> (L604)</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/worker_base.py</code></td><td><code>WorkerBase</code> (L34), <code>WorkerWrapperBase</code> (L175)</td></tr>
<tr><td><code>target/vllm/vllm/distributed/device_communicators/shm_broadcast.py</code></td><td><code>ShmRingBuffer</code> (L127), <code>MessageQueue</code> (L272)</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="gpumodelrunner"><a class="header" href="#gpumodelrunner">GPUModelRunner</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-15</p>
</blockquote>
<h2 id="概要-8"><a class="header" href="#概要-8">概要</a></h2>
<p>GPUModelRunnerは、推論パイプラインの実行中核を担う巨大クラス（約6,300行）である。SchedulerOutputを受け取り、入力テンソルの準備、モデルのforward実行、サンプリングを経て、ModelRunnerOutputを返す。<strong>2フェーズ実行パターン</strong>（<code>execute_model()</code> → <code>sample_tokens()</code>）を採用し、モデルフォワードとgrammar bitmask計算の並行実行を可能にしている。</p>
<h2 id="クラス定義"><a class="header" href="#クラス定義">クラス定義</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:329</code> (GPUModelRunner)</p>
<pre><code class="language-python">class GPUModelRunner(
    LoRAModelRunnerMixin,           # LoRAアダプタ管理
    KVConnectorModelRunnerMixin,    # KV Transfer対応
    ECConnectorModelRunnerMixin,    # エンコーダキャッシュ対応
):
</code></pre>
<h2 id="状態管理"><a class="header" href="#状態管理">状態管理</a></h2>
<p>GPUModelRunnerは2つのデータ構造でリクエスト状態を管理する:</p>
<ul>
<li><strong><code>self.requests: dict[str, CachedRequestState]</code></strong> — リクエストの論理状態（プリエンプション後も保持）</li>
<li><strong><code>self.input_batch: InputBatch</code></strong> — 永続バッチテンソル群（事前割り当て、差分更新）</li>
</ul>
<p>永続バッチ最適化により、連続stepで変更があったリクエストのデータのみCPU側で更新し、GPUへはDMA一括転送する。</p>
<p>詳細は <a href="#inputbatch-永続バッチと状態管理">InputBatch: 永続バッチと状態管理</a> を参照。</p>
<h2 id="kvcache-gpu-interface"><a class="header" href="#kvcache-gpu-interface">KVCache-GPU Interface</a></h2>
<p>KVCacheManager（Scheduler側）が割り当てた論理ブロックIDは、4段階の変換を経てAttentionカーネルが消費する形式になる:</p>
<pre><code>_update_states()        ← ブロックID取込（3ケース: 新規/追加/プリエンプション復帰）
  ↓
BlockTable              ← CPU側テーブル（CpuGpuBuffer）
  ↓
_prepare_inputs()       ← commit_block_table() DMA + compute_slot_mapping()
  ↓
_get_slot_mappings()    ← by_gid（AttentionMetadata用）/ by_layer（ForwardContext用）
  ↓
_build_attention_metadata() ← CommonAttentionMetadata → per-layer AttentionMetadata
</code></pre>
<p><strong>核心的な変換式</strong>: <code>slot = block_number * block_size + (position % block_size)</code></p>
<p>詳細は <a href="#kvcache-gpu-interface-ブロックテーブルとスロットマッピング">KVCache-GPU Interface</a> を参照。</p>
<h2 id="2フェーズ実行パターン"><a class="header" href="#2フェーズ実行パターン">2フェーズ実行パターン</a></h2>
<p>GPUModelRunnerの中核設計。<code>execute_model()</code>でモデルフォワードとlogits計算を行い、結果を<code>ExecuteModelState</code>に保存して<code>None</code>を返す。その後<code>sample_tokens()</code>が状態を復元してサンプリングを実行する。</p>
<pre class="mermaid">sequenceDiagram
    participant EC as EngineCore
    participant MR as GPUModelRunner

    EC-&gt;&gt;MR: execute_model(scheduler_output)
    Note over MR: 入力準備 → モデルフォワード&lt;br&gt;→ logits計算 → 状態保存
    MR--&gt;&gt;EC: None

    Note over EC: grammar bitmask 計算&lt;br&gt;（並行処理）

    EC-&gt;&gt;MR: sample_tokens(grammar_output)
    Note over MR: 状態復元 → grammar適用&lt;br&gt;→ サンプリング → 出力構築
    MR--&gt;&gt;EC: ModelRunnerOutput
</pre>

<h3 id="phase-1-execute_model"><a class="header" href="#phase-1-execute_model">Phase 1: execute_model()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:3312</code> (execute_model)</p>
<pre><code>execute_model(scheduler_output, intermediate_tensors=None)
  │
  ├─ 1. バッチ状態更新 _update_states()                     # L3376
  │     新規リクエスト登録、ブロックID更新、不要リクエスト除去
  │
  ├─ 2. 入力準備 _prepare_inputs()                          # L3383
  │     block_table DMA → slot_mapping計算 → positions/input_ids GPU転送
  │     ※ DMAオーバーラップ最適化（L1472-1474）
  │
  ├─ 3. CUDAGraph判定                                       # L3398
  │     _determine_batch_execution_and_padding()
  │     → CUDAGraphMode(FULL/PIECEWISE/NONE) + パディング量決定
  │
  ├─ 4. スロットマッピング2形式出力                            # L3468
  │     _get_slot_mappings() → by_gid + by_layer
  │
  ├─ 5. Attentionメタデータ構築                               # L3479
  │     _build_attention_metadata()
  │     → CommonAttentionMetadata → per-layer AttentionMetadata
  │
  ├─ 6. モデルフォワード                                      # L3538
  │     set_forward_context(slot_mapping=by_layer)
  │     → _model_forward() → model.forward() → logits
  │
  └─ 7. 状態保存                                              # L3605-3615
      ExecuteModelState に保存 → None を返す
</code></pre>
<p><strong>戻り値のパターン</strong>:</p>
<ul>
<li><code>None</code> — 通常ケース（sample_tokens()を後で呼ぶ）</li>
<li><code>ModelRunnerOutput</code> — プーリングモデル等（サンプリング不要）</li>
<li><code>IntermediateTensors</code> — Pipeline Parallelismの中間ランク</li>
<li><code>EMPTY_MODEL_RUNNER_OUTPUT</code> — スケジュールトークン0件</li>
</ul>
<h3 id="phase-2-sample_tokens"><a class="header" href="#phase-2-sample_tokens">Phase 2: sample_tokens()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:3621</code> (sample_tokens)</p>
<pre><code>sample_tokens(grammar_output)
  │
  ├─ 1. 状態復元 — ExecuteModelState から logits 等を復元     # L3643-3657
  ├─ 2. Grammar制約適用 — bitmask → logits                  # L3659-3663
  ├─ 3. サンプリング — _sample() → SamplerOutput             # L3665-3666
  ├─ 4. 後処理 — バッチ状態反映、PP broadcast、ドラフト提案    # L3668-3699
  └─ 5. ModelRunnerOutput構築                                # L3775-3787
</code></pre>
<h2 id="cudagraph統合-verified"><a class="header" href="#cudagraph統合-verified">CUDAGraph統合 [VERIFIED]</a></h2>
<h3 id="cudagraphmode"><a class="header" href="#cudagraphmode">CUDAGraphMode</a></h3>
<p>3つの実行モードが存在する:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>モード</th><th>説明</th><th>使用条件</th></tr>
</thead>
<tbody>
<tr><td><code>FULL</code></td><td>forward全体をキャプチャ</td><td>Attentionバックエンドが対応、cascade attention無効</td></tr>
<tr><td><code>PIECEWISE</code></td><td>Attention以外をキャプチャ（torch.compile統合）</td><td>Attention部分はコンパイル済みコードで実行</td></tr>
<tr><td><code>NONE</code></td><td>Eagerモード</td><td>CUDAGraph無効、バッチサイズ超過、calc_kv_scales時</td></tr>
</tbody>
</table>
</div>
<h3 id="cudagraphdispatcher"><a class="header" href="#cudagraphdispatcher">CudagraphDispatcher</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/cudagraph_dispatcher.py:14</code></p>
<p>事前キャプチャ済みCUDAGraphの中からランタイムで適切なグラフを選択する:</p>
<ol>
<li><code>cudagraph_keys: dict[CUDAGraphMode, set[BatchDescriptor]]</code> にキャプチャ済みのバッチ記述子を保持</li>
<li><code>dispatch()</code> は入力 <code>num_tokens</code> を最小のパディングサイズに丸め上げ</li>
<li>FULL → PIECEWISE → NONE の優先順序でキーを探索</li>
</ol>
<p><strong>バッチパディング</strong>: CUDAGraphは固定形状のテンソルを要求するため、実際のトークン数をキャプチャ済みサイズに丸め上げる。未使用スロットはslot_mapping <code>-1</code>（PAD_SLOT_ID）で埋められ、<code>reshape_and_cache()</code> がスキップする。</p>
<h3 id="_determine_batch_execution_and_padding"><a class="header" href="#_determine_batch_execution_and_padding">_determine_batch_execution_and_padding()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:3076</code></p>
<p>毎stepの実行モード判定:</p>
<ol>
<li><code>_is_uniform_decode()</code> — 全リクエストがデコードフェーズ（query_len=1）か判定</li>
<li><code>cudagraph_dispatcher.dispatch()</code> — モードとパディングサイズを決定</li>
<li>Data Parallel時は <code>coordinate_batch_across_dp()</code> で全ランク間で合意</li>
</ol>
<h2 id="executemodelstate"><a class="header" href="#executemodelstate">ExecuteModelState</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:313</code> (ExecuteModelState)</p>
<p>2フェーズ間の一時状態を保持するNamedTuple。GPUテンソルを含むため、シリアライズはされない。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>scheduler_output</code></td><td><code>SchedulerOutput</code></td><td>スケジュール結果</td></tr>
<tr><td><code>logits</code></td><td><code>torch.Tensor</code></td><td>モデル出力logits</td></tr>
<tr><td><code>spec_decode_metadata</code></td><td><code>SpecDecodeMetadata | None</code></td><td>Speculative Decoding情報</td></tr>
<tr><td><code>hidden_states</code></td><td><code>torch.Tensor</code></td><td>隠れ状態</td></tr>
<tr><td><code>sample_hidden_states</code></td><td><code>torch.Tensor</code></td><td>サンプリング用隠れ状態</td></tr>
<tr><td><code>aux_hidden_states</code></td><td><code>list[torch.Tensor] | None</code></td><td>補助隠れ状態</td></tr>
<tr><td><code>ec_connector_output</code></td><td><code>ECConnectorOutput | None</code></td><td>エンコーダ出力</td></tr>
<tr><td><code>cudagraph_stats</code></td><td><code>CUDAGraphStat | None</code></td><td>CUDAGraph統計</td></tr>
<tr><td><code>slot_mappings</code></td><td><code>dict | list | None</code></td><td>KVキャッシュスロットマッピング</td></tr>
</tbody>
</table>
</div>
<h2 id="6300行の内訳-verified"><a class="header" href="#6300行の内訳-verified">6,300行の内訳 [VERIFIED]</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>行範囲</th><th>セクション</th></tr>
</thead>
<tbody>
<tr><td>1-312</td><td>インポート、型エイリアス、Async出力クラス、ExecuteModelState</td></tr>
<tr><td>329-712</td><td><code>__init__</code>（設定、バッファ割当、状態初期化）</td></tr>
<tr><td>713-873</td><td>ライフサイクルヘルパー（<code>reset_mm_cache</code>, <code>init_fp8_kv_scales</code> 等）</td></tr>
<tr><td>874-1453</td><td>状態管理: <code>_update_states()</code>, <code>_update_states_after_model_execute()</code></td></tr>
<tr><td>1454-1672</td><td>入力準備: <code>_prepare_inputs()</code>, <code>_prepare_input_ids()</code></td></tr>
<tr><td>1673-2049</td><td><code>_build_attention_metadata()</code>（Attentionメタデータ構築）</td></tr>
<tr><td>2050-2557</td><td>位置計算、MMエンコーダ実行</td></tr>
<tr><td>2558-3311</td><td>モデルユーティリティ、<code>_get_slot_mappings()</code></td></tr>
<tr><td>3312-3620</td><td><code>execute_model()</code>（メインforward）</td></tr>
<tr><td>3621-3934</td><td><code>sample_tokens()</code>、PP broadcast、ドラフト提案</td></tr>
<tr><td>3935-4118</td><td>Speculative Decoding提案</td></tr>
<tr><td>4119-4609</td><td>モデルロード: <code>load_model()</code>, <code>reload_weights()</code></td></tr>
<tr><td>4610-5108</td><td>ダミー実行、プロファイリング</td></tr>
<tr><td>5109-5332</td><td>CUDAGraphキャプチャ: <code>capture_model()</code>, <code>_capture_cudagraphs()</code></td></tr>
<tr><td>5333-5596</td><td>Attentionバックエンド初期化、メタデータビルダー</td></tr>
<tr><td>5597-6152</td><td>KVキャッシュ初期化: <code>initialize_kv_cache()</code></td></tr>
<tr><td>6152-6273</td><td><code>get_kv_cache_spec()</code>, タイミング統計</td></tr>
</tbody>
</table>
</div>
<h2 id="マルチモーダル処理"><a class="header" href="#マルチモーダル処理">マルチモーダル処理</a></h2>
<p>マルチモーダル推論時、GPUModelRunnerは <code>execute_model()</code> 内で以下の追加処理を行う:</p>
<ol>
<li><strong><code>_execute_mm_encoder()</code></strong> (L2293): <code>model.embed_multimodal()</code> でビジョンエンコーダ実行。結果を <code>encoder_cache[mm_hash]</code> に格納</li>
<li><strong><code>_gather_mm_embeddings()</code></strong> (L2449): <code>encoder_cache</code> からスケジュール範囲に対応する埋め込みをスライス。チャンクPrefill対応</li>
<li><strong><code>embed_input_ids()</code></strong>: <code>masked_scatter_</code> でテキスト + ビジョン埋め込みをマージ → <code>inputs_embeds</code> として model.forward() に渡す</li>
</ol>
<p><code>encoder_cache: dict[str, torch.Tensor]</code> はGPU上のシンプルなdictキャッシュで、Schedulerの <code>free_encoder_mm_hashes</code> 指示で解放される。</p>
<p>詳細は <a href="#バックエンド-マルチモーダル処理パス-medium-verified">バックエンド MM処理パス</a> を参照。</p>
<h2 id="上流下流の関係-1"><a class="header" href="#上流下流の関係-1">上流・下流の関係</a></h2>
<ul>
<li><strong>上流</strong>: Worker（<code>execute_model()</code> / <code>sample_tokens()</code>経由で呼び出し）</li>
<li><strong>下流</strong>: モデル層（<code>model.forward()</code>）、Sampler</li>
</ul>
<h2 id="深堀り候補今後"><a class="header" href="#深堀り候補今後">深堀り候補（今後）</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>テーマ</th><th>関連メソッド</th><th>ユーザー関心</th></tr>
</thead>
<tbody>
<tr><td>サンプリング実装</td><td><code>_sample()</code>, Sampler</td><td>低</td></tr>
<tr><td>KV Transfer連携</td><td><code>KVConnectorModelRunnerMixin</code></td><td>高（ユーザー関心2位。次セッション候補）</td></tr>
<tr><td>Speculative Decoding</td><td><code>propose_draft_token_ids()</code></td><td>中</td></tr>
<tr><td>async_scheduling</td><td><code>_update_states_after_model_execute()</code></td><td>中</td></tr>
</tbody>
</table>
</div>
<h2 id="主要ファイル-1"><a class="header" href="#主要ファイル-1">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_model_runner.py</code></td><td><code>GPUModelRunner</code> (L329), <code>execute_model()</code> (L3312), <code>sample_tokens()</code> (L3621), <code>ExecuteModelState</code> (L313)</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_input_batch.py</code></td><td><code>CachedRequestState</code> (L30), <code>InputBatch</code> (L81)</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/block_table.py</code></td><td><code>BlockTable</code> (L16), <code>MultiGroupBlockTable</code> (L253)</td></tr>
<tr><td><code>target/vllm/vllm/v1/cudagraph_dispatcher.py</code></td><td><code>CudagraphDispatcher</code> (L14)</td></tr>
<tr><td><code>target/vllm/vllm/v1/utils.py</code></td><td><code>CpuGpuBuffer</code> (L105)</td></tr>
<tr><td><code>target/vllm/vllm/v1/outputs.py</code></td><td><code>ModelRunnerOutput</code> (L160), <code>AsyncModelRunnerOutput</code> (L200)</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="inputbatch-永続バッチと状態管理"><a class="header" href="#inputbatch-永続バッチと状態管理">InputBatch: 永続バッチと状態管理</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-14</p>
</blockquote>
<h2 id="概要-9"><a class="header" href="#概要-9">概要</a></h2>
<p>GPUModelRunnerは2つのデータ構造でリクエスト状態を管理する:</p>
<ul>
<li><strong><code>CachedRequestState</code></strong> — リクエストごとの論理状態（step間で永続、プリエンプション後も保持）</li>
<li><strong><code>InputBatch</code></strong> — 全リクエストの物理バッチテンソル群（事前割り当て、step間でCPU側を差分更新）</li>
</ul>
<p>永続バッチ最適化により、連続するstep間でリクエストの大部分が同じであることを前提に、差分のみを更新する。</p>
<h2 id="cachedrequeststate-verified"><a class="header" href="#cachedrequeststate-verified">CachedRequestState [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_input_batch.py:30</code></p>
<p><code>@dataclass</code> で定義されるリクエスト単位の状態。<code>GPUModelRunner.requests: dict[str, CachedRequestState]</code> に格納される。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>req_id</code></td><td><code>str</code></td><td>リクエストID</td></tr>
<tr><td><code>prompt_token_ids</code></td><td><code>list[int] | None</code></td><td>プロンプトトークンID列</td></tr>
<tr><td><code>prompt_embeds</code></td><td><code>Tensor | None</code></td><td>プロンプト埋め込み（embedsモード時）</td></tr>
<tr><td><code>mm_features</code></td><td><code>list[MultiModalFeatureSpec]</code></td><td>マルチモーダル特徴量</td></tr>
<tr><td><code>sampling_params</code></td><td><code>SamplingParams | None</code></td><td>サンプリングパラメータ</td></tr>
<tr><td><code>generator</code></td><td><code>torch.Generator | None</code></td><td>シード付き乱数生成器</td></tr>
<tr><td><code>block_ids</code></td><td><code>tuple[list[int], ...]</code></td><td>KVキャッシュグループごとのブロックID列</td></tr>
<tr><td><code>num_computed_tokens</code></td><td><code>int</code></td><td>計算済みトークン数（プレフィックスキャッシュ含む）</td></tr>
<tr><td><code>output_token_ids</code></td><td><code>list[int]</code></td><td>生成済みトークンID列</td></tr>
<tr><td><code>lora_request</code></td><td><code>LoRARequest | None</code></td><td>LoRAアダプタ</td></tr>
</tbody>
</table>
</div>
<p><strong>ライフサイクル</strong>: <code>_update_states()</code> で作成され、<code>finished_req_ids</code> で削除される。プリエンプション時もInputBatchからは除去されるが <code>self.requests</code> には保持され、復帰時に再利用される。</p>
<p><strong><code>num_tokens</code> プロパティ</strong>: <code>num_prompt_tokens + len(output_token_ids)</code> — 現在のリクエスト全体のトークン数。</p>
<h2 id="inputbatch-verified"><a class="header" href="#inputbatch-verified">InputBatch [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_input_batch.py:81</code></p>
<p>全リクエストの状態を事前割り当てテンソル群で管理するバッチマネージャー。</p>
<h3 id="主要テンソル群"><a class="header" href="#主要テンソル群">主要テンソル群</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>テンソル</th><th>shape</th><th>種別</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>token_ids_cpu_tensor</code></td><td><code>(max_num_reqs, max_model_len)</code></td><td>CPU (非pin)</td><td>全リクエストのトークンID。<strong>大きくなりうる</strong>（TODOコメントあり）</td></tr>
<tr><td><code>num_computed_tokens_cpu_tensor</code></td><td><code>(max_num_reqs,)</code></td><td>CPU (pin)</td><td>計算済みトークン数</td></tr>
<tr><td><code>num_tokens_no_spec</code></td><td><code>(max_num_reqs,)</code></td><td>numpy</td><td>Spec Decode以外のトークン数</td></tr>
<tr><td><code>num_prompt_tokens</code></td><td><code>(max_num_reqs,)</code></td><td>numpy</td><td>プロンプトトークン数</td></tr>
<tr><td><code>temperature_cpu</code> / <code>top_p_cpu</code> / <code>top_k_cpu</code></td><td><code>(max_num_reqs,)</code></td><td>CPU (pin)</td><td>サンプリングパラメータ</td></tr>
<tr><td><code>block_table</code></td><td><code>MultiGroupBlockTable</code></td><td>CpuGpuBuffer</td><td>KVキャッシュブロックテーブル</td></tr>
</tbody>
</table>
</div>
<h3 id="リクエスト管理"><a class="header" href="#リクエスト管理">リクエスト管理</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>データ構造</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>_req_ids: list[str | None]</code></td><td>インデックス→リクエストID。<code>None</code> は空スロット</td></tr>
<tr><td><code>req_id_to_index: dict[str, int]</code></td><td>リクエストID→インデックス。逆引き</td></tr>
<tr><td><code>num_reqs</code> プロパティ</td><td><code>len(req_id_to_index)</code> — 現在のリクエスト数</td></tr>
</tbody>
</table>
</div>
<h3 id="add_request--remove_request-verified"><a class="header" href="#add_request--remove_request-verified">add_request() / remove_request() [VERIFIED]</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_input_batch.py:304</code>, <code>469</code></p>
<p><strong><code>add_request()</code></strong>:</p>
<ol>
<li>空きインデックスを探す（末尾追加 or 空スロット再利用）</li>
<li><code>token_ids_cpu</code> にプロンプト+出力トークンIDをコピー</li>
<li><code>block_table.add_row()</code> でブロックIDを設定</li>
<li>サンプリングパラメータを各テンソルにコピー</li>
</ol>
<p><strong><code>remove_request()</code></strong>:</p>
<ol>
<li><code>req_id_to_index</code> から削除、<code>_req_ids[index] = None</code> で空スロット化</li>
<li><code>batch_update_builder.removed_append()</code> で空きインデックスを記録</li>
<li>サンプリング関連のset/dictからも除去</li>
<li>テンソル自体はクリアしない（次のadd_requestで上書きされる）</li>
</ol>
<h3 id="condense--バッチ圧縮-verified"><a class="header" href="#condense--バッチ圧縮-verified">condense() — バッチ圧縮 [VERIFIED]</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_input_batch.py:626</code></p>
<p><code>remove_request()</code> で生じた空スロットを埋める処理。末尾のリクエストを空スロットに移動し、連続した配列を保つ:</p>
<pre><code>[A, _, B, _, C]  →  [A, C, B]  （Cを空スロット1に移動）
</code></pre>
<p>移動対象: <code>token_ids_cpu</code>, <code>num_tokens_no_spec</code>, <code>num_prompt_tokens</code>, <code>num_computed_tokens_cpu</code>, <code>block_table.move_row()</code>, サンプリングパラメータ等。</p>
<h2 id="multigroupblocktable-verified"><a class="header" href="#multigroupblocktable-verified">MultiGroupBlockTable [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/block_table.py:253</code></p>
<p>KVキャッシュグループごとに <code>BlockTable</code> を保持するラッパー。Hybridモデル（異なるアテンションタイプが混在）では複数のBlockTableが存在する。</p>
<pre><code class="language-python"># アクセスパターン
input_batch.block_table[kv_cache_gid]  # → BlockTable
input_batch.block_table.append_row(block_ids, req_index)  # 全グループに委譲
input_batch.block_table.commit_block_table(num_reqs)       # 全グループDMA
</code></pre>
<p><code>block_ids</code> は <code>tuple[list[int], ...]</code> 型で、外側タプルのインデックスがKVキャッシュグループIDに対応。</p>
<h2 id="永続バッチ最適化の全体像-verified"><a class="header" href="#永続バッチ最適化の全体像-verified">永続バッチ最適化の全体像 [VERIFIED]</a></h2>
<pre class="mermaid">sequenceDiagram
    participant S as Scheduler
    participant US as _update_states()
    participant IB as InputBatch
    participant PI as _prepare_inputs()
    participant GPU as GPU Tensors

    S-&gt;&gt;US: SchedulerOutput
    Note over US: finished_req_ids → remove_request()
    Note over US: unscheduled → remove_request()
    Note over US: new_reqs → CachedRequestState作成
    Note over US: cached_reqs → block_ids更新
    US-&gt;&gt;IB: add_request() / append_row() / condense()
    Note over IB: CPU側テンソルを差分更新
    IB-&gt;&gt;PI: commit_block_table() (DMA)
    PI-&gt;&gt;GPU: slot_mapping, positions, input_ids etc.
</pre>

<p><strong>最適化の本質</strong>: 前stepと大部分が同じリクエスト群に対して、変更があったフィールド（新ブロックID、num_computed_tokens等）のみをCPU側で更新し、GPUへはDMA一括転送する。毎step全データを再構築する必要がない。</p>
<h2 id="主要ファイル-2"><a class="header" href="#主要ファイル-2">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_input_batch.py</code></td><td><code>CachedRequestState</code> (L30), <code>InputBatch</code> (L81), <code>add_request()</code> (L304), <code>remove_request()</code> (L469), <code>condense()</code> (L626)</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/block_table.py</code></td><td><code>MultiGroupBlockTable</code> (L253)</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kvcache-gpu-interface-ブロックテーブルとスロットマッピング"><a class="header" href="#kvcache-gpu-interface-ブロックテーブルとスロットマッピング">KVCache-GPU Interface: ブロックテーブルとスロットマッピング</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-14</p>
</blockquote>
<h2 id="概要-10"><a class="header" href="#概要-10">概要</a></h2>
<p>KVCacheManager（Scheduler側）が割り当てた論理ブロックIDは、GPUModelRunner内で4段階の変換を経てAttentionカーネルが消費できる形式になる。この文書では、ブロックIDの取り込みからAttentionMetadata構築までの完全なデータパスを追跡する。</p>
<pre class="mermaid">graph LR
    A["KVCacheManager&lt;br&gt;(論理ブロック割当)"] --&gt;|SchedulerOutput&lt;br&gt;new_block_ids| B["_update_states()&lt;br&gt;(ブロックID取込)"]
    B --&gt;|CachedRequestState&lt;br&gt;block_ids| C["BlockTable&lt;br&gt;(CPU側テーブル)"]
    C --&gt;|"commit_block_table()&lt;br&gt;DMA (non-blocking)"| D["GPU block_table"]
    C --&gt;|"compute_slot_mapping()"| E["CPU slot_mapping"]
    E --&gt;|"commit_slot_mapping()&lt;br&gt;DMA"| F["GPU slot_mapping"]
    D --&gt; G["_build_attention_metadata()"]
    F --&gt; G
    G --&gt;|"per-layer&lt;br&gt;AttentionMetadata"| H["Attention Kernel"]
    F --&gt;|"set_forward_context()&lt;br&gt;slot_mappings_by_layer"| I["reshape_and_cache()"]
</pre>

<h2 id="stage-a-ブロックid取込--_update_states-verified"><a class="header" href="#stage-a-ブロックid取込--_update_states-verified">Stage A: ブロックID取込 — <code>_update_states()</code> [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:874</code></p>
<p>SchedulerOutputには3種類のリクエストデータが含まれ、それぞれブロックIDの扱いが異なる:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ケース</th><th>block_ids の処理</th><th>InputBatch操作</th></tr>
</thead>
<tbody>
<tr><td><strong>新規リクエスト</strong></td><td><code>new_req_data.block_ids</code> → <code>CachedRequestState</code> 作成</td><td><code>add_request()</code> で追加</td></tr>
<tr><td><strong>継続（非プリエンプション）</strong></td><td><code>new_block_ids</code> を既存 <code>block_ids</code> に <code>extend()</code></td><td><code>block_table.append_row()</code> で差分追加</td></tr>
<tr><td><strong>プリエンプション復帰</strong></td><td><code>block_ids</code> を丸ごと <code>new_block_ids</code> で置換</td><td><code>add_request()</code> で再追加（前stepで <code>remove_request()</code> 済み）</td></tr>
</tbody>
</table>
</div>
<p><code>block_ids</code> の型は <code>list[list[int]]</code> — 外側のリストはKVキャッシュグループ（Hybridモデルで複数）、内側はそのグループのブロックID列。</p>
<p><strong>重要な最適化 — 永続バッチ</strong>: 前stepで存在し今回もスケジュールされたリクエストは、InputBatchに残り続ける。スケジュールされなかったリクエストだけが <code>remove_request()</code> で除去されるが、<code>CachedRequestState</code> は <code>self.requests</code> に保持される（L901-921）。</p>
<h2 id="stage-b-スロットマッピング計算--blocktablecompute_slot_mapping-verified"><a class="header" href="#stage-b-スロットマッピング計算--blocktablecompute_slot_mapping-verified">Stage B: スロットマッピング計算 — <code>BlockTable.compute_slot_mapping()</code> [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/block_table.py:133</code></p>
<p>トークンの論理位置を、KVキャッシュの物理GPUメモリスロットに変換する式:</p>
<pre><code>block_table_index = req_index * max_num_blocks_per_req + position // block_size
block_number = block_table[block_table_index]  # ravel()で1D化してルックアップ
block_offset = position % block_size
slot = block_number * block_size + block_offset
</code></pre>
<h3 id="hybrid-block対応"><a class="header" href="#hybrid-block対応">Hybrid Block対応</a></h3>
<p>KVCacheManagerの割当ブロックサイズとAttentionカーネルのブロックサイズが異なる場合（例: 割当=32トークン、カーネル=16トークン）:</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/block_table.py:203</code> (<code>map_to_kernel_blocks</code>)</p>
<pre><code class="language-python"># 割当ブロックID [0, 1, 2] → カーネルブロックID [0, 1, 2, 3, 4, 5]
# ブロック0 → [0, 1], ブロック1 → [2, 3], ブロック2 → [4, 5]
kernel_ids = block_id * blocks_per_kv_block + arange(blocks_per_kv_block)
</code></pre>
<p><code>append_row()</code> と <code>add_row()</code> は、ブロックIDを追加する前にこの変換を適用する。<code>compute_slot_mapping()</code> では変換後の <code>kernel_block_size</code> が <code>block_size</code> として使用される。</p>
<h3 id="context-parallel対応"><a class="header" href="#context-parallel対応">Context Parallel対応</a></h3>
<p>CP（DCP/PCP）有効時は「仮想ブロック」サイズ（= <code>block_size * cp_world_size</code>）を使ってブロックテーブルインデックスを算出し、ローカルランクに属さないトークンのスロットを <code>-1</code> にマスクする（L142-179）。</p>
<h2 id="stage-c-gpu転送--_prepare_inputs-verified"><a class="header" href="#stage-c-gpu転送--_prepare_inputs-verified">Stage C: GPU転送 — <code>_prepare_inputs()</code> [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:1454</code></p>
<p><code>_prepare_inputs()</code> は以下の順序でCPU→GPU転送を実行する:</p>
<pre><code>1. commit_block_table(num_reqs)      ← 最初に発行（DMAオーバーラップ最適化）
2. positions計算 (CPU)               ← DMA中にCPU演算
3. token_indices計算 (CPU)           ← DMA中にCPU演算
4. compute_slot_mapping()            ← CPU演算
5. commit_slot_mapping()             ← DMA発行
6. query_start_loc → GPU
7. seq_lens → GPU
8. input_ids → GPU
9. positions → GPU
</code></pre>
<p><strong>最適化ポイント</strong>: <code>commit_block_table()</code> をCPU演算の前に呼ぶことで、ブロックテーブルのDMA転送とCPU上のslot_mapping/positions計算を並行実行する（L1472-1474のコメント）。</p>
<h3 id="cpugpubuffer"><a class="header" href="#cpugpubuffer">CpuGpuBuffer</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/utils.py:105</code></p>
<p>ピン留めメモリCPUテンソル + GPUテンソルのペア。<code>np</code> 属性でnumpyビューを公開し、CPU側の演算をnumpy高速演算で行える。<code>copy_to_gpu(n)</code> は <code>non_blocking=True</code> でDMA転送を発行。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>属性</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>cpu</code></td><td><code>torch.Tensor</code> (pinned)</td><td>ピン留めCPUメモリ</td></tr>
<tr><td><code>gpu</code></td><td><code>torch.Tensor</code></td><td>GPU VRAM</td></tr>
<tr><td><code>np</code></td><td><code>np.ndarray</code></td><td><code>cpu</code> のnumpyビュー</td></tr>
</tbody>
</table>
</div>
<h2 id="stage-d-2形式のスロットマッピング出力--_get_slot_mappings-verified"><a class="header" href="#stage-d-2形式のスロットマッピング出力--_get_slot_mappings-verified">Stage D: 2形式のスロットマッピング出力 — <code>_get_slot_mappings()</code> [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:3237</code></p>
<p><code>_get_slot_mappings()</code> はスロットマッピングを2つの形式で返す:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>形式</th><th>キー</th><th>消費者</th></tr>
</thead>
<tbody>
<tr><td><code>slot_mappings_by_gid</code></td><td><code>dict[int, Tensor]</code> — KVキャッシュグループID</td><td><code>_build_attention_metadata()</code> → <code>CommonAttentionMetadata.slot_mapping</code></td></tr>
<tr><td><code>slot_mappings_by_layer</code></td><td><code>dict[str, Tensor]</code> — レイヤー名</td><td><code>set_forward_context()</code> → <code>reshape_and_cache()</code></td></tr>
</tbody>
</table>
</div>
<p>同一KVキャッシュグループ内のレイヤーは同じスロットマッピングテンソルを共有する。</p>
<p><strong>CUDAGraph FULLモード時のパディング</strong>: 未使用トークンスロットは <code>-1</code> で埋められる（L3283-3285）。これはMambaの <code>PAD_SLOT_ID</code> と一致し、<code>reshape_and_cache()</code> が <code>-1</code> スロットをスキップする。</p>
<h2 id="stage-e-attentionメタデータ構築--_build_attention_metadata-verified"><a class="header" href="#stage-e-attentionメタデータ構築--_build_attention_metadata-verified">Stage E: Attentionメタデータ構築 — <code>_build_attention_metadata()</code> [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:1673</code></p>
<h3 id="commonattentionmetadata"><a class="header" href="#commonattentionmetadata">CommonAttentionMetadata</a></h3>
<p>全KVキャッシュグループの共通基盤。グループ0のblock_table/slot_mappingで初期化し、他グループでは <code>copy()</code> (shallow copy) してblock_tableとslot_mappingのみ差し替える。</p>
<pre><code class="language-python">cm_base = CommonAttentionMetadata(
    query_start_loc=...,     # GPU tensor [num_reqs+1]
    seq_lens=...,            # GPU tensor [num_reqs]
    block_table_tensor=...,  # GPU tensor [num_reqs, max_blocks] ← グループ0
    slot_mapping=...,        # GPU tensor [num_tokens] ← グループ0
    max_query_len=...,
    max_seq_len=...,
    ...
)
</code></pre>
<h3 id="per-layer-attentionmetadata構築"><a class="header" href="#per-layer-attentionmetadata構築">Per-layer AttentionMetadata構築</a></h3>
<pre class="mermaid">graph TD
    A[CommonAttentionMetadata] --&gt;|"copy() + グループ別&lt;br&gt;block_table/slot_mapping"| B["cm (per kv_cache_group)"]
    B --&gt; C["AttentionMetadataBuilder.build()"]
    C --&gt; D["AttentionMetadata"]
    D --&gt;|"同一attn_group内の&lt;br&gt;全レイヤーが共有"| E["attn_metadata[layer_name]"]
</pre>

<p><code>_build_attn_group_metadata()</code> 内部関数が各<code>(kv_cache_gid, attn_gid)</code>ペアに対して:</p>
<ol>
<li><code>attn_group.get_metadata_builder()</code> でビルダーを取得</li>
<li><code>builder.build(common_attn_metadata=cm)</code> で <code>AttentionMetadata</code> を生成</li>
<li><code>attn_group.layer_names</code> の全レイヤーに同じメタデータを割り当て</li>
</ol>
<h3 id="キャッシュ最適化"><a class="header" href="#キャッシュ最適化">キャッシュ最適化</a></h3>
<p>Hybridモデルで同じ <code>KVCacheSpec</code> + 同じビルダー型の組み合わせが複数グループに現れる場合、2番目以降は <code>builder.update_block_table()</code> で block_table のみ差し替え、ビルド全体をスキップする（L1824-1832）。</p>
<h3 id="戻り値の型"><a class="header" href="#戻り値の型">戻り値の型</a></h3>
<pre><code class="language-python">PerLayerAttnMetadata = dict[str, AttentionMetadata]      # 通常
                     | list[dict[str, AttentionMetadata]] # UBatching(DBO)時
</code></pre>
<h2 id="execute_model-内の呼び出し順序-verified"><a class="header" href="#execute_model-内の呼び出し順序-verified">execute_model() 内の呼び出し順序 [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:3312</code></p>
<pre><code>execute_model(scheduler_output)
  │
  ├─ 1. _prepare_inputs()                    # L3383
  │     block_table DMA + slot_mapping計算 + GPU転送
  │
  ├─ 2. _determine_batch_execution_and_padding()  # L3398
  │     CUDAGraphモード判定 → num_tokens_padded, num_reqs_padded
  │
  ├─ 3. _get_slot_mappings()                  # L3468
  │     2形式出力: by_gid (→ attn_metadata), by_layer (→ ForwardContext)
  │
  ├─ 4. _build_attention_metadata()           # L3479
  │     CommonAttentionMetadata → per-layer AttentionMetadata
  │
  ├─ 5. set_forward_context(slot_mapping=slot_mappings_by_layer)  # L3524
  │     ForwardContext にスロットマッピングを設定
  │
  └─ 6. _model_forward()                      # L3538
        model.forward() → Attention layer が ForwardContext から
        slot_mapping を取得し reshape_and_cache() を実行
</code></pre>
<h2 id="主要ファイル-3"><a class="header" href="#主要ファイル-3">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_model_runner.py</code></td><td><code>_update_states()</code> (L874), <code>_prepare_inputs()</code> (L1454), <code>_build_attention_metadata()</code> (L1673), <code>_get_slot_mappings()</code> (L3237), <code>execute_model()</code> (L3312)</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/block_table.py</code></td><td><code>BlockTable</code> (L16), <code>compute_slot_mapping()</code> (L133), <code>map_to_kernel_blocks()</code> (L203), <code>MultiGroupBlockTable</code> (L253)</td></tr>
<tr><td><code>target/vllm/vllm/v1/utils.py</code></td><td><code>CpuGpuBuffer</code> (L105)</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="inputprocessor-サマリー"><a class="header" href="#inputprocessor-サマリー">InputProcessor サマリー</a></h1>
<blockquote>
<p><strong>深度</strong>: [SHALLOW]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-09</p>
</blockquote>
<h2 id="概要-11"><a class="header" href="#概要-11">概要</a></h2>
<p><code>InputProcessor</code>はユーザー入力（テキストプロンプト、SamplingParams等）を内部表現<code>EngineCoreRequest</code>に変換するコンポーネントである。トークナイズ、パラメータのバリデーションと正規化、マルチモーダル入力の前処理を担当する。AsyncLLMの初期化時に生成され、フロントエンドプロセスで動作する。</p>
<h2 id="アーキテクチャ-5"><a class="header" href="#アーキテクチャ-5">アーキテクチャ</a></h2>
<pre class="mermaid">graph LR
    Prompt["PromptType&lt;br&gt;(str / list[int] / dict)"] --&gt; IP["InputProcessor"]
    Params["SamplingParams"] --&gt; IP
    IP --&gt; IPP["InputPreprocessor&lt;br&gt;tokenizer.encode()"]
    IPP --&gt; PI["ProcessorInputs"]
    PI --&gt; IP
    IP --&gt; ECR["EngineCoreRequest"]
</pre>

<h2 id="主要コンポーネント-4"><a class="header" href="#主要コンポーネント-4">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>用途</th><th>ファイル</th></tr>
</thead>
<tbody>
<tr><td><code>InputProcessor</code></td><td>入力処理のメインクラス</td><td><code>target/vllm/vllm/v1/engine/input_processor.py:56</code></td></tr>
<tr><td><code>InputPreprocessor</code></td><td>トークナイズとマルチモーダル前処理</td><td><code>target/vllm/vllm/v1/engine/input_processor.py</code> (内部利用)</td></tr>
<tr><td><code>ProcessorInputs</code></td><td>前処理結果の中間データ構造</td><td><code>target/vllm/vllm/v1/engine/input_processor.py</code></td></tr>
</tbody>
</table>
</div>
<h2 id="主要メソッド-5"><a class="header" href="#主要メソッド-5">主要メソッド</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>入力</th><th>出力</th></tr>
</thead>
<tbody>
<tr><td><code>process_inputs()</code></td><td>L521</td><td><code>request_id</code>, <code>prompt</code>, <code>params</code></td><td><code>EngineCoreRequest</code></td></tr>
<tr><td><code>assign_request_id()</code></td><td>(別メソッド)</td><td><code>EngineCoreRequest</code></td><td>None (内部IDを付与)</td></tr>
<tr><td><code>_validate_lora()</code></td><td>L535付近</td><td><code>LoRARequest</code></td><td>バリデーション</td></tr>
<tr><td><code>_validate_params()</code></td><td>L536付近</td><td><code>SamplingParams</code></td><td>バリデーション</td></tr>
</tbody>
</table>
</div>
<h2 id="process_inputs-の処理フロー"><a class="header" href="#process_inputs-の処理フロー">process_inputs() の処理フロー</a></h2>
<pre><code>process_inputs(request_id, prompt, params)
  1. バリデーション
     ├─ LoRAリクエスト検証
     ├─ パラメータ検証
     └─ data_parallel_rank 範囲チェック
  2. arrival_time 設定
  3. input_preprocessor.preprocess(prompt)
     → テキストをトークナイズ → ProcessorInputs
  4. split_enc_dec_inputs()
     → エンコーダ/デコーダ入力を分離
  5. SamplingParams 正規化
     ├─ clone() で複製
     ├─ max_tokens 未設定時: max_model_len - seq_len
     ├─ update_from_generation_config()
     └─ update_from_tokenizer()
  6. EngineCoreRequest を構築して返す
</code></pre>
<h2 id="設定-3"><a class="header" href="#設定-3">設定</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>model_config.max_model_len</code></td><td>モデル依存</td><td>max_tokens未指定時の上限計算に使用</td></tr>
<tr><td><code>cache_config.enable_prefix_caching</code></td><td>—</td><td>マルチモーダルUUID生成方式に影響</td></tr>
</tbody>
</table>
</div>
<h2 id="呼び出しフロー-3"><a class="header" href="#呼び出しフロー-3">呼び出しフロー</a></h2>
<pre><code>AsyncLLM.add_request() / LLM._add_request()
  → InputProcessor.process_inputs()
    → InputPreprocessor.preprocess()
      → tokenizer.encode()
    → EngineCoreRequest を返す
  → InputProcessor.assign_request_id()
    → 外部IDを external_req_id に退避
    → 内部ID（外部ID + 8文字ランダム）を request_id に設定
</code></pre>
<h2 id="マルチモーダル処理-1"><a class="header" href="#マルチモーダル処理-1">マルチモーダル処理</a></h2>
<p>テキスト入力に加えて画像等のマルチモーダルデータがある場合、InputProcessorは以下の追加処理を行う:</p>
<ol>
<li><strong>MM Registry から <code>BaseMultiModalProcessor</code> を取得</strong>（<code>_get_mm_processor()</code>）</li>
<li><strong>マルチモーダルデータのパース</strong>: <code>mm_processor.info.parse_mm_data()</code> → <code>MultiModalDataItems</code></li>
<li><strong>HF Processor 実行</strong>: <code>mm_processor.apply()</code> → <code>MultiModalInputs</code>（トークン列 + テンソル + ハッシュ + PlaceholderRange）</li>
<li><strong>ProcessorCache</strong>: <code>mm_processor_cache</code> によるHF処理結果のキャッシュ（4種類: processor_only/lru/shm/none）</li>
<li><strong>MultiModalFeatureSpec 構築</strong>: データ + 位置情報 + ハッシュを <code>EngineCoreRequest.mm_features</code> にセット</li>
</ol>
<p>詳細は <a href="#フロントエンド-マルチモーダル処理パス-mediumdeep3-verified">マルチモーダル フロントエンド処理</a> を参照。</p>
<h2 id="関連ドキュメント-3"><a class="header" href="#関連ドキュメント-3">関連ドキュメント</a></h2>
<ul>
<li><a href="#エントリポイント-asyncllm--llm-サマリー">エントリポイント</a></li>
<li><a href="#テキスト推論データフロー">データフロー</a></li>
<li><a href="#マルチモーダル処理パイプライン-サマリー">マルチモーダル処理パイプライン</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kvcachemanager-サマリー"><a class="header" href="#kvcachemanager-サマリー">KVCacheManager サマリー</a></h1>
<blockquote>
<p><strong>深度</strong>: [DEEP]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-12"><a class="header" href="#概要-12">概要</a></h2>
<p><code>KVCacheManager</code> は PagedAttention に基づく KV キャッシュブロックの割り当て・解放・プレフィックスキャッシュ検索を管理するクラスである。4層の階層設計（<code>KVCacheManager</code> → <code>KVCacheCoordinator</code> → <code>SingleTypeKVCacheManager</code> → <code>BlockPool</code>）でマルチグループ KV キャッシュを統括する。Scheduler から呼び出され、各リクエストに必要な GPU メモリブロックを確保する。</p>
<h2 id="アーキテクチャ-6"><a class="header" href="#アーキテクチャ-6">アーキテクチャ</a></h2>
<h3 id="クラス階層-1"><a class="header" href="#クラス階層-1">クラス階層</a></h3>
<pre class="mermaid">graph TD
    KVM["KVCacheManager&lt;br&gt;公開 API"]
    Factory["get_kv_cache_coordinator()&lt;br&gt;ファクトリ関数"]

    NPC["NoPrefixCache&lt;br&gt;キャッシュ無効時"]
    UC["UnitaryCoordinator&lt;br&gt;単一グループ"]
    HC["HybridCoordinator&lt;br&gt;複数グループ"]

    FAM["FullAttentionManager"]
    SWM["SlidingWindowManager"]
    CLM["ChunkedLocalManager"]
    MM["MambaManager"]
    CAM["CrossAttentionManager"]
    SFM["SinkFullAttentionManager"]

    BP["BlockPool&lt;br&gt;物理ブロック管理"]
    BH["BlockHashToBlockMap"]
    FQ["FreeKVCacheBlockQueue"]
    KB["KVCacheBlock"]

    KVM --&gt; Factory
    Factory --&gt; NPC
    Factory --&gt; UC
    Factory --&gt; HC

    UC --&gt; FAM
    UC --&gt; SWM
    HC --&gt; FAM
    HC --&gt; SWM
    HC --&gt; CLM
    HC --&gt; MM
    HC --&gt; CAM
    HC --&gt; SFM

    FAM --&gt; BP
    SWM --&gt; BP
    CLM --&gt; BP
    MM --&gt; BP
    CAM --&gt; BP
    SFM --&gt; BP

    BP --&gt; BH
    BP --&gt; FQ
    FQ --&gt; KB
</pre>

<h3 id="coordinator-選択ロジック"><a class="header" href="#coordinator-選択ロジック">Coordinator 選択ロジック</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_coordinator.py:542</code></p>
<pre><code>get_kv_cache_coordinator()
  ├─ enable_caching == False  → KVCacheCoordinatorNoPrefixCache
  ├─ kv_cache_groups == 1     → UnitaryKVCacheCoordinator
  └─ kv_cache_groups &gt; 1      → HybridKVCacheCoordinator
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Coordinator</th><th>用途</th><th>find_longest_cache_hit</th></tr>
</thead>
<tbody>
<tr><td><code>NoPrefixCache</code></td><td>キャッシュ無効</td><td>空リスト、0 トークン</td></tr>
<tr><td><code>Unitary</code></td><td>単一アテンションタイプ</td><td>単一 Manager に委譲</td></tr>
<tr><td><code>Hybrid</code></td><td>複数アテンションタイプ</td><td>反復固定点アルゴリズム</td></tr>
</tbody>
</table>
</div>
<h3 id="kv-キャッシュグループ"><a class="header" href="#kv-キャッシュグループ">KV キャッシュグループ</a></h3>
<p><strong>KV キャッシュグループ</strong>とは、同一の <code>KVCacheSpec</code>（アテンションタイプ・ブロックサイズ）を共有するモデルレイヤーの集合である。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>モデル例</th><th>グループ構成</th></tr>
</thead>
<tbody>
<tr><td>全層 Full Attention</td><td>1 グループ</td></tr>
<tr><td>12 層 Full + 12 層 Sliding Window</td><td>2 グループ</td></tr>
<tr><td>デコーダ + クロスアテンション</td><td>2-3 グループ</td></tr>
</tbody>
</table>
</div>
<p>各グループは独立した <code>SingleTypeKVCacheManager</code> を持ち、異なるキャッシュ検索アルゴリズム・スキップポリシーを適用する。</p>
<h2 id="kvcacheblocks-deep-verified"><a class="header" href="#kvcacheblocks-deep-verified">KVCacheBlocks [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_manager.py:21</code></p>
<p>Scheduler と KVCacheManager のインターフェース。内部データ構造を隠蔽する。</p>
<pre><code class="language-python">@dataclass
class KVCacheBlocks:
    blocks: tuple[Sequence[KVCacheBlock], ...]
    # blocks[i][j] = i 番目の kv_cache_group、j 番目のブロック
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>__add__</code></td><td>2 つの KVCacheBlocks を結合</td></tr>
<tr><td><code>get_block_ids()</code></td><td><code>tuple[list[int], ...]</code> に変換（GPU カーネル用）</td></tr>
<tr><td><code>get_unhashed_block_ids()</code></td><td>未ハッシュブロックの ID リスト（ドラフトトークン用）</td></tr>
<tr><td><code>new_empty()</code></td><td>空の KVCacheBlocks を生成</td></tr>
</tbody>
</table>
</div>
<p><strong>GC 最適化</strong>: <code>KVCacheManager</code> は <code>empty_kv_cache_blocks</code> を事前生成し、空の結果を返す際に再利用する。</p>
<h3 id="ブロック配置図allocate_slots"><a class="header" href="#ブロック配置図allocate_slots">ブロック配置図（allocate_slots）</a></h3>
<p><code>allocate_slots()</code> がリクエストに割り当てるブロックの論理構造:</p>
<pre><code>|  comp  | new_comp | ext_comp |   new   | lookahead |
|&lt;------ 既計算トークン ------&gt;|&lt;-- 新規計算対象 --&gt;|
                               |&lt;- 割り当て対象 -&gt;|
</code></pre>
<ul>
<li><code>comp</code>: <code>request.num_computed_tokens</code> — 前ステップまでに計算済み</li>
<li><code>new_comp</code>: <code>num_new_computed_tokens</code> — プレフィックスキャッシュから新規にヒットしたトークン</li>
<li><code>ext_comp</code>: <code>num_external_computed_tokens</code> — KV コネクタ（LMCache 等）から取得したトークン</li>
<li><code>new</code>: <code>num_new_tokens</code> — 今回計算するトークン</li>
<li><code>lookahead</code>: <code>num_lookahead_tokens</code> — Speculative Decoding 用の先読みトークン</li>
</ul>
<h2 id="主要コンポーネント-5"><a class="header" href="#主要コンポーネント-5">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>用途</th><th>ファイル</th></tr>
</thead>
<tbody>
<tr><td><code>KVCacheManager</code></td><td>Scheduler 向け公開 API</td><td><code>target/vllm/vllm/v1/core/kv_cache_manager.py:94</code></td></tr>
<tr><td><code>KVCacheCoordinator</code></td><td>マルチグループ統括（3 実装）</td><td><code>target/vllm/vllm/v1/core/kv_cache_coordinator.py:28</code></td></tr>
<tr><td><code>SingleTypeKVCacheManager</code></td><td>アテンションタイプ別管理（7 実装）</td><td><code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:24</code></td></tr>
<tr><td><code>BlockPool</code></td><td>物理ブロック割り当て・解放・キャッシュ管理</td><td><code>target/vllm/vllm/v1/core/block_pool.py:128</code></td></tr>
<tr><td><code>KVCacheBlock</code></td><td>ブロックメタデータ（block_id, ref_cnt, block_hash）</td><td><code>target/vllm/vllm/v1/core/kv_cache_utils.py:107</code></td></tr>
<tr><td><code>BlockHashToBlockMap</code></td><td>プレフィックスキャッシュ用ハッシュ→ブロック対応表</td><td><code>target/vllm/vllm/v1/core/block_pool.py:32</code></td></tr>
<tr><td><code>FreeKVCacheBlockQueue</code></td><td>LRU 順序の空きブロック管理（双方向リンクリスト）</td><td><code>target/vllm/vllm/v1/core/kv_cache_utils.py:156</code></td></tr>
</tbody>
</table>
</div>
<h2 id="主要メソッド-6"><a class="header" href="#主要メソッド-6">主要メソッド</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>allocate_slots()</code></td><td>L206</td><td>リクエストに KV キャッシュブロックを割り当て。成功時 <code>KVCacheBlocks</code>、失敗時 <code>None</code></td></tr>
<tr><td><code>get_computed_blocks()</code></td><td>L164</td><td>プレフィックスキャッシュから最長ヒットを検索。<code>(KVCacheBlocks, int)</code></td></tr>
<tr><td><code>free()</code></td><td>L378</td><td>リクエストのブロックをプールに返却</td></tr>
<tr><td><code>usage</code> (property)</td><td>L143</td><td>KV キャッシュ使用率 (0.0-1.0)</td></tr>
<tr><td><code>reset_prefix_cache()</code></td><td>L409</td><td>プレフィックスキャッシュ全体をリセット</td></tr>
<tr><td><code>get_num_common_prefix_blocks()</code></td><td>L425</td><td>全リクエスト共通の先頭ブロック数（Cascade Attention 用）</td></tr>
<tr><td><code>cache_blocks()</code></td><td>L475</td><td>ブロックをプレフィックスキャッシュに登録</td></tr>
</tbody>
</table>
</div>
<h2 id="allocate_slots-の-5-段階フロー-deep-verified"><a class="header" href="#allocate_slots-の-5-段階フロー-deep-verified">allocate_slots() の 5 段階フロー [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_manager.py:206</code></p>
<pre><code>allocate_slots(request, num_new_tokens, ...)
  │
  ├─ Stage 1: スキップブロック解放（Sliding Window 用）
  │  └─ coordinator.remove_skipped_blocks(request_id, total_computed_tokens)
  │
  ├─ Stage 2: 容量チェック
  │  ├─ coordinator.get_num_blocks_to_allocate(...)
  │  └─ 空きブロック不足 → return None（プリエンプション誘発）
  │
  ├─ Stage 3: キャッシュヒットブロック割り当て
  │  └─ new_computed_blocks 非空 or external_computed &gt; 0:
  │     └─ coordinator.allocate_new_computed_blocks(...)
  │
  ├─ Stage 4: 新規ブロック割り当て
  │  └─ coordinator.allocate_new_blocks(request_id, num_tokens_need_slot, ...)
  │
  └─ Stage 5: キャッシュ登録判定
     ├─ NOT enable_caching or delay_cache_blocks → スキップ
     └─ coordinator.cache_blocks(request, num_tokens_to_cache)
        ※ num_tokens_to_cache はドラフトトークンを除外
</code></pre>
<p><strong>delay_cache_blocks</strong>: P/D（Prefill/Decode 分離）構成で KV Transfer 完了前にキャッシュ登録を遅延する。</p>
<h2 id="プレフィックスキャッシュ"><a class="header" href="#プレフィックスキャッシュ">プレフィックスキャッシュ</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_manager.py:164</code> (get_computed_blocks)</p>
<p>プロンプトトークン列をブロックサイズ単位でハッシュ化し、<code>BlockHashToBlockMap</code> で過去に計算済みのブロックを検索する。ハッシュチェーン（各ブロックのハッシュが前ブロックのハッシュに依存）により、プレフィックスの最長一致を効率的に検索する。</p>
<pre><code>get_computed_blocks(request)
  → coordinator.find_longest_cache_hit(request.block_hashes, max_length)
    → アテンションタイプ別の検索アルゴリズム
  → (キャッシュ済みブロック, ヒットトークン数) を返却
</code></pre>
<p><strong>制約</strong>: 全トークンがキャッシュヒットしても、logits 取得のため最後の 1 トークンは再計算が必要（<code>max_cache_hit_length = request.num_tokens - 1</code>）。</p>
<p>→ 詳細は <a href="#プレフィックスキャッシュ詳細">プレフィックスキャッシュ詳細</a> を参照</p>
<h2 id="参照カウントと-eviction"><a class="header" href="#参照カウントと-eviction">参照カウントと Eviction</a></h2>
<p><strong>KVCacheBlock</strong> (L107) の <code>ref_cnt</code> フィールドでブロックの使用状況を管理:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ref_cnt</th><th>状態</th></tr>
</thead>
<tbody>
<tr><td>0</td><td>空きブロックキュー（<code>FreeKVCacheBlockQueue</code>）内。Eviction 候補</td></tr>
<tr><td>≥ 1</td><td>リクエストに使用中。Eviction 対象外</td></tr>
</tbody>
</table>
</div>
<ul>
<li><strong>touch()</strong>: キャッシュヒット時に <code>ref_cnt</code> を増加し、空きキューから除外</li>
<li><strong>free_blocks()</strong>: <code>ref_cnt</code> を減少。0 になったら空きキューに戻す（逆順追加で LRU 効率化）</li>
<li><strong>_maybe_evict_cached_block()</strong>: 新規ブロック要求時に空きキューの先頭（最古）から Evict。ハッシュメタデータをリセット</li>
</ul>
<p>→ 詳細は <a href="#blockpool-詳細">BlockPool 詳細</a> を参照</p>
<h2 id="アテンションタイプ対応"><a class="header" href="#アテンションタイプ対応">アテンションタイプ対応</a></h2>
<p>7 種の <code>SingleTypeKVCacheManager</code> がアテンションタイプごとのブロック管理を担当:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Manager</th><th>KVCacheSpec</th><th>スキップ計算</th><th>キャッシュ検索</th></tr>
</thead>
<tbody>
<tr><td><code>FullAttentionManager</code></td><td>FullAttention / MLA</td><td>0</td><td>左→右</td></tr>
<tr><td><code>SlidingWindowManager</code></td><td>SlidingWindow</td><td><code>max(0, n-w+1)</code></td><td>右→左（連続）</td></tr>
<tr><td><code>ChunkedLocalAttentionManager</code></td><td>ChunkedLocal</td><td><code>(n//c)*c</code></td><td>null_pad + 左→右</td></tr>
<tr><td><code>MambaManager</code></td><td>Mamba</td><td><code>n - 1</code></td><td>右→左（単一）</td></tr>
<tr><td><code>CrossAttentionManager</code></td><td>CrossAttention</td><td>N/A</td><td>非対応</td></tr>
<tr><td><code>SinkFullAttentionManager</code></td><td>SinkFullAttention</td><td>0</td><td>左→右</td></tr>
</tbody>
</table>
</div>
<p>→ 詳細は <a href="#アテンションタイプ別-manager-詳細">アテンションタイプ別 Manager</a> を参照</p>
<h2 id="設定-4"><a class="header" href="#設定-4">設定</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>block_size</code></td><td>モデル依存</td><td>1 ブロックあたりのトークン数</td></tr>
<tr><td><code>enable_caching</code></td><td>設定依存</td><td>プレフィックスキャッシュの有効化</td></tr>
<tr><td><code>num_gpu_blocks</code></td><td>プロファイリングで決定</td><td>GPU メモリから算出される総ブロック数</td></tr>
<tr><td><code>hash_block_size</code></td><td>block_size と同値</td><td>ハッシュ計算に使用するブロックサイズ</td></tr>
<tr><td><code>prefix_caching_hash_algo</code></td><td><code>sha256_cbor</code></td><td>ハッシュ関数（sha256/sha256_cbor/xxhash/xxhash_cbor）</td></tr>
<tr><td><code>enable_kv_cache_events</code></td><td>False</td><td>KV Transfer 用イベント発行</td></tr>
</tbody>
</table>
</div>
<h2 id="呼び出しフロー-4"><a class="header" href="#呼び出しフロー-4">呼び出しフロー</a></h2>
<pre><code>Scheduler.schedule()
  ├─ kv_cache_manager.get_computed_blocks(request)     # プレフィックスキャッシュ検索
  ├─ kv_cache_manager.allocate_slots(request, ...)     # ブロック割り当て
  │   └─ None の場合 → プリエンプション実行
  └─ （完了時）kv_cache_manager.free(request)           # ブロック解放
</code></pre>
<h2 id="ソースファイル一覧"><a class="header" href="#ソースファイル一覧">ソースファイル一覧</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>行数</th><th>内容</th></tr>
</thead>
<tbody>
<tr><td><code>kv_cache_manager.py</code></td><td>490</td><td>KVCacheManager、KVCacheBlocks</td></tr>
<tr><td><code>kv_cache_coordinator.py</code></td><td>586</td><td>Coordinator 3 実装</td></tr>
<tr><td><code>single_type_kv_cache_manager.py</code></td><td>1065</td><td>Manager 7 種</td></tr>
<tr><td><code>block_pool.py</code></td><td>490</td><td>BlockPool、BlockHashToBlockMap</td></tr>
<tr><td><code>kv_cache_utils.py</code></td><td>1644</td><td>KVCacheBlock、Queue、ハッシュ計算</td></tr>
<tr><td><code>kv_cache_metrics.py</code></td><td>96</td><td>メトリクス収集</td></tr>
</tbody>
</table>
</div>
<h2 id="詳細ドキュメント"><a class="header" href="#詳細ドキュメント">詳細ドキュメント</a></h2>
<ul>
<li><strong><a href="#blockpool-詳細">BlockPool 詳細</a></strong> — FreeKVCacheBlockQueue、BlockHashToBlockMap、KVCacheBlock ライフサイクル、Eviction、KV Cache Events</li>
<li><strong><a href="#プレフィックスキャッシュ詳細">プレフィックスキャッシュ詳細</a></strong> — ハッシュチェーン計算、Extra Keys、Lookup アルゴリズム、Hybrid fixed-point</li>
<li><strong><a href="#アテンションタイプ別-manager-詳細">アテンションタイプ別 Manager</a></strong> — 7 種 Manager の詳細、スキップ計算、キャッシュ検索アルゴリズム</li>
</ul>
<h2 id="関連ドキュメント-4"><a class="header" href="#関連ドキュメント-4">関連ドキュメント</a></h2>
<ul>
<li><a href="#scheduler-サマリー">Scheduler</a></li>
<li><a href="#enginecore-サマリー">EngineCore</a></li>
<li><a href="#テキスト推論データフロー">データフロー</a></li>
<li><a href="#用語集">用語集</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="アテンションタイプ別-manager-詳細"><a class="header" href="#アテンションタイプ別-manager-詳細">アテンションタイプ別 Manager 詳細</a></h1>
<blockquote>
<p><strong>深度</strong>: [DEEP]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-13"><a class="header" href="#概要-13">概要</a></h2>
<p><code>SingleTypeKVCacheManager</code> は1種類のアテンションタイプの KV キャッシュ管理ロジックを担当する抽象基底クラスである。アテンションタイプごとにサブクラスが存在し、ブロックの割り当て・解放・プレフィックスキャッシュ検索をそれぞれのセマンティクスに合わせて実装する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py</code></p>
<h2 id="spec_manager_map-deep-verified"><a class="header" href="#spec_manager_map-deep-verified">spec_manager_map [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:1049</code></p>
<pre><code class="language-python">spec_manager_map = {
    FullAttentionSpec:            FullAttentionManager,
    MLAAttentionSpec:             FullAttentionManager,       # MLA も Full 扱い
    SlidingWindowSpec:            SlidingWindowManager,
    ChunkedLocalAttentionSpec:    ChunkedLocalAttentionManager,
    MambaSpec:                    MambaManager,
    CrossAttentionSpec:           CrossAttentionManager,
    SinkFullAttentionSpec:        SinkFullAttentionManager,
}
</code></pre>
<p><code>get_manager_for_kv_cache_spec(spec, **kwargs)</code> ファクトリ関数がこのマップから Manager クラスをディスパッチする。</p>
<h2 id="基底クラス-singletypekvcachemanager-deep-verified"><a class="header" href="#基底クラス-singletypekvcachemanager-deep-verified">基底クラス: SingleTypeKVCacheManager [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:24</code></p>
<h3 id="状態管理-1"><a class="header" href="#状態管理-1">状態管理</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>req_to_blocks</code></td><td><code>defaultdict[str, list[KVCacheBlock]]</code></td><td>リクエスト ID → 割り当て済みブロックリスト</td></tr>
<tr><td><code>num_cached_block</code></td><td><code>dict[str, int]</code></td><td>リクエスト ID → キャッシュ登録済みブロック数。RUNNING リクエストのみ追跡</td></tr>
<tr><td><code>block_size</code></td><td><code>int</code></td><td>1 ブロックあたりのトークン数。DCP/PCP &gt; 1 の場合は乗算される</td></tr>
</tbody>
</table>
</div>
<h3 id="コンストラクタ"><a class="header" href="#コンストラクタ">コンストラクタ</a></h3>
<pre><code class="language-python">def __init__(self, kv_cache_spec, block_pool, enable_caching,
             kv_cache_group_id, dcp_world_size=1, pcp_world_size=1):
    self.block_size = kv_cache_spec.block_size
    if dcp_world_size * pcp_world_size &gt; 1:
        self.block_size *= dcp_world_size * pcp_world_size
</code></pre>
<p>DCP（Decode Context Parallelism）/ PCP（Prefill Context Parallelism）ではブロックサイズが並列度倍に拡大される。</p>
<h3 id="get_num_blocks_to_allocate-deep-verified"><a class="header" href="#get_num_blocks_to_allocate-deep-verified">get_num_blocks_to_allocate() [DEEP] [VERIFIED]</a></h3>
<p><strong>参照</strong>: L73</p>
<p>リクエストに必要な新規ブロック数を算出する。2 つのパスが存在:</p>
<pre><code>get_num_blocks_to_allocate(request_id, num_tokens, new_computed_blocks, ...)
  │
  ├─ Fast-path: request_id in num_cached_block（RUNNING リクエスト）
  │  └─ max(num_required_blocks - num_req_blocks, 0)
  │     ※ Speculative Decoding のリジェクトで num_req_blocks &gt; num_required_blocks もあり得る
  │
  └─ Slow-path: 新規リクエスト（プレフィックスキャッシュヒットあり）
     ├─ num_skipped_tokens = get_num_skipped_tokens(total_computed_tokens)
     ├─ num_skipped_blocks = num_skipped_tokens // block_size
     ├─ num_new_blocks = max(required - max(skipped, local_computed), 0)
     ├─ num_evictable_blocks = Σ(ref_cnt==0 かつ非null)
     │  ← touch() 時にキューから除去されるブロック分を加算
     └─ return num_new_blocks + num_evictable_blocks
</code></pre>
<p><strong>Evictable blocks の加算理由</strong>: <code>new_computed_blocks</code> 内のブロックが空きキュー内（<code>ref_cnt == 0</code>）にある場合、<code>touch()</code> で空きキューから除去されるため、実質的に空きブロック数が減る。この分を事前に計上する。</p>
<h3 id="allocate_new_computed_blocks-deep-verified"><a class="header" href="#allocate_new_computed_blocks-deep-verified">allocate_new_computed_blocks() [DEEP] [VERIFIED]</a></h3>
<p><strong>参照</strong>: L137</p>
<p>プレフィックスキャッシュヒットしたブロックをリクエストに追加する。</p>
<pre><code>allocate_new_computed_blocks(request_id, new_computed_blocks, ...)
  │
  ├─ RUNNING → assert len(new_computed_blocks) == 0 → return
  │
  └─ 新規リクエスト:
     ├─ num_skipped_blocks 計算
     ├─ スキップ分を new_computed_blocks から除去
     ├─ enable_caching → block_pool.touch(new_computed_blocks)
     ├─ req_blocks に null_block × num_skipped_blocks を追加
     ├─ req_blocks に new_computed_blocks を追加
     ├─ num_cached_block[request_id] = len(req_blocks)
     └─ external_computed_tokens &gt; 0 → 追加ブロック割り当て
</code></pre>
<h3 id="allocate_new_blocks-verified"><a class="header" href="#allocate_new_blocks-verified">allocate_new_blocks() [VERIFIED]</a></h3>
<p><strong>参照</strong>: L208</p>
<pre><code class="language-python">def allocate_new_blocks(self, request_id, num_tokens, num_tokens_main_model):
    num_required_blocks = cdiv(num_tokens, self.block_size)
    num_new_blocks = num_required_blocks - len(req_blocks)
    if num_new_blocks &lt;= 0:
        return []
    new_blocks = self.block_pool.get_new_blocks(num_new_blocks)
    req_blocks.extend(new_blocks)
    return new_blocks  # 新規分のみ返す
</code></pre>
<h3 id="cache_blocks-verified"><a class="header" href="#cache_blocks-verified">cache_blocks() [VERIFIED]</a></h3>
<p><strong>参照</strong>: L235</p>
<pre><code class="language-python">def cache_blocks(self, request, num_tokens):
    num_full_blocks = num_tokens // self.block_size
    if num_cached_blocks &gt;= num_full_blocks:
        return  # 既に登録済み
    block_pool.cache_full_blocks(request, req_blocks, num_cached_blocks,
                                  num_full_blocks, block_size, group_id)
    num_cached_block[request_id] = num_full_blocks
</code></pre>
<h3 id="free-verified"><a class="header" href="#free-verified">free() [VERIFIED]</a></h3>
<p><strong>参照</strong>: L261</p>
<pre><code class="language-python">def free(self, request_id):
    req_blocks = self.req_to_blocks.pop(request_id, [])
    ordered_blocks = reversed(req_blocks)  # 逆順でLRU最適化
    self.block_pool.free_blocks(ordered_blocks)
    self.num_cached_block.pop(request_id, None)
</code></pre>
<p><strong>逆順の理由</strong>: チェーン末尾（最新トークン）がキューの先頭側に来ることで、次の割り当て時に最初に evict される。先頭ブロック（プレフィックス）は長く残り、共有確率が上がる。</p>
<h3 id="remove_skipped_blocks-verified"><a class="header" href="#remove_skipped_blocks-verified">remove_skipped_blocks() [VERIFIED]</a></h3>
<p><strong>参照</strong>: L343</p>
<pre><code class="language-python">def remove_skipped_blocks(self, request_id, total_computed_tokens):
    num_skipped_tokens = self.get_num_skipped_tokens(total_computed_tokens)
    if num_skipped_tokens &lt;= 0:
        return  # Full Attention: 何もしない
    # 後方から走査して null_block に遭遇したら停止
    for i in range(num_skipped_blocks - 1, -1, -1):
        if blocks[i] == null_block:
            break  # 前回の呼び出しで既に解放済み
        removed_blocks.append(blocks[i])
        blocks[i] = null_block
    block_pool.free_blocks(removed_blocks)
</code></pre>
<h3 id="get_num_skipped_tokens-verified"><a class="header" href="#get_num_skipped_tokens-verified">get_num_skipped_tokens() [VERIFIED]</a></h3>
<p><strong>参照</strong>: L386</p>
<p>デフォルト実装は <code>return 0</code>（全トークンがアテンション対象）。サブクラスでオーバーライド。</p>
<h2 id="fullattentionmanager-deep-verified"><a class="header" href="#fullattentionmanager-deep-verified">FullAttentionManager [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:400</code></p>
<p>標準的な全トークンアテンション。基底クラスの動作をそのまま継承し、2 つのメソッドのみ実装。</p>
<h3 id="find_longest_cache_hit"><a class="header" href="#find_longest_cache_hit">find_longest_cache_hit()</a></h3>
<p><strong>参照</strong>: L401</p>
<pre><code>左→右にブロックハッシュを走査:
  キャッシュヒット → computed_blocks に追加
  キャッシュミス → break（チェーンが途切れたら以降は必ずミス）

EAGLE 使用時:
  最後のブロックを削除（hidden states 再計算が必要）

alignment_tokens でアライメント調整:
  Hybrid モデルで LCM ブロックサイズの倍数に切り詰め
</code></pre>
<p><strong>Downward-closed 性質</strong>: Full Attention では blocks[0..n] がヒットするなら blocks[0..n-1] も必ずヒットする。この性質により、左→右の貪欲スキャンで最適解が得られる。</p>
<h3 id="get_num_common_prefix_blocks"><a class="header" href="#get_num_common_prefix_blocks">get_num_common_prefix_blocks()</a></h3>
<p><strong>参照</strong>: L450</p>
<pre><code class="language-python">def get_num_common_prefix_blocks(self, running_request_id):
    for block in blocks:
        if block.ref_cnt == len(self.req_to_blocks):
            num_common_blocks += 1
        else:
            break
    return num_common_blocks
</code></pre>
<p><strong>原理</strong>: <code>ref_cnt == 全リクエスト数</code> なら、そのブロックは全リクエストで共有されている → 共通プレフィックス。Cascade Attention で共通プレフィックスの再計算をスキップするために使用。</p>
<h2 id="slidingwindowmanager-deep-verified"><a class="header" href="#slidingwindowmanager-deep-verified">SlidingWindowManager [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:461</code></p>
<p>Sliding Window Attention 用。ウィンドウ外のトークンの KV キャッシュを解放してメモリを節約する。</p>
<h3 id="コンストラクタ-1"><a class="header" href="#コンストラクタ-1">コンストラクタ</a></h3>
<pre><code class="language-python">def __init__(self, kv_cache_spec: SlidingWindowSpec, **kwargs):
    super().__init__(kv_cache_spec, **kwargs)
    self.sliding_window = kv_cache_spec.sliding_window
</code></pre>
<h3 id="get_num_skipped_tokens"><a class="header" href="#get_num_skipped_tokens">get_num_skipped_tokens()</a></h3>
<p><strong>参照</strong>: L556</p>
<pre><code class="language-python">def get_num_skipped_tokens(self, num_computed_tokens):
    return max(0, num_computed_tokens - self.sliding_window + 1)
</code></pre>
<pre><code>例: sliding_window=4, num_computed_tokens=7

Tokens: [0 1 2 3 4 5 6 | 7]
                          ↑ 次に計算するトークン
                  [4 5 6 7]  ← sliding window（サイズ4）
        [0 1 2 3]            ← skipped（4トークン）
</code></pre>
<h3 id="find_longest_cache_hit-1"><a class="header" href="#find_longest_cache_hit-1">find_longest_cache_hit()</a></h3>
<p><strong>参照</strong>: L466</p>
<pre><code>右→左にブロックハッシュを走査:
  キャッシュヒット → computed_blocks[i] にセット、連続カウント++
  キャッシュミス → 連続カウント = 0（リセット）

  連続カウント &gt;= sliding_window_contiguous_blocks:
    末尾をトリミングして break

sliding_window_contiguous_blocks = ceil((window - 1) / block_size)
</code></pre>
<p><strong>Right-to-left の理由</strong>: Sliding Window は最新のトークン付近のブロックが重要。右端から連続ヒットを探すことで、ウィンドウ内の有用なキャッシュを効率的に発見する。</p>
<p><strong>初期値</strong>: <code>computed_blocks</code> は <code>null_block</code> で埋められ、ヒットした位置のみ実ブロックで置換される。</p>
<p><strong>制約事項</strong>:</p>
<ul>
<li>DCP/PCP 非対応 (<code>assert dcp_world_size == 1</code>)</li>
<li>EAGLE 使用時は <code>sliding_window_contiguous_blocks += 1</code></li>
</ul>
<h3 id="get_num_common_prefix_blocks-1"><a class="header" href="#get_num_common_prefix_blocks-1">get_num_common_prefix_blocks()</a></h3>
<p><strong>参照</strong>: L584</p>
<p>常に <code>0</code> を返す。プレフィックスブロックは全て null_block に置換されているため、Cascade Attention は使用不可。</p>
<h2 id="chunkedlocalattentionmanager-deep-verified"><a class="header" href="#chunkedlocalattentionmanager-deep-verified">ChunkedLocalAttentionManager [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:594</code></p>
<p>チャンク境界でアテンションが分割されるモデル用。各チャンク内のトークンのみが互いにアテンションする。</p>
<h3 id="コンストラクタ-2"><a class="header" href="#コンストラクタ-2">コンストラクタ</a></h3>
<pre><code class="language-python">def __init__(self, kv_cache_spec: ChunkedLocalAttentionSpec, **kwargs):
    super().__init__(kv_cache_spec, **kwargs)
    self.attention_chunk_size = kv_cache_spec.attention_chunk_size
</code></pre>
<h3 id="get_num_skipped_tokens-1"><a class="header" href="#get_num_skipped_tokens-1">get_num_skipped_tokens()</a></h3>
<p><strong>参照</strong>: L691</p>
<pre><code class="language-python">def get_num_skipped_tokens(self, num_computed_tokens):
    return (num_computed_tokens // self.attention_chunk_size) * self.attention_chunk_size
</code></pre>
<pre><code>例1: chunk_size=8, computed=13 → skipped=8  (チャンク[0,7]全体)
例2: chunk_size=8, computed=8  → skipped=8  (チャンク[0,7]全体)
例3: chunk_size=8, computed=7  → skipped=0  (まだチャンク内)
</code></pre>
<h3 id="find_longest_cache_hit-2"><a class="header" href="#find_longest_cache_hit-2">find_longest_cache_hit()</a></h3>
<p><strong>参照</strong>: L599</p>
<pre><code>1. local_attention_start_idx = (max_length // chunk_size) * chunk_size
2. computed_blocks = [null_block] × (start_idx // block_size)  ← ウィンドウ外
3. start_idx から max_num_blocks まで左→右スキャン:
   ヒット → append、ミス → break
</code></pre>
<p>ウィンドウ外のブロックは null_block でパディングし、ウィンドウ内のみ FullAttention と同様の左→右スキャンを行う。</p>
<p><strong>制約事項</strong>:</p>
<ul>
<li>EAGLE 非対応 (<code>assert use_eagle is False</code>)</li>
<li>DCP/PCP 非対応</li>
<li>異なるブロックサイズの混在非対応</li>
</ul>
<h2 id="mambamanager-deep-verified"><a class="header" href="#mambamanager-deep-verified">MambaManager [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:744</code></p>
<p>Mamba（State Space Model / 線形アテンション）用の Manager。Transformer ベースのアテンションとは根本的に異なり、KV キャッシュではなく「状態」を管理する。</p>
<h3 id="2つのキャッシュモード"><a class="header" href="#2つのキャッシュモード">2つのキャッシュモード</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>モード</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>none</code>（デフォルト）</td><td>基底クラスの動作 + speculative blocks 追加</td></tr>
<tr><td><code>align</code></td><td>最後の状態ブロックのみ追跡、null_block パディング、speculative blocks 再利用</td></tr>
</tbody>
</table>
</div>
<h3 id="追加状態align-モード"><a class="header" href="#追加状態align-モード">追加状態（align モード）</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>last_state_block_idx</code></td><td><code>dict[str, int]</code></td><td>前ステップで割り当てた状態ブロックのインデックス</td></tr>
<tr><td><code>_allocated_block_reqs</code></td><td><code>set[str]</code></td><td>ブロック割り当て済みリクエストの集合</td></tr>
<tr><td><code>num_speculative_blocks</code></td><td><code>int</code></td><td>Speculative Decoding 用の余分なブロック数</td></tr>
</tbody>
</table>
</div>
<h3 id="get_num_skipped_tokens-2"><a class="header" href="#get_num_skipped_tokens-2">get_num_skipped_tokens()</a></h3>
<p><strong>参照</strong>: L967</p>
<pre><code class="language-python">def get_num_skipped_tokens(self, num_computed_tokens):
    return num_computed_tokens - 1  # 最後の状態のみ必要
</code></pre>
<p>Mamba は状態の累積的な更新なので、最後のトークンの状態さえあれば以前のトークンの状態は不要。</p>
<h3 id="find_longest_cache_hit-3"><a class="header" href="#find_longest_cache_hit-3">find_longest_cache_hit()</a></h3>
<p><strong>参照</strong>: L756</p>
<pre><code>右→左に走査:
  最初のヒットで即座に break
  ヒット位置の前を null_block で埋める
</code></pre>
<p>最後の状態のみ必要なため、最右のヒット 1 つで十分。</p>
<h3 id="get_num_blocks_to_allocatealign-モード-deep-verified"><a class="header" href="#get_num_blocks_to_allocatealign-モード-deep-verified">get_num_blocks_to_allocate()（align モード） [DEEP] [VERIFIED]</a></h3>
<p><strong>参照</strong>: L832</p>
<pre><code>align モード:
  ├─ 既存リクエスト（_allocated_block_reqs に存在）:
  │  └─ 最大 1 ブロック追加（speculative blocks 再利用のため）
  │
  └─ 新規リクエスト:
     └─ 1 + num_speculative_blocks ブロック
</code></pre>
<h3 id="allocate_new_blocksalign-モード-deep-verified"><a class="header" href="#allocate_new_blocksalign-モード-deep-verified">allocate_new_blocks()（align モード） [DEEP] [VERIFIED]</a></h3>
<p><strong>参照</strong>: L885</p>
<p>align モードの割り当ては複雑:</p>
<pre><code>1. num_tokens を main model 分に制限（lookahead 除外）
2. last_state_block_idx を記録:
   - 既存: prev_len - 1 - num_speculative_blocks
   - 新規（キャッシュヒット有）: prev_len - 1
3. null_block でスキップ位置をパディング
4. 既存リクエスト: speculative blocks をスキップ位置に移動して再利用
5. 残りの新規ブロックを割り当て
</code></pre>
<h3 id="remove_skipped_blocksalign-モード"><a class="header" href="#remove_skipped_blocksalign-モード">remove_skipped_blocks()（align モード）</a></h3>
<p><strong>参照</strong>: L804</p>
<p>基底クラスの <code>remove_skipped_blocks()</code> に加え、<code>last_state_block_idx</code> のブロックも解放する:</p>
<pre><code class="language-python">if last_state_block_idx &lt; cdiv(num_computed_tokens, block_size) - 1:
    block_pool.free_blocks([blocks[last_state_block_idx]])
    blocks[last_state_block_idx] = null_block
</code></pre>
<p>2 ステップ前のブロックが不要になるタイミングで解放する。</p>
<h3 id="free"><a class="header" href="#free">free()</a></h3>
<p><strong>参照</strong>: L961</p>
<pre><code class="language-python">def free(self, request_id):
    if self.mamba_cache_mode == "align":
        self._allocated_block_reqs.discard(request_id)
        self.last_state_block_idx.pop(request_id, None)
    super().free(request_id)
</code></pre>
<h2 id="crossattentionmanager-deep-verified"><a class="header" href="#crossattentionmanager-deep-verified">CrossAttentionManager [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:976</code></p>
<p>エンコーダ-デコーダモデル（Whisper 等）のクロスアテンション用。エンコーダ出力はリクエスト固有（異なる音声/画像入力）のため、プレフィックスキャッシュの恩恵がない。</p>
<h3 id="制約"><a class="header" href="#制約">制約</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>動作</th></tr>
</thead>
<tbody>
<tr><td><code>allocate_new_computed_blocks()</code></td><td><code>assert len(new_computed_blocks) == 0</code></td></tr>
<tr><td><code>cache_blocks()</code></td><td><code>raise ValueError</code></td></tr>
<tr><td><code>find_longest_cache_hit()</code></td><td><code>raise NotImplementedError</code></td></tr>
<tr><td><code>get_num_common_prefix_blocks()</code></td><td><code>return 0</code></td></tr>
</tbody>
</table>
</div>
<p>エンコーダブロックはリクエスト開始時に <code>num_encoder_tokens</code> に基づいて静的に割り当てられ、デコード中は変化しない。</p>
<h2 id="sinkfullattentionmanager-deep-verified"><a class="header" href="#sinkfullattentionmanager-deep-verified">SinkFullAttentionManager [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:1025</code></p>
<p>StreamingLLM のための Attention Sink 実装。<code>FullAttentionManager</code> を継承し、初期化時に先頭の sink ブロックを事前確保する。</p>
<h3 id="コンストラクタ-3"><a class="header" href="#コンストラクタ-3">コンストラクタ</a></h3>
<pre><code class="language-python">class SinkFullAttentionManager(FullAttentionManager):
    def __init__(self, kv_cache_spec: SinkFullAttentionSpec, ...):
        super().__init__(...)
        sink_len = kv_cache_spec.sink_len
        assert sink_len &gt; 0 and sink_len % self.block_size == 0
        num_sink_block = sink_len // self.block_size
        self.sink_blocks = self.block_pool.free_block_queue.popleft_n(num_sink_block)
</code></pre>
<p><strong>特徴</strong>:</p>
<ul>
<li><code>sink_len</code> は <code>block_size</code> の倍数でなければならない</li>
<li>sink ブロックは初期化時に <code>popleft_n()</code> で確保され、以降解放されない</li>
<li>FullAttentionManager の <code>find_longest_cache_hit()</code> と <code>get_num_common_prefix_blocks()</code> をそのまま使用</li>
</ul>
<h2 id="各-manager-の比較表-verified"><a class="header" href="#各-manager-の比較表-verified">各 Manager の比較表 [VERIFIED]</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Manager</th><th>スキップ計算</th><th>キャッシュ検索</th><th>Cascade</th><th>DCP/PCP</th><th>EAGLE</th></tr>
</thead>
<tbody>
<tr><td><strong>FullAttention</strong></td><td>0（全トークン）</td><td>左→右</td><td>ref_cnt 基準</td><td>対応</td><td>対応</td></tr>
<tr><td><strong>SlidingWindow</strong></td><td><code>max(0, n-w+1)</code></td><td>右→左（連続）</td><td>非対応</td><td>非対応</td><td>対応</td></tr>
<tr><td><strong>ChunkedLocal</strong></td><td><code>(n//c)*c</code></td><td>null_pad + 左→右</td><td>非対応</td><td>非対応</td><td>非対応</td></tr>
<tr><td><strong>Mamba</strong></td><td><code>n - 1</code></td><td>右→左（単一）</td><td>非対応</td><td>非対応</td><td>-</td></tr>
<tr><td><strong>CrossAttention</strong></td><td>0</td><td>非対応</td><td>非対応</td><td>-</td><td>-</td></tr>
<tr><td><strong>SinkFullAttention</strong></td><td>0</td><td>左→右</td><td>ref_cnt 基準</td><td>対応</td><td>対応</td></tr>
</tbody>
</table>
</div>
<h2 id="関連ドキュメント-5"><a class="header" href="#関連ドキュメント-5">関連ドキュメント</a></h2>
<ul>
<li><a href="#kvcachemanager-サマリー">KVCacheManager サマリー</a></li>
<li><a href="#blockpool-詳細">BlockPool 詳細</a></li>
<li><a href="#プレフィックスキャッシュ詳細">プレフィックスキャッシュ詳細</a></li>
<li><a href="#scheduler-サマリー">Scheduler</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="blockpool-詳細"><a class="header" href="#blockpool-詳細">BlockPool 詳細</a></h1>
<blockquote>
<p><strong>深度</strong>: [DEEP]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-14"><a class="header" href="#概要-14">概要</a></h2>
<p><code>BlockPool</code> はKVキャッシュの物理ブロックを管理するクラスである。ブロックの割り当て・解放・プレフィックスキャッシュ索引を一元管理し、LRU Eviction によるメモリ再利用を実現する。3つの内部データ構造（<code>FreeKVCacheBlockQueue</code>、<code>BlockHashToBlockMap</code>、<code>KVCacheBlock</code>）で構成される。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:128</code></p>
<h2 id="kvcacheblock-deep-verified"><a class="header" href="#kvcacheblock-deep-verified">KVCacheBlock [DEEP] [VERIFIED]</a></h2>
<p>KVキャッシュブロック1つのメタデータを保持する dataclass。物理メモリ自体は GPU 上にあり、このオブジェクトは CPU 側のメタデータのみを管理する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:107</code></p>
<h3 id="フィールド"><a class="header" href="#フィールド">フィールド</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>block_id</code></td><td><code>int</code></td><td>0 〜 <code>num_gpu_blocks - 1</code> の一意識別子</td></tr>
<tr><td><code>ref_cnt</code></td><td><code>int</code></td><td>参照カウント。0なら空きキュー内（Eviction候補）</td></tr>
<tr><td><code>_block_hash</code></td><td><code>BlockHashWithGroupId | None</code></td><td>プレフィックスキャッシュ用ハッシュキー。fullブロックでキャッシュ登録済みの場合のみ設定</td></tr>
<tr><td><code>prev_free_block</code></td><td><code>KVCacheBlock | None</code></td><td>空きキューの前ノードポインタ</td></tr>
<tr><td><code>next_free_block</code></td><td><code>KVCacheBlock | None</code></td><td>空きキューの次ノードポインタ</td></tr>
<tr><td><code>is_null</code></td><td><code>bool</code></td><td>null_block フラグ。True の場合は解放・Eviction 対象外</td></tr>
</tbody>
</table>
</div>
<h3 id="block_hash-プロパティ"><a class="header" href="#block_hash-プロパティ">block_hash プロパティ</a></h3>
<pre><code class="language-python">@block_hash.setter
def block_hash(self, block_hash: BlockHashWithGroupId):
    assert self.block_hash is None  # 二重設定を禁止
    self._block_hash = block_hash

def reset_hash(self):
    self._block_hash = None  # Eviction時にリセット
</code></pre>
<p><strong>制約</strong>: setter は <code>assert self.block_hash is None</code> で二重設定を防止する。ハッシュのリセットは <code>reset_hash()</code> のみで行う。これによりブロックのライフサイクルが「未設定 → 設定 → リセット → 再設定」の順序で制御される。</p>
<h3 id="ライフサイクル"><a class="header" href="#ライフサイクル">ライフサイクル</a></h3>
<pre><code>1. 生成: BlockPool.__init__() で全ブロック生成（ref_cnt=0）
2. 割り当て: get_new_blocks() → ref_cnt=1
3. キャッシュ登録: cache_full_blocks() → block_hash 設定
4. 再利用（キャッシュヒット）: touch() → ref_cnt++
5. 解放: free_blocks() → ref_cnt-- → 0なら空きキューへ
6. Eviction: _maybe_evict_cached_block() → hash リセット → 再割り当て
</code></pre>
<h2 id="freekvcacheblockqueue-deep-verified"><a class="header" href="#freekvcacheblockqueue-deep-verified">FreeKVCacheBlockQueue [DEEP] [VERIFIED]</a></h2>
<p>空きブロックを LRU 順序で管理する<strong>双方向リンクリスト</strong>。Python 組み込みの <code>deque</code> ではなく独自実装を採用している。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:156</code></p>
<h3 id="なぜ独自実装か"><a class="header" href="#なぜ独自実装か">なぜ独自実装か</a></h3>
<p><code>deque</code> では中間要素の削除が O(n) であるのに対し、この実装では O(1) で削除できる。<code>touch()</code> でキャッシュヒットしたブロックを空きキューの中間から即座に除去する必要があるため、O(1) の <code>remove()</code> が不可欠である。</p>
<p>また、Python オブジェクトのアロケーションを行わず、<code>KVCacheBlock</code> の <code>prev_free_block</code>/<code>next_free_block</code> ポインタを直接操作するため、GC 負荷が低い。</p>
<h3 id="センチネルノード"><a class="header" href="#センチネルノード">センチネルノード</a></h3>
<pre><code>fake_head ⇄ block_0 ⇄ block_1 ⇄ ... ⇄ block_n ⇄ fake_tail
</code></pre>
<ul>
<li><code>fake_free_list_head</code>: <code>block_id=-1</code> のダミーノード。先頭の前に配置</li>
<li><code>fake_free_list_tail</code>: <code>block_id=-1</code> のダミーノード。末尾の後に配置</li>
<li><strong>目的</strong>: null チェックの分岐を減らし、コードを簡素化</li>
</ul>
<h3 id="操作一覧"><a class="header" href="#操作一覧">操作一覧</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>計算量</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>popleft()</code></td><td>L208</td><td>O(1)</td><td>先頭ブロックを取り出し</td></tr>
<tr><td><code>popleft_n(n)</code></td><td>L245</td><td>O(n)</td><td>先頭から n 個を一括取り出し</td></tr>
<tr><td><code>remove(block)</code></td><td>L278</td><td>O(1)</td><td>中間のブロックを除去（touch 用）</td></tr>
<tr><td><code>append(block)</code></td><td>L298</td><td>O(1)</td><td>末尾にブロックを追加</td></tr>
<tr><td><code>append_n(blocks)</code></td><td>L321</td><td>O(n)</td><td>末尾に複数ブロックを一括追加</td></tr>
<tr><td><code>get_all_free_blocks()</code></td><td>L346</td><td>O(m)</td><td>全空きブロック取得（テスト用）</td></tr>
</tbody>
</table>
</div>
<h3 id="lru-順序の維持"><a class="header" href="#lru-順序の維持">LRU 順序の維持</a></h3>
<ul>
<li><strong>初期状態</strong>: <code>block_id</code> 順（0, 1, 2, …）</li>
<li><strong>再挿入時</strong>: <code>free_blocks()</code> でブロックが返却される際に<strong>逆順</strong>で追加される
<ul>
<li>理由: リクエストのブロックチェーンの末尾（最新トークン）は先に evict されるべき。先頭（古いプレフィックス）は他のリクエストと共有される可能性が高いため後回し</li>
<li>逆順操作は <code>SingleTypeKVCacheManager.free()</code> 側で実行される（BlockPool 外）</li>
</ul>
</li>
</ul>
<h2 id="blockhashtoblockmap-deep-verified"><a class="header" href="#blockhashtoblockmap-deep-verified">BlockHashToBlockMap [DEEP] [VERIFIED]</a></h2>
<p>プレフィックスキャッシュのハッシュ→ブロック対応表。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:32</code></p>
<h3 id="データ構造"><a class="header" href="#データ構造">データ構造</a></h3>
<pre><code class="language-python">_cache: dict[BlockHashWithGroupId, KVCacheBlock | dict[int, KVCacheBlock]]
</code></pre>
<p><strong>Union 型の最適化</strong>: 大半のハッシュキーには 1 ブロックしか対応しないため、単一ブロックは直接格納し、2つ以上の場合のみ内部 dict に昇格する。これにより内部 dict の GC コストを削減。</p>
<h3 id="重複排除なし設計"><a class="header" href="#重複排除なし設計">重複排除なし設計</a></h3>
<p>同一ハッシュのブロックが複数存在しても<strong>重複排除しない</strong>。理由: ブロック ID をリクエストに割り当てた後は追加のみ（append-only）を保証するため。重複排除するとブロック ID が変わり、ブロックテーブルの安定性が崩れる。</p>
<h3 id="操作"><a class="header" href="#操作">操作</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>get_one_block(key)</code></td><td>L60</td><td>ハッシュキーに対応する任意の1ブロックを返す。複数あれば先頭</td></tr>
<tr><td><code>insert(key, block)</code></td><td>L73</td><td>ブロックをキャッシュに追加。1→dict 昇格を自動処理</td></tr>
<tr><td><code>pop(key, block_id)</code></td><td>L91</td><td>特定の block_id を除去。残りがあれば dict を復元</td></tr>
<tr><td><code>__len__()</code></td><td>L121</td><td>ハッシュキー数（ブロック数ではない）</td></tr>
</tbody>
</table>
</div>
<h3 id="insert-の分岐"><a class="header" href="#insert-の分岐">insert の分岐</a></h3>
<pre><code>key なし       → _cache[key] = block          (単一格納)
key に 1 block → _cache[key] = {id: blk, ...}  (dict 昇格)
key に dict    → dict[block.block_id] = block   (dict 追加)
</code></pre>
<h2 id="null_block-deep-verified"><a class="header" href="#null_block-deep-verified">null_block [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:174</code></p>
<h3 id="特性"><a class="header" href="#特性">特性</a></h3>
<ul>
<li><strong>block_id = 0</strong>: 初期化時に空きキューから最初に popleft される</li>
<li><strong>is_null = True</strong>: 解放・Eviction の対象外</li>
<li><strong>ref_cnt 未管理</strong>: <code>touch()</code> や <code>free_blocks()</code> で特別にスキップされる</li>
</ul>
<h3 id="用途"><a class="header" href="#用途">用途</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>用途</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td>Sliding Window Attention</td><td>ウィンドウ外のブロック位置を埋める。物理メモリを消費しない</td></tr>
<tr><td>Mamba (align モード)</td><td>スキップされたブロック位置のパディング</td></tr>
<tr><td>ブロックテーブルの長さ統一</td><td>Attention カーネルにはブロックテーブルの連続性が必要。null_block で長さを揃える</td></tr>
</tbody>
</table>
</div>
<h3 id="ガード条件"><a class="header" href="#ガード条件">ガード条件</a></h3>
<pre><code class="language-python"># touch() (L383)
if block.ref_cnt == 0 and not block.is_null:
    self.free_block_queue.remove(block)

# free_blocks() (L401-402)
[block for block in blocks_list if block.ref_cnt == 0 and not block.is_null]

# cache_full_blocks() (L260-261)
if blk.is_null:
    continue  # null ブロックはキャッシュしない
</code></pre>
<h2 id="ブロック割り当てフロー-deep-verified"><a class="header" href="#ブロック割り当てフロー-deep-verified">ブロック割り当てフロー [DEEP] [VERIFIED]</a></h2>
<h3 id="get_new_blocks"><a class="header" href="#get_new_blocks">get_new_blocks()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:300</code></p>
<pre><code>get_new_blocks(num_blocks)
  │
  ├─ 空きブロック数チェック → 不足なら ValueError
  │
  ├─ free_block_queue.popleft_n(num_blocks)
  │  └─ LRU 先頭（最古）から取り出し
  │
  ├─ enable_caching の場合:
  │  ├─ _maybe_evict_cached_block(block)  ← ハッシュクリア
  │  ├─ assert block.ref_cnt == 0
  │  ├─ block.ref_cnt += 1
  │  └─ metrics_collector.on_block_allocated(block)  ← サンプリング
  │
  └─ enable_caching でない場合:
     ├─ assert block.ref_cnt == 0
     ├─ block.ref_cnt += 1
     └─ metrics_collector.on_block_allocated(block)
</code></pre>
<p><strong>注意</strong>: この関数はキャッシュ検索を行わない。キャッシュヒットの確認は <code>get_cached_block()</code> で別途行う。</p>
<h3 id="get_cached_block"><a class="header" href="#get_cached_block">get_cached_block()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:182</code></p>
<pre><code>get_cached_block(block_hash, kv_cache_group_ids)
  │
  ├─ 各 group_id について:
  │  ├─ make_block_hash_with_group_id(block_hash, group_id)
  │  ├─ cached_block_hash_to_block.get_one_block(hash_with_id)
  │  └─ 1つでも miss → None を返す（全グループ一致が必須）
  │
  └─ 全ヒット → list[KVCacheBlock] を返す
</code></pre>
<p><strong>All-or-nothing セマンティクス</strong>: 複数の KV キャッシュグループがある場合、全グループでヒットしなければキャッシュミスとして扱う。</p>
<h2 id="ブロック解放フロー-deep-verified"><a class="header" href="#ブロック解放フロー-deep-verified">ブロック解放フロー [DEEP] [VERIFIED]</a></h2>
<h3 id="free_blocks"><a class="header" href="#free_blocks">free_blocks()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:389</code></p>
<pre><code class="language-python">def free_blocks(self, ordered_blocks: Iterable[KVCacheBlock]) -&gt; None:
    blocks_list = list(ordered_blocks)           # イテレータを実体化
    for block in blocks_list:
        block.ref_cnt -= 1                       # 参照カウント減少
    self.free_block_queue.append_n(
        [block for block in blocks_list
         if block.ref_cnt == 0 and not block.is_null]  # 0 到達 &amp; 非 null のみ
    )
</code></pre>
<p><strong>逆順解放の理由</strong>: 呼び出し元（<code>SingleTypeKVCacheManager.free()</code>）がブロックを逆順にして渡す。チェーン末尾（最新トークン）がキューの先頭側に来るため、次回の <code>get_new_blocks()</code> で最初に evict される。先頭ブロック（プレフィックス）は末尾側に来るため、プレフィックスキャッシュとして長く生き残る。</p>
<h3 id="touch"><a class="header" href="#touch">touch()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:372</code></p>
<p>プレフィックスキャッシュヒット時に呼ばれ、ブロックの再利用を記録する。</p>
<pre><code>touch(blocks)
  │
  ├─ ref_cnt == 0 かつ非 null の場合:
  │  └─ free_block_queue.remove(block)  ← 空きキューから除去
  │
  ├─ ref_cnt += 1
  │
  └─ metrics_collector.on_block_accessed(block)
</code></pre>
<h2 id="eviction-メカニズム-deep-verified"><a class="header" href="#eviction-メカニズム-deep-verified">Eviction メカニズム [DEEP] [VERIFIED]</a></h2>
<h3 id="_maybe_evict_cached_block"><a class="header" href="#_maybe_evict_cached_block">_maybe_evict_cached_block()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:332</code></p>
<p>新規ブロック割り当て時に、そのブロックがプレフィックスキャッシュに登録されている場合にキャッシュから除去する。</p>
<pre><code>_maybe_evict_cached_block(block)
  │
  ├─ metrics_collector.on_block_evicted(block)  ← 先にメトリクス記録
  │
  ├─ block.block_hash is None → return False（キャッシュ未登録）
  │
  ├─ cached_block_hash_to_block.pop(hash, block_id)
  │  └─ None → return False（マップに不在）
  │
  ├─ block.reset_hash()  ← ハッシュをクリア
  │
  ├─ enable_kv_cache_events の場合:
  │  └─ kv_event_queue.append(BlockRemoved(...))
  │
  └─ return True
</code></pre>
<p><strong>タイミング</strong>: <code>get_new_blocks()</code> 内で <code>ref_cnt</code> をインクリメントする<strong>前</strong>に呼ばれる。</p>
<h3 id="evict_blocks"><a class="header" href="#evict_blocks">evict_blocks()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:405</code></p>
<p>外部（KV コネクタ）から特定の block_id 群を明示的に evict する。<code>_maybe_evict_cached_block()</code> を各ブロックに対して呼ぶ。ブロックは空きキューからは除去しない（ハッシュの除去のみ）。</p>
<h3 id="reset_prefix_cache"><a class="header" href="#reset_prefix_cache">reset_prefix_cache()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:424</code></p>
<p>全プレフィックスキャッシュのリセット。RLHF でモデル重み更新後にキャッシュを無効化する用途。</p>
<p><strong>前提条件</strong>: 使用中のブロックが null_block のみ（<code>num_used_blocks == 1</code>）。条件を満たさない場合は <code>False</code> を返して失敗。</p>
<pre><code>reset_prefix_cache()
  ├─ 使用中ブロック数 != 1 → return False
  ├─ cached_block_hash_to_block を新規インスタンスで置換
  ├─ 全ブロックの hash をリセット
  ├─ metrics_collector.reset()
  ├─ kv_event_queue.append(AllBlocksCleared())
  └─ return True
</code></pre>
<h2 id="kv-cache-events-deep-verified"><a class="header" href="#kv-cache-events-deep-verified">KV Cache Events [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:177-178, 480-490</code></p>
<p>KV Transfer（Disaggregated Prefill）連携のためのイベントシステム。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>イベント型</th><th>発行タイミング</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>BlockStored</code></td><td><code>cache_full_blocks()</code></td><td>新規ブロックがキャッシュ登録された</td></tr>
<tr><td><code>BlockRemoved</code></td><td><code>_maybe_evict_cached_block()</code></td><td>ブロックがキャッシュから除去された</td></tr>
<tr><td><code>AllBlocksCleared</code></td><td><code>reset_prefix_cache()</code></td><td>全キャッシュがリセットされた</td></tr>
</tbody>
</table>
</div>
<pre><code class="language-python">def take_events(self) -&gt; list[KVCacheEvent]:
    """アトミックにイベントキューを排出"""
    events = self.kv_event_queue
    self.kv_event_queue = []  # 新規リストで置換（参照スワップ）
    return events
</code></pre>
<h2 id="キャッシュ使用率-verified"><a class="header" href="#キャッシュ使用率-verified">キャッシュ使用率 [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:467</code></p>
<pre><code class="language-python">def get_usage(self) -&gt; float:
    total_gpu_blocks = self.num_gpu_blocks - 1  # null_block を除外
    return 1.0 - (self.get_num_free_blocks() / total_gpu_blocks)
</code></pre>
<p>null_block は常に「使用中」だが、使用率の計算からは除外される。</p>
<h2 id="メトリクス収集-deep-verified"><a class="header" href="#メトリクス収集-deep-verified">メトリクス収集 [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_metrics.py:46</code></p>
<p><code>KVCacheMetricsCollector</code> はサンプリングベースのブロック滞留メトリクスを収集する。</p>
<h3 id="blockmetricsstate"><a class="header" href="#blockmetricsstate">BlockMetricsState</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_metrics.py:16</code></p>
<p>個別ブロックのライフサイクル指標:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>birth_time_ns</code></td><td>割り当て時刻（<code>time.monotonic_ns()</code>）</td></tr>
<tr><td><code>last_access_ns</code></td><td>最終アクセス時刻</td></tr>
<tr><td><code>access_history</code></td><td>アクセス履歴（最大4件、<code>deque(maxlen=4)</code>）</td></tr>
</tbody>
</table>
</div>
<h3 id="サンプリング"><a class="header" href="#サンプリング">サンプリング</a></h3>
<pre><code class="language-python">sample_rate: float = 0.01  # デフォルト1%
def should_sample_block(self) -&gt; bool:
    return random.random() &lt; self.sample_rate
</code></pre>
<p>全ブロックを追跡するとオーバーヘッドが大きいため、割り当て時に確率的にサンプリングする。サンプリングされたブロックのみ <code>BlockMetricsState</code> が生成される。</p>
<h3 id="イベントフック"><a class="header" href="#イベントフック">イベントフック</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フック</th><th>タイミング</th><th>処理</th></tr>
</thead>
<tbody>
<tr><td><code>on_block_allocated(block)</code></td><td><code>get_new_blocks()</code></td><td>サンプル判定、<code>BlockMetricsState</code> 生成</td></tr>
<tr><td><code>on_block_accessed(block)</code></td><td><code>touch()</code></td><td><code>record_access()</code> 呼び出し</td></tr>
<tr><td><code>on_block_evicted(block)</code></td><td><code>_maybe_evict_cached_block()</code></td><td><code>KVCacheEvictionEvent</code> を生成・蓄積</td></tr>
</tbody>
</table>
</div>
<p>Eviction 時に生成される <code>KVCacheEvictionEvent</code> には <code>lifetime_seconds</code>、<code>idle_seconds</code>、<code>reuse_gaps_seconds</code> が含まれ、<code>drain_events()</code> で一括取得できる。</p>
<h2 id="blockpool-初期化-verified"><a class="header" href="#blockpool-初期化-verified">BlockPool 初期化 [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:147</code></p>
<pre><code class="language-python">def __init__(self, num_gpu_blocks, enable_caching, hash_block_size,
             enable_kv_cache_events=False, metrics_collector=None):
    self.blocks = [KVCacheBlock(idx) for idx in range(num_gpu_blocks)]
    self.free_block_queue = FreeKVCacheBlockQueue(self.blocks)
    self.cached_block_hash_to_block = BlockHashToBlockMap()
    self.null_block = self.free_block_queue.popleft()  # block_id=0
    self.null_block.is_null = True
</code></pre>
<ul>
<li><code>hash_block_size</code>: ハッシュ計算に使うブロックサイズ。通常は実際のブロックサイズと一致するが、Hybrid モデル（異なるブロックサイズの KV キャッシュグループ）では異なる場合がある</li>
<li><code>enable_kv_cache_events</code>: KV Transfer 連携用のイベント発行を有効化</li>
<li><code>metrics_collector</code>: サンプリングベースの滞留メトリクス収集（オプション）</li>
</ul>
<h2 id="関連ドキュメント-6"><a class="header" href="#関連ドキュメント-6">関連ドキュメント</a></h2>
<ul>
<li><a href="#kvcachemanager-サマリー">KVCacheManager サマリー</a></li>
<li><a href="#プレフィックスキャッシュ詳細">プレフィックスキャッシュ詳細</a></li>
<li><a href="#アテンションタイプ別-manager-詳細">アテンションタイプ別 Manager</a></li>
<li><a href="#scheduler-サマリー">Scheduler</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="プレフィックスキャッシュ詳細"><a class="header" href="#プレフィックスキャッシュ詳細">プレフィックスキャッシュ詳細</a></h1>
<blockquote>
<p><strong>深度</strong>: [DEEP]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-15"><a class="header" href="#概要-15">概要</a></h2>
<p>プレフィックスキャッシュは、異なるリクエスト間で共通するプロンプトプレフィックスの KV キャッシュブロックを再利用する機構である。トークン列をブロック単位でハッシュ化し、ハッシュチェーン（各ブロックのハッシュが前のブロックのハッシュに依存）を構築することで、プレフィックスの最長一致を効率的に検索する。</p>
<h2 id="ハッシュチェーン計算-deep-verified"><a class="header" href="#ハッシュチェーン計算-deep-verified">ハッシュチェーン計算 [DEEP] [VERIFIED]</a></h2>
<h3 id="hash_block_tokens"><a class="header" href="#hash_block_tokens">hash_block_tokens()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:525</code></p>
<p>各ブロックのハッシュは 3 要素のタプルから計算される:</p>
<pre><code class="language-python">def hash_block_tokens(hash_function, parent_block_hash, curr_block_token_ids,
                      extra_keys=None) -&gt; BlockHash:
    if not parent_block_hash:
        parent_block_hash = NONE_HASH      # 先頭ブロック用のシード
    curr_block_token_ids_tuple = tuple(curr_block_token_ids)
    return BlockHash(
        hash_function((parent_block_hash, curr_block_token_ids_tuple, extra_keys))
    )
</code></pre>
<p><strong>ハッシュ入力の 3 要素</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>要素</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>parent_block_hash</code></td><td>前ブロックのハッシュ（先頭ブロックは <code>NONE_HASH</code>）</td></tr>
<tr><td><code>curr_block_token_ids_tuple</code></td><td>現ブロックのトークン ID 列（tuple 化）</td></tr>
<tr><td><code>extra_keys</code></td><td>LoRA、マルチモーダル、cache_salt、prompt_embeds（後述）</td></tr>
</tbody>
</table>
</div>
<h3 id="チェーン依存性"><a class="header" href="#チェーン依存性">チェーン依存性</a></h3>
<pre><code>Block 0: hash(NONE_HASH, tokens[0:B], extra)   → H0
Block 1: hash(H0,        tokens[B:2B], extra)  → H1
Block 2: hash(H1,        tokens[2B:3B], extra) → H2
  ...
</code></pre>
<p><strong>なぜチェーンか</strong>: 各ハッシュが全ての先行ブロックに依存するため、プレフィックスが異なれば後続のハッシュも必ず異なる。これにより左から右へのスキャンで「最初のミスで停止」すれば最長プレフィックス一致が得られる。</p>
<h3 id="none_hash"><a class="header" href="#none_hash">NONE_HASH</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:77</code></p>
<p>チェーンの起点となるシード値:</p>
<pre><code class="language-python">def init_none_hash(hash_fn):
    hash_seed = os.getenv("PYTHONHASHSEED")
    if hash_seed is None:
        NONE_HASH = BlockHash(os.urandom(32))    # ランダム 32 バイト
    else:
        NONE_HASH = BlockHash(hash_fn(hash_seed)) # 決定論的
</code></pre>
<ul>
<li><strong>PYTHONHASHSEED 未設定</strong>: ランダムシード → プロセス間でハッシュが一致しない</li>
<li><strong>PYTHONHASHSEED 設定済み</strong>: 決定論的 → プロセス間でハッシュを共有可能（KV Transfer で必要）</li>
<li>CBOR ベースのハッシュ関数で PYTHONHASHSEED 未設定の場合は警告が出る</li>
</ul>
<h2 id="blockhash-型-deep-verified"><a class="header" href="#blockhash-型-deep-verified">BlockHash 型 [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:34</code></p>
<h3 id="型階層"><a class="header" href="#型階層">型階層</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>型</th><th>定義</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>BlockHash</code></td><td><code>NewType("BlockHash", bytes)</code></td><td>ブロック単体のハッシュ値</td></tr>
<tr><td><code>BlockHashWithGroupId</code></td><td><code>NewType("BlockHashWithGroupId", bytes)</code></td><td>ハッシュ + KV キャッシュグループ ID（4 バイト BE）</td></tr>
<tr><td><code>ExternalBlockHash</code></td><td><code>bytes | int</code></td><td>外部向けハッシュ（後方互換性のための Union）</td></tr>
</tbody>
</table>
</div>
<h3 id="blockhashwithgroupid-のパッキング"><a class="header" href="#blockhashwithgroupid-のパッキング">BlockHashWithGroupId のパッキング</a></h3>
<pre><code class="language-python">def make_block_hash_with_group_id(block_hash, group_id):
    return BlockHashWithGroupId(
        block_hash + group_id.to_bytes(4, "big", signed=False)
    )

def get_block_hash(key):     return BlockHash(key[:-4])
def get_group_id(key):       return int.from_bytes(key[-4:], "big")
</code></pre>
<p><strong>設計</strong>: tuple ではなく bytes 結合でパッキングすることで、Python オブジェクト生成を回避し、GC 負荷を低減。</p>
<h2 id="ハッシュ関数-deep-verified"><a class="header" href="#ハッシュ関数-deep-verified">ハッシュ関数 [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/utils/hashing.py</code></p>
<p>4 種類のハッシュ関数が利用可能:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>名前</th><th>シリアライゼーション</th><th>ハッシュ</th><th>出力サイズ</th><th>特徴</th></tr>
</thead>
<tbody>
<tr><td><code>sha256</code></td><td>pickle</td><td>SHA-256</td><td>32 bytes</td><td>Python 依存</td></tr>
<tr><td><code>sha256_cbor</code></td><td>CBOR (canonical)</td><td>SHA-256</td><td>32 bytes</td><td><strong>デフォルト</strong>。言語非依存・再現可能</td></tr>
<tr><td><code>xxhash</code></td><td>pickle</td><td>xxh3_128</td><td>16 bytes</td><td>高速、Python 依存</td></tr>
<tr><td><code>xxhash_cbor</code></td><td>CBOR (canonical)</td><td>xxh3_128</td><td>16 bytes</td><td>高速、言語非依存</td></tr>
</tbody>
</table>
</div>
<p><strong>デフォルト</strong>: <code>sha256_cbor</code> — CBOR の canonical モードにより、PYTHONHASHSEED に依存しないシリアライゼーションが可能。プロセス間でハッシュを共有する KV Transfer に適している。</p>
<p>設定: <code>vllm_config.cache_config.prefix_caching_hash_algo</code></p>
<h2 id="extra-keys-deep-verified"><a class="header" href="#extra-keys-deep-verified">Extra Keys [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:367</code></p>
<p>同一トークン列でも異なる KV キャッシュを持つ場合に、追加情報をハッシュに含める。</p>
<h3 id="必要判定"><a class="header" href="#必要判定">必要判定</a></h3>
<pre><code class="language-python">def need_extra_keys(request):
    return (bool(request.mm_features)           # マルチモーダル
            or (request.lora_request is not None) # LoRA
            or (request.cache_salt is not None))  # キャッシュソルト
</code></pre>
<h3 id="各-extra-key-の生成"><a class="header" href="#各-extra-key-の生成">各 Extra Key の生成</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>生成関数</th><th>行</th><th>内容</th><th>適用範囲</th></tr>
</thead>
<tbody>
<tr><td><code>_gen_mm_extra_hash_keys()</code></td><td>L387</td><td>MM 入力の <code>identifier</code></td><td>ブロックと重なる MM 入力のみ</td></tr>
<tr><td><code>_gen_lora_extra_hash_keys()</code></td><td>L451</td><td>LoRA アダプタの <code>lora_name</code></td><td>全ブロック共通</td></tr>
<tr><td><code>cache_salt</code></td><td>L508-509</td><td>ユーザー指定のキャッシュソルト</td><td><strong>先頭ブロックのみ</strong> (<code>start_token_idx == 0</code>)</td></tr>
<tr><td><code>_gen_prompt_embeds_extra_hash_keys()</code></td><td>L466</td><td>プロンプト埋め込みの生テンソルバイト</td><td>ブロック範囲分のスライス</td></tr>
</tbody>
</table>
</div>
<h3 id="結合順序"><a class="header" href="#結合順序">結合順序</a></h3>
<pre><code class="language-python">extra_keys = lora_extra_keys + mm_extra_keys + cache_salt_keys + prompt_embeds_keys
</code></pre>
<p>空の場合は <code>None</code> を返し、ハッシュ計算の <code>extra_keys</code> 引数として渡される。</p>
<h3 id="マルチモーダル-extra-keys-の詳細"><a class="header" href="#マルチモーダル-extra-keys-の詳細">マルチモーダル Extra Keys の詳細</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:387</code></p>
<p><code>_gen_mm_extra_hash_keys()</code> はブロックの [start, end) トークン範囲と MM 入力の [offset, offset+length) 範囲の重なりを検出する:</p>
<pre><code>MM入力: [offset ─────── offset+length]
Block:         [start ──── end]
               ↑ 重なりあり → identifier を extra_keys に追加
</code></pre>
<ul>
<li><code>mm_features</code> は <code>mm_position.offset</code> でソート済みと仮定</li>
<li><code>start_mm_idx</code> で走査位置を追跡し、毎回先頭から検索しない</li>
<li><code>start_mm_idx = -1</code> は「最後の MM 入力」を示す（生成トークンが増えるデコードフェーズで使用）</li>
</ul>
<h2 id="リクエストブロックハッシャー-deep-verified"><a class="header" href="#リクエストブロックハッシャー-deep-verified">リクエストブロックハッシャー [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:555</code></p>
<h3 id="ファクトリ関数"><a class="header" href="#ファクトリ関数">ファクトリ関数</a></h3>
<pre><code class="language-python">def get_request_block_hasher(block_size, caching_hash_fn):
    def request_block_hasher(request) -&gt; list[BlockHash]:
        start_token_idx = len(request.block_hashes) * block_size
        # full ブロックのみハッシュ（不完全ブロックはスキップ）
        if start_token_idx + block_size &gt; request.num_tokens:
            return []
        # ...ハッシュチェーンを走査して新規 full ブロックのハッシュを計算
    return request_block_hasher
</code></pre>
<h3 id="遅延インクリメンタル計算"><a class="header" href="#遅延インクリメンタル計算">遅延・インクリメンタル計算</a></h3>
<ol>
<li><strong>初期化時</strong>: <code>Request.__init__()</code> で <code>block_hasher</code> が渡された場合、即座に <code>get_hash_new_full_blocks()</code> を呼び、プロンプトの full ブロック分のハッシュを計算</li>
<li><strong>トークン追加時</strong>: <code>Request.append_output_token_ids()</code> で新トークンが追加されるたびに <code>get_hash_new_full_blocks()</code> を呼び、新たに full になったブロックのハッシュをインクリメンタルに追加</li>
</ol>
<pre><code class="language-python"># Request.__init__()
self.block_hashes = self.get_hash_new_full_blocks()  # 初期ハッシュ

# Request.append_output_token_ids()
self.block_hashes.extend(self.get_hash_new_full_blocks())  # 増分追加
</code></pre>
<p><strong>制約</strong>: 不完全ブロック（最後のブロックが block_size 未満）はハッシュされない。これによりプレフィックスキャッシュは常にブロック境界単位で一致する。</p>
<h3 id="チェーンの継続"><a class="header" href="#チェーンの継続">チェーンの継続</a></h3>
<pre><code class="language-python">prev_block_hash_value = request.block_hashes[-1] if request.block_hashes else None
</code></pre>
<p>前回計算済みの最後のハッシュを <code>parent_block_hash</code> として使い、チェーンを継続する。</p>
<h2 id="blockhashlistwithblocksize-deep-verified"><a class="header" href="#blockhashlistwithblocksize-deep-verified">BlockHashListWithBlockSize [DEEP] [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:1571</code></p>
<p>Hybrid モデル（異なるブロックサイズの KV キャッシュグループ）でハッシュ粒度を変換するアダプタ。</p>
<h3 id="動作原理"><a class="header" href="#動作原理">動作原理</a></h3>
<pre><code>hash_block_size = 16, target_block_size = 32 の場合:

元のハッシュ:  [H0, H1, H2, H3]  (各16トークン)
変換後:        [H0+H1, H2+H3]     (各32トークン)
                  ↑ bytes 結合
</code></pre>
<pre><code class="language-python">def _get_value_at(self, idx):
    base = idx * self.scale_factor   # scale_factor = target / hash
    end = base + self.scale_factor
    merged_hash = self.block_hashes[base]
    for i in range(base + 1, end):
        merged_hash += self.block_hashes[i]  # bytes 結合
    return BlockHash(merged_hash)
</code></pre>
<p><strong>遅延評価</strong>: アクセス時にのみ変換を実行。<code>__getitem__</code>、<code>__iter__</code>、<code>__len__</code> をサポートし、通常の <code>list[BlockHash]</code> と同じインターフェースで使える。</p>
<h2 id="lookup-アルゴリズム-deep-verified"><a class="header" href="#lookup-アルゴリズム-deep-verified">Lookup アルゴリズム [DEEP] [VERIFIED]</a></h2>
<p>プレフィックスキャッシュの検索は <code>SingleTypeKVCacheManager</code> のサブクラスごとに異なるアルゴリズムを持つ。</p>
<h3 id="fullattentionmanager-左右スキャン"><a class="header" href="#fullattentionmanager-左右スキャン">FullAttentionManager: 左→右スキャン</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:401</code></p>
<pre><code>for block_hash in block_hashes[:max_num_blocks]:
    if cache hit:
        computed_blocks.append(cached_block)
    else:
        break  ← 最初のミスで停止
</code></pre>
<ul>
<li><strong>Downward-closed 性質</strong>: blocks[0..n] がヒットするなら blocks[0..n-1] も必ずヒットする</li>
<li>EAGLE 使用時は最後のブロックを削除（hidden states が必要なため）</li>
<li><code>alignment_tokens</code> でアライメント調整（Hybrid モデルでの LCM ブロックサイズ）</li>
</ul>
<h3 id="slidingwindowmanager-右左スキャン"><a class="header" href="#slidingwindowmanager-右左スキャン">SlidingWindowManager: 右→左スキャン</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:466</code></p>
<pre><code>sliding_window_contiguous_blocks = ceil((sliding_window - 1) / block_size)

for i in range(max_num_blocks - 1, -1, -1):  # 右→左
    if cache hit:
        computed_blocks[i] = cached_block
        num_contiguous_blocks += 1
        if num_contiguous_blocks &gt;= required:
            break  ← ウィンドウ分の連続ブロック確保
    else:
        num_contiguous_blocks = 0  ← 連続性リセット
</code></pre>
<ul>
<li><strong>連続ブロックが必須</strong>: Sliding Window Attention はウィンドウ内の連続したトークンにのみアテンションするため、不連続なブロックは使えない</li>
<li><code>computed_blocks</code> は初期値として <code>null_block</code> で埋められ、ヒットした位置のみ実ブロックで置換される</li>
<li>アライメントチェック: 右端のブロックがアライメント境界に合わない場合はスキップ</li>
</ul>
<h3 id="chunkedlocalattentionmanager"><a class="header" href="#chunkedlocalattentionmanager">ChunkedLocalAttentionManager</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:594</code></p>
<p>チャンク境界でアテンションが分割されるモデル用。ウィンドウ外のブロックは null_block でパディングし、ウィンドウ内のみ左→右スキャンで検索する。</p>
<h3 id="mambamanager-右左単一一致"><a class="header" href="#mambamanager-右左単一一致">MambaManager: 右→左、単一一致</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:744</code></p>
<p>Mamba（線形アテンション）は最後の状態のみが必要なため、最初のヒットで即座に停止する。</p>
<h3 id="hybridkvcachecoordinator-反復固定点"><a class="header" href="#hybridkvcachecoordinator-反復固定点">HybridKVCacheCoordinator: 反復固定点</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_coordinator.py:448</code></p>
<p>複数のアテンションタイプが混在する Hybrid モデルでは、各グループの最長ヒット長が相互に制約し合う:</p>
<pre><code>hit_length = max_cache_hit_length

while True:
    curr_hit_length = hit_length
    for each attention_group:
        if is_full_attention and cached:
            # Downward-closed: 既存結果を再利用
            curr_hit_length = (curr_hit_length // block_size) * block_size
        else:
            hit = find_longest_cache_hit(curr_hit_length)
            curr_hit_length = len(hit) * block_size

    if curr_hit_length &gt;= hit_length:
        break  ← 収束（もう減らない）
    hit_length = curr_hit_length

    if is_simple_hybrid:  # FullAttn + 1種のみ
        break  ← 1回で十分
</code></pre>
<p><strong>収束保証</strong>: <code>hit_length</code> は単調減少するため、有限回で収束する。
<strong>最適化</strong>: Full Attention は downward-closed なので、他グループの結果に合わせてカットするだけでよい。2グループの simple hybrid ケースでは 1 イテレーションで確定。</p>
<h2 id="キャッシュ登録-deep-verified"><a class="header" href="#キャッシュ登録-deep-verified">キャッシュ登録 [DEEP] [VERIFIED]</a></h2>
<h3 id="cache_full_blocks"><a class="header" href="#cache_full_blocks">cache_full_blocks()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:209</code></p>
<p>計算済みの full ブロックをプレフィックスキャッシュに登録する。</p>
<pre><code>cache_full_blocks(request, blocks, num_cached_blocks, num_full_blocks, ...)
  │
  ├─ num_cached_blocks &gt;= num_full_blocks → return（既に登録済み）
  │
  ├─ new_full_blocks = blocks[num_cached_blocks:num_full_blocks]
  │
  ├─ block_size == hash_block_size の場合:
  │  └─ block_hashes = request.block_hashes（直接使用）
  │
  ├─ block_size != hash_block_size の場合:
  │  └─ block_hashes = BlockHashListWithBlockSize(...)（粒度変換）
  │
  └─ 各 new_full_block について:
     ├─ is_null → skip
     ├─ assert block_hash is None（二重登録防止）
     ├─ block_hash_with_group_id を生成
     ├─ blk.block_hash = block_hash_with_group_id
     ├─ cached_block_hash_to_block.insert(...)
     └─ enable_kv_cache_events → BlockStored イベント発行
</code></pre>
<h3 id="num_cached_block-トラッキング"><a class="header" href="#num_cached_block-トラッキング">num_cached_block トラッキング</a></h3>
<p><code>SingleTypeKVCacheManager</code> が <code>num_cached_block[request_id]</code> で各リクエストの登録済みブロック数を追跡する。<code>cache_blocks()</code> 呼び出し時に <code>num_cached_blocks &gt;= num_full_blocks</code> なら何もせず、新たに full になったブロックのみを登録する。</p>
<h2 id="データフロー全体-verified"><a class="header" href="#データフロー全体-verified">データフロー全体 [VERIFIED]</a></h2>
<pre><code>EngineCore.__init__()
  └─ init_none_hash(hash_fn)        ← グローバル NONE_HASH 初期化
  └─ get_request_block_hasher(...)   ← ハッシャークロージャ生成

Request.__init__(block_hasher=...)
  └─ block_hashes = get_hash_new_full_blocks()  ← プロンプトの full ブロックを即時ハッシュ

Request.append_output_token_ids()
  └─ block_hashes.extend(get_hash_new_full_blocks())  ← 増分ハッシュ

Scheduler.schedule()
  ├─ kv_cache_manager.get_computed_blocks(request)
  │  └─ coordinator.find_longest_cache_hit(request.block_hashes, max_length)
  │     └─ block_pool.get_cached_block(hash, group_ids)
  │        └─ BlockHashToBlockMap.get_one_block(hash_with_group_id)
  │
  └─ kv_cache_manager.allocate_slots(request, ...)
     └─ coordinator.cache_blocks(request, num_tokens_to_cache)
        └─ block_pool.cache_full_blocks(request, blocks, ...)
           └─ BlockHashToBlockMap.insert(hash_with_group_id, block)
</code></pre>
<h2 id="関連ドキュメント-7"><a class="header" href="#関連ドキュメント-7">関連ドキュメント</a></h2>
<ul>
<li><a href="#kvcachemanager-サマリー">KVCacheManager サマリー</a></li>
<li><a href="#blockpool-詳細">BlockPool 詳細</a></li>
<li><a href="#アテンションタイプ別-manager-詳細">アテンションタイプ別 Manager</a></li>
<li><a href="#用語集">用語集</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kv-transfer-medium-verified"><a class="header" href="#kv-transfer-medium-verified">KV Transfer [MEDIUM] [VERIFIED]</a></h1>
<blockquote>
<p><strong>最終更新</strong>: 2026-02-15
<strong>対象ソース</strong>: <code>target/vllm/vllm/distributed/kv_transfer/</code>, <code>target/vllm/vllm/v1/worker/kv_connector_model_runner_mixin.py</code></p>
</blockquote>
<h2 id="概要-16"><a class="header" href="#概要-16">概要</a></h2>
<p>KV Transferは、vLLMインスタンス間またはストレージ間でデコーダKVキャッシュを転送するためのプラグインフレームワーク。Disaggregated Prefill（P/D分離）、KVキャッシュのオフロード、外部キャッシュ（LMCache等）との連携を実現する。</p>
<p><strong>ECConnector（<code>ec_transfer/</code>）とは完全に独立した系統</strong>。ECConnectorがエンコーダキャッシュ専用であるのに対し、KV Transferはデコーダ側のKVキャッシュのみを対象とする。ただし、設計パターン（2ロール分離、Factory、Mixin統合）は共通している。</p>
<h2 id="アーキテクチャ-7"><a class="header" href="#アーキテクチャ-7">アーキテクチャ</a></h2>
<pre class="mermaid">graph TD
    subgraph Scheduler["Scheduler プロセス"]
        S[Scheduler] --&gt;|"get_num_new_matched_tokens()"| KC_S[KVConnector&lt;br/&gt;SCHEDULER ロール]
        S --&gt;|"update_state_after_alloc()"| KC_S
        S --&gt;|"build_connector_meta()"| KC_S
        S --&gt;|"request_finished()"| KC_S
        S --&gt;|"take_events()"| KC_S
    end

    subgraph Worker["Worker プロセス"]
        MR[GPUModelRunner] --&gt;|"Mixin"| KC_W[KVConnector&lt;br/&gt;WORKER ロール]
        KC_W --&gt;|"start_load_kv()"| EXT[外部ストレージ&lt;br/&gt;LMCache / NIXL / etc.]
        KC_W --&gt;|"save_kv_layer()"| EXT
        ATT[Attention層] --&gt;|"wait_for_layer_load()"| KC_W
        ATT --&gt;|"save_kv_layer()"| KC_W
    end

    KC_S -.-&gt;|"KVConnectorMetadata&lt;br/&gt;(via SchedulerOutput)"| KC_W
    KC_W -.-&gt;|"KVConnectorOutput&lt;br/&gt;(via ModelRunnerOutput)"| KC_S
</pre>

<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/base.py:147</code> (KVConnectorBase_V1)</p>
<h3 id="2ロール分離"><a class="header" href="#2ロール分離">2ロール分離</a></h3>
<p>KVConnectorBase_V1は<strong>同一クラス</strong>だが、SchedulerプロセスとWorkerプロセスで<strong>別インスタンス</strong>が生成される。KVConnectorFactory.create_connector()がロールを引数に取り、各プロセスで適切なインスタンスを構築する。</p>
<pre><code class="language-python"># KVConnectorRole enum
class KVConnectorRole(enum.Enum):
    SCHEDULER = 0  # Schedulerプロセス内
    WORKER = 1     # Workerプロセス内
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/base.py:121</code></p>
<h3 id="グローバル状態管理"><a class="header" href="#グローバル状態管理">グローバル状態管理</a></h3>
<p>Worker側のKVConnectorインスタンスはグローバル変数で管理される。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>関数</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>get_kv_transfer_group()</code></td><td>現在のコネクタインスタンス取得（assertで非None保証）</td></tr>
<tr><td><code>has_kv_transfer_group()</code></td><td>コネクタ初期化済みか確認</td></tr>
<tr><td><code>ensure_kv_transfer_initialized()</code></td><td>未初期化なら初期化（<code>is_kv_transfer_instance=True</code>時のみ）</td></tr>
<tr><td><code>ensure_kv_transfer_shutdown()</code></td><td>コネクタ停止・グローバル変数クリア</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_transfer_state.py:16-74</code></p>
<h2 id="kvconnectorbase_v1-抽象基底クラス"><a class="header" href="#kvconnectorbase_v1-抽象基底クラス">KVConnectorBase_V1 抽象基底クラス</a></h2>
<h3 id="abstract-メソッド7つ"><a class="header" href="#abstract-メソッド7つ">Abstract メソッド（7つ）</a></h3>
<h4 id="worker側4つ"><a class="header" href="#worker側4つ">Worker側（4つ）</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>シグネチャ</th><th>呼び出し元</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>start_load_kv()</code></td><td><code>(ForwardContext, **kwargs) → None</code></td><td>Mixin（forward前）</td><td>KVキャッシュの非同期ロード開始</td></tr>
<tr><td><code>wait_for_layer_load()</code></td><td><code>(layer_name: str) → None</code></td><td>Attention層内</td><td>レイヤー別ロード完了待機</td></tr>
<tr><td><code>save_kv_layer()</code></td><td><code>(layer_name, kv_layer, attn_metadata, **kwargs) → None</code></td><td>Attention層内</td><td>レイヤー別KVの非同期セーブ開始</td></tr>
<tr><td><code>wait_for_save()</code></td><td><code>() → None</code></td><td>Mixin（forward後）</td><td>全セーブ完了待機</td></tr>
</tbody>
</table>
</div>
<p><strong>レイヤーバイレイヤー・パイプライニング</strong>: <code>start_load_kv()</code>で全レイヤーのロードを非同期開始し、各Attention層のforward内で<code>wait_for_layer_load()</code>を呼ぶことで、前のレイヤーの計算中に次レイヤーのKVをロードできる。セーブも<code>save_kv_layer()</code>で即座に非同期開始し、<code>wait_for_save()</code>で全体の完了を保証する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/base.py:275-338</code></p>
<h4 id="scheduler側3つ"><a class="header" href="#scheduler側3つ">Scheduler側（3つ）</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>シグネチャ</th><th>呼び出し元</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>get_num_new_matched_tokens()</code></td><td><code>(Request, int) → (int|None, bool)</code></td><td>Scheduler._schedule_waiting()</td><td>外部KVキャッシュで利用可能なトークン数を返す</td></tr>
<tr><td><code>update_state_after_alloc()</code></td><td><code>(Request, KVCacheBlocks, int) → None</code></td><td>Scheduler._schedule_waiting()</td><td>ブロック割り当て後のコネクタ状態更新</td></tr>
<tr><td><code>build_connector_meta()</code></td><td><code>(SchedulerOutput) → KVConnectorMetadata</code></td><td>Scheduler.schedule()</td><td>ステップ用メタデータ構築（状態リセットを伴う）</td></tr>
</tbody>
</table>
</div>
<p><strong>get_num_new_matched_tokens() の戻り値</strong>:</p>
<ul>
<li><code>(N, False)</code> — N個の外部トークンが同期的に利用可能</li>
<li><code>(N, True)</code> — N個の外部トークンが非同期ロード（WAITING_FOR_REMOTE_KVS状態へ遷移）</li>
<li><code>(None, _)</code> — まだ判定不能（次回再問い合わせ）</li>
<li><code>(0, False)</code> — 外部キャッシュなし</li>
</ul>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/base.py:416-485</code></p>
<h3 id="非abstract-重要メソッド"><a class="header" href="#非abstract-重要メソッド">非Abstract 重要メソッド</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>用途</th><th>デフォルト動作</th></tr>
</thead>
<tbody>
<tr><td><code>bind_connector_metadata()</code></td><td>Scheduler→Workerメタデータ設定</td><td><code>_connector_metadata</code>に保存</td></tr>
<tr><td><code>clear_connector_metadata()</code></td><td>メタデータクリア</td><td>Noneに設定</td></tr>
<tr><td><code>register_kv_caches()</code></td><td>KVキャッシュテンソル事前登録（NIXL等で必要）</td><td>no-op</td></tr>
<tr><td><code>register_cross_layers_kv_cache()</code></td><td>全レイヤー一括KVテンソル登録</td><td>no-op</td></tr>
<tr><td><code>set_host_xfer_buffer_ops()</code></td><td>ホスト↔デバイス間コピー操作設定</td><td>no-op</td></tr>
<tr><td><code>handle_preemptions()</code></td><td>プリエンプション通知（ブロック上書き前）</td><td>no-op</td></tr>
<tr><td><code>get_finished()</code></td><td>非同期転送完了リクエストID取得</td><td><code>(None, None)</code></td></tr>
<tr><td><code>get_block_ids_with_load_errors()</code></td><td>ロード失敗ブロックID取得</td><td><code>set()</code></td></tr>
<tr><td><code>request_finished()</code></td><td>リクエスト完了通知・遅延解放制御</td><td><code>(False, None)</code></td></tr>
<tr><td><code>take_events()</code></td><td>KVキャッシュイベント取得</td><td>空</td></tr>
<tr><td><code>update_connector_output()</code></td><td>Worker出力でScheduler状態更新</td><td>no-op</td></tr>
</tbody>
</table>
</div>
<h3 id="補助クラスインタフェース"><a class="header" href="#補助クラスインタフェース">補助クラス・インタフェース</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>クラス</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>KVConnectorMetadata</code> (ABC)</td><td>Scheduler→Worker間通信メタデータ</td></tr>
<tr><td><code>KVConnectorHandshakeMetadata</code> (ABC)</td><td>P/Dワーカー間帯域外ハンドシェイク</td></tr>
<tr><td><code>SupportsHMA</code> (ABC)</td><td>Hybrid Memory Allocator対応インタフェース</td></tr>
<tr><td><code>CopyBlocksOp</code> (Callable)</td><td><code>(s_tensors, d_tensors, s_indices, d_indices, direction) → None</code></td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/base.py:82-144</code></p>
<h3 id="cross-layer-blocks"><a class="header" href="#cross-layer-blocks">Cross-Layer Blocks</a></h3>
<p><code>prefer_cross_layer_blocks</code>プロパティが<code>True</code>のコネクタは、全レイヤーのKVを1つの連続テンソルにまとめたレイアウトを使用する。これにより、ブロック単位で全レイヤーのKVデータを一括転送でき、転送効率が向上する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/kv_connector_model_runner_mixin.py:113-177</code></p>
<h2 id="kvconnectorfactory"><a class="header" href="#kvconnectorfactory">KVConnectorFactory</a></h2>
<p>遅延ロードパターンのファクトリ。<code>module_path</code> + <code>class_name</code>を登録し、使用時にimportする。</p>
<h3 id="登録済みコネクタ10個"><a class="header" href="#登録済みコネクタ10個">登録済みコネクタ（10個）</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>名前</th><th>用途</th><th>特徴</th></tr>
</thead>
<tbody>
<tr><td><code>ExampleConnector</code></td><td>デバッグ用</td><td>safetensorsでディスク保存</td></tr>
<tr><td><code>LMCacheConnectorV1</code></td><td>LMCache統合</td><td>チャンク単位KV保存・3層ストレージ</td></tr>
<tr><td><code>LMCacheMPConnector</code></td><td>LMCacheマルチプロセス版</td><td>別プロセスでLMCache実行</td></tr>
<tr><td><code>NixlConnector</code></td><td>NIXL (RDMA)</td><td>高速GPU間転送</td></tr>
<tr><td><code>P2pNcclConnector</code></td><td>P2P NCCL</td><td>NCCL経由の直接GPU転送</td></tr>
<tr><td><code>OffloadingConnector</code></td><td>KVオフロード</td><td>CPU/ディスクへのオフロード</td></tr>
<tr><td><code>MultiConnector</code></td><td>複合コネクタ</td><td>複数バックエンドを束ねる</td></tr>
<tr><td><code>MoRIIOConnector</code></td><td>MORIIO</td><td>MORIIOフレームワーク</td></tr>
<tr><td><code>MooncakeConnector</code></td><td>Mooncake</td><td>分散学習フレームワーク</td></tr>
<tr><td><code>DecodeBenchConnector</code></td><td>ベンチマーク用</td><td>デコード性能測定</td></tr>
</tbody>
</table>
</div>
<p><strong>動的ロード</strong>: <code>kv_connector_module_path</code>設定で未登録クラスも使用可能。旧2引数シグネチャとの互換性チェック（<code>supports_kw()</code>）あり。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/factory.py:27-203</code></p>
<h2 id="scheduler統合"><a class="header" href="#scheduler統合">Scheduler統合</a></h2>
<h3 id="外部キャッシュ問い合わせフロー"><a class="header" href="#外部キャッシュ問い合わせフロー">外部キャッシュ問い合わせフロー</a></h3>
<p>WAITINGリクエストのスケジューリング時、ローカルプレフィックスキャッシュに加えて外部KVキャッシュも問い合わせる。</p>
<pre class="mermaid">sequenceDiagram
    participant S as Scheduler
    participant KV as KVConnector (SCHEDULER)
    participant CM as KVCacheManager

    Note over S: _schedule_waiting()
    S-&gt;&gt;CM: get_computed_blocks(request)
    CM--&gt;&gt;S: local_computed_tokens

    S-&gt;&gt;KV: get_num_new_matched_tokens(request, local_computed_tokens)
    KV--&gt;&gt;S: (external_tokens, is_async)

    Note over S: total = local + external
    S-&gt;&gt;CM: allocate_slots(request, num_new_tokens, ..., delay_cache_blocks=is_async)
    CM--&gt;&gt;S: new_blocks

    S-&gt;&gt;KV: update_state_after_alloc(request, blocks, external_tokens)

    alt is_async=True
        Note over S: request.status = WAITING_FOR_REMOTE_KVS
    else is_async=False
        Note over S: request.status = RUNNING
    end

    Note over S: schedule()末尾
    S-&gt;&gt;KV: build_connector_meta(scheduler_output)
    KV--&gt;&gt;S: KVConnectorMetadata
</pre>

<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:608-772</code></p>
<h3 id="waiting_for_remote_kvs-状態管理"><a class="header" href="#waiting_for_remote_kvs-状態管理">WAITING_FOR_REMOTE_KVS 状態管理</a></h3>
<p>非同期KVロード中のリクエストは<code>WAITING_FOR_REMOTE_KVS</code>状態に遷移する。</p>
<ol>
<li><strong><code>_update_from_kv_xfer_finished()</code></strong>: Worker側コネクタの<code>finished_recving</code>/<code>finished_sending</code>を処理
<ul>
<li><code>finished_recving</code> → <code>finished_recving_kv_req_ids</code>に追加（次stepで処理）</li>
<li><code>finished_sending</code> → ブロック即時解放</li>
</ul>
</li>
<li><strong><code>_update_waiting_for_remote_kv()</code></strong>: WAITING_FOR_REMOTE_KVS状態のリクエストを確認
<ul>
<li>受信完了 → <code>kv_cache_manager.cache_blocks()</code>でキャッシュ → WAITING状態に戻す</li>
<li>ロードエラー → 有効なcomputed_tokensだけキャッシュ、またはブロック解放</li>
</ul>
</li>
</ol>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:1961-2034</code></p>
<h3 id="リクエスト完了時の遅延解放"><a class="header" href="#リクエスト完了時の遅延解放">リクエスト完了時の遅延解放</a></h3>
<p><code>_connector_finished()</code>は<code>request_finished()</code>を呼び、コネクタがブロックの非同期送信を引き受けるかを確認する。</p>
<pre><code class="language-python">delay_free, kv_xfer_params = connector.request_finished(request, block_ids)
# delay_free=True → ブロックはget_finished()で送信完了報告後に解放
# kv_xfer_params → リクエスト出力に含めるKV転送パラメータ
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:1930-1959</code></p>
<h2 id="workergpumodelrunner-統合"><a class="header" href="#workergpumodelrunner-統合">Worker/GPUModelRunner 統合</a></h2>
<h3 id="kvconnectormodelrunnermixin"><a class="header" href="#kvconnectormodelrunnermixin">KVConnectorModelRunnerMixin</a></h3>
<p>GPUModelRunnerにミックスインされ、KVコネクタのライフサイクルを管理する。</p>
<h4 id="_get_kv_connector_output--コアライフサイクル"><a class="header" href="#_get_kv_connector_output--コアライフサイクル"><code>_get_kv_connector_output()</code> — コアライフサイクル</a></h4>
<pre><code class="language-python">@contextmanager
def _get_kv_connector_output(scheduler_output, wait_for_save=True):
    output = KVConnectorOutput()
    kv_connector = get_kv_transfer_group()

    # 1. Scheduler側メタデータをバインド
    kv_connector.bind_connector_metadata(scheduler_output.kv_connector_metadata)

    # 2. 非同期KVロード開始（forward前）
    kv_connector.start_load_kv(get_forward_context())

    try:
        yield output  # ← ここでモデルforward実行（save_kv_layer含む）
    finally:
        # 3. セーブ完了待機
        if wait_for_save:
            kv_connector.wait_for_save()

        # 4. 完了・エラー情報収集
        output.finished_sending, output.finished_recving = (
            kv_connector.get_finished(scheduler_output.finished_req_ids))
        output.invalid_block_ids = kv_connector.get_block_ids_with_load_errors()
        output.kv_connector_stats = kv_connector.get_kv_connector_stats()
        output.kv_cache_events = kv_connector.get_kv_connector_kv_cache_events()

        # 5. メタデータクリア
        kv_connector.clear_connector_metadata()
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/kv_connector_model_runner_mixin.py:80-111</code></p>
<h4 id="execute_model-内の呼び出し位置"><a class="header" href="#execute_model-内の呼び出し位置">execute_model() 内の呼び出し位置</a></h4>
<pre><code class="language-python"># GPUModelRunner.execute_model()

# (1) プリエンプション処理
if scheduler_output.preempted_req_ids and has_kv_transfer_group():
    get_kv_transfer_group().handle_preemptions(scheduler_output.preempted_req_ids)

# (2) forward不要時のKV転送
if num_reqs == 0:
    return self.kv_connector_no_forward(scheduler_output, self.vllm_config)

# (3) モデルforward + KVコネクタ
with self.maybe_get_kv_connector_output(scheduler_output) as kv_connector_output:
    model_output = self._model_forward(...)

# (4) sample_tokens()に引き渡し
self.kv_connector_output = kv_connector_output  # ephemeral state
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:3330-3617</code></p>
<h3 id="kvコネクタ初期化"><a class="header" href="#kvコネクタ初期化">KVコネクタ初期化</a></h3>
<p>GPUModelRunner.<code>_initialize_kv_caches()</code>内で、KVキャッシュ確保後にコネクタを登録する。</p>
<pre><code class="language-python">if has_kv_transfer_group():
    kv_transfer_group = get_kv_transfer_group()
    if self.cross_layers_kv_cache is not None:
        kv_transfer_group.register_cross_layers_kv_cache(
            self.cross_layers_kv_cache, self.cross_layers_attn_backend)
    else:
        kv_transfer_group.register_kv_caches(kv_caches)
    kv_transfer_group.set_host_xfer_buffer_ops(copy_kv_blocks)
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:6079-6088</code></p>
<h2 id="kvconnectoroutput"><a class="header" href="#kvconnectoroutput">KVConnectorOutput</a></h2>
<p>Worker→Schedulerへのフィードバック構造体。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>finished_sending</code></td><td><code>set[str] | None</code></td><td>送信完了リクエストID</td></tr>
<tr><td><code>finished_recving</code></td><td><code>set[str] | None</code></td><td>受信完了リクエストID</td></tr>
<tr><td><code>kv_connector_stats</code></td><td><code>KVConnectorStats | None</code></td><td>転送統計</td></tr>
<tr><td><code>kv_cache_events</code></td><td><code>KVConnectorKVEvents | None</code></td><td>ブロック保存/削除イベント</td></tr>
<tr><td><code>invalid_block_ids</code></td><td><code>set[int]</code></td><td>ロード失敗ブロックID</td></tr>
<tr><td><code>expected_finished_count</code></td><td><code>int</code></td><td>ハンドシェイクベースコネクタ用</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/outputs.py:123-148</code></p>
<h2 id="kv-cache-events"><a class="header" href="#kv-cache-events">KV Cache Events</a></h2>
<p>外部システム（ルーティング、モニタリング等）へのKVキャッシュ状態通知システム。</p>
<h3 id="イベント型"><a class="header" href="#イベント型">イベント型</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>イベント</th><th>フィールド</th><th>発行タイミング</th></tr>
</thead>
<tbody>
<tr><td><code>BlockStored</code></td><td><code>block_hashes, parent_block_hash, token_ids, block_size, lora_id, medium</code></td><td>KVブロック保存時</td></tr>
<tr><td><code>BlockRemoved</code></td><td><code>block_hashes, medium</code></td><td>KVブロック削除時</td></tr>
<tr><td><code>AllBlocksCleared</code></td><td>なし</td><td>全ブロッククリア時</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_events.py:49-84</code></p>
<h3 id="イベントフロー"><a class="header" href="#イベントフロー">イベントフロー</a></h3>
<ol>
<li>Worker側コネクタが<code>BlockStored</code>/<code>BlockRemoved</code>イベントを生成</li>
<li><code>_get_kv_connector_output()</code>がイベントを<code>KVConnectorOutput.kv_cache_events</code>に収集</li>
<li>Scheduler側で<code>update_connector_output()</code>→<code>KVEventAggregator</code>で全Worker共通イベントを集約</li>
<li><code>take_events()</code>で集約済みイベントを取得</li>
<li><code>EventPublisher</code>（ZMQ PUB/ROUTERまたはNull）で外部に配信</li>
</ol>
<h3 id="eventpublisher"><a class="header" href="#eventpublisher">EventPublisher</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>実装</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>NullEventPublisher</code></td><td>no-op（デフォルト）</td></tr>
<tr><td><code>ZmqEventPublisher</code></td><td>ZMQ PUB/ROUTERで配信。インメモリreplay buffer付き</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_events.py:205-473</code></p>
<h2 id="kvtransferconfig"><a class="header" href="#kvtransferconfig">KVTransferConfig</a></h2>
<p>KV Transferの設定。<code>--kv-transfer-config</code>で指定する。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>デフォルト</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>kv_connector</code></td><td>None</td><td>コネクタ名（“LMCacheConnectorV1“等）</td></tr>
<tr><td><code>kv_role</code></td><td>None</td><td><code>"kv_producer"</code> / <code>"kv_consumer"</code> / <code>"kv_both"</code></td></tr>
<tr><td><code>engine_id</code></td><td>UUID</td><td>エンジン識別子</td></tr>
<tr><td><code>kv_buffer_device</code></td><td>“cuda”</td><td>バッファデバイス</td></tr>
<tr><td><code>kv_buffer_size</code></td><td>1e9</td><td>バッファサイズ（バイト）</td></tr>
<tr><td><code>kv_rank</code></td><td>None</td><td>P/D内のランク（0=prefill, 1=decode）</td></tr>
<tr><td><code>kv_parallel_size</code></td><td>1</td><td>並列インスタンス数</td></tr>
<tr><td><code>kv_ip</code> / <code>kv_port</code></td><td>“127.0.0.1” / 14579</td><td>接続先</td></tr>
<tr><td><code>kv_connector_extra_config</code></td><td>{}</td><td>コネクタ固有追加設定</td></tr>
<tr><td><code>kv_connector_module_path</code></td><td>None</td><td>動的ロード用モジュールパス</td></tr>
<tr><td><code>kv_load_failure_policy</code></td><td>“recompute”</td><td>ロード失敗時ポリシー（“recompute” or “fail”）</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/config/kv_transfer.py:17-117</code></p>
<h2 id="ecconnectorとの比較"><a class="header" href="#ecconnectorとの比較">ECConnectorとの比較</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>観点</th><th>KV Transfer</th><th>ECConnector</th></tr>
</thead>
<tbody>
<tr><td>対象</td><td>デコーダKVキャッシュ</td><td>エンコーダキャッシュ</td></tr>
<tr><td>基底クラス</td><td><code>KVConnectorBase_V1</code></td><td><code>ECConnectorBase</code></td></tr>
<tr><td>abstractメソッド数</td><td>7</td><td>5</td></tr>
<tr><td>ロール分離</td><td>SCHEDULER / WORKER</td><td>SCHEDULER / WORKER</td></tr>
<tr><td>Factory</td><td><code>KVConnectorFactory</code></td><td><code>ECConnectorFactory</code></td></tr>
<tr><td>Mixin</td><td><code>KVConnectorModelRunnerMixin</code></td><td><code>ECConnectorModelRunnerMixin</code></td></tr>
<tr><td>レイヤー別操作</td><td>あり（save/load per layer）</td><td>なし（エンコーダ出力一括）</td></tr>
<tr><td>非同期ロード</td><td>あり（WAITING_FOR_REMOTE_KVS）</td><td>なし</td></tr>
<tr><td>イベント通知</td><td>あり（BlockStored/Removed）</td><td>なし</td></tr>
<tr><td>登録済み実装数</td><td>10</td><td>2（Example, SHM）</td></tr>
<tr><td>設定クラス</td><td><code>KVTransferConfig</code></td><td><code>ECTransferConfig</code></td></tr>
</tbody>
</table>
</div>
<h2 id="ディレクトリ構造"><a class="header" href="#ディレクトリ構造">ディレクトリ構造</a></h2>
<pre><code>target/vllm/vllm/distributed/kv_transfer/
├── kv_connector/
│   ├── base.py                    # KVConnectorBase リダイレクト
│   ├── factory.py                 # KVConnectorFactory + 10コネクタ登録
│   ├── utils.py
│   └── v1/
│       ├── base.py                # KVConnectorBase_V1 (抽象基底)
│       ├── metrics.py             # KVConnectorStats, Prometheus
│       ├── example_connector.py   # safetensorsデバッグ用
│       ├── lmcache_connector.py   # LMCacheラッパー
│       ├── lmcache_mp_connector.py
│       ├── nixl_connector.py
│       ├── offloading_connector.py
│       ├── multi_connector.py     # 複合コネクタ
│       ├── decode_bench_connector.py
│       ├── lmcache_integration/   # native LMCache実装
│       │   ├── vllm_v1_adapter.py
│       │   ├── multi_process_adapter.py
│       │   └── utils.py
│       ├── p2p/                   # P2P NCCLコネクタ
│       ├── moriio/                # MORIIOコネクタ
│       └── mooncake/              # Mooncakeコネクタ
├── kv_transfer_state.py           # グローバル状態管理
└── __init__.py

target/vllm/vllm/distributed/kv_events.py  # イベントシステム
target/vllm/vllm/config/kv_transfer.py     # KVTransferConfig
target/vllm/vllm/v1/worker/kv_connector_model_runner_mixin.py  # Mixin
</code></pre>
<h2 id="依存関係"><a class="header" href="#依存関係">依存関係</a></h2>
<h3 id="上流kv-transferを使う側"><a class="header" href="#上流kv-transferを使う側">上流（KV Transferを使う側）</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>使い方</th></tr>
</thead>
<tbody>
<tr><td>Scheduler</td><td>外部キャッシュ問い合わせ、メタデータ構築、完了処理</td></tr>
<tr><td>GPUModelRunner</td><td>Mixinでライフサイクル管理、KVキャッシュ登録</td></tr>
<tr><td>Attention層</td><td><code>wait_for_layer_load()</code> / <code>save_kv_layer()</code> 呼び出し</td></tr>
</tbody>
</table>
</div>
<h3 id="下流kv-transferが使う側"><a class="header" href="#下流kv-transferが使う側">下流（KV Transferが使う側）</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>使い方</th></tr>
</thead>
<tbody>
<tr><td>KVCacheManager</td><td>ブロック割り当て・解放情報の取得</td></tr>
<tr><td>ForwardContext</td><td>KVキャッシュテンソルへのアクセス</td></tr>
<tr><td>LMCache / NIXL / etc.</td><td>実際のKVデータ保存・取得</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="マルチモーダル処理パイプライン-サマリー"><a class="header" href="#マルチモーダル処理パイプライン-サマリー">マルチモーダル処理パイプライン サマリー</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-17"><a class="header" href="#概要-17">概要</a></h2>
<p>vLLMのマルチモーダル処理パイプラインは、画像・音声・動画等の非テキストデータをLLMの推論に統合するシステムである。フロントエンド（P0）でのメディア前処理・キャッシュと、バックエンド（P1）でのエンコーダ実行・埋め込みマージの2段構成で動作する。</p>
<h2 id="エンドツーエンド-データフロー"><a class="header" href="#エンドツーエンド-データフロー">エンドツーエンド データフロー</a></h2>
<pre class="mermaid">graph TD
    A["API Request&lt;br&gt;(messages + images)"] --&gt; B["ChatTemplate 適用&lt;br&gt;プレースホルダー挿入"]
    B --&gt; C["InputPreprocessor&lt;br&gt;トークナイズ + HF Processor"]

    C --&gt; D{"ProcessorCache&lt;br&gt;ヒット?"}
    D --&gt;|HIT| E["キャッシュから取得&lt;br&gt;(HF処理スキップ)"]
    D --&gt;|MISS| F["HF Processor 実行&lt;br&gt;pixel_values テンソル生成"]
    E --&gt; G["MultiModalFeatureSpec 構築"]
    F --&gt; G

    G --&gt; H["EngineCoreRequest&lt;br&gt;ZMQ IPC 送信"]

    H --&gt; I["Scheduler"]
    I --&gt; J{"EncoderCacheManager&lt;br&gt;ヒット?"}
    J --&gt;|HIT| K["エンコーダ計算スキップ"]
    J --&gt;|MISS| L["encoder_compute_budget&lt;br&gt;から割り当て"]

    K --&gt; M["GPUModelRunner"]
    L --&gt; M

    M --&gt; N["_execute_mm_encoder()&lt;br&gt;model.embed_multimodal()"]
    N --&gt; O["encoder_cache に格納"]
    O --&gt; P["_gather_mm_embeddings()&lt;br&gt;キャッシュからスライス"]
    P --&gt; Q["embed_input_ids()&lt;br&gt;text + vision マージ&lt;br&gt;(masked_scatter_)"]
    Q --&gt; R["model.forward()&lt;br&gt;統合推論"]
</pre>

<h2 id="主要コンポーネント-6"><a class="header" href="#主要コンポーネント-6">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>場所</th><th>役割</th></tr>
</thead>
<tbody>
<tr><td><code>MULTIMODAL_REGISTRY</code></td><td><code>vllm/multimodal/registry.py</code></td><td>モデルごとのプロセッサ/情報を登録・取得</td></tr>
<tr><td><code>BaseMultiModalProcessor</code></td><td><code>vllm/multimodal/processing/processor.py</code></td><td>HFプロセッサ実行、プロンプト更新管理</td></tr>
<tr><td><code>MultiModalHasher</code></td><td><code>vllm/multimodal/hasher.py</code></td><td>コンテンツベースハッシュ（blake3）</td></tr>
<tr><td><code>ProcessorCache</code> (4種)</td><td><code>vllm/multimodal/cache.py</code></td><td>P0側のHF処理結果キャッシュ</td></tr>
<tr><td><code>EncoderCacheManager</code></td><td><code>vllm/v1/core/encoder_cache_manager.py</code></td><td>P1側のエンコーダ出力の論理管理</td></tr>
<tr><td><code>encoder_cache</code></td><td><code>vllm/v1/worker/gpu_model_runner.py:439</code></td><td>GPU上のエンコーダ出力テンソルキャッシュ</td></tr>
</tbody>
</table>
</div>
<h2 id="キャッシュの3層構造"><a class="header" href="#キャッシュの3層構造">キャッシュの3層構造</a></h2>
<pre><code>P0（フロントエンド）               P1（Scheduler）              P1（GPU）
┌──────────────────┐           ┌─────────────────┐        ┌──────────────┐
│ ProcessorCache   │           │ EncoderCache    │        │ encoder_cache│
│ LRU, サイズベース │           │ Manager         │        │ dict[str,    │
│                  │           │ RefCount + FIFO │        │  Tensor]     │
│ 何をキャッシュ:   │           │                 │        │              │
│ HF処理済みテンソル │           │ 何を管理:       │        │ 何をキャッシュ:│
│ + prompt_updates │           │ 容量・参照カウント│       │ エンコーダ出力│
│                  │           │ Evictionリスト   │        │ (GPUテンソル) │
└──────────────────┘           └─────────────────┘        └──────────────┘
  ヒット時:                      ヒット時:                   ヒット時:
  HF処理スキップ                 エンコーダ計算スキップ        テンソル再利用
  IPC転送量削減                  予算節約                    再計算不要
</code></pre>
<h2 id="テキスト推論との主な差分"><a class="header" href="#テキスト推論との主な差分">テキスト推論との主な差分</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>処理段階</th><th>テキスト推論</th><th>マルチモーダル推論</th></tr>
</thead>
<tbody>
<tr><td>入力前処理</td><td>tokenize のみ</td><td>tokenize + HF Processor + ハッシュ + キャッシュ</td></tr>
<tr><td>プロンプト</td><td>テキストトークンのみ</td><td>テキスト + プレースホルダートークン（<code>&lt;start_of_image&gt;</code> 等）</td></tr>
<tr><td>EngineCoreRequest</td><td><code>mm_features = None</code></td><td><code>mm_features = [MultiModalFeatureSpec, ...]</code></td></tr>
<tr><td>Scheduler</td><td>KVキャッシュ予算のみ</td><td>+ エンコーダ計算予算管理</td></tr>
<tr><td>GPUModelRunner</td><td>input_ids → model.forward()</td><td>encoder実行 → embed_input_ids(masked_scatter_) → inputs_embeds → model.forward()</td></tr>
</tbody>
</table>
</div>
<h2 id="gemma3-固有の特徴"><a class="header" href="#gemma3-固有の特徴">Gemma3 固有の特徴</a></h2>
<ul>
<li><strong>ビジョンエンコーダ</strong>: SiglipVisionModel（SIGLIP ViT、双方向Attention）</li>
<li><strong>プロジェクタ</strong>: AvgPool2d → GemmaRMSNorm → Linear（vision → text空間）</li>
<li><strong>プレースホルダー</strong>: <code>&lt;start_of_image&gt;</code> → <code>image_token × 256</code> に展開</li>
<li><strong>Pan-and-Scan</strong>: アスペクト比が大きい画像を複数クロップ（V1では簡略化されたアテンション）</li>
<li><strong>改行トークン結合</strong>: <code>\n</code> + <code>\n\n</code> → <code>\n\n\n</code> 等の特殊処理</li>
</ul>
<h2 id="詳細ドキュメント-1"><a class="header" href="#詳細ドキュメント-1">詳細ドキュメント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ドキュメント</th><th>内容</th></tr>
</thead>
<tbody>
<tr><td><a href="#フロントエンド-マルチモーダル処理パス-mediumdeep3-verified">mm-processing.md</a></td><td>フロントエンド: チャットテンプレート、プレースホルダー、MMハッシュ[DEEP]（hash_kwargs/serialize_item/iter_item_to_bytes詳細、identifier vs mm_hash使い分け、プレフィックスキャッシュ連携）、プロセッサキャッシュ4種、ZMQ送信データ</td></tr>
<tr><td><a href="#バックエンド-マルチモーダル処理パス-medium-verified">mm-engine-gpu.md</a></td><td>バックエンド: EncoderCacheManager、Schedulerエンコーダ予算、GPUModelRunnerエンコーダ実行・キャッシュ・埋め込みマージ</td></tr>
<tr><td><a href="#gemma3-ビジョンエンコーダと画像処理-medium-verified">gemma3-vision.md</a></td><td>Gemma3: SiglipVisionModel、MultiModalProjector、Pan-and-Scan、masked_scatter_マージ</td></tr>
</tbody>
</table>
</div>
<h2 id="上流下流"><a class="header" href="#上流下流">上流・下流</a></h2>
<ul>
<li><strong>上流</strong>: <a href="#エントリポイント-asyncllm--llm-サマリー">エントリポイント</a> → <a href="#inputprocessor-サマリー">InputProcessor</a></li>
<li><strong>下流</strong>: <a href="#scheduler-サマリー">Scheduler</a> → <a href="#gpumodelrunner">GPUModelRunner</a> → モデル層</li>
</ul>
<h2 id="主要ファイル-4"><a class="header" href="#主要ファイル-4">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>概要</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/multimodal/</code></td><td>マルチモーダル処理の基盤（レジストリ、ハッシュ、キャッシュ、パース）</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/input_processor.py</code></td><td>フロントエンドでのMM処理統合</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/encoder_cache_manager.py</code></td><td>バックエンドのエンコーダキャッシュ管理</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_model_runner.py</code></td><td>GPU上でのエンコーダ実行と埋め込みマージ</td></tr>
<tr><td><code>target/vllm/vllm/model_executor/models/gemma3_mm.py</code></td><td>Gemma3のマルチモーダル実装</td></tr>
<tr><td><code>target/vllm/vllm/model_executor/models/siglip.py</code></td><td>SiglipVisionModel（ビジョンエンコーダ）</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="gemma3-ビジョンエンコーダと画像処理-medium-verified"><a class="header" href="#gemma3-ビジョンエンコーダと画像処理-medium-verified">Gemma3 ビジョンエンコーダと画像処理 [MEDIUM] [VERIFIED]</a></h1>
<blockquote>
<p><strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<p>Gemma3 モデルにおけるビジョンエンコーダ（SiglipVisionModel）、プロジェクタ（Gemma3MultiModalProjector）、および text + vision 埋め込みマージの詳細。</p>
<h2 id="モデルアーキテクチャ全体像"><a class="header" href="#モデルアーキテクチャ全体像">モデルアーキテクチャ全体像</a></h2>
<pre class="mermaid">graph TD
    subgraph "Gemma3ForConditionalGeneration"
        subgraph "vision_tower (SiglipVisionModel)"
            PE["SiglipVisionEmbeddings&lt;br&gt;Conv2d patch embedding&lt;br&gt;+ position embedding"]
            ENC["SiglipEncoder&lt;br&gt;N層 Transformer&lt;br&gt;(双方向Attention)"]
            PE --&gt; ENC
        end

        PROJ["Gemma3MultiModalProjector&lt;br&gt;AvgPool2d → RMSNorm → Linear"]
        ENC --&gt; PROJ

        subgraph "language_model (Gemma3ForCausalLM)"
            EMB["embed_tokens()&lt;br&gt;テキスト埋め込み"]
            LM["Transformer Decoder&lt;br&gt;(因果的Attention)"]
            EMB --&gt; LM
        end

        MERGE["masked_scatter_&lt;br&gt;text + vision マージ"]
        PROJ --&gt; MERGE
        EMB --&gt; MERGE
        MERGE --&gt; LM
    end
</pre>

<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:481-694</code></p>
<h2 id="1-siglipvisionmodelビジョンエンコーダ"><a class="header" href="#1-siglipvisionmodelビジョンエンコーダ">1. SiglipVisionModel（ビジョンエンコーダ）</a></h2>
<h3 id="構造"><a class="header" href="#構造">構造</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/siglip.py:848-894</code></p>
<pre><code>SiglipVisionModel
  └─ SiglipVisionTransformer (L681)
      ├─ SiglipVisionEmbeddings (L282)
      │   ├─ patch_embedding: Conv2d(3, hidden_size, kernel=patch_size, stride=patch_size)
      │   └─ position_embedding: Embedding(num_patches, hidden_size)
      ├─ SiglipEncoder (L520)
      │   └─ layers: ModuleList[SiglipEncoderLayer] × N
      │       ├─ layer_norm1 → SiglipAttention (MMEncoderAttention)
      │       └─ layer_norm2 → SiglipMLP
      └─ post_layernorm: LayerNorm（最終層のみ）
</code></pre>
<h3 id="パッチ埋め込み-siglipvisionembeddings"><a class="header" href="#パッチ埋め込み-siglipvisionembeddings">パッチ埋め込み (SiglipVisionEmbeddings)</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/siglip.py:282-352</code></p>
<pre><code class="language-python"># 入力: pixel_values (batch, 3, H, W)
patch_embeds = self.patch_embedding(pixel_values)  # Conv2d: (batch, hidden_size, grid, grid)
embeddings = patch_embeds.flatten(2).transpose(1, 2)  # (batch, num_patches, hidden_size)
embeddings += self.position_embedding(position_ids)    # 位置埋め込みを加算
</code></pre>
<ul>
<li><code>image_size = 256</code> の場合、<code>patch_size = 16</code> → <code>num_patches = (256/16)² = 256</code></li>
<li>各パッチは <code>16×16×3 = 768</code> ピクセルから <code>hidden_size</code> 次元のベクトルに変換</li>
<li>位置埋め込みは学習済みの <code>nn.Embedding</code>（補間対応あり）</li>
</ul>
<h3 id="エンコーダ層-siglipencoder"><a class="header" href="#エンコーダ層-siglipencoder">エンコーダ層 (SiglipEncoder)</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/siglip.py:520-567</code></p>
<p>各エンコーダ層の構造:</p>
<pre><code>SiglipEncoderLayer:
    residual = hidden_states
    hidden_states = layer_norm1(hidden_states)
    hidden_states = self_attn(hidden_states)     ← MMEncoderAttention（双方向）
    hidden_states = residual + hidden_states
    residual = hidden_states
    hidden_states = layer_norm2(hidden_states)
    hidden_states = mlp(hidden_states)
    hidden_states = residual + hidden_states
</code></pre>
<ul>
<li><strong>アテンション型</strong>: <code>MMEncoderAttention</code>（双方向、因果マスクなし）</li>
<li>全パッチが全パッチを参照できる（テキストの因果アテンションとは異なる）</li>
</ul>
<h3 id="forward-フロー"><a class="header" href="#forward-フロー">forward() フロー</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/siglip.py:755-788</code></p>
<pre><code class="language-python">def forward(self, pixel_values, *, select_layers=None, feature_select_strategy=None):
    hidden_states = self.embeddings(pixel_values)      # (batch, num_patches, hidden_size)
    encoder_outputs = self.encoder(inputs_embeds=hidden_states)  # N層 Transformer
    encoder_outputs = resolve_visual_encoder_outputs(   # 特徴選択
        encoder_outputs, select_layers, feature_select_strategy
    )
    return self.last_hs_proc(encoder_outputs)           # post_layernorm + head
</code></pre>
<p>出力: <code>(batch, num_patches, hidden_size)</code> — 典型的に <code>(batch, 256, 1152)</code></p>
<h2 id="2-gemma3multimodalprojector投射層"><a class="header" href="#2-gemma3multimodalprojector投射層">2. Gemma3MultiModalProjector（投射層）</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:432-473</code></p>
<p>ビジョンエンコーダの出力をテキスト埋め込み空間に変換する。</p>
<h3 id="パラメータ"><a class="header" href="#パラメータ">パラメータ</a></h3>
<pre><code class="language-python">mm_input_projection_weight: (vision_hidden_size, text_hidden_size)  # 例: (1152, 2048)
mm_soft_emb_norm: GemmaRMSNorm(vision_hidden_size)
avg_pool: AvgPool2d(kernel_size, stride=kernel_size)
</code></pre>
<h3 id="設定値の計算"><a class="header" href="#設定値の計算">設定値の計算</a></h3>
<pre><code class="language-python">patches_per_image = image_size // patch_size           # 256 // 16 = 16
tokens_per_side = int(mm_tokens_per_image ** 0.5)      # int(256 ** 0.5) = 16
kernel_size = patches_per_image // tokens_per_side     # 16 // 16 = 1
</code></pre>
<p><code>kernel_size = 1</code> の場合、AvgPool2dは実質的にno-op（パッチ数が変わらない）。<code>mm_tokens_per_image</code> が小さい設定ではダウンサンプリングが発生する。</p>
<h3 id="forward-フロー-1"><a class="header" href="#forward-フロー-1">forward() フロー</a></h3>
<pre><code class="language-python">def forward(self, vision_outputs):
    # vision_outputs: (batch, hidden_size, num_patches) — エンコーダ出力のtranspose形式
    batch_size, _, seq_length = vision_outputs.shape

    # 1. 2Dグリッドへリシェイプ
    reshaped = vision_outputs.transpose(1, 2).reshape(
        batch_size, seq_length, patches_per_image, patches_per_image
    )  # (batch, seq_length, 16, 16)

    # 2. 平均プーリング（ダウンサンプリング）
    pooled = self.avg_pool(reshaped)  # kernel=1の場合: (batch, seq_length, 16, 16)
    pooled = pooled.flatten(2).transpose(1, 2)  # (batch, mm_tokens_per_image, seq_length)

    # 3. RMS正規化
    normed = self.mm_soft_emb_norm(pooled)

    # 4. 線形投射: vision_hidden_size → text_hidden_size
    projected = torch.matmul(normed, self.mm_input_projection_weight)
    # (batch, mm_tokens_per_image, text_hidden_size)

    return projected.type_as(vision_outputs)
</code></pre>
<p><strong>重要な注意</strong>: テキスト埋め込みの正規化とは異なり、ビジョン埋め込みには <code>mm_soft_emb_norm</code> のみが適用される。vocab embedding に適用されるスケーリング（<code>embed_tokens * normalizer</code>）はビジョン埋め込みには適用 <strong>されない</strong>。</p>
<h2 id="3-embed_multimodal-と-_process_image_input"><a class="header" href="#3-embed_multimodal-と-_process_image_input">3. embed_multimodal() と _process_image_input()</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:567-594</code></p>
<p>GPUModelRunner の <code>_execute_mm_encoder()</code> から呼ばれるメインのエンコーダ実行パス:</p>
<pre><code class="language-python">def embed_multimodal(self, **kwargs):
    image_input = self._parse_and_validate_image_input(**kwargs)  # pixel_values 取得
    if image_input is None:
        return []
    return self._process_image_input(image_input)

def _process_image_input(self, image_input):
    pixel_values = image_input["pixel_values"]    # (total_patches, 3, image_size, image_size)
    num_patches = image_input["num_patches"]       # (num_images,) — 画像ごとのパッチ数

    # ビジョンエンコーダ実行
    image_features = self._image_pixels_to_features(self.vision_tower, pixel_values)

    # プロジェクタで投射
    image_embeds = self.multi_modal_projector(image_features)

    # 画像ごとに分割 + flatten
    return [e.flatten(0, 1) for e in image_embeds.split(num_patches.tolist())]
    # list[Tensor(mm_tokens_per_image, text_hidden_size)]
</code></pre>
<h3 id="テンソル形状の追跡"><a class="header" href="#テンソル形状の追跡">テンソル形状の追跡</a></h3>
<p>画像1枚、Pan-and-Scan なし（<code>num_patches=1</code>）の場合:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ステージ</th><th>形状</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td>入力 pixel_values</td><td><code>(1, 3, 256, 256)</code></td><td>RGB画像</td></tr>
<tr><td>patch_embedding</td><td><code>(1, 1152, 16, 16)</code></td><td>Conv2d出力</td></tr>
<tr><td>flatten + transpose</td><td><code>(1, 256, 1152)</code></td><td>パッチシーケンス</td></tr>
<tr><td>+ position_embedding</td><td><code>(1, 256, 1152)</code></td><td>位置情報付加</td></tr>
<tr><td>SiglipEncoder (N層)</td><td><code>(1, 256, 1152)</code></td><td>Transformer処理</td></tr>
<tr><td>Projector reshape</td><td><code>(1, 256, 16, 16)</code></td><td>2Dグリッド</td></tr>
<tr><td>AvgPool2d (k=1)</td><td><code>(1, 256, 16, 16)</code></td><td>no-op</td></tr>
<tr><td>flatten + transpose</td><td><code>(1, 256, 256)</code></td><td>?</td></tr>
<tr><td>mm_soft_emb_norm</td><td><code>(1, 256, 1152)</code></td><td>RMS正規化</td></tr>
<tr><td>matmul(projection)</td><td><code>(1, 256, 2048)</code></td><td>テキスト空間</td></tr>
<tr><td>flatten(0,1)</td><td><code>(256, 2048)</code></td><td>最終出力</td></tr>
</tbody>
</table>
</div>
<h2 id="4-テキスト--ビジョン埋め込みのマージ"><a class="header" href="#4-テキスト--ビジョン埋め込みのマージ">4. テキスト + ビジョン埋め込みのマージ</a></h2>
<h3 id="embed_input_ids"><a class="header" href="#embed_input_ids">embed_input_ids()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:596-614</code></p>
<pre><code class="language-python">def embed_input_ids(self, input_ids, multimodal_embeddings=None, *, is_multimodal=None, ...):
    if multimodal_embeddings is None or is_multimodal is None:
        return super().embed_input_ids(input_ids)  # テキストのみ
    return super().embed_input_ids(
        input_ids, multimodal_embeddings=multimodal_embeddings,
        is_multimodal=is_multimodal, handle_oov_mm_token=True,
    )
</code></pre>
<h3 id="_merge_multimodal_embeddings"><a class="header" href="#_merge_multimodal_embeddings">_merge_multimodal_embeddings()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/utils.py:445-487</code></p>
<pre><code class="language-python">def _merge_multimodal_embeddings(inputs_embeds, multimodal_embeddings, is_multimodal):
    mm_embeds_flat = _flatten_embeddings(multimodal_embeddings)
    # in-place 置換: is_multimodal=True の位置を mm_embeds_flat で上書き
    inputs_embeds.masked_scatter_(
        is_multimodal.unsqueeze(-1),
        mm_embeds_flat.to(dtype=inputs_embeds.dtype)
    )
    return inputs_embeds
</code></pre>
<p><strong><code>masked_scatter_</code> の動作</strong>:</p>
<ol>
<li><code>is_multimodal</code>: <code>(seq_len,)</code> のboolテンソル（True = 画像プレースホルダー位置）</li>
<li><code>is_multimodal.unsqueeze(-1)</code>: <code>(seq_len, 1)</code> → ブロードキャストで <code>(seq_len, hidden_size)</code> に展開</li>
<li><code>mm_embeds_flat</code>: True位置の数 × hidden_size の連続テンソル</li>
<li>True位置に順番に mm_embeds_flat の値を書き込む</li>
</ol>
<p><strong>制約</strong>: <code>is_multimodal.sum() == len(mm_embeds_flat)</code> でなければランタイムエラー</p>
<h3 id="handle_oov_mm_token"><a class="header" href="#handle_oov_mm_token">handle_oov_mm_token</a></h3>
<p>Gemma3 は <code>handle_oov_mm_token=True</code> を指定。これは image_token_id が vocab の範囲外の場合でも安全に処理するための仕組み。</p>
<h2 id="5-pan-and-scanパノラマクロップ"><a class="header" href="#5-pan-and-scanパノラマクロップ">5. Pan-and-Scan（パノラマクロップ）</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:109-176</code></p>
<p>アスペクト比が大きい画像に対して、複数のクロップを生成して詳細認識を向上させる仕組み。</p>
<h3 id="クロップ数の計算"><a class="header" href="#クロップ数の計算">クロップ数の計算</a></h3>
<pre><code class="language-python">def get_num_crops(self, *, image_width, image_height, processor):
    # 横長画像の場合
    if image_width &gt;= image_height:
        if width/height &lt; min_ratio:  return 0  # 比率が小さすぎる
        num_crops_w = min(floor(width/min_crop_size), floor(w/h + 0.5))
        num_crops_w = max(2, num_crops_w)
        num_crops_w = min(max_num_crops, num_crops_w)
        num_crops_h = 1

    # 縦長画像の場合は逆
    ...

    # クロップサイズが小さすぎる場合は無効
    if min(crop_size_w, crop_size_h) &lt; min_crop_size:
        return 0

    return num_crops_w * num_crops_h
</code></pre>
<h3 id="pan-and-scan時のプロンプト"><a class="header" href="#pan-and-scan時のプロンプト">Pan-and-Scan時のプロンプト</a></h3>
<pre><code>"Here is the original image &lt;full_image_seq&gt; and here are some crops to help you see better &lt;full_image_seq&gt; &lt;full_image_seq&gt;"
</code></pre>
<p>各 <code>&lt;full_image_seq&gt;</code> は <code>image_seq_length</code>（=256）トークンを消費。</p>
<p><strong>V1での制限</strong>: Pan-and-Scan は簡略化されたアテンションパターンを使用するため、最適ではない結果になる可能性がある。</p>
<h2 id="6-forward--最終推論"><a class="header" href="#6-forward--最終推論">6. forward() — 最終推論</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:616-635</code></p>
<pre><code class="language-python">def forward(self, input_ids, positions, intermediate_tensors=None, inputs_embeds=None, **kwargs):
    if intermediate_tensors is not None:
        inputs_embeds = None  # Pipeline Parallelism の中間ランク

    hidden_states = self.language_model.model(
        input_ids,
        positions,
        intermediate_tensors,
        inputs_embeds=inputs_embeds,  # text + vision マージ済み
        **kwargs,
    )
    return hidden_states
</code></pre>
<p>マルチモーダル時は <code>inputs_embeds</code> が渡され、<code>input_ids</code> は使用されない（embed_input_ids で既に埋め込み済みのため）。</p>
<h2 id="データフロー全体"><a class="header" href="#データフロー全体">データフロー全体</a></h2>
<pre><code>pixel_values: (total_patches, 3, 256, 256)
      │
      ▼
SiglipVisionEmbeddings
  Conv2d(3→1152, k=16, s=16) + position_embedding
      │
      ▼
(total_patches, 256, 1152)
      │
      ▼
SiglipEncoder (N層 Transformer, 双方向Attention)
      │
      ▼
(total_patches, 256, 1152)
      │
      ▼
Gemma3MultiModalProjector
  reshape → AvgPool2d → RMSNorm → matmul(1152→2048)
      │
      ▼
(total_patches, 256, 2048)
      │
      ▼
split by num_patches → list[(mm_tokens, 2048)]
      │
      ▼
encoder_cache[mm_hash] に格納
      │
      ▼
_gather_mm_embeddings() でスライス
      │
      ▼
embed_input_ids():
  text_embeds = embed_tokens(input_ids)     # (seq_len, 2048)
  merged = masked_scatter_(text_embeds, is_multimodal, mm_embeds)
      │
      ▼
language_model.model(inputs_embeds=merged)  # Gemma3 Decoder
</code></pre>
<h2 id="主要ファイル-5"><a class="header" href="#主要ファイル-5">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th><th>行</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/model_executor/models/gemma3_mm.py</code></td><td><code>Gemma3ForConditionalGeneration</code></td><td>L481</td></tr>
<tr><td><code>target/vllm/vllm/model_executor/models/gemma3_mm.py</code></td><td><code>Gemma3MultiModalProjector</code></td><td>L432</td></tr>
<tr><td><code>target/vllm/vllm/model_executor/models/gemma3_mm.py</code></td><td><code>Gemma3ProcessingInfo</code>, <code>Gemma3MultiModalProcessor</code></td><td>L77, L276</td></tr>
<tr><td><code>target/vllm/vllm/model_executor/models/siglip.py</code></td><td><code>SiglipVisionModel</code></td><td>L848</td></tr>
<tr><td><code>target/vllm/vllm/model_executor/models/siglip.py</code></td><td><code>SiglipVisionEmbeddings</code></td><td>L282</td></tr>
<tr><td><code>target/vllm/vllm/model_executor/models/siglip.py</code></td><td><code>SiglipEncoder</code></td><td>L520</td></tr>
<tr><td><code>target/vllm/vllm/model_executor/models/utils.py</code></td><td><code>_merge_multimodal_embeddings()</code></td><td>L445</td></tr>
</tbody>
</table>
</div>
<h2 id="関連ドキュメント-8"><a class="header" href="#関連ドキュメント-8">関連ドキュメント</a></h2>
<ul>
<li><a href="#フロントエンド-マルチモーダル処理パス-mediumdeep3-verified">フロントエンド MM処理パス</a></li>
<li><a href="#バックエンド-マルチモーダル処理パス-medium-verified">バックエンド MM処理パス</a></li>
<li><a href="#マルチモーダル処理パイプライン-サマリー">マルチモーダル処理パイプライン概要</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="バックエンド-マルチモーダル処理パス-medium-verified"><a class="header" href="#バックエンド-マルチモーダル処理パス-medium-verified">バックエンド マルチモーダル処理パス [MEDIUM] [VERIFIED]</a></h1>
<blockquote>
<p><strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<p>EngineCore（P1）でマルチモーダルリクエストがどのように処理されるかを追跡する。EncoderCacheManager、Schedulerのエンコーダ予算管理、GPUModelRunnerのエンコーダ実行・キャッシュ・埋め込みマージを含む。</p>
<h2 id="全体フロー"><a class="header" href="#全体フロー">全体フロー</a></h2>
<pre class="mermaid">sequenceDiagram
    participant S as Scheduler
    participant ECM as EncoderCacheManager
    participant MR as GPUModelRunner
    participant M as Model

    Note over S: schedule() 実行中
    S-&gt;&gt;ECM: check_and_update_cache(req, i)
    alt キャッシュヒット
        ECM--&gt;&gt;S: True（エンコード不要）
    else キャッシュミス
        ECM--&gt;&gt;S: False
        S-&gt;&gt;ECM: can_allocate(req, i, budget, scheduled)
        alt 空き/回収可能
            ECM--&gt;&gt;S: True（Eviction実行の可能性あり）
            S-&gt;&gt;ECM: allocate(req, i)
            Note over S: scheduled_encoder_inputs[req_id].append(i)
        else 不足
            ECM--&gt;&gt;S: False
            Note over S: num_new_tokens を調整
        end
    end

    Note over S: SchedulerOutput 構築
    S-&gt;&gt;MR: execute_model(scheduler_output)

    MR-&gt;&gt;MR: _batch_mm_inputs_from_scheduler()
    MR-&gt;&gt;M: embed_multimodal(pixel_values)
    M--&gt;&gt;MR: encoder_outputs
    MR-&gt;&gt;MR: encoder_cache[mm_hash] = output

    MR-&gt;&gt;MR: _gather_mm_embeddings()
    Note over MR: encoder_cache からスライス
    MR-&gt;&gt;M: embed_input_ids(ids, mm_embeds, is_multimodal)
    Note over M: masked_scatter_ で text + vision マージ

    MR-&gt;&gt;MR: free_encoder_mm_hashes の処理
    Note over MR: encoder_cache.pop(mm_hash)
</pre>

<h2 id="1-encodercachemanager"><a class="header" href="#1-encodercachemanager">1. EncoderCacheManager</a></h2>
<h3 id="概要-18"><a class="header" href="#概要-18">概要</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:17-267</code></p>
<p>ビジョンエンコーダの出力（埋め込みテンソル）のライフサイクルを管理する。リファレンスカウント方式でリクエスト間のキャッシュ共有を実現し、遅延Evictionでメモリ効率を高める。</p>
<h3 id="データ構造-1"><a class="header" href="#データ構造-1">データ構造</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>cache_size</code></td><td><code>int</code></td><td>キャッシュ容量（エンコーダ埋め込み数単位）</td></tr>
<tr><td><code>num_free_slots</code></td><td><code>int</code></td><td>現在の空き容量</td></tr>
<tr><td><code>num_freeable_slots</code></td><td><code>int</code></td><td>回収可能な容量（参照なしエントリ含む）</td></tr>
<tr><td><code>cached</code></td><td><code>dict[str, set[str]]</code></td><td>mm_hash → 参照中のrequest_id集合</td></tr>
<tr><td><code>freeable</code></td><td><code>OrderedDict[str, int]</code></td><td>mm_hash → 埋め込み数（参照なし、回収可能）</td></tr>
<tr><td><code>freed</code></td><td><code>list[str]</code></td><td>実際にEvictされたmm_hashのリスト</td></tr>
</tbody>
</table>
</div>
<h3 id="主要操作"><a class="header" href="#主要操作">主要操作</a></h3>
<h4 id="check_and_update_cacherequest-input_id--bool"><a class="header" href="#check_and_update_cacherequest-input_id--bool">check_and_update_cache(request, input_id) → bool</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:91-117</code></p>
<pre><code>1. mm_hash が cached にない → False（キャッシュミス）
2. cached[mm_hash] が空集合（参照なし）→ freeable から除去、num_freeable_slots 減算
3. cached[mm_hash] に request_id 追加 → True（キャッシュヒット）
</code></pre>
<p>キャッシュヒット時、エンコーダ計算が <strong>完全にスキップ</strong> される。</p>
<h4 id="can_allocaterequest-input_id-budget-scheduled--bool"><a class="header" href="#can_allocaterequest-input_id-budget-scheduled--bool">can_allocate(request, input_id, budget, scheduled) → bool</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:119-178</code></p>
<pre><code>1. num_embeds &gt; encoder_compute_budget → False（予算不足）
2. total ≤ num_free_slots → True（空きあり）
3. total &gt; num_freeable_slots → False（回収しても不足）
4. total &gt; num_free_slots かつ ≤ num_freeable_slots
   → Eviction 実行: freeable から oldest-first で popitem(last=False)
   → cached から削除、freed に追加
   → num_free_slots 回復 → True
</code></pre>
<p><strong>Eviction ポリシー</strong>: FIFO順（OrderedDict の挿入順）。最も古い unreferenced エントリを先にEvict。</p>
<h4 id="allocaterequest-input_id"><a class="header" href="#allocaterequest-input_id">allocate(request, input_id)</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:180-205</code></p>
<p>キャッシュスペースを予約（論理的な簿記のみ）。物理メモリの割り当てはGPUModelRunnerで行われる。</p>
<h4 id="freerequest"><a class="header" href="#freerequest">free(request)</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:242-253</code></p>
<p>リクエスト完了時に呼ばれる。全エンコーダ入力の参照を解放。参照セットが空になったエントリは <code>freeable</code> に移動する（物理メモリは解放しない）。</p>
<h4 id="get_freed_mm_hashes--liststr"><a class="header" href="#get_freed_mm_hashes--liststr">get_freed_mm_hashes() → list[str]</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:255-266</code></p>
<p>Evictされたmm_hashを返してクリア。SchedulerOutputに含められ、GPUModelRunnerに通知される。</p>
<h3 id="キャッシュ予算の計算"><a class="header" href="#キャッシュ予算の計算">キャッシュ予算の計算</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:269-316</code></p>
<pre><code class="language-python">encoder_compute_budget = max(max_num_encoder_input_tokens, max_tokens_per_mm_item)
encoder_cache_size = max(encoder_cache_size_config, max_tokens_per_mm_item)
</code></pre>
<ul>
<li><code>encoder_compute_budget</code>: 1ステップあたりのエンコーダ計算量上限（埋め込み数）</li>
<li><code>encoder_cache_size</code>: キャッシュ全体の容量</li>
</ul>
<h2 id="2-scheduler-のエンコーダスケジューリング"><a class="header" href="#2-scheduler-のエンコーダスケジューリング">2. Scheduler のエンコーダスケジューリング</a></h2>
<h3 id="_get_encoder_budget"><a class="header" href="#_get_encoder_budget">_get_encoder_budget()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:1060-1215</code></p>
<p>各リクエストのエンコーダ入力をスケジューリングするロジック。<code>mm_features</code> の各アイテムについて:</p>
<pre><code>for i, mm_feature in enumerate(mm_features):
    1. 位置チェック: start_pos + num_encoder_tokens がスケジュール範囲内か
    2. 重複チェック: 同じ mm_hash が既にスケジュール済みか
    3. キャッシュチェック: encoder_cache_manager.check_and_update_cache() → True ならスキップ
    4. チャンクMMチェック: disable_chunked_mm_input の場合、部分スケジュール禁止
    5. 割り当てチェック: can_allocate() → False ならトークン数調整して break
    6. ECConnectorチェック: 外部キャッシュにある場合は external_load_encoder_input に追加
    7. encoder_inputs_to_schedule に追加、予算減算
</code></pre>
<h3 id="scheduleroutput-のmm関連フィールド"><a class="header" href="#scheduleroutput-のmm関連フィールド">SchedulerOutput のMM関連フィールド</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/output.py:207-218</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>scheduled_encoder_inputs</code></td><td><code>dict[str, list[int]]</code></td><td>req_id → エンコーダ入力インデックスのリスト</td></tr>
<tr><td><code>free_encoder_mm_hashes</code></td><td><code>list[str]</code></td><td>解放すべきmm_hashのリスト</td></tr>
</tbody>
</table>
</div>
<h3 id="スケジューリングの統合"><a class="header" href="#スケジューリングの統合">スケジューリングの統合</a></h3>
<p>schedule() のRUNNINGフェーズとWAITINGフェーズの両方で <code>_get_encoder_budget()</code> が呼ばれる:</p>
<pre><code>schedule()
  ├─ encoder_compute_budget = max_num_encoder_input_tokens  # 初期予算
  │
  ├─ RUNNING リクエスト処理
  │   └─ _get_encoder_budget(request, ...)
  │       → encoder_inputs_to_schedule, 調整後の num_new_tokens
  │       → allocate() 実行
  │
  ├─ WAITING リクエスト処理
  │   └─ _get_encoder_budget(request, ...)
  │       → 同上
  │
  └─ SchedulerOutput 構築
      ├─ scheduled_encoder_inputs = {req_id: [input_ids], ...}
      └─ free_encoder_mm_hashes = encoder_cache_manager.get_freed_mm_hashes()
</code></pre>
<h2 id="3-gpumodelrunner-のエンコーダ実行"><a class="header" href="#3-gpumodelrunner-のエンコーダ実行">3. GPUModelRunner のエンコーダ実行</a></h2>
<h3 id="encoder_cache"><a class="header" href="#encoder_cache">encoder_cache</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:439</code></p>
<pre><code class="language-python">self.encoder_cache: dict[str, torch.Tensor] = {}
</code></pre>
<p>mm_hash をキーとして、エンコーダ出力テンソル（GPU上）を保持する単純なdictキャッシュ。</p>
<h3 id="_batch_mm_inputs_from_scheduler"><a class="header" href="#_batch_mm_inputs_from_scheduler">_batch_mm_inputs_from_scheduler()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:2250-2291</code></p>
<p>SchedulerOutput の <code>scheduled_encoder_inputs</code> から、エンコーダに渡すデータをバッチにまとめる。</p>
<pre><code>入力: scheduled_encoder_inputs = {req_id: [input_ids], ...}
出力: (mm_hashes, mm_kwargs, mm_lora_refs)

for req_id, encoder_input_ids in scheduled_encoder_inputs.items():
    for mm_input_id in encoder_input_ids:
        mm_feature = req_state.mm_features[mm_input_id]
        if mm_feature.data is None:  # P0キャッシュヒットで省略された場合
            continue
        mm_hashes.append(mm_feature.identifier)
        mm_kwargs.append((modality, mm_feature.data))
</code></pre>
<h3 id="_execute_mm_encoder"><a class="header" href="#_execute_mm_encoder">_execute_mm_encoder()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:2293-2447</code></p>
<pre><code>1. mm_kwargs をモダリティごとにグループ化（group_mm_kwargs_by_modality）
2. 各グループに対して model.embed_multimodal(**mm_kwargs_group) を実行
3. LoRA tower mapping が必要な場合は事前にセット
4. 出力を encoder_cache[mm_hash] に格納
5. ECConnector があれば maybe_save_ec_to_connector() で外部キャッシュにも保存
</code></pre>
<p><strong>バッチ処理</strong>: 同一モダリティのアイテムはバッチ実行される。異なるモダリティは別グループとして処理。</p>
<h3 id="_gather_mm_embeddings"><a class="header" href="#_gather_mm_embeddings">_gather_mm_embeddings()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:2449-2556</code></p>
<p>スケジュールされたトークン範囲に対応するエンコーダ出力をキャッシュから取得し、マージ用のデータを準備する。</p>
<pre><code>for req_id in input_batch.req_ids:
    for mm_feature in req_state.mm_features:
        1. プレースホルダーの範囲 [start_pos, start_pos + num_encoder_tokens)
        2. スケジュールされた範囲 [num_computed_tokens, num_computed + num_scheduled)
        3. 重複部分を計算 → start_idx, end_idx
        4. encoder_cache[mm_hash] からスライス → mm_embeds_item
        5. is_mm_embed マスクの対応位置を True に設定
</code></pre>
<p><strong>チャンクPrefill対応</strong>: <code>num_computed_tokens</code> と <code>num_scheduled_tokens</code> に基づいて部分的な埋め込み取得が可能。</p>
<p>出力:</p>
<ul>
<li><code>mm_embeds: list[torch.Tensor]</code> — 全リクエストの埋め込みスライスを連結</li>
<li><code>is_mm_embed: torch.Tensor</code> — <code>(total_num_scheduled_tokens,)</code> のboolマスク</li>
</ul>
<h3 id="統合-_model_forward-内の処理"><a class="header" href="#統合-_model_forward-内の処理">統合: _model_forward() 内の処理</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:2756-2777</code></p>
<pre><code class="language-python"># 1. エンコーダ実行 + キャッシュ格納
self._execute_mm_encoder(scheduler_output)

# 2. キャッシュから必要な埋め込みを収集
mm_embeds, is_mm_embed = self._gather_mm_embeddings(scheduler_output)

# 3. テキスト + ビジョン埋め込みのマージ
inputs_embeds_scheduled = self.model.embed_input_ids(
    self.input_ids.gpu[:num_scheduled_tokens],
    multimodal_embeddings=mm_embeds,
    is_multimodal=is_mm_embed,
)
</code></pre>
<p><code>embed_input_ids()</code> は内部で <code>masked_scatter_</code> を使い、<code>is_multimodal=True</code> の位置を <code>mm_embeds</code> で置換する。</p>
<h3 id="エンコーダキャッシュの解放"><a class="header" href="#エンコーダキャッシュの解放">エンコーダキャッシュの解放</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:898-899</code></p>
<pre><code class="language-python">for mm_hash in scheduler_output.free_encoder_mm_hashes:
    self.encoder_cache.pop(mm_hash, None)
</code></pre>
<p>SchedulerOutput に含まれる <code>free_encoder_mm_hashes</code> に基づいて、GPUメモリ上のテンソルを解放。</p>
<h2 id="4-テキスト推論との差分まとめ"><a class="header" href="#4-テキスト推論との差分まとめ">4. テキスト推論との差分まとめ</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>処理ステップ</th><th>テキスト推論</th><th>マルチモーダル推論</th></tr>
</thead>
<tbody>
<tr><td>Scheduler: _get_encoder_budget</td><td>不要（has_encoder_inputs=False）</td><td>エンコーダ入力のスケジューリング</td></tr>
<tr><td>Scheduler: encoder_compute_budget</td><td>消費しない</td><td>ステップごとに予算管理</td></tr>
<tr><td>SchedulerOutput</td><td>scheduled_encoder_inputs={}</td><td>{req_id: [input_ids]}</td></tr>
<tr><td>GPUModelRunner: encoder実行</td><td>なし</td><td>_execute_mm_encoder()</td></tr>
<tr><td>GPUModelRunner: 埋め込み</td><td>input_ids をそのまま使用</td><td>embed_input_ids() で text + vision マージ</td></tr>
<tr><td>GPUModelRunner: キャッシュ</td><td>なし</td><td>encoder_cache dict</td></tr>
<tr><td>モデルforward入力</td><td>input_ids</td><td>inputs_embeds（テンソル）</td></tr>
</tbody>
</table>
</div>
<h2 id="5-キャッシュの3層構造"><a class="header" href="#5-キャッシュの3層構造">5. キャッシュの3層構造</a></h2>
<pre><code>P0（フロントエンド）          P1（バックエンド/Scheduler）      P1（バックエンド/GPU）
┌────────────────────┐     ┌───────────────────────┐      ┌──────────────────┐
│ ProcessorCache     │     │ EncoderCacheManager   │      │ encoder_cache    │
│ (LRU, mm_hash)     │     │ (RefCount, mm_hash)   │      │ (dict, mm_hash)  │
│                    │     │                       │      │                  │
│ キャッシュ対象:     │     │ キャッシュ対象:        │      │ キャッシュ対象:   │
│ HF処理済みテンソル  │     │ 論理的な存在管理      │      │ GPU上のテンソル   │
│ + prompt_updates   │     │ (容量・参照カウント)   │      │ (エンコーダ出力)  │
│                    │     │                       │      │                  │
│ Eviction: LRU      │     │ Eviction: FIFO        │      │ Eviction:        │
│ (サイズベース)      │     │ (OrderedDict oldest)  │      │ Scheduler指示    │
└────────────────────┘     └───────────────────────┘      └──────────────────┘
        ↓                           ↓                            ↓
  HF処理スキップ            エンコーダ計算スキップ          テンソル再利用
</code></pre>
<h2 id="主要ファイル-6"><a class="header" href="#主要ファイル-6">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th><th>行</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/v1/core/encoder_cache_manager.py</code></td><td><code>EncoderCacheManager</code></td><td>L17</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/sched/scheduler.py</code></td><td><code>_get_encoder_budget()</code></td><td>L1060</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/sched/output.py</code></td><td><code>SchedulerOutput</code> (MM fields)</td><td>L207, L218</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_model_runner.py</code></td><td><code>_execute_mm_encoder()</code>, <code>_gather_mm_embeddings()</code></td><td>L2293, L2449</td></tr>
</tbody>
</table>
</div>
<h2 id="関連ドキュメント-9"><a class="header" href="#関連ドキュメント-9">関連ドキュメント</a></h2>
<ul>
<li><a href="#フロントエンド-マルチモーダル処理パス-mediumdeep3-verified">フロントエンド MM処理パス</a></li>
<li><a href="#gemma3-ビジョンエンコーダと画像処理-medium-verified">Gemma3 ビジョンエンコーダ</a></li>
<li><a href="#gpumodelrunner">GPUModelRunner</a></li>
<li><a href="#scheduler-サマリー">Scheduler</a></li>
<li><a href="#kvcachemanager-サマリー">KVCacheManager</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="フロントエンド-マルチモーダル処理パス-mediumdeep3-verified"><a class="header" href="#フロントエンド-マルチモーダル処理パス-mediumdeep3-verified">フロントエンド マルチモーダル処理パス [MEDIUM→DEEP(§3)] [VERIFIED]</a></h1>
<blockquote>
<p><strong>最終更新</strong>: 2026-02-17</p>
</blockquote>
<p>APIリクエストに含まれる画像データが、フロントエンドプロセス（P0）でどのように処理され、EngineCoreRequest として ZMQ 経由でバックエンド（P1）へ送信されるかを追跡する。テキスト推論パスとの差分を中心に記述する。</p>
<h2 id="全体フロー-1"><a class="header" href="#全体フロー-1">全体フロー</a></h2>
<pre class="mermaid">graph TD
    A["API Request&lt;br&gt;(messages + images)"] --&gt; B["ChatTemplate適用"]
    B --&gt; C["InputPreprocessor.preprocess()"]
    C --&gt; D["_process_multimodal()"]
    D --&gt; E["mm_processor.info.parse_mm_data()"]
    E --&gt; F["mm_processor.apply()"]
    F --&gt; G{"ProcessorCache&lt;br&gt;ヒット?"}
    G --&gt;|HIT| H["キャッシュからitem+prompt_updates取得&lt;br&gt;(HF処理スキップ)"]
    G --&gt;|MISS| I["HF Processor実行&lt;br&gt;pixel_values等テンソル生成"]
    I --&gt; J["キャッシュに格納"]
    H --&gt; K["MultiModalInputs 構築"]
    J --&gt; K
    K --&gt; L["InputProcessor.process_inputs()"]
    L --&gt; M["MultiModalFeatureSpec 構築"]
    M --&gt; N["EngineCoreRequest&lt;br&gt;(mm_features)"]
    N --&gt; O["ZMQ IPC 送信"]
</pre>

<h2 id="1-チャットテンプレートとプレースホルダー"><a class="header" href="#1-チャットテンプレートとプレースホルダー">1. チャットテンプレートとプレースホルダー</a></h2>
<h3 id="テンプレート適用前後の文字列gemma3の例"><a class="header" href="#テンプレート適用前後の文字列gemma3の例">テンプレート適用前後の文字列（Gemma3の例）</a></h3>
<p><strong>適用前</strong>（OpenAI形式のメッセージ）:</p>
<pre><code class="language-json">{
  "messages": [
    {"role": "user", "content": [
      {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}},
      {"type": "text", "text": "この画像は何ですか？"}
    ]}
  ]
}
</code></pre>
<p><strong>チャットテンプレート適用後</strong>（テキスト）:</p>
<pre><code>&lt;bos&gt;&lt;start_of_turn&gt;user
&lt;start_of_image&gt;この画像は何ですか？&lt;end_of_turn&gt;
&lt;start_of_turn&gt;model
</code></pre>
<p>ここで <code>&lt;start_of_image&gt;</code> がプレースホルダートークンとなる。</p>
<h3 id="プレースホルダーの展開"><a class="header" href="#プレースホルダーの展開">プレースホルダーの展開</a></h3>
<p><code>Gemma3ProcessingInfo.get_image_repl()</code> がプレースホルダーを展開する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:178</code></p>
<pre><code>&lt;start_of_image&gt; → processor.full_image_sequence
</code></pre>
<p><code>processor.full_image_sequence</code> はHuggingFace Gemma3Processorが定義する完全なトークン列で、<code>&lt;start_of_image&gt;</code> + image_token × image_seq_length + <code>&lt;end_of_image&gt;</code> の形式。</p>
<p><strong>Pan-and-Scan有効時</strong>（複数クロップ）:</p>
<pre><code>"Here is the original image &lt;full_image_seq&gt; and here are some crops to help you see better &lt;full_image_seq&gt; &lt;full_image_seq&gt;"
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:196-211</code></p>
<h3 id="画像1枚あたりのトークン数"><a class="header" href="#画像1枚あたりのトークン数">画像1枚あたりのトークン数</a></h3>
<pre><code class="language-python">num_tokens = (num_crops + 1) * image_seq_length
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:213-230</code></p>
<ul>
<li><code>image_seq_length</code>: Gemma3Processorの設定値（典型的に256）</li>
<li><code>num_crops</code>: Pan-and-Scan無効時は0、有効時はアスペクト比に基づいて計算（最大 <code>pan_and_scan_max_num_crops</code>）</li>
<li>よって画像1枚で256〜1280+トークンを消費</li>
</ul>
<h3 id="トークン列の構造"><a class="header" href="#トークン列の構造">トークン列の構造</a></h3>
<p>テキスト推論ではトークン列は純粋なテキストトークンのみ。マルチモーダルでは以下の構造になる:</p>
<pre><code>[BOS] [start_of_turn] [user] [\n]
[start_of_image] [image_token × 256] [end_of_image]    ← 画像プレースホルダー
[テキストトークン列...]                                    ← "この画像は何ですか？"
[end_of_turn] [\n] [start_of_turn] [model] [\n]
</code></pre>
<p><code>image_token</code> の位置がマスクで追跡され（<code>PlaceholderRange</code>）、後にビジョンエンコーダの出力で置換される。</p>
<h3 id="改行トークンの結合処理"><a class="header" href="#改行トークンの結合処理">改行トークンの結合処理</a></h3>
<p>Gemma3固有の問題：<code>\n\n\n</code> と <code>\n\n\n\n</code> が単一トークンとして存在する。画像置換テキストに <code>\n\n</code> が挿入されると、隣接する <code>\n</code> と結合が必要。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:351-385</code></p>
<pre><code class="language-python"># _apply_token_matches() でトークンの結合を実行
replace_token_matches(token_ids, [newline_1, newline_2], [newline_3])  # \n + \n\n → \n\n\n
replace_token_matches(token_ids, [newline_2, newline_1], [newline_3])  # \n\n + \n → \n\n\n
replace_token_matches(token_ids, [newline_2, newline_2], [newline_4])  # \n\n + \n\n → \n\n\n\n
</code></pre>
<h2 id="2-マルチモーダルデータの処理パイプライン"><a class="header" href="#2-マルチモーダルデータの処理パイプライン">2. マルチモーダルデータの処理パイプライン</a></h2>
<h3 id="inputpreprocessor_process_multimodal"><a class="header" href="#inputpreprocessor_process_multimodal">InputPreprocessor._process_multimodal()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/inputs/preprocess.py:193-232</code></p>
<pre><code>_process_multimodal(prompt, mm_data, mm_processor_kwargs, mm_uuids)
  1. mm_processor.info.parse_mm_data(mm_data)
     → MultiModalDataItems（モダリティごとに型付きデータアイテムに変換）
  2. mm_processor.apply(prompt, mm_items, ...)
     → MultiModalInputs（トークン列 + テンソルデータ + プレースホルダー位置 + ハッシュ）
</code></pre>
<h3 id="basemultimodalprocessorapply"><a class="header" href="#basemultimodalprocessorapply">BaseMultiModalProcessor.apply()</a></h3>
<p>HFプロセッサ実行、プロンプト更新（プレースホルダー検出・展開）、キャッシュ管理を統合的に処理する。</p>
<p>主な出力（<code>MultiModalInputs</code>）:</p>
<ul>
<li><code>prompt_token_ids</code>: プレースホルダー展開済みのトークン列</li>
<li><code>mm_kwargs</code>: <code>dict[modality, list[MultiModalKwargsItem]]</code> — 処理済みテンソルデータ</li>
<li><code>mm_hashes</code>: <code>dict[modality, list[str]]</code> — 各アイテムのハッシュ値</li>
<li><code>mm_placeholders</code>: <code>dict[modality, list[PlaceholderRange]]</code> — プレースホルダー位置情報</li>
</ul>
<h3 id="gemma3multimodalprocessor_call_hf_processor"><a class="header" href="#gemma3multimodalprocessor_call_hf_processor">Gemma3MultiModalProcessor._call_hf_processor()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:277-310</code></p>
<p>親クラスのHFプロセッサ呼び出し後、<code>num_patches</code> を追加計算する。HFプロセッサはこの値をpopしてしまうため、vLLM側で再計算が必要。</p>
<pre><code class="language-python">num_crops = [self.info.get_num_crops(...) for size in image_sizes]
processed_outputs["num_patches"] = torch.tensor(num_crops) + 1  # +1 for original
</code></pre>
<p>HFプロセッサの出力:</p>
<ul>
<li><code>pixel_values</code>: <code>(total_patches, 3, image_size, image_size)</code> — 全パッチのピクセルテンソル</li>
<li><code>num_patches</code>: <code>(num_images,)</code> — 画像ごとのパッチ数（= num_crops + 1）</li>
</ul>
<h2 id="3-mmハッシュ-deep-verified"><a class="header" href="#3-mmハッシュ-deep-verified">3. MMハッシュ [DEEP] [VERIFIED]</a></h2>
<p>マルチモーダル入力の同一性を判定するためのコンテンツベースハッシュ。2つの用途がある:</p>
<ol>
<li><strong>ProcessorCache</strong> — HFプロセッサの処理結果キャッシュ（同じ画像の再処理回避）</li>
<li><strong>プレフィックスキャッシュのextra_keys</strong> — KVキャッシュブロックハッシュ計算時にブロック内のMMトークンを識別</li>
</ol>
<h3 id="31-ハッシュ計算の全体フロー"><a class="header" href="#31-ハッシュ計算の全体フロー">3.1 ハッシュ計算の全体フロー</a></h3>
<pre class="mermaid">graph TD
    A["BaseMultiModalProcessor&lt;br&gt;apply() / _cached_apply_hf_processor()"] --&gt; B["_hash_mm_items()"]
    B --&gt; C{"mm_uuids&lt;br&gt;提供?"}
    C --&gt;|"あり + kwargs空"| D["item_uuid をそのまま使用"]
    C --&gt;|"なし or kwargs非空"| E["MultiModalHasher.hash_kwargs()"]
    E --&gt; F["kwargs をキー名でソート"]
    F --&gt; G["iter_item_to_bytes() で再帰的バイト変換"]
    G --&gt; H["serialize_item() で型別シリアライズ"]
    H --&gt; I["hasher.update() に逐次投入"]
    I --&gt; J["hasher.hexdigest() → mm_hash"]
</pre>

<h3 id="32-_hash_mm_items--ハッシュ入力の構成"><a class="header" href="#32-_hash_mm_items--ハッシュ入力の構成">3.2 <code>_hash_mm_items()</code> — ハッシュ入力の構成</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/processing/processor.py:1300-1364</code></p>
<p>各モダリティ（画像の場合 <code>"image"</code>）ごとに、アイテム1つずつハッシュを計算する。</p>
<pre><code class="language-python">MultiModalHasher.hash_kwargs(
    model_id=model_id,         # e.g. "google/gemma-3-27b-it"
    image=item,                # PIL.Image.Image or MediaWithBytes
    **hf_processor_mm_kwargs,  # HFプロセッサに渡すkwargs（PaS設定等）
    **tokenization_kwargs,     # トークナイザkwargs
)
</code></pre>
<p><strong>重要</strong>: ハッシュ入力は <code>model_id</code> + モダリティ名付きの画像データ + プロセッサkwargs + トークナイザkwargs。同じ画像でもプロセッサkwargsが異なればハッシュが変わる。</p>
<h4 id="mm_uuids-による分岐"><a class="header" href="#mm_uuids-による分岐">mm_uuids による分岐</a></h4>
<p><code>_hash_mm_items()</code> はアイテムごとに3つのパスを持つ:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>条件</th><th>動作</th></tr>
</thead>
<tbody>
<tr><td><code>item_uuid</code> が <code>None</code></td><td>画像データから <code>hash_kwargs()</code> を計算</td></tr>
<tr><td><code>item_uuid</code> あり + <code>hf_processor_mm_kwargs</code> or <code>tokenization_kwargs</code> が非空</td><td><code>item_uuid</code> 文字列を item として <code>hash_kwargs()</code> に投入（画像デコード回避）</td></tr>
<tr><td><code>item_uuid</code> あり + kwargs 空</td><td><code>item_uuid</code> をそのまま mm_hash として使用（ハッシュ計算スキップ）</td></tr>
</tbody>
</table>
</div>
<p>2番目のパスは最適化: UUID文字列のハッシュは画像ピクセルのハッシュより高速だが、kwargsとの組み合わせで一意性を保つ必要がある。</p>
<h4 id="get_item_for_hash--ハッシュ用アイテムの取得"><a class="header" href="#get_item_for_hash--ハッシュ用アイテムの取得"><code>get_item_for_hash()</code> — ハッシュ用アイテムの取得</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/parse.py:118-120</code></p>
<p><code>ProcessorBatchItems.get_item_for_hash()</code> は <code>get()</code> と異なり、<code>MediaWithBytes</code> ラッパーを<strong>剥がさずにそのまま返す</strong>。</p>
<pre><code class="language-python"># get() → self._unwrap(self.data[index])  → PIL.Image（ラッパー除去）
# get_item_for_hash() → self.data[index]  → MediaWithBytes[PIL.Image]（ラッパー保持）
</code></pre>
<p>これにより <code>serialize_item()</code> で元のバイト列（JPEG/PNG等）からハッシュを計算でき、PIL画像のピクセルデコードを回避できる。</p>
<h3 id="33-multimodalhasherhash_kwargs--ハッシュ関数本体"><a class="header" href="#33-multimodalhasherhash_kwargs--ハッシュ関数本体">3.3 <code>MultiModalHasher.hash_kwargs()</code> — ハッシュ関数本体</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/hasher.py:154-162</code></p>
<pre><code class="language-python">@classmethod
def hash_kwargs(cls, **kwargs: object) -&gt; str:
    hasher = _get_hasher_factory(envs.VLLM_MM_HASHER_ALGORITHM)()
    for k, v in sorted(kwargs.items(), key=lambda kv: kv[0]):  # キー名でソート
        for bytes_ in cls.iter_item_to_bytes(k, v):
            hasher.update(bytes_)
    return hasher.hexdigest()
</code></pre>
<p><strong>ハッシュアルゴリズム</strong>: <code>VLLM_MM_HASHER_ALGORITHM</code> 環境変数で設定（<code>target/vllm/vllm/envs.py:73,793</code>）</p>
<ul>
<li><code>blake3</code>（デフォルト）: 高速</li>
<li><code>sha256</code> / <code>sha512</code>: FIPS準拠用（政府・企業デプロイメント向け）</li>
</ul>
<h3 id="34-iter_item_to_bytes--再帰的バイト変換"><a class="header" href="#34-iter_item_to_bytes--再帰的バイト変換">3.4 <code>iter_item_to_bytes()</code> — 再帰的バイト変換</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/hasher.py:134-151</code></p>
<p>dict/list/tupleを再帰的に展開し、<strong>キー名のバイト列 + 値のバイト列</strong> を交互にyieldする。</p>
<pre><code class="language-python"># 例: image=PIL.Image(mode="RGB", data=...) の場合
# iter_item_to_bytes("image", {"mode": "RGB", "data": &lt;numpy&gt;})
#   → "image.mode".encode() + "RGB".encode()
#   → "image.data".encode() + &lt;numpy raw bytes&gt;
</code></pre>
<p>キー名がプレフィックスとして含まれるため、<strong>異なるkwargsキーの値が衝突しない</strong>（例: <code>model_id</code> の値と <code>image</code> の一部が同じバイト列でも区別される）。</p>
<h3 id="35-serialize_item--型別シリアライズ"><a class="header" href="#35-serialize_item--型別シリアライズ">3.5 <code>serialize_item()</code> — 型別シリアライズ</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/hasher.py:52-131</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>データ型</th><th>シリアライズ方法</th><th>備考</th></tr>
</thead>
<tbody>
<tr><td><code>bytes</code> / <code>memoryview</code></td><td>そのまま返す</td><td></td></tr>
<tr><td><code>str</code></td><td>UTF-8エンコード</td><td>model_id 等</td></tr>
<tr><td><code>int</code> / <code>float</code></td><td><code>np.array(obj).tobytes()</code></td><td></td></tr>
<tr><td><code>PIL.Image</code> (EXIF UUID)</td><td><code>exif[ImageID].bytes</code> (16バイト)</td><td>高速パス</td></tr>
<tr><td><code>PIL.Image</code> (通常)</td><td><code>{"mode": mode, "data": np.asarray(obj)}</code> + palette</td><td>全ピクセル読み込み</td></tr>
<tr><td><code>MediaWithBytes(Image)</code> (EXIF UUID)</td><td>同上の高速パス</td><td></td></tr>
<tr><td><code>MediaWithBytes(Image)</code> (通常)</td><td><code>original_bytes</code></td><td><strong>エンコード済みバイト列。デコード不要で高速</strong></td></tr>
<tr><td><code>torch.Tensor</code></td><td>numpy変換。bfloat16は <code>view(uint8)</code> 経由</td><td><code>{"original_dtype", "original_shape", "data"}</code></td></tr>
<tr><td><code>np.ndarray</code></td><td><code>{"dtype": str, "shape": tuple, "data": raw}</code></td><td>C-contiguous なら zero-copy</td></tr>
<tr><td>その他</td><td><code>pickle.dumps()</code> (警告ログ)</td><td>フォールバック</td></tr>
</tbody>
</table>
</div>
<h4 id="画像の3つのシリアライズパス"><a class="header" href="#画像の3つのシリアライズパス">画像の3つのシリアライズパス</a></h4>
<pre class="mermaid">graph TD
    A["serialize_item(obj)"] --&gt; B{"EXIF ImageID&lt;br&gt;UUID?"}
    B --&gt;|Yes| C["UUID.bytes (16バイト)&lt;br&gt;最速"]
    B --&gt;|No| D{"MediaWithBytes?"}
    D --&gt;|Yes| E["original_bytes&lt;br&gt;(JPEG/PNG等エンコード済み)&lt;br&gt;高速"]
    D --&gt;|No| F["mode + np.asarray(obj)&lt;br&gt;(全ピクセル展開)&lt;br&gt;低速"]
</pre>

<h3 id="36-uuidオーバーライド"><a class="header" href="#36-uuidオーバーライド">3.6 UUIDオーバーライド</a></h3>
<p>キャッシュとプレフィックスキャッシュの両方が無効な場合、コンテンツハッシュの代わりに <code>{request_id}-{modality}-{index}</code> 形式のUUIDを使用する（ハッシュ計算コストの回避）。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/input_processor.py:551-574</code></p>
<h3 id="37-lora対応のidentifier"><a class="header" href="#37-lora対応のidentifier">3.7 LoRA対応のidentifier</a></h3>
<p>LoRAのtower_connector_loraが有効な場合、同じ画像でもLoRAによって埋め込みが変わるため、<code>identifier</code> に LoRA名をプレフィックスとして付加する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/input_processor.py:273-289</code></p>
<pre><code class="language-python">def _get_mm_identifier(self, mm_hash, lora_request):
    if lora_request is None or not enable_tower_connector_lora:
        return mm_hash
    return f"{lora_request.lora_name}:{mm_hash}"
</code></pre>
<h3 id="38-mm_hash-vs-identifier--2つのハッシュ値の使い分け"><a class="header" href="#38-mm_hash-vs-identifier--2つのハッシュ値の使い分け">3.8 mm_hash vs identifier — 2つのハッシュ値の使い分け</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>属性</th><th><code>mm_hash</code></th><th><code>identifier</code></th></tr>
</thead>
<tbody>
<tr><td>定義場所</td><td><code>MultiModalFeatureSpec.mm_hash</code></td><td><code>MultiModalFeatureSpec.identifier</code></td></tr>
<tr><td>LoRAプレフィックス</td><td>なし</td><td>あり（<code>enable_tower_connector_lora</code>時）</td></tr>
<tr><td>主な用途</td><td>ProcessorCache のキー</td><td>EncoderCache / プレフィックスキャッシュのキー</td></tr>
<tr><td>理由</td><td>ProcessorCacheはLoRA非依存（ピクセルデータのみ）</td><td>エンコーダ出力・KVキャッシュはLoRAに依存</td></tr>
</tbody>
</table>
</div>
<h3 id="39-プレフィックスキャッシュでの使用"><a class="header" href="#39-プレフィックスキャッシュでの使用">3.9 プレフィックスキャッシュでの使用</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:388-444</code></p>
<p>ブロックハッシュ計算時に、ブロックのトークン範囲にMMプレースホルダが含まれる場合、<code>mm_feature.identifier</code> がextra_keysとして追加される。</p>
<pre><code class="language-python">def _gen_mm_extra_hash_keys(request, start_token_idx, end_token_idx, start_mm_idx):
    # ブロック範囲とMMプレースホルダ範囲の重なりをチェック
    while curr_mm_idx &lt; len(mm_features):
        mm_feature = mm_features[curr_mm_idx]
        offset = mm_feature.mm_position.offset
        length = mm_feature.mm_position.length
        if end_token_idx &gt; offset:
            extra_keys.append(mm_feature.identifier)  # ★ identifierを追加
            ...
</code></pre>
<p>これにより、<strong>同じトークン列でも異なる画像が挿入されたブロック</strong>は別のハッシュを持つ。逆に、同じ画像（同じidentifier）であればKVキャッシュのプレフィックスヒットが可能。</p>
<h3 id="310-gemma3における具体例"><a class="header" href="#310-gemma3における具体例">3.10 Gemma3における具体例</a></h3>
<p>Gemma3は <code>_hash_mm_items()</code> を<strong>オーバーライドしていない</strong>。<code>BaseMultiModalProcessor</code> のデフォルト実装がそのまま使われる。</p>
<pre><code>ハッシュ入力例:
  model_id = "google/gemma-3-27b-it"
  image = &lt;MediaWithBytes[PIL.Image]&gt;  (元のJPEGバイト列)
  ※ hf_processor_mm_kwargs, tokenization_kwargs は通常空

→ hash_kwargs() 内部:
  sorted keys: ["image", "model_id"]
  1. "image" → iter_item_to_bytes("image", MediaWithBytes)
     → serialize_item() → original_bytes (JPEG/PNGバイト列)
  2. "model_id" → iter_item_to_bytes("model_id", "google/gemma-3-27b-it")
     → "model_id".encode() + "google/gemma-3-27b-it".encode()

→ blake3.hexdigest() → "a1b2c3..." (mm_hash)
</code></pre>
<p><strong>Pan-and-Scan（PaS）との関係</strong>: PaSによる画像分割はHFプロセッサ内で行われるため、<strong>ハッシュ計算の後</strong>に実行される。同じ画像 + 同じmodel_id + 同じkwargs → 同じmm_hash（PaS分割後の結果もキャッシュされる）。</p>
<h2 id="4-プロセッサキャッシュp0側"><a class="header" href="#4-プロセッサキャッシュp0側">4. プロセッサキャッシュ（P0側）</a></h2>
<h3 id="キャッシュタイプの選択"><a class="header" href="#キャッシュタイプの選択">キャッシュタイプの選択</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/registry.py:284-320</code></p>
<pre><code>mm_processor_cache_gb &lt;= 0         → None（キャッシュ無効）
IPC非対応 or API process &gt; 1       → processor_only
mm_processor_cache_type == "lru"   → lru（Sender + Receiver）
mm_processor_cache_type == "shm"   → shm（共有メモリ）
</code></pre>
<h3 id="4種類のキャッシュ実装"><a class="header" href="#4種類のキャッシュ実装">4種類のキャッシュ実装</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/cache.py</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>実装</th><th>場所</th><th>格納内容</th><th>キャッシュヒット時の動作</th></tr>
</thead>
<tbody>
<tr><td><code>MultiModalProcessorOnlyCache</code> (L326)</td><td>P0のみ</td><td>テンソルデータ + prompt_updates</td><td>キャッシュから item + prompt_updates を返す（HF処理スキップ）</td></tr>
<tr><td><code>MultiModalProcessorSenderCache</code> (L379)</td><td>P0</td><td>サイズメタデータ + prompt_updates</td><td>item=None を返す（P1にデータあり、IPC不要）</td></tr>
<tr><td><code>ShmObjectStoreSenderCache</code> (L437)</td><td>P0</td><td>共有メモリ参照 + prompt_updates</td><td>item=None を返す（共有メモリ経由でP1に渡す）</td></tr>
<tr><td><code>MultiModalReceiverCache</code> (L614)</td><td>P1</td><td>テンソルデータ</td><td>lru タイプ時に P1 側で使用</td></tr>
</tbody>
</table>
</div>
<h3 id="p0-p1-キャッシュの整合性"><a class="header" href="#p0-p1-キャッシュの整合性">P0-P1 キャッシュの整合性</a></h3>
<p><strong>設計の核心</strong>: P0とP1のキャッシュは <strong>同一のEviction順序</strong> を維持する。</p>
<pre><code>                 is_cached() × N    get_and_update()
P0: From API ───────────────────&gt; ────────────────&gt; To P1

                get_and_update()
P1: From P0 ───────────────────&gt; To model
</code></pre>
<ul>
<li><code>is_cached()</code> はP0キャッシュのみを参照（Eviction順序を変えない）</li>
<li><code>get_and_update()</code> は P0 と P1 で順番に呼ぶ必要がある（Eviction順序を同期）</li>
<li>これにより、P0のキャッシュ状態を見るだけでP1のキャッシュ状態を推定できる（IPC不要）</li>
</ul>
<h3 id="キャッシュヒット時にスキップされる処理"><a class="header" href="#キャッシュヒット時にスキップされる処理">キャッシュヒット時にスキップされる処理</a></h3>
<ol>
<li><strong>HF Processor実行</strong>（画像のリサイズ、正規化、パッチ分割 → <code>pixel_values</code> テンソル生成）</li>
<li><strong>テンソルデータのIPC送信</strong>（<code>SenderCache</code>/<code>ShmCache</code> 使用時、<code>data=None</code> にしてZMQ転送量削減）</li>
<li><strong>プロンプト更新の再計算は常に必要</strong>（キャッシュにprompt_updatesが保存されているため計算はスキップだが、取得は必要）</li>
</ol>
<h2 id="5-enginecorerequest-への組み立て"><a class="header" href="#5-enginecorerequest-への組み立て">5. EngineCoreRequest への組み立て</a></h2>
<h3 id="multimodalfeaturespec-構築"><a class="header" href="#multimodalfeaturespec-構築">MultiModalFeatureSpec 構築</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/input_processor.py:627-654</code></p>
<pre><code class="language-python">mm_features = []
for modality, idx in sorted_mm_idxs:
    base_mm_hash = decoder_mm_hashes[modality][idx]
    mm_features.append(
        MultiModalFeatureSpec(
            data=decoder_mm_inputs[modality][idx],     # MultiModalKwargsItem | None
            modality=modality,                          # "image"
            identifier=_get_mm_identifier(base_mm_hash, lora_request),
            mm_position=decoder_mm_positions[modality][idx],  # PlaceholderRange
            mm_hash=base_mm_hash,
        )
    )
</code></pre>
<p><code>sorted_mm_idxs</code> は <code>argsort_mm_positions()</code> でプロンプト内の出現順にソートされる。</p>
<h3 id="multimodalfeaturespec-の構造"><a class="header" href="#multimodalfeaturespec-の構造">MultiModalFeatureSpec の構造</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/inputs.py:337-381</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>data</code></td><td><code>MultiModalKwargsItem | None</code></td><td>処理済みテンソルデータ。P0キャッシュヒット時は <code>None</code></td></tr>
<tr><td><code>modality</code></td><td><code>str</code></td><td><code>"image"</code>, <code>"video"</code>, <code>"audio"</code> 等</td></tr>
<tr><td><code>identifier</code></td><td><code>str</code></td><td>エンコーダキャッシュ用ハッシュ（LoRAプレフィックス付きの場合あり）</td></tr>
<tr><td><code>mm_position</code></td><td><code>PlaceholderRange</code></td><td>プロンプト内のプレースホルダー位置</td></tr>
<tr><td><code>mm_hash</code></td><td><code>str | None</code></td><td>プロセッサキャッシュ用ハッシュ（LoRAプレフィックスなし）</td></tr>
</tbody>
</table>
</div>
<h3 id="placeholderrange-の構造"><a class="header" href="#placeholderrange-の構造">PlaceholderRange の構造</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/inputs.py:170-240</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>offset</code></td><td><code>int</code></td><td>プロンプト内の開始位置</td></tr>
<tr><td><code>length</code></td><td><code>int</code></td><td>プレースホルダーの長さ（トークン数）</td></tr>
<tr><td><code>is_embed</code></td><td><code>Tensor[bool] | None</code></td><td>各位置が埋め込みを受け取るかのマスク</td></tr>
</tbody>
</table>
</div>
<p><code>get_num_embeds()</code> は実際のエンコーダ出力の埋め込み数を返す（<code>is_embed</code> のTrue数、またはlength）。</p>
<h3 id="enginecorerequest-1"><a class="header" href="#enginecorerequest-1">EngineCoreRequest</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/__init__.py:55-101</code></p>
<p>テキスト推論との差分:</p>
<ul>
<li><code>mm_features: list[MultiModalFeatureSpec] | None</code> — マルチモーダル時に設定される</li>
<li><code>prompt_token_ids</code> にはプレースホルダー展開済みのトークン列が入る</li>
</ul>
<p>ZMQ送信時は <code>msgspec</code> によるバイナリシリアライゼーション。テンソルデータは <code>MultiModalKwargsItem</code> に含まれ、カスタムエンコーダで処理される。</p>
<h2 id="6-キャッシュタイプ別のデータフロー"><a class="header" href="#6-キャッシュタイプ別のデータフロー">6. キャッシュタイプ別のデータフロー</a></h2>
<h3 id="processor_onlyp0完結"><a class="header" href="#processor_onlyp0完結">processor_only（P0完結）</a></h3>
<pre><code>P0: hash → cache miss → HF処理 → cache store(tensor+prompt) → tensor をリクエストに含めて送信
P0: hash → cache hit  → cache get(tensor+prompt) → tensor をリクエストに含めて送信
</code></pre>
<ul>
<li>テンソルデータは常にZMQ経由で送信される</li>
</ul>
<h3 id="lrup0-sender--p1-receiver"><a class="header" href="#lrup0-sender--p1-receiver">lru（P0 Sender + P1 Receiver）</a></h3>
<pre><code>P0: hash → cache miss → HF処理 → meta store(size+prompt) → tensor をリクエストに含めて送信
P1: hash → cache miss → tensor を受信 → cache store(tensor)

P0: hash → cache hit  → meta get(prompt) → data=None で送信（テンソル省略）
P1: hash → cache hit  → cache get(tensor)
</code></pre>
<ul>
<li>キャッシュヒット時は <strong>テンソルデータのIPC転送がスキップ</strong> される</li>
</ul>
<h3 id="shm共有メモリ"><a class="header" href="#shm共有メモリ">shm（共有メモリ）</a></h3>
<pre><code>P0: hash → cache miss → HF処理 → 共有メモリに書き込み → data=None で送信
P1: hash → cache miss → 共有メモリから読み取り

P0: hash → cache hit  → data=None で送信
P1: hash → cache hit  → 共有メモリから読み取り（ringバッファ）
</code></pre>
<h2 id="主要ファイル-7"><a class="header" href="#主要ファイル-7">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th><th>行</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/v1/engine/input_processor.py</code></td><td><code>InputProcessor</code>, <code>process_inputs()</code>, <code>_get_mm_identifier()</code></td><td>L56, L521, L490</td></tr>
<tr><td><code>target/vllm/vllm/inputs/preprocess.py</code></td><td><code>InputPreprocessor</code>, <code>_process_multimodal()</code>, <code>_get_mm_processor()</code></td><td>L60, L193, L182</td></tr>
<tr><td><code>target/vllm/vllm/multimodal/hasher.py</code></td><td><code>MultiModalHasher</code>, <code>hash_kwargs()</code>, <code>serialize_item()</code></td><td>L50, L154, L52</td></tr>
<tr><td><code>target/vllm/vllm/multimodal/cache.py</code></td><td><code>MultiModalProcessorOnlyCache</code>, <code>SenderCache</code>, <code>ShmCache</code>, <code>ReceiverCache</code></td><td>L326, L379, L437, L614</td></tr>
<tr><td><code>target/vllm/vllm/multimodal/registry.py</code></td><td><code>MULTIMODAL_REGISTRY</code>, <code>processor_cache_from_config()</code></td><td>L305</td></tr>
<tr><td><code>target/vllm/vllm/multimodal/inputs.py</code></td><td><code>MultiModalFeatureSpec</code>, <code>PlaceholderRange</code></td><td>L337, L170</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/__init__.py</code></td><td><code>EngineCoreRequest</code></td><td>L55</td></tr>
<tr><td><code>target/vllm/vllm/model_executor/models/gemma3_mm.py</code></td><td><code>Gemma3MultiModalProcessor</code>, <code>Gemma3ProcessingInfo</code></td><td>L276, L77</td></tr>
</tbody>
</table>
</div>
<h2 id="関連ドキュメント-10"><a class="header" href="#関連ドキュメント-10">関連ドキュメント</a></h2>
<ul>
<li><a href="#マルチモーダル処理パイプライン-サマリー">マルチモーダル処理パイプライン概要</a></li>
<li><a href="#バックエンド-マルチモーダル処理パス-medium-verified">バックエンド MM処理パス</a></li>
<li><a href="#gemma3-ビジョンエンコーダと画像処理-medium-verified">Gemma3 ビジョンエンコーダ</a></li>
<li><a href="#inputprocessor-サマリー">InputProcessor</a></li>
<li><a href="#テキスト推論データフロー">データフロー</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="outputprocessor"><a class="header" href="#outputprocessor">OutputProcessor</a></h1>
<blockquote>
<p><strong>深度</strong>: [SHALLOW]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-19"><a class="header" href="#概要-19">概要</a></h2>
<p>OutputProcessorは<strong>フロントエンドプロセス</strong>で動作し、バックエンド（EngineCore）からZMQ経由で受信した<code>EngineCoreOutput</code>を、ユーザー向けの<code>RequestOutput</code>に変換する。主な処理はインクリメンタルデトークナイズ、停止文字列判定、logprobs処理である。AsyncLLMの<code>output_handler</code>バックグラウンドタスクから呼び出される。</p>
<h2 id="process_outputs-フロー"><a class="header" href="#process_outputs-フロー">process_outputs() フロー</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/output_processor.py:582</code> (process_outputs)</p>
<pre><code>OutputProcessor.process_outputs(engine_core_outputs)       # L582
  │
  for each engine_core_output:
    │
    ├─ req_state = request_states[req_id]                  # RequestState取得
    │   （abortされていればスキップ）
    │
    ├─ 統計情報更新                                         # L620-622
    │
    ├─ デトークナイズ + 停止文字列判定                       # L637-639
    │   stop_string = detokenizer.update(
    │       new_token_ids, stop_terminated)
    │   → トークン→テキスト変換（インクリメンタル）
    │   → 停止文字列検出時は finish_reason = STOP
    │
    ├─ logprobs処理                                         # L646
    │   logprobs_processor.update_from_output(output)
    │
    ├─ RequestOutput構築                                    # L649-656
    │   req_state.make_request_output(
    │       new_token_ids, finish_reason, stop_reason, ...)
    │   → CompletionOutput + RequestOutput
    │
    ├─ 出力配信                                             # L660-665
    │   ├─ AsyncLLM: req_state.queue.put(request_output)
    │   └─ LLM: request_outputs.append(request_output)
    │
    └─ 完了処理                                             # L668-687
        if finish_reason is not None:
          _finish_request(req_state)
          → リクエスト解放、統計記録
</code></pre>
<h2 id="requeststate"><a class="header" href="#requeststate">RequestState</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/output_processor.py:116</code> (RequestState)</p>
<p>各リクエストのフロントエンド側状態を保持する。OutputProcessor.add_request()で作成される。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>external_req_id</code></td><td><code>str</code></td><td>外部リクエストID（クライアント向け）</td></tr>
<tr><td><code>detokenizer</code></td><td><code>IncrementalDetokenizer</code></td><td>デトークナイザインスタンス</td></tr>
<tr><td><code>logprobs_processor</code></td><td><code>LogprobsProcessor</code></td><td>logprobs処理インスタンス</td></tr>
<tr><td><code>output_kind</code></td><td><code>RequestOutputKind</code></td><td>出力モード（CUMULATIVE/DELTA/FINAL_ONLY）</td></tr>
<tr><td><code>queue</code></td><td><code>RequestOutputCollector | None</code></td><td>AsyncLLM用出力キュー</td></tr>
<tr><td><code>prompt_token_ids</code></td><td><code>list[int]</code></td><td>プロンプトトークン（出力に含める用）</td></tr>
</tbody>
</table>
</div>
<h3 id="make_request_output"><a class="header" href="#make_request_output">make_request_output()</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/output_processor.py:269</code> (make_request_output)</p>
<pre><code>make_request_output(new_token_ids, finish_reason, ...)
  │
  ├─ FINAL_ONLY モードかつ未完了 → None（出力なし）
  │
  ├─ プーリングモデル → PoolingRequestOutput
  │
  └─ テキスト生成 → RequestOutput
      ├─ _new_completion_output()                          # L377
      │   ├─ detokenizer.get_next_output_text(finished, delta)
      │   │   → DELTAモード: 新規テキストのみ
      │   │   → CUMULATIVEモード: 全テキスト
      │   └─ CompletionOutput(text, token_ids, logprobs, ...)
      └─ RequestOutput(request_id, outputs, finished, ...)
</code></pre>
<h2 id="detokenizerインクリメンタルデトークナイズ-1"><a class="header" href="#detokenizerインクリメンタルデトークナイズ-1">Detokenizer（インクリメンタルデトークナイズ）</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/detokenizer.py:30</code> (IncrementalDetokenizer)</p>
<h3 id="クラス階層-2"><a class="header" href="#クラス階層-2">クラス階層</a></h3>
<pre><code>IncrementalDetokenizer (基底・No-op)               L30
└── BaseIncrementalDetokenizer (ABC)                L65
    ├── FastIncrementalDetokenizer                  L169
    │   → HF tokenizersの DecodeStream 使用
    └── SlowIncrementalDetokenizer                  L258
        → detokenize_incrementally() 使用
</code></pre>
<p>ファクトリメソッド <code>from_new_request()</code> がトークナイザの種類に応じて適切な実装を選択する。</p>
<h3 id="update-メソッド"><a class="header" href="#update-メソッド">update() メソッド</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/detokenizer.py:65</code> (BaseIncrementalDetokenizer.update)</p>
<pre><code>update(new_token_ids, stop_terminated) → stop_string | None
  │
  for each new_token_id:
    ├─ token_ids.append(new_token_id)
    └─ output_text += decode_next(new_token_id)  # 抽象メソッド
  │
  └─ check_stop_strings(output_text, ...)         # L316
      → (stop_string, truncate_offset) | None
</code></pre>
<h3 id="停止文字列判定"><a class="header" href="#停止文字列判定">停止文字列判定</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/detokenizer.py:316</code> (check_stop_strings)</p>
<p><code>check_stop_strings()</code>は累積テキストの末尾付近で停止文字列を検索する。検出時はテキストをトランケートし、停止文字列と切り詰め位置を返す。<code>include_stop_str_in_output</code>フラグで停止文字列を出力に含めるか制御する。</p>
<h2 id="logprobsprocessor"><a class="header" href="#logprobsprocessor">LogprobsProcessor</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/logprobs.py:28</code> (LogprobsProcessor)</p>
<p><code>SamplingParams.logprobs</code> / <code>prompt_logprobs</code> の設定に基づいて初期化される。<code>update_from_output()</code>で<code>EngineCoreOutput</code>からlogprobs情報を抽出し、累積対数確率を更新する。</p>
<h2 id="出力モードrequestoutputkind"><a class="header" href="#出力モードrequestoutputkind">出力モード（RequestOutputKind）</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/sampling_params.py:108</code> (RequestOutputKind)</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>モード</th><th>値</th><th>動作</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>CUMULATIVE</code></td><td>0</td><td>毎回全出力テキスト/トークンを返す</td><td>デフォルト</td></tr>
<tr><td><code>DELTA</code></td><td>1</td><td>差分（新規テキスト/トークン）のみ返す</td><td>ストリーミング</td></tr>
<tr><td><code>FINAL_ONLY</code></td><td>2</td><td>完了時のみ出力を返す</td><td>バッチ処理</td></tr>
</tbody>
</table>
</div>
<h2 id="asyncllmとの連携"><a class="header" href="#asyncllmとの連携">AsyncLLMとの連携</a></h2>
<pre><code>AsyncLLM._run_output_handler()                     # async_llm.py:662
  while True:
    outputs = await engine_core.get_output_async()  # ZMQ受信
    for chunk in outputs.outputs:
      output_processor.process_outputs(chunk, ...)  # ← ここで呼ばれる
      → RequestOutputがper-requestキューにpush
      → generate()がキューからyield
</code></pre>
<h2 id="上流下流の関係-2"><a class="header" href="#上流下流の関係-2">上流・下流の関係</a></h2>
<ul>
<li><strong>上流</strong>: AsyncLLM（output_handlerタスクから呼び出し）、EngineCoreOutputs（ZMQ経由受信）</li>
<li><strong>下流</strong>: APIサーバー（RequestOutputをyield）</li>
</ul>
<h2 id="phase-2-深堀り候補-1"><a class="header" href="#phase-2-深堀り候補-1">Phase 2 深堀り候補</a></h2>
<ul>
<li><code>RequestOutputCollector</code>のキューイング実装</li>
<li>ストリーミングモード（DELTA）時のテキスト差分計算詳細</li>
<li>n&gt;1サンプリング時のParentRequest管理</li>
</ul>
<h2 id="主要ファイル-8"><a class="header" href="#主要ファイル-8">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/v1/engine/output_processor.py</code></td><td><code>OutputProcessor</code> (L73), <code>process_outputs()</code> (L582), <code>RequestState</code> (L116), <code>make_request_output()</code> (L269)</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/detokenizer.py</code></td><td><code>IncrementalDetokenizer</code> (L30), <code>FastIncrementalDetokenizer</code> (L169), <code>SlowIncrementalDetokenizer</code> (L258), <code>check_stop_strings()</code> (L316)</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/logprobs.py</code></td><td><code>LogprobsProcessor</code> (L28)</td></tr>
<tr><td><code>target/vllm/vllm/outputs.py</code></td><td><code>RequestOutput</code> (L86), <code>CompletionOutput</code> (L23)</td></tr>
<tr><td><code>target/vllm/vllm/sampling_params.py</code></td><td><code>RequestOutputKind</code> (L108)</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="scheduler-サマリー"><a class="header" href="#scheduler-サマリー">Scheduler サマリー</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<h2 id="概要-20"><a class="header" href="#概要-20">概要</a></h2>
<p><code>Scheduler</code>はContinuous Batchingの中核コンポーネントであり、各ステップでどのリクエストにどれだけのトークンを計算させるかを決定する。<code>schedule()</code>メソッドは3フェーズ（RUNNING処理 → WAITING処理 → Output構築）で構成され、トークン予算の範囲内で最大限のリクエストをスケジュールする。Unified Compute Modelを採用し、Prefill/Decodeを明示的に区別せず <code>num_computed_tokens</code> の進捗で統一的に管理する。</p>
<h2 id="アーキテクチャ-8"><a class="header" href="#アーキテクチャ-8">アーキテクチャ</a></h2>
<pre class="mermaid">graph TD
    subgraph schedule 3フェーズ
        P1["Phase 1: RUNNING&lt;br&gt;既実行リクエスト処理&lt;br&gt;L350-517"]
        P2["Phase 2: WAITING&lt;br&gt;新規リクエスト受け入れ&lt;br&gt;L532-800"]
        P3["Phase 3: Output構築&lt;br&gt;SchedulerOutput生成&lt;br&gt;L827-896"]
    end

    AR["add_request()"] --&gt;|"WAITINGキューに追加"| P2
    P1 --&gt;|"トークン予算消費"| P2
    P2 --&gt;|"トークン予算消費"| P3
    P1 --&gt;|"プリエンプション"| KVM["KVCacheManager"]
    P2 --&gt;|"allocate_slots()"| KVM
    P2 --&gt;|"get_computed_blocks()"| KVM
    P3 --&gt;|"SchedulerOutput"| EC["EngineCore"]

    UO["update_from_output()"] --&gt;|"ModelRunnerOutput"| OUT["EngineCoreOutputs"]
</pre>

<h2 id="主要コンポーネント-7"><a class="header" href="#主要コンポーネント-7">主要コンポーネント</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>用途</th><th>ファイル</th></tr>
</thead>
<tbody>
<tr><td><code>Scheduler</code></td><td>スケジューリング本体</td><td><code>target/vllm/vllm/v1/core/sched/scheduler.py</code></td></tr>
<tr><td><code>Request</code></td><td>リクエスト内部状態</td><td><code>target/vllm/vllm/v1/request.py</code></td></tr>
<tr><td><code>SchedulerOutput</code></td><td>スケジュール結果（Executor向け）</td><td><code>target/vllm/vllm/v1/core/sched/output.py:184</code></td></tr>
<tr><td><code>NewRequestData</code></td><td>初回スケジュールのフルデータ</td><td><code>target/vllm/vllm/v1/core/sched/output.py:34</code></td></tr>
<tr><td><code>CachedRequestData</code></td><td>既スケジュール済みの差分データ</td><td><code>target/vllm/vllm/v1/core/sched/output.py:114</code></td></tr>
</tbody>
</table>
</div>
<h2 id="主要メソッド-7"><a class="header" href="#主要メソッド-7">主要メソッド</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>行</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>schedule()</code></td><td>L321</td><td>メイン: 3フェーズスケジューリング → SchedulerOutput</td></tr>
<tr><td><code>add_request()</code></td><td>L1644</td><td>WAITINGキューにリクエスト登録</td></tr>
<tr><td><code>update_from_output()</code></td><td>L1241</td><td>ModelRunnerOutputから出力生成 → EngineCoreOutputs</td></tr>
<tr><td><code>finish_requests()</code></td><td>L1666</td><td>リクエストを完了/中止状態にする</td></tr>
<tr><td><code>_preempt_request()</code></td><td>L898</td><td>プリエンプション実行（ブロック解放→WAITINGに戻す）</td></tr>
<tr><td><code>_make_cached_request_data()</code></td><td>L999</td><td>CachedRequestData（差分データ）構築</td></tr>
<tr><td><code>_update_request_with_output()</code></td><td>L1538</td><td>生成トークンをリクエストに追加、停止判定</td></tr>
</tbody>
</table>
</div>
<h2 id="schedule-3フェーズ"><a class="header" href="#schedule-3フェーズ">schedule() 3フェーズ</a></h2>
<h3 id="phase-1-running-リクエストのスケジューリングl350-517"><a class="header" href="#phase-1-running-リクエストのスケジューリングl350-517">Phase 1: RUNNING リクエストのスケジューリング（L350-517）</a></h3>
<p>既に実行中のリクエストに対してトークンを割り当てる。</p>
<pre><code>while req_index &lt; len(self.running) and token_budget &gt; 0:
    request = self.running[req_index]

    # 計算すべき新規トークン数
    num_new_tokens = (
        request.num_tokens_with_spec           # 最終目標
        + request.num_output_placeholders      # asyncプレースホルダ
        - request.num_computed_tokens           # 既計算分を差引
    )
    num_new_tokens = min(num_new_tokens, token_budget)

    # KVキャッシュブロック割り当て
    new_blocks = kv_cache_manager.allocate_slots(request, num_new_tokens)

    if new_blocks is None:
        # → プリエンプション: 最低優先度リクエストを解放して再試行
</code></pre>
<p><strong>プリエンプション</strong>: ブロック割り当て失敗時、Priority/FIFOポリシーで最低優先度のリクエストを選びブロック解放。解放後に再試行する。</p>
<h3 id="phase-2-waiting-リクエストのスケジューリングl532-800"><a class="header" href="#phase-2-waiting-リクエストのスケジューリングl532-800">Phase 2: WAITING リクエストのスケジューリング（L532-800）</a></h3>
<p>WAITINGキューから新規リクエストを受け入れる。</p>
<pre><code>for request in self.waiting:
    # スキップ条件チェック
    #   - WAITING_FOR_REMOTE_KVS: 非同期KV受信待ち
    #   - WAITING_FOR_FSM: 構造化出力のFSMコンパイル待ち
    #   - WAITING_FOR_STREAMING_REQ: ストリーミング入力待ち
    #   - LoRA制約超過

    # プレフィックスキャッシュ検索（初回のみ）
    if request.num_computed_tokens == 0:
        computed_blocks, num_hits = kv_cache_manager.get_computed_blocks(request)
        # KVコネクタ（LMCache等）による外部キャッシュも検索

    # 計算対象トークン数
    num_new_tokens = request.num_tokens - num_computed_tokens
    num_new_tokens = min(num_new_tokens, token_budget)

    # KVキャッシュブロック割り当て
    new_blocks = kv_cache_manager.allocate_slots(request, num_new_tokens, ...)
    if new_blocks is None:
        break  # ← RUNNINGと異なりプリエンプションせずループ終了

    # RUNNINGキューに追加
    self.running.append(request)
    request.status = RequestStatus.RUNNING
    token_budget -= num_new_tokens
</code></pre>
<h3 id="phase-3-scheduleroutput-構築l827-896"><a class="header" href="#phase-3-scheduleroutput-構築l827-896">Phase 3: SchedulerOutput 構築（L827-896）</a></h3>
<pre><code># 新規リクエスト → NewRequestData（フルデータ）
new_reqs_data = [NewRequestData.from_request(req, block_ids) for req in scheduled_new_reqs]

# 既実行リクエスト → CachedRequestData（差分のみ）
cached_reqs_data = self._make_cached_request_data(running_reqs, resumed_reqs, ...)

return SchedulerOutput(
    scheduled_new_reqs=new_reqs_data,
    scheduled_cached_reqs=cached_reqs_data,
    num_scheduled_tokens=num_scheduled_tokens,
    total_num_scheduled_tokens=total,
    ...
)
</code></pre>
<h2 id="unified-compute-model"><a class="header" href="#unified-compute-model">Unified Compute Model</a></h2>
<p>vLLM v1のSchedulerはPrefillとDecodeを明示的に区別しない。各リクエストの <code>num_computed_tokens</code> が <code>num_tokens_with_spec</code>（プロンプト長 + 出力長 + スペキュレーショントークン）に追いつくまでトークンを割り当てる。</p>
<p>このアプローチにより以下が統一的に扱える:</p>
<ul>
<li><strong>Chunked Prefill</strong>: 大きなプロンプトを複数ステップに分割</li>
<li><strong>Prefix Caching</strong>: キャッシュヒット分を <code>num_computed_tokens</code> に反映</li>
<li><strong>Speculative Decoding</strong>: ドラフトトークンを <code>num_tokens_with_spec</code> に含める</li>
</ul>
<h2 id="トークン予算"><a class="header" href="#トークン予算">トークン予算</a></h2>
<pre><code class="language-python">token_budget = self.max_num_scheduled_tokens  # ステップあたりの上限
</code></pre>
<ul>
<li>各リクエストのスケジュール時に <code>token_budget -= num_new_tokens</code> で消費</li>
<li>Phase 1（RUNNING）→ Phase 2（WAITING）の順で消費</li>
<li>枯渇時: RUNNING側は continue（次リクエスト試行）、WAITING側は break（ループ終了）</li>
</ul>
<h2 id="プリエンプション"><a class="header" href="#プリエンプション">プリエンプション</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:898</code> (_preempt_request)</p>
<p>KVキャッシュブロック不足時にRUNNINGリクエストに対してのみ発動:</p>
<ol>
<li>ポリシーに基づき最低優先度のリクエストを選択
<ul>
<li><strong>Priority</strong>: <code>(priority, arrival_time)</code> が最大のリクエスト</li>
<li><strong>FIFO</strong>: 最後のリクエスト</li>
</ul>
</li>
<li><code>kv_cache_manager.free(request)</code> でブロック解放</li>
<li><code>request.status = RequestStatus.PREEMPTED</code>、<code>num_computed_tokens = 0</code> にリセット</li>
<li>WAITINGキューの先頭に戻す（LIFO順序で優先再スケジュール）</li>
</ol>
<h2 id="request-ステータス遷移"><a class="header" href="#request-ステータス遷移">Request ステータス遷移</a></h2>
<pre class="mermaid">stateDiagram-v2
    [*] --&gt; WAITING: add_request()
    WAITING --&gt; WAITING_FOR_FSM: FSMコンパイル待ち
    WAITING --&gt; WAITING_FOR_REMOTE_KVS: リモートKV受信待ち
    WAITING --&gt; WAITING_FOR_STREAMING_REQ: ストリーミング入力待ち
    WAITING_FOR_FSM --&gt; WAITING: コンパイル完了
    WAITING_FOR_REMOTE_KVS --&gt; WAITING: 受信完了
    WAITING_FOR_STREAMING_REQ --&gt; WAITING: 入力完了
    WAITING --&gt; RUNNING: schedule()で選択
    RUNNING --&gt; PREEMPTED: KVキャッシュ不足
    PREEMPTED --&gt; WAITING: キュー先頭に戻す
    RUNNING --&gt; FINISHED_STOPPED: EOS/stop_token検出
    RUNNING --&gt; FINISHED_LENGTH_CAPPED: max_tokens到達
    RUNNING --&gt; FINISHED_ABORTED: ユーザーによる中止
    RUNNING --&gt; FINISHED_ERROR: エラー発生
</pre>

<h2 id="update_from_output-フロー"><a class="header" href="#update_from_output-フロー">update_from_output() フロー</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:1241</code></p>
<pre><code>update_from_output(scheduler_output, model_runner_output)
  → dict[int, EngineCoreOutputs]

処理:
  for req_id in scheduler_output.scheduled requests:
    # Speculative Decoding リジェクション処理
    #   → 不採用トークン分 num_computed_tokens を巻き戻し

    # 生成トークンをリクエストに追加
    new_token_ids, stopped = _update_request_with_output(request, tokens)

    # 完了判定
    if stopped:
      finish_reason = request.get_finished_reason()
      _free_request(request)  # ブロック解放

    # EngineCoreOutput を構築
    outputs[client_index].append(EngineCoreOutput(
      request_id, new_token_ids, finish_reason, logprobs, ...
    ))

  return {client_index: EngineCoreOutputs(outputs=outs) for ...}
</code></pre>
<h2 id="設定-5"><a class="header" href="#設定-5">設定</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>max_num_seqs</code></td><td>設定依存</td><td>同時実行リクエスト数上限</td></tr>
<tr><td><code>max_num_batched_tokens</code></td><td>設定依存</td><td>ステップあたりのトークン予算上限</td></tr>
<tr><td><code>enable_chunked_prefill</code></td><td>設定依存</td><td>Chunked Prefillの有効化</td></tr>
<tr><td><code>long_prefill_token_threshold</code></td><td>0</td><td>長プロンプト分割閾値（0=無効）</td></tr>
<tr><td><code>scheduling_policy</code></td><td>Priority</td><td>プリエンプション選択ポリシー</td></tr>
</tbody>
</table>
</div>
<h2 id="呼び出しフロー-5"><a class="header" href="#呼び出しフロー-5">呼び出しフロー</a></h2>
<pre><code>EngineCore.add_request(request)
  → scheduler.add_request(request)       # WAITINGキューに登録

EngineCore.step()
  ├─ scheduler.schedule()                 # → SchedulerOutput
  │   ├─ kv_cache_manager.get_computed_blocks()  # プレフィックスキャッシュ
  │   └─ kv_cache_manager.allocate_slots()       # ブロック割り当て
  ├─ executor.execute_model(scheduler_output)     # GPU実行
  └─ scheduler.update_from_output(output)         # → EngineCoreOutputs
</code></pre>
<h2 id="関連ドキュメント-11"><a class="header" href="#関連ドキュメント-11">関連ドキュメント</a></h2>
<ul>
<li><a href="#kvcachemanager-サマリー">KVCacheManager</a></li>
<li><a href="#enginecore-サマリー">EngineCore</a></li>
<li><a href="#エントリポイント-asyncllm--llm-サマリー">エントリポイント</a></li>
<li><a href="#テキスト推論データフロー">データフロー</a></li>
<li><a href="#用語集">用語集: Continuous Batching, Unified Compute Model</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cacheblend-github-議論調査"><a class="header" href="#cacheblend-github-議論調査">CacheBlend GitHub 議論調査</a></h1>
<p><strong>調査日</strong>: 2026-02-14
<strong>対象リポジトリ</strong>: vllm-project/vllm, LMCache/LMCache, LMCache/LMCache-Ascend</p>
<h2 id="エグゼクティブサマリー"><a class="header" href="#エグゼクティブサマリー">エグゼクティブサマリー</a></h2>
<p>CacheBlendは、プレフィックス一致に限定せず、部分的・非連続的なKVキャッシュの再利用を可能にする技術（<a href="https://arxiv.org/abs/2405.16444">論文: arXiv:2405.16444</a>）。LMCacheプロジェクトがvLLM向けの実装を提供しているが、<strong>vLLM本体への統合はまだ実現していない</strong>。</p>
<p>最大の未解決課題は <strong>オンライン推論（<code>vllm serve</code>）でのCacheBlend利用</strong>であり、2026年2月時点でも安定した動作は実現されていない。オフライン（<code>LLM.generate()</code>直接呼び出し）では動作するが、HTTP APIを通じた利用では複数の技術的障壁がある。</p>
<h2 id="主要な議論論点"><a class="header" href="#主要な議論論点">主要な議論・論点</a></h2>
<h3 id="1-vllm本体への-generalized-kv-cache-reuse-rfc"><a class="header" href="#1-vllm本体への-generalized-kv-cache-reuse-rfc">1. vLLM本体への Generalized KV Cache Reuse RFC</a></h3>
<ul>
<li><strong>Issue</strong>: <a href="https://github.com/vllm-project/vllm/issues/25950">vllm-project/vllm#25950</a> (open)</li>
<li><strong>状態</strong>: 停滞中（staleラベル後にunstaleされたが、実装は未着手）</li>
</ul>
<p>プレフィックス一致に限定しない一般化されたKVキャッシュ再利用をvLLMに組み込む提案。3領域の変更を提案:</p>
<ol>
<li><strong>Attention Kernel</strong>: per-tokenマスキングパラメータの追加（FlashInferは既にサポート）</li>
<li><strong>KV Connector</strong>: 成功ビットマップの返却（現在のsequentialなprefix長ではなく）</li>
<li><strong>Scheduler</strong>: 穴あきKVキャッシュを持つトークンの適切な処理</li>
</ol>
<p><strong>2026年1月12日の重要な進展</strong>: カーネルやグルーロジックを変更せずに実装する方法を発見。<strong>プロンプトを複数のサブリクエストに分割してバッチ内で処理する</strong>アプローチ。コードは「soon」とされたが、まだ公開されていない。</p>
<p>LMCacheチーム（@ApostaC）は「CacheBlendは既にLMCacheに実装済み」と返答。提案者（@iddo10）は「LMCacheの実装は別のforwardパスを書いているが、この提案はvLLM本体に組み込む」と差別化を主張。</p>
<h3 id="2-vllm-serve-でのcacheblendサポートオンライン推論-最重要"><a class="header" href="#2-vllm-serve-でのcacheblendサポートオンライン推論-最重要">2. <code>vllm serve</code> でのCacheBlendサポート（オンライン推論） [最重要]</a></h3>
<ul>
<li><strong>Issue</strong>: <a href="https://github.com/LMCache/LMCache/issues/1936">LMCache#1936</a> (open)</li>
<li><strong>関連</strong>: <a href="https://github.com/LMCache/LMCache/issues/1136">#1136</a>, <a href="https://github.com/LMCache/LMCache/issues/1290">#1290</a>, <a href="https://github.com/LMCache/LMCache/issues/1682">#1682</a> (いずれもclosed/stale)</li>
<li><strong>状態</strong>: 未解決。繰り返し報告されており、最も重要な未解決課題</li>
</ul>
<p>CacheBlendは現在 <code>LLM.generate()</code> によるオフライン推論でのみ動作。<code>vllm serve</code> 経由のHTTP APIでは以下の技術的障壁がある:</p>
<ol>
<li><strong>トークン化の不一致</strong>: <code>blend_special_str</code>（例: <code>" # #"</code>）が前後のコンテキストにより異なるトークンIDに変換される。HTTP API経由ではトークン化を制御不可能</li>
<li><strong><code>/v1/chat/completions</code> は <code>input_ids</code> を受け付けない</strong>: セグメント境界の正確な指定が不可能</li>
<li><strong><code>/v1/completions</code> で <code>input_ids</code> を渡してもキャッシュのロードが発生しない</strong>: ストアは行われるがリユースなし</li>
<li><strong>vLLM GPUワーカーへのパッチが必要</strong>: <code>gpu_worker.py</code> の手動修正が必要で、バージョン間で互換性が壊れる</li>
</ol>
<p><strong>ワークアラウンド</strong>（2026年2月3日 @rick-heig）: Qwen3ファミリーでは <code>&lt;|file_sep|&gt;</code> のような常に同じトークンIDに変換される特殊トークンをblend区切り文字として使うことで、テキストレベルでの操作が可能になる。</p>
<h3 id="3-cacheblend-v1の品質安定性バグ"><a class="header" href="#3-cacheblend-v1の品質安定性バグ">3. CacheBlend V1の品質・安定性バグ</a></h3>
<ul>
<li><strong>ガーブル出力</strong> (<a href="https://github.com/LMCache/LMCache/issues/2496">#2496</a>, open): 複数エントリ処理時、最初以外の出力が文字化け。GPU/CPUメモリのクリア不足の疑い</li>
<li><strong>キャッシュヒット後の保存漏れ</strong> (<a href="https://github.com/LMCache/LMCache/issues/2029">#2029</a>, open/stale): 部分ヒット時、新規計算トークンが保存されない → 後続リクエストのヒット率低下</li>
<li><strong>先頭ミス時の探索打ち切り</strong> (<a href="https://github.com/LMCache/LMCache/issues/2029">#2029</a>): 最初のチャンクがキャッシュミスすると後続チャンクの探索なし → RAGで致命的</li>
<li><strong>layerwiseモードでのKVキャッシュ破損</strong> (PR#2329コメント, 2026-02-03): layerwise有効時、温度0でも異なる応答</li>
</ul>
<h3 id="4-lmcache側の安定化pr"><a class="header" href="#4-lmcache側の安定化pr">4. LMCache側の安定化PR</a></h3>
<ul>
<li><strong><a href="https://github.com/LMCache/LMCache/pull/762">PR#762</a></strong> (2025-06マージ): CacheBlend V1初期実装</li>
<li><strong><a href="https://github.com/LMCache/LMCache/pull/2329">PR#2329</a></strong> (open, コンフリクトあり): layerwise/blendingエッジケース修正 + vLLMパッチヘルパー。テスト中にlayerwiseのKVキャッシュ破損が新たに報告されており、マージ見通し不明</li>
</ul>
<h3 id="5-バージョン互換性"><a class="header" href="#5-バージョン互換性">5. バージョン互換性</a></h3>
<p>LMCache-Ascendチームが整理した互換性マトリクス (<a href="https://github.com/LMCache/LMCache-Ascend/issues/154">#154</a>):</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>vLLM</th><th>LMCache</th><th>CacheBlend</th></tr>
</thead>
<tbody>
<tr><td>0.9.2</td><td>0.3.3 / 0.3.7</td><td>Production Ready</td></tr>
<tr><td>0.10.0</td><td>0.3.7</td><td>Not supported</td></tr>
<tr><td>0.11.0</td><td>0.3.7</td><td>Not supported</td></tr>
<tr><td>0.10.0</td><td>0.3.12</td><td>Production Ready</td></tr>
<tr><td>0.11.0</td><td>0.3.12</td><td>Production Ready</td></tr>
</tbody>
</table>
</div>
<p>※ Ascend NPU版マトリクス。GPU版LMCacheとは異なる可能性あり。</p>
<h2 id="タイムライン"><a class="header" href="#タイムライン">タイムライン</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>日付</th><th>イベント</th></tr>
</thead>
<tbody>
<tr><td>2025-06-07</td><td>CacheBlend V1 初期実装が LMCache にマージ (<a href="https://github.com/LMCache/LMCache/pull/762">PR#762</a>)</td></tr>
<tr><td>2025-07-24</td><td>オンライン推論での動作不良が初報告 (<a href="https://github.com/LMCache/LMCache/issues/1136">#1136</a>)</td></tr>
<tr><td>2025-08-05</td><td>トークン化不一致問題の根本原因が特定</td></tr>
<tr><td>2025-09-30</td><td>vLLM本体に Generalized KV Cache Reuse RFC提出 (<a href="https://github.com/vllm-project/vllm/issues/25950">#25950</a>)</td></tr>
<tr><td>2025-10-31</td><td><code>vllm serve</code> サポートの明確な Feature Request (<a href="https://github.com/LMCache/LMCache/issues/1936">#1936</a>)</td></tr>
<tr><td>2025-12-29</td><td>layerwise/blending修正PR提出 (<a href="https://github.com/LMCache/LMCache/pull/2329">PR#2329</a>)</td></tr>
<tr><td>2026-01-12</td><td>vLLM RFC#25950にてサブリクエスト分割アプローチ発見の報告</td></tr>
<tr><td>2026-01-27</td><td>ガーブル出力バグ報告 (<a href="https://github.com/LMCache/LMCache/issues/2496">#2496</a>)</td></tr>
<tr><td>2026-02-03</td><td>layerwise KVキャッシュ破損の新規報告 + 特殊トークンワークアラウンド提案</td></tr>
</tbody>
</table>
</div>
<h2 id="結論所見"><a class="header" href="#結論所見">結論・所見</a></h2>
<h3 id="オンライン推論vllm-serveの現状"><a class="header" href="#オンライン推論vllm-serveの現状">オンライン推論（<code>vllm serve</code>）の現状</a></h3>
<p><strong>動作しない</strong>。オフライン専用の状態が約8ヶ月続いている。根本的な問題はCacheBlendのセグメント区切りがトークンレベルの精密な制御を要求するのに対し、HTTP APIがテキストレベルの入力しか受け付けない点にある。</p>
<p>2つのアプローチが存在するが、いずれも未完成:</p>
<ol>
<li><strong>LMCache側のアプローチ</strong>: vLLMのworkerにパッチを当てて対応。バージョン間の互換性維持が困難</li>
<li><strong>vLLM本体側のアプローチ</strong>: RFC#25950のサブリクエスト分割方式。コード未公開。実現すればvLLM本体の機能としてCacheBlendが使えるようになる可能性があるが、不透明</li>
</ol>
<h3 id="プラグイン開発への示唆"><a class="header" href="#プラグイン開発への示唆">プラグイン開発への示唆</a></h3>
<p>CacheBlendの現状は、vLLMのKV Connectorインターフェースの限界を示している:</p>
<ul>
<li>現在のKV ConnectorはPrefix Caching前提（連続ブロックの転送）</li>
<li>非連続キャッシュ再利用にはScheduler・Attention Kernelレベルの変更が必要</li>
<li>RFC#25950のサブリクエスト分割アプローチが実現すれば、KV Connector層のみで対応可能になる</li>
</ul>
<p>独自プラグイン作成を検討する場合、CacheBlendの統合方式の行方は重要な参考情報となる。</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cacheblend-実装調査報告-medium-verified"><a class="header" href="#cacheblend-実装調査報告-medium-verified">CacheBlend 実装調査報告 [MEDIUM] [VERIFIED]</a></h1>
<blockquote>
<p><strong>最終更新</strong>: 2026-02-15
<strong>対象ソース</strong>: <code>target/LMCache/lmcache/v1/compute/blend/</code>, <code>target/LMCache/lmcache/v1/compute/models/</code>, <code>target/LMCache/lmcache/v1/compute/attention/</code>, <code>target/LMCache/lmcache/v1/gpu_connector/gpu_connectors.py</code>, <code>target/LMCache/lmcache/integration/vllm/vllm_v1_adapter.py</code></p>
</blockquote>
<h2 id="調査目的"><a class="header" href="#調査目的">調査目的</a></h2>
<p>CacheBlendの実装が論文の主張（RAGチャンク間のクロスアテンション、選択的KV再計算、レイヤー別KV選択、KVロード/再計算パイプライン）をどの程度実現しているか、またvLLM本体へのパッチなしにKV Transferプラグインだけで動作するかを明らかにする。</p>
<h2 id="結論"><a class="header" href="#結論">結論</a></h2>
<p><strong>CacheBlendはvLLM本体への ad-hoc パッチが必要</strong>。KV Transferプラグインの枠組みだけでは完結しない。理由は、CacheBlendがvLLMのモデルオブジェクト（Transformerレイヤー群）に直接アクセスして独自のforward計算を行うため。</p>
<h2 id="論文機能-vs-実装状況"><a class="header" href="#論文機能-vs-実装状況">論文機能 vs 実装状況</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>論文の主張</th><th>実装状態</th><th>備考</th></tr>
</thead>
<tbody>
<tr><td>RAGチャンク間のKV再利用</td><td><strong>実装済み</strong></td><td>セパレータトークンで段落分割、段落単位でKV保存・ルックアップ</td></tr>
<tr><td>重要token同定（選択的KV再計算）</td><td><strong>実装済み</strong></td><td>topk by K差分のL2ノルム。<code>check_layers</code>で指定レイヤーで判定</td></tr>
<tr><td>レイヤー別KV選択</td><td><strong>部分実装</strong></td><td><code>check_layers</code>で判定レイヤーを指定可能だが、判定結果は以降の全レイヤーに一律適用</td></tr>
<tr><td>KVロード/再計算パイプライン</td><td><strong>実装済み</strong></td><td>ジェネレータでレイヤー別にロード→RoPE補正→paged memory書込みをオーバーラップ</td></tr>
<tr><td>レイヤーごとの異なるrecomp_ratio</td><td><strong>未実装 (TODO)</strong></td><td><code>recomp_ratios[0]</code>のハードコード。TODOコメントあり</td></tr>
<tr><td>閾値ベースのblending</td><td><strong>未実装 (TODO)</strong></td><td><code>blend_thresholds</code>設定は存在するが未使用</td></tr>
<tr><td>TP/PP対応</td><td><strong>未実装 (TODO)</strong></td><td><code>base.py</code>冒頭にTODOコメント</td></tr>
<tr><td>マルチモーダル対応</td><td><strong>未実装 (TODO)</strong></td><td>同上</td></tr>
<tr><td>プレフィックスキャッシュとの互換</td><td><strong>未実装 (TODO)</strong></td><td><code>vllm_v1_adapter.py:803</code>にTODOコメント</td></tr>
</tbody>
</table>
</div>
<h2 id="アーキテクチャ-9"><a class="header" href="#アーキテクチャ-9">アーキテクチャ</a></h2>
<h3 id="全体構成"><a class="header" href="#全体構成">全体構成</a></h3>
<pre class="mermaid">graph TD
    subgraph vLLM_Worker["vLLM Worker (パッチ必要)"]
        GMR[GPUModelRunner] --&gt;|"model参照を登録"| VMT[VLLMModelTracker]
    end

    subgraph LMCache_Blend["LMCache CacheBlend"]
        VMT --&gt;|"get_model()"| LMB[LMCBlender]
        LMB --&gt;|"layerwise_model"| LMM[LMCBaseModel&lt;br/&gt;独自forward実行]
        LMM --&gt;|"process_qkv()"| LMB
        LMB --&gt;|"get_kv(layer_id)"| GPU_BUF[VLLMBufferLayerwiseGPUConnector&lt;br/&gt;中間バッファ]
        GPU_BUF --&gt;|"batched_to_gpu()"| KVC[vLLM KVCache&lt;br/&gt;paged memory]
    end

    subgraph LMCache_Storage["LMCache Storage"]
        CE[LMCacheEngine] --&gt;|"retrieve_layer()"| GPU_BUF
        CE --&gt;|"チャンク取得"| SM[StorageManager&lt;br/&gt;CPU/Disk/Remote]
    end

    LMB --&gt;|"blend()"| CE
</pre>

<h3 id="cacheblendの実行フロー"><a class="header" href="#cacheblendの実行フロー">CacheBlendの実行フロー</a></h3>
<p><code>LMCacheConnectorV1Impl.start_load_kv()</code> 内で <code>self.blender.blend()</code> が呼ばれる:</p>
<ol>
<li><strong>セパレータ分割</strong>: 入力トークン列をRAGチャンク区切り（<code>" # # "</code> など）で段落に分割</li>
<li><strong>外部KVロード</strong>: 段落単位でLMCacheストレージからKVキャッシュを取得 → 中間GPUバッファに配置</li>
<li><strong>RoPE位置補正</strong>: 保存時の位置エンコーディングを新しい位置に変換（FusedRope カスタムCUDAカーネル）</li>
<li><strong>独自forward実行</strong>: vLLMモデルのレイヤー群を使って入力を再計算</li>
<li><strong>重要token同定</strong>: <code>check_layers</code>レイヤーで新旧Kの差分L2ノルムを計算、topk個を選択</li>
<li><strong>選択的アテンション再計算</strong>: 選択されたtokenのみでattentionを再計算（FlashAttn or FlashInfer sparse）</li>
<li><strong>KVマージ</strong>: 古いKV（外部ストレージ由来）に重要tokenのKVのみ上書き</li>
<li><strong>paged memory書込み</strong>: 最終KVをvLLMのpaged memoryに書き込み</li>
</ol>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/compute/blend/blender.py:59-120</code></p>
<h3 id="重要token同定アルゴリズム"><a class="header" href="#重要token同定アルゴリズム">重要token同定アルゴリズム</a></h3>
<pre><code class="language-python"># blender.py:88-110 (process_qkv)
if layer_id in self.common_metadata.check_layers:
    # 新しいK（再計算）と古いK（ストレージ由来）のL2距離
    diff_k = torch.sum(
        (k.to(torch.float32) - old_k.to(torch.float32)) ** 2, dim=[1]
    )
    # recomp_ratio割合のtop-kを選択
    topk_num = int(total_len * self.common_metadata.recomp_ratios[0])
    top_indices = torch.topk(diff_k, k=topk_num).indices
    top_indices, _ = torch.sort(top_indices)

    # 選択されたtokenのみQ, K, Vを抽出
    k, v = k[top_indices], v[top_indices]
    q = q[top_indices]
    # 古いKVの該当位置に新しいKVを書き込み
    old_k[self.metadata.imp_indices] = k
    old_v[self.metadata.imp_indices] = v
</code></pre>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/compute/blend/blender.py:88-120</code></p>
<h3 id="独自モデルforwardlmcbasemodelcompute_layer"><a class="header" href="#独自モデルforwardlmcbasemodelcompute_layer">独自モデルforward（LMCBaseModel.compute_layer）</a></h3>
<p>CacheBlendはvLLMのnormalなforward pathを使わず、<strong>vLLMのモデルオブジェクトのレイヤーを直接操作する独自のforward</strong>を実行する:</p>
<pre><code class="language-python"># base.py:66-141 (compute_layer, ジェネレータ)
@torch.compile
def compute_layer(self, input_ids):
    hidden_states = self.vllm_model.get_input_embeddings(input_ids)
    for idx, layer in enumerate(self.vllm_model.model.layers[...]):
        # input_layernorm
        hidden_states, residual = layer.input_layernorm(hidden_states, residual)
        # QKV projection
        qkv, _ = layer.self_attn.qkv_proj(hidden_states)
        q, k, v = qkv.split([q_size, kv_size, kv_size], dim=-1)
        # モデル固有QKV処理（Qwen3のq_norm/k_norm等）
        q, k, v = self._process_qkv(q, k, v, layer)
        # ★ blenderによる重要token選択・KVマージ
        q, k, v, residual, attn_output, attn_metadata = self.blender.process_qkv(...)
        # 独自attentionバックエンド（flash_attn_varlen_func）
        attn_output = self.lmc_attn_layers[idx].forward_contiguous(q, k, v, ...)
        # output projection + MLP
        hidden_states, _ = layer.self_attn.o_proj(attn_output)
        hidden_states, residual = layer.post_attention_layernorm(hidden_states, residual)
        hidden_states = layer.mlp(hidden_states)
        yield  # レイヤーごとにジェネレータで制御を返す
</code></pre>
<p><strong>重要</strong>: これはvLLMの通常のforward pathとは完全に別のコードパス。vLLMのAttention層、CUDAGraph、InputBatch最適化等は一切使われない。</p>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/compute/models/base.py:66-141</code></p>
<h3 id="対応モデル"><a class="header" href="#対応モデル">対応モデル</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>モデル</th><th>クラス</th><th>QKV処理</th></tr>
</thead>
<tbody>
<tr><td>Llama</td><td><code>LMCLlamaModel</code></td><td>なし（そのまま）</td></tr>
<tr><td>Qwen2</td><td><code>LMCLlamaModel</code>（流用）</td><td>なし（そのまま）</td></tr>
<tr><td>Qwen3</td><td><code>LMCQwen3Model</code></td><td>q_norm/k_norm 適用</td></tr>
</tbody>
</table>
</div>
<p><strong>3モデルのみ</strong>。新モデル追加には <code>LMCBaseModel</code> の継承と <code>_process_qkv()</code> 実装が必要。</p>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/compute/models/utils.py:14-35</code></p>
<h3 id="アテンションバックエンド"><a class="header" href="#アテンションバックエンド">アテンションバックエンド</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>バックエンド</th><th>クラス</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td>FlashAttention</td><td><code>LMCFlashAttnBackend</code></td><td>密なアテンション計算（デフォルト）</td></tr>
<tr><td>FlashInfer Sparse</td><td><code>HackBSAWrapper</code> + <code>LMCFlashInferSparseMetadata</code></td><td>スパースブロックアテンション（<code>enable_sparse=True</code>時）</td></tr>
</tbody>
</table>
</div>
<p><code>enable_sparse=True</code> を指定すると、FlashInferのVariableBlockSparseAttentionWrapperをハックした <code>HackBSAWrapper</code> を使用して、選択されたtokenブロックのみスパースにアテンション計算を行う。</p>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/compute/attention/flash_attn.py</code>, <code>target/LMCache/lmcache/v1/compute/attention/flash_infer_sparse.py</code></p>
<h2 id="vllm本体パッチの分析"><a class="header" href="#vllm本体パッチの分析">vLLM本体パッチの分析</a></h2>
<h3 id="必要なパッチexamplesblend_kv_v1readmemd-に明記"><a class="header" href="#必要なパッチexamplesblend_kv_v1readmemd-に明記">必要なパッチ（examples/blend_kv_v1/README.md に明記）</a></h3>
<pre><code class="language-python"># vllm/v1/worker/gpu_worker.py の load_model() 末尾に追加が必要:
from lmcache.v1.compute.models.utils import VLLMModelTracker
from lmcache.integration.vllm.utils import ENGINE_NAME

VLLMModelTracker.register_model(ENGINE_NAME, self.model_runner.model)
ensure_kv_transfer_initialized(self.vllm_config)
</code></pre>
<p>さらに <code>init_worker_distributed_environment()</code> 内の <code>ensure_kv_transfer_initialized()</code> をコメントアウトし、上記の <code>load_model()</code> に移動する必要がある。</p>
<p><strong>参照</strong>: <code>target/LMCache/examples/blend_kv_v1/README.md</code></p>
<h3 id="パッチが必要な理由"><a class="header" href="#パッチが必要な理由">パッチが必要な理由</a></h3>
<ol>
<li><strong>モデルオブジェクトへの直接アクセス</strong>: <code>VLLMModelTracker</code> にvLLMのモデルオブジェクトを登録する必要がある。CacheBlendはこのオブジェクトの <code>.model.layers[i]</code> に直接アクセスして独自forwardを実行する</li>
<li><strong>初期化順序の変更</strong>: KV Transfer初期化をモデルロード後に移動する必要がある（モデルオブジェクトが存在しないとblender構築に失敗）</li>
<li><strong>KVConnectorのAPIでは不十分</strong>: KVConnectorBase_V1の<code>start_load_kv()</code>/<code>wait_for_layer_load()</code>/<code>save_kv_layer()</code>インタフェースは、KVの読み書きのみを想定。CacheBlendが行う「独自forward＋選択的再計算」はスコープ外</li>
</ol>
<h3 id="プラグイン境界の問題"><a class="header" href="#プラグイン境界の問題">プラグイン境界の問題</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>KV Transferプラグインの範囲</th><th>CacheBlendが必要とすること</th><th>ギャップ</th></tr>
</thead>
<tbody>
<tr><td>KVキャッシュの読み書き</td><td>モデルのforward計算</td><td>forwardはWorker/GPUModelRunnerの責任</td></tr>
<tr><td>レイヤー別KVロード/セーブ</td><td>レイヤー内部のQKV取得・attention再計算</td><td>レイヤー内部ロジックへのアクセス不可</td></tr>
<tr><td>メタデータ受け渡し</td><td>vLLMモデルオブジェクトへの参照</td><td>メタデータでは渡せない</td></tr>
<tr><td>slot_mapping操作</td><td>独自のattention計算結果のpaged memory書込み</td><td>通常のattention pathをバイパス</td></tr>
</tbody>
</table>
</div>
<h2 id="vllmbufferlayerwisegpuconnector--blending専用gpuコネクタ"><a class="header" href="#vllmbufferlayerwisegpuconnector--blending専用gpuコネクタ">VLLMBufferLayerwiseGPUConnector — Blending専用GPUコネクタ</a></h2>
<p><code>enable_blending=True</code> 時に選択される専用コネクタ。通常のLayerwiseGPUConnectorとの違い:</p>
<ol>
<li><strong>中間GPUバッファ</strong>: ロード結果をpaged memoryではなく連続バッファに保持（<code>get_kv(layer_id)</code>でblenderがアクセス）</li>
<li><strong>RoPE位置補正</strong>: <code>batched_to_gpu()</code>ジェネレータ内で <code>fused_rotary_emb</code> による位置エンコーディング変換を実行</li>
<li><strong>ギャップゼロイング</strong>: RAGチャンク間のセパレータ位置をゼロで埋める（<code>current_gap_positions</code>）</li>
<li><strong>ダブルバッファ</strong>: compute/loadバッファのping-pongでロードと計算をオーバーラップ</li>
</ol>
<h3 id="パイプライン動作"><a class="header" href="#パイプライン動作">パイプライン動作</a></h3>
<pre><code>Layer 0: [ロード: CPU→GPU buf_load]
         [compute: ─────────────] [RoPE補正: buf_load→buf_compute]
Layer 1: [ロード: CPU→GPU buf_load]
         [compute: ─────────────] [RoPE補正: buf_load→buf_compute] [buf→paged: Layer 0]
Layer 2: [ロード: CPU→GPU buf_load]
         [compute: ─────────────] [RoPE補正: buf_load→buf_compute] [buf→paged: Layer 1]
...
Layer N+1: [buf→paged: Layer N-1]
Layer N+2: (yield完了)
</code></pre>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/gpu_connector/gpu_connectors.py:552-836</code></p>
<h2 id="blendserverマルチプロセス版"><a class="header" href="#blendserverマルチプロセス版">BlendServer（マルチプロセス版）</a></h2>
<p><code>blend_server.py</code> は <code>MPCacheEngine</code> を継承した <code>BlendEngine</code> クラスを提供する。ZMQメッセージキューで別プロセスとして動作し、以下の追加APIを持つ:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>API</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>cb_register_kv_cache</code></td><td>blendエンジンのKVキャッシュ登録</td></tr>
<tr><td><code>cb_lookup_pre_computed</code></td><td>セパレータで段落分割→段落単位ルックアップ</td></tr>
<tr><td><code>cb_store_pre_computed</code></td><td>段落単位の事前計算KV保存（BLEND_HASH_PREFIX付き）</td></tr>
<tr><td><code>cb_retrieve_pre_computed</code></td><td>段落単位の事前計算KV取得→GPUバッファ書込み</td></tr>
<tr><td><code>cb_store_final</code></td><td>最終KV保存（通常ハッシュ、通常モードLLMからも利用可能）</td></tr>
</tbody>
</table>
</div>
<p><code>ParallelPatternMatcher</code>（Cで実装）でセパレータトークンを高速マッチング。</p>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/multiprocess/blend_server.py</code></p>
<h2 id="設定-6"><a class="header" href="#設定-6">設定</a></h2>
<h3 id="lmcache側"><a class="header" href="#lmcache側">LMCache側</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>設定</th><th>デフォルト</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>enable_blending</code></td><td><code>False</code></td><td>CacheBlendモード有効化</td></tr>
<tr><td><code>blend_check_layers</code></td><td><code>None</code></td><td>重要token判定を行うレイヤーIDリスト</td></tr>
<tr><td><code>blend_recompute_ratios</code></td><td><code>None</code></td><td>再計算するtokenの割合リスト（現在は<code>[0]</code>のみ使用）</td></tr>
<tr><td><code>blend_thresholds</code></td><td><code>None</code></td><td>閾値ベースblending用（<strong>未実装</strong>）</td></tr>
<tr><td><code>enable_sparse</code> (extra_config)</td><td><code>False</code></td><td>FlashInfer スパースアテンション使用</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/config.py:101-118</code></p>
<h3 id="暗黙の前提"><a class="header" href="#暗黙の前提">暗黙の前提</a></h3>
<ul>
<li><code>use_layerwise=True</code> が必須（blending有効時に自動選択: <code>VLLMBufferLayerwiseGPUConnector</code>）</li>
<li><code>save_unfull_chunk=True</code> が自動設定される</li>
<li><code>SegmentTokenDatabase</code> が使用される（<code>ChunkedTokenDatabase</code>ではなく）</li>
</ul>
<h2 id="制約未実装機能"><a class="header" href="#制約未実装機能">制約・未実装機能</a></h2>
<ol>
<li><strong>vLLM本体パッチ必須</strong> — プラグインのみでは動作しない</li>
<li><strong>対応モデル3種のみ</strong> — Llama, Qwen2, Qwen3。新モデルはLMCBaseModel継承が必要</li>
<li><strong>RoPE制約</strong> — <code>rotary_dim == head_size</code>、<code>rope_scaling=None</code>、<code>partial_rotary_factor=1.0</code> のみ</li>
<li><strong>プレフィックスキャッシュ非互換</strong> — TODOコメント、blending後のプレフィックスキャッシュスキップ</li>
<li><strong>TP/PP未対応</strong> — TODOコメント</li>
<li><strong>マルチモーダル未対応</strong> — TODOコメント</li>
<li><strong>バッチサイズ1前提</strong> — FlashAttnMetadata初期化で <code>batch_size=1</code> ハードコード</li>
<li><strong>レイヤー別ratio未実装</strong> — <code>recomp_ratios[0]</code>のみ使用</li>
<li><strong>CUDAGraph未対応</strong> — 独自forward pathのため<code>torch.compile</code>のみ</li>
</ol>
<h2 id="関連ドキュメント-12"><a class="header" href="#関連ドキュメント-12">関連ドキュメント</a></h2>
<ul>
<li><a href="#lmcache-統合調査報告-medium-verified">LMCache統合調査報告</a> — LMCache全体のvLLM統合</li>
<li><a href="#kv-transfer-medium-verified">KV Transfer summary</a> — KVConnectorBase_V1のAPI</li>
<li><a href="#gpumodelrunner">GPUModelRunner summary</a> — 通常のforward path</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ecconnector-github-議論調査レポート"><a class="header" href="#ecconnector-github-議論調査レポート">ECConnector GitHub 議論調査レポート</a></h1>
<blockquote>
<p><strong>ステータス</strong>: 調査完了
<strong>作成日</strong>: 2026-02-14
<strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>関連ドキュメント</strong>: <a href="#encodercache-永続化と階層キャッシュ-調査報告">EncoderCache 永続化と階層キャッシュ</a>、<a href="#kvcachemanager-サマリー">KVCacheManager</a></p>
</blockquote>
<hr>
<h2 id="1-概要"><a class="header" href="#1-概要">1. 概要</a></h2>
<p>ECConnector は、マルチモーダルモデルの <strong>Encode-Prefill-Decode (EPD) 分離推論</strong> を実現するために導入されたインフラストラクチャである。エンコーダ出力（画像埋め込みなど）を別プロセス間で転送・キャッシュするための抽象インタフェース <code>ECConnectorBase</code> を定義し、プラグイン可能な設計になっている。</p>
<p>2025年11月に基盤PR (<a href="https://github.com/vllm-project/vllm/pull/25233">#25233</a>) がマージされた後、encoder-onlyモード、リモートキャッシュチェック最適化、ec_both ロールが順次マージされた。現在は<strong>高性能転送バックエンド</strong>（SHMConnector vs Mooncake ECConnector）の方向性が議論中で、メンテナーは Mooncake ベースの統一的なソリューションを志向している。</p>
<h2 id="2-マージ済み-pr確定した設計"><a class="header" href="#2-マージ済み-pr確定した設計">2. マージ済み PR（確定した設計）</a></h2>
<h3 id="21-基盤-epd分離-25233--2025-11-12-マージ-verified"><a class="header" href="#21-基盤-epd分離-25233--2025-11-12-マージ-verified">2.1 基盤: EPD分離 (#25233) — 2025-11-12 マージ [VERIFIED]</a></h3>
<p><strong>主導</strong>: Chenguang Zheng (fake0fan)</p>
<p>ECConnectorBase はスケジューラ側とワーカー側の責務を明確に分離:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>責務</th><th>メソッド</th><th>実行場所</th></tr>
</thead>
<tbody>
<tr><td>キャッシュ存在確認</td><td><code>check_caches_exist</code></td><td>スケジューラ</td></tr>
<tr><td>割当後状態更新</td><td><code>update_state_after_alloc</code></td><td>スケジューラ</td></tr>
<tr><td>メタデータ構築</td><td><code>build_connector_meta</code></td><td>スケジューラ</td></tr>
<tr><td>リクエスト完了通知</td><td><code>request_finished</code></td><td>スケジューラ</td></tr>
<tr><td>キャッシュ読み込み</td><td><code>start_load_caches</code></td><td>ワーカー</td></tr>
<tr><td>キャッシュ保存</td><td><code>save_caches</code></td><td>ワーカー</td></tr>
</tbody>
</table>
</div>
<p>参照実装として <code>ECExampleConnector</code>（safetensors でディスク保存）を同梱。<code>ec_connector_module_path</code> によるOOTプラグインロードもサポート。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_connector/</code></p>
<h3 id="22-encoder-only-モード-30242--2025-12-22-マージ-verified"><a class="header" href="#22-encoder-only-モード-30242--2025-12-22-マージ-verified">2.2 Encoder-only モード (#30242) — 2025-12-22 マージ [VERIFIED]</a></h3>
<p>EPD分離時にエンコーダ専用インスタンスがLLM重みまでロードしてしまう問題を解決。<code>--convert mm_encoder_only</code> オプションを追加。</p>
<ul>
<li>当初 <code>VisionOnly</code> という名称 → オーディオ等も対象のため <code>MMEncoderOnly</code> に変更</li>
<li>A100でのGPUメモリ使用量: <strong>44GB → 3.7GB</strong> に削減</li>
<li><code>adapters.py</code> の pooling モデルと同様のアプローチ</li>
</ul>
<h3 id="23-リモートキャッシュチェック最適化-32585--2026-01-22-マージ-verified"><a class="header" href="#23-リモートキャッシュチェック最適化-32585--2026-01-22-マージ-verified">2.3 リモートキャッシュチェック最適化 (#32585) — 2026-01-22 マージ [VERIFIED]</a></h3>
<p>スケジューラでのリモートエンコーダキャッシュ存在確認を最適化。EPD追跡RFCのタスクリストの一環。</p>
<h3 id="24-ec_both-ロール-34182--2026-02-10-マージ-verified"><a class="header" href="#24-ec_both-ロール-34182--2026-02-10-マージ-verified">2.4 ec_both ロール (#34182) — 2026-02-10 マージ [VERIFIED]</a></h3>
<p><strong>貢献</strong>: Qi Wang (furionw, NVIDIA)</p>
<p>従来は producer/consumer のどちらか一方のみだったが、集約ノード（EPD一体型）でのエンベディングオフロード（GPU → CPU → GPU）を可能にするため <code>ec_both</code> ロールを追加。KV Connector の both ロールと同様の概念。変更は3ファイル・計10行程度の小規模PR。</p>
<h2 id="3-進行中の議論"><a class="header" href="#3-進行中の議論">3. 進行中の議論</a></h2>
<h3 id="31-高性能転送バックエンド-shmconnector-vs-mooncake-inferred"><a class="header" href="#31-高性能転送バックエンド-shmconnector-vs-mooncake-inferred">3.1 高性能転送バックエンド: SHMConnector vs Mooncake [INFERRED]</a></h3>
<p><strong>関連 PR</strong>: <a href="https://github.com/vllm-project/vllm/pull/33714">#33714</a> (open)</p>
<p>PiratePai が共有メモリ + PyTorch RPC ベースの <code>SHMConnector</code> を提案:</p>
<ul>
<li>RTX 4090 での1E-1PD構成で ExampleConnector より <strong>8-9% の TTFT 改善</strong> を実証</li>
</ul>
<p>しかしメンテナー (NickLucche) は「vllm のツリー内コネクタは少数の高品質ソリューションに絞りたい」と慎重姿勢。fake0fan は <strong>Mooncake ベースの統一 ECConnector</strong> を提案し、RDMA/NVLink/TCP フォールバックの複数バックエンドを1つのコネクタで対応する構想を示している。</p>
<h4 id="技術的課題-エンコーダキャッシュの固定アドレス問題"><a class="header" href="#技術的課題-エンコーダキャッシュの固定アドレス問題">技術的課題: エンコーダキャッシュの固定アドレス問題</a></h4>
<p>エンコーダキャッシュテンソルは KVキャッシュと異なり事前割り当てされた固定アドレスを持たない。NickLucche は2つの選択肢を提示:</p>
<ol>
<li><strong>中間バッファ方式</strong>: 転送前にコピーする</li>
<li><strong>事前割り当て方式</strong>: ECConnector 使用時にはエンコーダキャッシュを固定アドレスバッファに切り替える（現在の dict ベースを置き換え）</li>
</ol>
<h3 id="32-nixlセグメントツリーベースのepd分離-26009-shallow"><a class="header" href="#32-nixlセグメントツリーベースのepd分離-26009-shallow">3.2 NIXL/セグメントツリーベースのEPD分離 (#26009) [SHALLOW]</a></h3>
<p>H. Jhoo (MerHS) が NIXL コミュニケータを使った P2P 直接通信方式を提案。セグメントツリーベースのエンコーダキャッシュマネージャ（ベストフィット割り当て）、動的ロールスイッチング、エンコーダ専用スケジューラなど大規模な変更を含む。</p>
<ul>
<li>90日間の非活動で stale クローズ（2026-02-07）</li>
<li>fake0fan の fork に NIXL ECConnector の作業が継続中</li>
</ul>
<h3 id="33-マルチモーダル前処理の重複排除-27094-inferred"><a class="header" href="#33-マルチモーダル前処理の重複排除-27094-inferred">3.3 マルチモーダル前処理の重複排除 (#27094) [INFERRED]</a></h3>
<p>EPD分離時に全ワーカーで画像前処理が繰り返される問題:</p>
<ul>
<li>プロファイリングで前処理が推論時間の <strong>約50%</strong>（Qwen2.5-VL-3B で 143ms × 3回 ≈ 430ms）を占める</li>
<li><code>image_meta</code> タイプの導入を提案し、エンコーダ処理後は形状メタデータのみを後続ワーカーに転送する案</li>
<li>DarkLight1337 は前処理をエンコーダプロセスに完全移動する方針を示唆</li>
</ul>
<h3 id="34-ecキャッシュの解放メカニズム-32659-shallow"><a class="header" href="#34-ecキャッシュの解放メカニズム-32659-shallow">3.4 ECキャッシュの解放メカニズム (#32659) [SHALLOW]</a></h3>
<p>EPD追跡RFCのタスクの一つ。エンコーダキャッシュの明示的解放メカニズムが未実装で、リソースリークの原因となりうる。fake0fan の fork で作業中。</p>
<h2 id="4-タイムライン"><a class="header" href="#4-タイムライン">4. タイムライン</a></h2>
<pre class="mermaid">gantt
    title ECConnector 開発タイムライン
    dateFormat YYYY-MM-DD
    section 基盤
    初期EPD PR (#21740)          :done, 2025-09-01, 2025-09-19
    EPD基盤 (#25233)             :done, 2025-09-19, 2025-11-12
    section 機能拡張
    NIXL方式提案 (#26009)         :done, 2025-10-01, 2026-02-07
    Encoder-only (#30242)        :done, 2025-12-01, 2025-12-22
    キャッシュチェック最適化 (#32585) :done, 2026-01-15, 2026-01-22
    ec_both ロール (#34182)      :done, 2026-02-03, 2026-02-10
    section 進行中
    EPD追跡RFC (#32659)          :active, 2026-01-20, 2026-03-01
    SHMConnector (#33714)        :active, 2026-02-03, 2026-03-01
    MM前処理重複排除 (#27094)     :active, 2025-10-17, 2026-03-01
</pre>

<h2 id="5-未解決の課題一覧"><a class="header" href="#5-未解決の課題一覧">5. 未解決の課題一覧</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>課題</th><th>Issue/PR</th><th>状態</th><th>備考</th></tr>
</thead>
<tbody>
<tr><td>高性能転送バックエンド</td><td><a href="https://github.com/vllm-project/vllm/pull/33714">#33714</a></td><td>方向性議論中</td><td>Mooncake統一案が有力</td></tr>
<tr><td>ECキャッシュ解放</td><td><a href="https://github.com/vllm-project/vllm/issues/32659">#32659</a></td><td>作業中</td><td>fake0fan fork</td></tr>
<tr><td>EPDプロキシ最適化</td><td>#31017</td><td>未着手</td><td>—</td></tr>
<tr><td>MM前処理重複排除</td><td><a href="https://github.com/vllm-project/vllm/issues/27094">#27094</a></td><td>RFC</td><td>前処理の50%コスト</td></tr>
<tr><td>エンコーダキャッシュ事前割り当て</td><td>#33714 コメント</td><td>提案段階</td><td>dict→固定バッファ</td></tr>
<tr><td>NIXL ECConnector</td><td>#32659 タスク</td><td>fork作業中</td><td>—</td></tr>
</tbody>
</table>
</div>
<h2 id="6-主要コントリビューター"><a class="header" href="#6-主要コントリビューター">6. 主要コントリビューター</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>名前</th><th>GitHub</th><th>役割</th></tr>
</thead>
<tbody>
<tr><td>Chenguang Zheng</td><td>fake0fan</td><td>ECConnector基盤設計者、EPDフォローアップ主導</td></tr>
<tr><td>NickLucche</td><td>NickLucche</td><td>vLLM Collaborator、アーキテクチャ方針決定者</td></tr>
<tr><td>Cyrus Leung</td><td>DarkLight1337</td><td>vLLM Member、マルチモーダルレビューアー</td></tr>
<tr><td>Qi Wang</td><td>furionw (NVIDIA)</td><td>ec_both ロール貢献</td></tr>
<tr><td>H. Jhoo</td><td>MerHS</td><td>NIXL方式提案者</td></tr>
<tr><td>PiratePai</td><td>PiratePai</td><td>SHMConnector 実装者</td></tr>
</tbody>
</table>
</div>
<h2 id="7-プラグイン開発への示唆-inferred"><a class="header" href="#7-プラグイン開発への示唆-inferred">7. プラグイン開発への示唆 [INFERRED]</a></h2>
<p>独自ECConnectorプラグインを開発する場合の留意点:</p>
<ol>
<li><strong>現在の安定インタフェース</strong>: <code>ECConnectorBase</code> の6メソッドは安定しているが、今後のリファクタリング（事前割り当て方式への移行）で変更の可能性あり</li>
<li><strong>OOTプラグイン</strong>: <code>ec_connector_module_path</code> で外部モジュールロード可能。<code>ECConnectorFactory</code> 経由で登録</li>
<li><strong>転送方式の選択</strong>: Mooncake方式が統一バックエンドとして推奨される方向。独自実装よりMooncakeの上に構築する方が将来的に有利</li>
<li><strong>エンコーダキャッシュ管理の変更予定</strong>: dict ベースから事前割り当て型への移行が検討中。プラグインはこの変更に備えるべき</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="encodercache-永続化と階層キャッシュ-調査報告"><a class="header" href="#encodercache-永続化と階層キャッシュ-調査報告">EncoderCache 永続化と階層キャッシュ: 調査報告</a></h1>
<blockquote>
<p><strong>ステータス</strong>: 調査完了 — ECConnector 既存インフラの発見により設計方針が確定
<strong>作成日</strong>: 2026-02-14
<strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>関連ドキュメント</strong>: <a href="#gemma3-ビジョンパイプライン-キャッシュ機構-medium-verified">Gemma3 ビジョンパイプライン: キャッシュ機構</a>、<a href="#バックエンド-マルチモーダル処理パス-medium-verified">マルチモーダル バックエンド MM 処理</a></p>
</blockquote>
<hr>
<h2 id="1-背景と動機"><a class="header" href="#1-背景と動機">1. 背景と動機</a></h2>
<h3 id="現状の-encodercache"><a class="header" href="#現状の-encodercache">現状の EncoderCache</a></h3>
<p>vLLM の EncoderCache は、ビジョンエンコーダ（例: SiglipVisionModel + Projector）の GPU 上の出力テンソルをキャッシュする。Gemma3 27B の場合、出力形状は <code>(N×256, 5376)</code> で、1 画像あたり約 2.6 MB（FP16）。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>項目</th><th>現状</th></tr>
</thead>
<tbody>
<tr><td>格納先</td><td>GPU メモリ（<code>gpu_model_runner.encoder_cache</code>）</td></tr>
<tr><td>キャッシュキー</td><td><code>mm_hash</code> or <code>{lora_name}:{mm_hash}</code></td></tr>
<tr><td>Eviction 方式</td><td><strong>FIFO</strong>（<code>OrderedDict.popitem(last=False)</code>）</td></tr>
<tr><td>容量設定</td><td><code>encoder_cache_size</code>（エンベディング数単位）</td></tr>
<tr><td>永続性</td><td>なし（プロセス終了で消失）</td></tr>
<tr><td>管理</td><td><code>EncoderCacheManager</code>（CPU 側論理管理）+ <code>encoder_cache</code> dict（GPU 側物理格納）</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py</code> (EncoderCacheManager)、<code>target/vllm/vllm/v1/worker/gpu_model_runner.py:439</code> (encoder_cache dict)</p>
<h3 id="rag-ユースケースにおける課題"><a class="header" href="#rag-ユースケースにおける課題">RAG ユースケースにおける課題</a></h3>
<p>RAG では同一ドキュメント画像が異なるクエリから繰り返し参照される。</p>
<ol>
<li><strong>FIFO Eviction との相性の悪さ</strong>: 高頻度アクセス画像でも新しいエントリが来れば古い順に追い出される</li>
<li><strong>GPU メモリの有限性</strong>: <code>encoder_cache_size</code> を大きくしてもデコーダ KV Cache と競合</li>
<li><strong>再起動耐性がない</strong>: vLLM プロセス再起動で全キャッシュが消失</li>
</ol>
<h3 id="エンコーダ処理のコスト"><a class="header" href="#エンコーダ処理のコスト">エンコーダ処理のコスト</a></h3>
<p>EncoderCache がスキップする GPU 上の処理:</p>
<ul>
<li>SiglipVisionModel: Conv2d + position_embedding + <strong>27 層 Transformer Encoder</strong>（双方向 Attention）+ post_layernorm</li>
<li>Gemma3MultiModalProjector: AvgPool2d + GemmaRMSNorm + Linear(1152→5376)</li>
<li>split + flatten</li>
</ul>
<p>RAG コーパスが数千〜数万画像規模の場合、毎回の再計算コストは無視できない。</p>
<hr>
<h2 id="2-重要な発見-ecconnector-既存インフラ"><a class="header" href="#2-重要な発見-ecconnector-既存インフラ">2. 重要な発見: ECConnector 既存インフラ</a></h2>
<h3 id="kv-transfer-ではなく-ecconnector-が正解"><a class="header" href="#kv-transfer-ではなく-ecconnector-が正解">KV Transfer ではなく ECConnector が正解</a></h3>
<p>当初の仮説では「KV Transfer の枠組み（LMCache 等）を活用」する方針だったが、調査の結果、<strong>エンコーダキャッシュの外部ストレージ永続化のために設計された専用インフラ「ECConnector」が既に存在</strong>することが判明した。</p>
<h3 id="ecconnector-の全体像"><a class="header" href="#ecconnector-の全体像">ECConnector の全体像</a></h3>
<pre><code>target/vllm/vllm/distributed/ec_transfer/
  __init__.py                      -- get_ec_transfer(), has_ec_transfer()
  ec_transfer_state.py             -- グローバルシングルトン管理
  ec_connector/
    __init__.py
    base.py                        -- ECConnectorBase (抽象基底クラス)
    factory.py                     -- ECConnectorFactory (プラグイン登録)
    example_connector.py           -- ECExampleConnector (参照実装, 199行)
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_connector/base.py:59</code> (ECConnectorBase)</p>
<h3 id="ecconnectorbase-の抽象メソッド-verified"><a class="header" href="#ecconnectorbase-の抽象メソッド-verified">ECConnectorBase の抽象メソッド [VERIFIED]</a></h3>
<p><code>ECConnectorBase</code> は <code>KVConnectorBase_V1</code> とは<strong>完全に独立</strong>した抽象基底クラス。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>分類</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>start_load_caches(encoder_cache, **kwargs)</code></td><td>Worker</td><td>外部ストレージから <code>encoder_cache</code> dict にテンソルをロード</td></tr>
<tr><td><code>save_caches(encoder_cache, mm_hash, **kwargs)</code></td><td>Worker</td><td><code>encoder_cache</code> から外部ストレージにテンソルを保存</td></tr>
<tr><td><code>has_cache_item(identifier)</code></td><td>Scheduler</td><td>外部ストレージにキャッシュが存在するか確認</td></tr>
<tr><td><code>update_state_after_alloc(request, index)</code></td><td>Scheduler</td><td>アロケーション後の内部状態更新</td></tr>
<tr><td><code>build_connector_meta(scheduler_output)</code></td><td>Scheduler</td><td>Scheduler → Worker 間のメタデータ構築</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_connector/base.py:126-224</code></p>
<h3 id="ecconnector-の統合ポイント-verified"><a class="header" href="#ecconnector-の統合ポイント-verified">ECConnector の統合ポイント [VERIFIED]</a></h3>
<p>既にGPUModelRunnerとSchedulerに統合済み:</p>
<p><strong>Scheduler 側</strong> (<code>target/vllm/vllm/v1/core/sched/scheduler.py:1197-1203</code>):</p>
<pre><code class="language-python">if self.ec_connector is not None and self.ec_connector.has_cache_item(
    item_identifier
):
    mm_hashes_to_schedule.add(item_identifier)
    external_load_encoder_input.append(i)
    num_embeds_to_schedule += num_encoder_embeds
    continue
</code></pre>
<p>→ ECConnector にキャッシュが存在する場合、エンコーダ計算バジェットを消費せず、ロード予約のみ行う。</p>
<p><strong>Worker 側</strong> (<code>target/vllm/vllm/v1/worker/gpu_model_runner.py:2444-2445</code>):</p>
<pre><code class="language-python">self.encoder_cache[mm_hash] = output
self.maybe_save_ec_to_connector(self.encoder_cache, mm_hash)
</code></pre>
<p>→ エンコーダ実行後、結果を GPU dict に格納すると同時に ECConnector にも保存。</p>
<p><strong>Worker 側コンテキストマネージャ</strong> (<code>target/vllm/vllm/v1/worker/ec_connector_model_runner_mixin.py:62-85</code>):</p>
<pre><code class="language-python">ec_connector.bind_connector_metadata(scheduler_output.ec_connector_metadata)
if not ec_connector.is_producer:
    ec_connector.start_load_caches(encoder_cache, **kwargs)
try:
    yield output   # _execute_mm_encoder() + _gather_mm_embeddings() が実行される
finally:
    output.finished_sending, output.finished_recving = (
        ec_connector.get_finished(scheduler_output.finished_req_ids)
    )
    ec_connector.clear_connector_metadata()
</code></pre>
<p>→ <code>start_load_caches()</code> でストレージからロード → エンコーダ実行（ロード済みはスキップ）→ 完了通知。</p>
<h3 id="ectransferconfig-verified"><a class="header" href="#ectransferconfig-verified">ECTransferConfig [VERIFIED]</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/config/ec_transfer.py</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>ec_connector</code></td><td><code>str | None</code></td><td>None</td><td>コネクタ名（例: <code>"ECExampleConnector"</code>）</td></tr>
<tr><td><code>ec_role</code></td><td><code>ECRole | None</code></td><td>None</td><td><code>"ec_producer"</code> or <code>"ec_consumer"</code></td></tr>
<tr><td><code>ec_connector_extra_config</code></td><td><code>dict</code></td><td><code>{}</code></td><td>コネクタ固有の追加設定</td></tr>
<tr><td><code>ec_connector_module_path</code></td><td><code>str | None</code></td><td>None</td><td>動的ロード用モジュールパス</td></tr>
<tr><td><code>engine_id</code></td><td><code>str | None</code></td><td>uuid4 自動生成</td><td>エンジンID</td></tr>
</tbody>
</table>
</div>
<p>起動時パラメータ例: <code>--ec-connector ECExampleConnector --ec-role ec_producer --ec-connector-extra-config '{"shared_storage_path": "/mnt/cache"}'</code></p>
<h3 id="ecconnectorfactory-verified"><a class="header" href="#ecconnectorfactory-verified">ECConnectorFactory [VERIFIED]</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_connector/factory.py</code></p>
<ul>
<li><code>register_connector(name, module_path, class_name)</code> で遅延ロード登録</li>
<li><code>ec_connector_module_path</code> による動的ロード（外部モジュール対応）</li>
<li>現在の登録済みコネクタ: <code>ECExampleConnector</code> のみ</li>
</ul>
<hr>
<h2 id="3-ecexampleconnector-参照実装の分析-verified"><a class="header" href="#3-ecexampleconnector-参照実装の分析-verified">3. ECExampleConnector 参照実装の分析 [VERIFIED]</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/ec_transfer/ec_connector/example_connector.py</code></p>
<h3 id="概要-21"><a class="header" href="#概要-21">概要</a></h3>
<p>safetensors を使ってディスクにエンコーダ出力テンソルを保存/読込するデバッグ用実装。全 199 行。</p>
<h3 id="ストレージ構造-1"><a class="header" href="#ストレージ構造-1">ストレージ構造</a></h3>
<pre><code>{shared_storage_path}/
  {mm_hash}/
    encoder_cache.safetensors
</code></pre>
<h3 id="保存-save_caches-l98-118"><a class="header" href="#保存-save_caches-l98-118">保存 (save_caches, L98-118)</a></h3>
<pre><code class="language-python">def save_caches(self, encoder_cache, mm_hash, **kwargs) -&gt; None:
    if not self.is_producer:
        return
    filename = self._generate_filename_debug(mm_hash)
    ec_cache = encoder_cache[mm_hash]
    tensors = {"ec_cache": ec_cache.detach().cpu()}  # GPU→CPU コピー
    safetensors.torch.save_file(tensors, filename)
</code></pre>
<ul>
<li>Producer ロールの場合のみ保存</li>
<li><code>detach().cpu()</code> で GPU テンソルを CPU に移動してからシリアライズ</li>
<li><strong>テンソル形状に一切依存しない</strong></li>
</ul>
<h3 id="ロード-start_load_caches-l63-96"><a class="header" href="#ロード-start_load_caches-l63-96">ロード (start_load_caches, L63-96)</a></h3>
<pre><code class="language-python">def start_load_caches(self, encoder_cache, **kwargs) -&gt; None:
    metadata = self._get_connector_metadata()
    for mm_data in metadata.mm_datas:
        if mm_data.mm_hash in encoder_cache:
            continue  # 既に GPU dict にあればスキップ
        filename = self._generate_filename_debug(mm_data.mm_hash)
        ec_cache = safetensors.torch.load_file(filename, device=...)["ec_cache"]
        encoder_cache[mm_data.mm_hash] = ec_cache  # dict に直接格納
</code></pre>
<ul>
<li>メタデータ（Scheduler が構築）に基づいてロード対象を決定</li>
<li><code>encoder_cache</code> dict に直接格納 → <code>_gather_mm_embeddings()</code> でそのまま読める</li>
</ul>
<h3 id="存在チェック-has_cache_item-l120-133"><a class="header" href="#存在チェック-has_cache_item-l120-133">存在チェック (has_cache_item, L120-133)</a></h3>
<pre><code class="language-python">def has_cache_item(self, identifier: str) -&gt; bool:
    return self._found_match_for_mm_data(identifier)
    # → os.path.exists(filename)
</code></pre>
<ul>
<li>ファイルの存在確認のみ（同期的）</li>
</ul>
<h3 id="メタデータ管理"><a class="header" href="#メタデータ管理">メタデータ管理</a></h3>
<p><code>ECExampleConnectorMetadata</code> (L35-42): ロードすべき <code>mm_hash</code> と <code>num_token</code> のリスト。</p>
<p><code>update_state_after_alloc()</code> (L135-146): Scheduler が allocate() 後に呼び出し、<code>_mm_datas_need_loads</code> にロード対象を追加。</p>
<p><code>build_connector_meta()</code> (L148-164): <code>_mm_datas_need_loads</code> からメタデータを構築し、Worker に伝達。呼び出し後にクリア。</p>
<hr>
<h2 id="4-kv-transfer-との比較-verified"><a class="header" href="#4-kv-transfer-との比較-verified">4. KV Transfer との比較 [VERIFIED]</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>評価項目</th><th>KV Transfer</th><th>ECConnector</th></tr>
</thead>
<tbody>
<tr><td><strong>設計目的</strong></td><td>デコーダ KV Cache の転送・永続化</td><td>エンコーダ出力テンソルの転送・永続化</td></tr>
<tr><td><strong>テンソル粒度</strong></td><td>レイヤー別、ブロック単位、トークン粒度</td><td><code>mm_hash</code> 単位、任意形状テンソル</td></tr>
<tr><td><strong>テンソル形状依存</strong></td><td>あり (<code>num_layer, 2, chunk_size, num_kv_heads, head_size</code>)</td><td><strong>なし</strong></td></tr>
<tr><td><strong>エンコーダ出力への適合性</strong></td><td>不適合</td><td><strong>最適</strong></td></tr>
<tr><td><strong>既存統合ポイント</strong></td><td>Attention 層デコレータ経由</td><td>GPUModelRunner の <code>_execute_mm_encoder</code> 直後</td></tr>
<tr><td><strong>新規実装量</strong></td><td>大（7 abstract メソッド + KV 概念適合）</td><td><strong>小</strong>（5 abstract メソッド、参照実装 199 行）</td></tr>
<tr><td><strong>ストレージ実装</strong></td><td>LMCache/NIXL/Mooncake（全て KV 前提）</td><td>Example（safetensors/ディスク）、拡張容易</td></tr>
</tbody>
</table>
</div>
<p><strong>結論</strong>: エンコーダ出力テンソルの永続化には ECConnector を使うべき。KV Transfer はデコーダ KV Cache に特化しており、エンコーダ出力の形状・粒度に合わない。</p>
<p>LMCache の KV 形状ハードコード箇所:</p>
<ul>
<li><code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py:477</code>
<pre><code class="language-python">kv_shape = (num_layer, 1 if use_mla else 2, chunk_size, num_kv_heads, head_size)
</code></pre>
</li>
</ul>
<hr>
<h2 id="5-fifo--lru-変更の具体的設計"><a class="header" href="#5-fifo--lru-変更の具体的設計">5. FIFO → LRU 変更の具体的設計</a></h2>
<h3 id="現状の-fifo-実装-verified"><a class="header" href="#現状の-fifo-実装-verified">現状の FIFO 実装 [VERIFIED]</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py</code></p>
<p><strong>データ構造</strong>:</p>
<pre><code class="language-python"># L72-77
self.cached: dict[str, set[str]] = {}           # mm_hash → {request_ids}
self.freeable: OrderedDict[str, int] = OrderedDict()  # mm_hash → num_embeds (挿入順)
self.freed: list[str] = []                       # evict 済みリスト
</code></pre>
<p><strong>FIFO の核心</strong> (L173-177):</p>
<pre><code class="language-python">while num_embeds &gt; self.num_free_slots:
    mm_hash, num_free_embeds = self.freeable.popitem(last=False)  # 最も古いエントリから
    del self.cached[mm_hash]
    self.freed.append(mm_hash)
    self.num_free_slots += num_free_embeds
</code></pre>
<p><code>OrderedDict.popitem(last=False)</code> で<strong>最初に挿入された（＝最も早く参照解放された）エントリ</strong>から Evict。</p>
<h3 id="現状の-fifo-が-lru-と異なる点"><a class="header" href="#現状の-fifo-が-lru-と異なる点">現状の FIFO が LRU と異なる点</a></h3>
<p>FIFO は「最も早く <code>freeable</code> に追加されたものから Evict」する。LRU は「最も長期間アクセスされていないものから Evict」する。</p>
<p>差が出るケース:</p>
<ol>
<li>画像 A が freeable に入る（参照解放）</li>
<li>画像 B が freeable に入る</li>
<li>画像 A が再度参照される → freeable から取り出されて active に戻る</li>
<li>画像 A が再度 freeable に入る → <strong>FIFO: 末尾に追加（最新扱い）、LRU: 末尾に追加（最新扱い）</strong></li>
</ol>
<p>実は、<strong>現状の FIFO は「参照解放順」であり、再参照された画像は freeable の末尾に再挿入される</strong>ため、RAG での繰り返しアクセスパターンでは擬似 LRU として機能する部分もある。</p>
<p>しかし、<strong>active 状態（参照中）のエントリ間でのアクセス頻度は考慮されない</strong>。複数リクエストが同時に異なる画像を参照し、それらが一斉に freeable になった場合、「最後にアクセスされた時刻」ではなく「最後に参照解放された時刻」で順序が決まる。</p>
<h3 id="lru-への変更方法"><a class="header" href="#lru-への変更方法">LRU への変更方法</a></h3>
<p><strong>変更箇所</strong>: <code>encoder_cache_manager.py</code> の 1 ファイルのみ。Scheduler 側・GPUModelRunner 側の変更は不要。</p>
<p><strong>方法 A: アクセス時刻の追跡</strong>（推奨）</p>
<pre><code class="language-python">class EncoderCacheManager:
    def __init__(self, cache_size: int):
        # ... 既存フィールド ...
        self._access_order: dict[str, int] = {}  # mm_hash → monotonic counter
        self._access_counter: int = 0

    def check_and_update_cache(self, request, input_id) -&gt; bool:
        mm_hash = request.mm_features[input_id].identifier
        if mm_hash not in self.cached:
            return False
        if not self.cached[mm_hash]:
            num_encoder_embeds = self.freeable.pop(mm_hash)
            self.num_freeable_slots -= num_encoder_embeds
        self.cached[mm_hash].add(request.request_id)
        # ★ アクセス時刻を更新
        self._access_counter += 1
        self._access_order[mm_hash] = self._access_counter
        return True

    def free_encoder_input(self, request, input_id) -&gt; None:
        req_id = request.request_id
        mm_hash = request.mm_features[input_id].identifier
        if not self.cached.get(mm_hash, None):
            return
        self.cached[mm_hash].discard(req_id)
        if not self.cached[mm_hash]:
            num_encoder_embeds = request.get_num_encoder_embeds(input_id)
            self.freeable[mm_hash] = num_encoder_embeds
            self.num_freeable_slots += num_encoder_embeds
            # ★ アクセス時刻でソートされた位置に挿入
            # OrderedDictを再ソート: 古いアクセスが先頭に来るようにする
            self.freeable.move_to_end(mm_hash)  # 末尾に追加（最新アクセス）

    def allocate(self, request, input_id) -&gt; None:
        mm_hash = request.mm_features[input_id].identifier
        # ... 既存ロジック ...
        # ★ アクセス時刻を記録
        self._access_counter += 1
        self._access_order[mm_hash] = self._access_counter
</code></pre>
<p><strong>方法 B: 簡易 LRU（move_to_end のみ）</strong></p>
<p>現状の実装でも、<code>freeable</code> への再挿入は末尾に行われるため、ほぼ LRU として機能する。唯一の改善点は、<code>check_and_update_cache()</code> で freeable から復活する際のタイムスタンプ更新のみ。実質的に方法 A と同等の効果が得られる。</p>
<h3 id="変更の影響範囲"><a class="header" href="#変更の影響範囲">変更の影響範囲</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コンポーネント</th><th>変更</th></tr>
</thead>
<tbody>
<tr><td><code>encoder_cache_manager.py</code></td><td><code>check_and_update_cache()</code> と <code>free_encoder_input()</code> の 2 メソッド修正</td></tr>
<tr><td><code>can_allocate()</code></td><td>変更不要（<code>popitem(last=False)</code> は同じ）</td></tr>
<tr><td>Scheduler</td><td>変更不要（API は同じ）</td></tr>
<tr><td>GPUModelRunner</td><td>変更不要</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="6-階層キャッシュの実装設計"><a class="header" href="#6-階層キャッシュの実装設計">6. 階層キャッシュの実装設計</a></h2>
<h3 id="アーキテクチャ-10"><a class="header" href="#アーキテクチャ-10">アーキテクチャ</a></h3>
<pre><code>リクエスト到着
    │
    ▼
Scheduler: _try_schedule_encoder_inputs()
    │
    ├── check_and_update_cache() → L1 HIT (GPU dict) → スキップ
    │
    ├── L1 MISS → ec_connector.has_cache_item() → L2 HIT (Storage)
    │       │
    │       └── external_load_encoder_input に追加 → Worker でロード予約
    │
    └── L1/L2 MISS → encoder_inputs_to_schedule に追加 → エンコーダ計算
    │
    ▼
Worker: execute_model()
    │
    ├── start_load_caches() → L2 からテンソルを GPU dict にロード
    │
    ├── _execute_mm_encoder() → L1/L2 MISS 分のみエンコーダ実行
    │       └── save_caches() → 新規計算結果を L2 に保存
    │
    └── _gather_mm_embeddings() → GPU dict からテンソル取得
</code></pre>
<h3 id="2-層キャッシュの役割分担"><a class="header" href="#2-層キャッシュの役割分担">2 層キャッシュの役割分担</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th></th><th>L1: GPU dict（ホット）</th><th>L2: ECConnector（コールド）</th></tr>
</thead>
<tbody>
<tr><td>格納先</td><td>GPU メモリ</td><td>Redis / ディスク / NFS 等</td></tr>
<tr><td>容量</td><td>小（<code>encoder_cache_size</code>）</td><td>大（コーパス全体）</td></tr>
<tr><td>レイテンシ</td><td>ナノ秒</td><td>マイクロ〜ミリ秒</td></tr>
<tr><td>Eviction</td><td><strong>LRU</strong>（提案変更後）</td><td>TTL or LRU or なし</td></tr>
<tr><td>永続性</td><td>なし</td><td>あり</td></tr>
<tr><td>管理</td><td><code>EncoderCacheManager</code></td><td>カスタム <code>ECConnectorBase</code> 実装</td></tr>
</tbody>
</table>
</div>
<h3 id="カスタム-ecconnector-の実装ガイド"><a class="header" href="#カスタム-ecconnector-の実装ガイド">カスタム ECConnector の実装ガイド</a></h3>
<p>新しい ECConnector を作成するには、<code>ECConnectorBase</code> を継承して 5 つの abstract メソッドを実装する。</p>
<pre><code class="language-python"># my_ec_connector.py
from vllm.distributed.ec_transfer.ec_connector.base import (
    ECConnectorBase, ECConnectorMetadata, ECConnectorRole
)

class RedisECConnector(ECConnectorBase):
    def __init__(self, vllm_config, role):
        super().__init__(vllm_config=vllm_config, role=role)
        self._redis_url = vllm_config.ec_transfer_config.get_from_extra_config(
            "redis_url", "redis://localhost:6379"
        )
        # Redis クライアント初期化...

    # Worker側: ストレージからGPU dictにロード
    def start_load_caches(self, encoder_cache, **kwargs):
        metadata = self._get_connector_metadata()
        for mm_data in metadata.mm_datas:
            if mm_data.mm_hash in encoder_cache:
                continue
            tensor_bytes = self._redis.get(mm_data.mm_hash)
            if tensor_bytes:
                encoder_cache[mm_data.mm_hash] = deserialize(tensor_bytes)

    # Worker側: GPU dictからストレージに保存
    def save_caches(self, encoder_cache, mm_hash, **kwargs):
        if not self.is_producer:
            return
        tensor = encoder_cache[mm_hash].detach().cpu()
        self._redis.set(mm_hash, serialize(tensor))

    # Scheduler側: ストレージにキャッシュが存在するか
    def has_cache_item(self, identifier):
        return self._redis.exists(identifier)

    # Scheduler側: アロケーション後の状態更新
    def update_state_after_alloc(self, request, index):
        mm_hash = request.mm_features[index].identifier
        num_token = request.get_num_encoder_embeds(index)
        self._need_loads[mm_hash] = num_token

    # Scheduler側: メタデータ構築
    def build_connector_meta(self, scheduler_output):
        meta = MyECConnectorMetadata()
        for mm_hash, num_token in self._need_loads.items():
            meta.add(mm_hash, num_token)
        self._need_loads.clear()
        return meta
</code></pre>
<p><strong>登録方法</strong>:</p>
<ol>
<li>ファクトリ登録: <code>ECConnectorFactory.register_connector("RedisECConnector", "my.module", "RedisECConnector")</code></li>
<li>または動的ロード: <code>--ec-connector RedisECConnector --ec-connector-module-path my.module</code></li>
</ol>
<h3 id="テンソルサイズの見積もり"><a class="header" href="#テンソルサイズの見積もり">テンソルサイズの見積もり</a></h3>
<p>Gemma3 27B、1 画像あたり:</p>
<pre><code>256 tokens × 5376 dim × 2 bytes (FP16) = 2,752,512 bytes ≈ 2.6 MB/画像
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コーパス規模</th><th>L2 ストレージ必要量（FP16）</th></tr>
</thead>
<tbody>
<tr><td>1,000 画像</td><td>≈ 2.6 GB</td></tr>
<tr><td>10,000 画像</td><td>≈ 26 GB</td></tr>
<tr><td>100,000 画像</td><td>≈ 260 GB</td></tr>
</tbody>
</table>
</div>
<h3 id="プリコンピュート運用"><a class="header" href="#プリコンピュート運用">プリコンピュート運用</a></h3>
<p>ECConnector を活用したオフラインプリコンピュートの流れ:</p>
<ol>
<li><strong>Producer モード</strong>で vLLM を起動し、コーパス全画像を含むダミーリクエストを送信</li>
<li><code>save_caches()</code> でエンコーダ出力がストレージに蓄積される</li>
<li><strong>Consumer モード</strong>で本番 vLLM を起動</li>
<li>リクエスト到着時に <code>has_cache_item()</code> → <code>start_load_caches()</code> でストレージからロード</li>
<li>エンコーダ計算をスキップし、ストレージからの読み出し + GPU 転送のみで処理</li>
</ol>
<hr>
<h2 id="7-残る設計上の考慮事項"><a class="header" href="#7-残る設計上の考慮事項">7. 残る設計上の考慮事項</a></h2>
<h3 id="71-ecexampleconnector-の同期-io"><a class="header" href="#71-ecexampleconnector-の同期-io">7.1 ECExampleConnector の同期 I/O</a></h3>
<p>現在の <code>ECExampleConnector</code> の <code>start_load_caches()</code> は同期的な <code>safetensors.torch.load_file()</code> を呼ぶ。ディスク I/O がブロッキングとなり、エンコーダ実行前のレイテンシに直接影響する。</p>
<p><strong>対策案</strong>:</p>
<ul>
<li><code>start_load_caches()</code> を非同期化（別スレッドでロード開始、<code>_gather_mm_embeddings()</code> 前に完了待ち）</li>
<li>Redis 等のインメモリストレージを使い、I/O レイテンシを最小化</li>
<li>EngineCore.step() のスケジューリングとモデル実行の間の時間的ギャップを活用</li>
</ul>
<h3 id="72-lru-とストレージ-eviction-の相互作用"><a class="header" href="#72-lru-とストレージ-eviction-の相互作用">7.2 LRU とストレージ Eviction の相互作用</a></h3>
<p>L1（GPU dict）から LRU で Evict されたテンソルは、L2（ストレージ）には残る。次にアクセスされた時:</p>
<ol>
<li>Scheduler: <code>check_and_update_cache()</code> → L1 MISS</li>
<li>Scheduler: <code>ec_connector.has_cache_item()</code> → L2 HIT</li>
<li>Worker: <code>start_load_caches()</code> → L2 から L1 にロード</li>
</ol>
<p>→ エンコーダ再計算は不要だが、ストレージ→GPU 転送のレイテンシが発生する。</p>
<h3 id="73-producerconsumer-ロールの運用"><a class="header" href="#73-producerconsumer-ロールの運用">7.3 Producer/Consumer ロールの運用</a></h3>
<p>ECConnector は P/D 分離を想定した設計。RAG ユースケースでは:</p>
<ul>
<li><strong>ec_producer</strong>: プリコンピュート用インスタンス（エンコーダ出力をストレージに書き込み）</li>
<li><strong>ec_consumer</strong>: 本番サービング用インスタンス（ストレージからロード）</li>
<li>Producer と Consumer で同じストレージパスを共有する必要がある</li>
</ul>
<h3 id="74-キャッシュ無効化"><a class="header" href="#74-キャッシュ無効化">7.4 キャッシュ無効化</a></h3>
<p>モデル重み更新（LoRA ホットスワップ等）時:</p>
<ul>
<li>L1: <code>EncoderCacheManager.reset()</code> + <code>encoder_cache.clear()</code> で対応済み</li>
<li>L2: ストレージ側のキャッシュクリアが必要（<code>identifier</code> に LoRA プレフィックスが含まれるため、LoRA 別に無効化可能）</li>
</ul>
<hr>
<h2 id="8-次のステップ"><a class="header" href="#8-次のステップ">8. 次のステップ</a></h2>
<ol>
<li><strong>FIFO→LRU の実装</strong>: <code>encoder_cache_manager.py</code> の 2 メソッドを修正（変更量: 数行）</li>
<li><strong>カスタム ECConnector の実装</strong>: Redis バックエンドの ECConnector を作成（参照: ECExampleConnector の 199 行）</li>
<li><strong>ベンチマーク</strong>: RAG ワークロードでの比較
<ul>
<li>ベースライン: FIFO + インメモリのみ</li>
<li>改善 1: LRU + インメモリのみ</li>
<li>改善 2: LRU + Redis ECConnector</li>
</ul>
</li>
<li><strong>コミュニティ調査</strong>: vLLM の Issue/PR で ECConnector 関連の議論を確認</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="gemma3-ビジョンパイプライン-キャッシュ機構-medium-verified"><a class="header" href="#gemma3-ビジョンパイプライン-キャッシュ機構-medium-verified">Gemma3 ビジョンパイプライン: キャッシュ機構 [MEDIUM] [VERIFIED]</a></h1>
<blockquote>
<p><strong>最終更新</strong>: 2026-02-11</p>
</blockquote>
<p><a href="#gemma3-27b-ビジョンパイプライン-形状フローと数値まとめ">gemma3-vision-pipeline.md</a> で追跡した Gemma3 27B ビジョンパイプライン上には、3つの独立したキャッシュ層が存在する。各キャッシュは異なるステップの重い処理をスキップし、同一画像の再利用や同一プロンプトの再送時に大幅な計算量削減を実現する。</p>
<blockquote>
<p><strong>関連</strong>: EncoderCache の永続化・階層キャッシュ化については <a href="#encodercache-永続化と階層キャッシュ-調査報告">encoder-cache-persistence.md</a> を参照。</p>
</blockquote>
<hr>
<h2 id="1-パイプラインとキャッシュの位置関係"><a class="header" href="#1-パイプラインとキャッシュの位置関係">1. パイプラインとキャッシュの位置関係</a></h2>
<pre><code>                      Step 1: API Request
                             │
                      Step 2: chat_template 適用
                             │
                ┌────────────┴────────────────┐
                │  Step 3: Gemma3Processor     │
                │  (CPU, P0 フロントエンド)      │
                │                              │
                │  3a. image_processor          │
                │      resize(896×896)          │
                │      rescale(×1/255)          │  ◀── ProcessorCache ヒット時
                │      normalize(0.5, 0.5)      │      Step 3 全体をスキップ
                │      Pan-and-Scan crop        │
                │  3b. num_crops 取得            │
                │  3c. プロンプト書き換え         │
                │  3d. boi→full_image_seq 展開   │
                │  3e. tokenize                 │
                │  3f. token_type_ids 生成       │
                └────────────┬────────────────┘
                             │
                  pixel_values: (N, 3, 896, 896)
                  prompt_token_ids, mm_hashes
                             │
              ═══════════════╪═══════════════ CPU → GPU (ZMQ IPC)
                             │
                ┌────────────┴────────────────┐
                │  Step 4: SiglipVisionModel   │
                │  (GPU, P1 バックエンド)        │
                │  Conv2d → 4096 patches        │
                │  + position_embedding          │  ◀── EncoderCache ヒット時
                │  SiglipEncoder × 27層          │      Step 4+5+6 をスキップ
                │  post_layernorm               │
                ├───────────────────────────────┤
                │  Step 5: Projector            │
                │  AvgPool2d(k=4) → 256 tokens  │
                │  GemmaRMSNorm                  │
                │  Linear(1152→5376)             │
                ├───────────────────────────────┤
                │  Step 6: split + flatten       │
                └────────────┬────────────────┘
                             │
                  encoder output: (N×256, 5376)
                             │
                ┌────────────┴────────────────┐
                │  Step 7: embed_input_ids     │
                │  text_embeds × normalizer     │
                │  masked_scatter_(mm_embeds)    │  ◀── KVプレフィックスキャッシュ ヒット時
                ├───────────────────────────────┤      プレフィックス一致分の
                │  Step 8: Gemma3 Decoder       │      Step 7+8 をスキップ
                │  62層 Transformer              │      (KV再計算不要)
                └──────────────────────────────┘
</code></pre>
<hr>
<h2 id="2-キャッシュ比較テーブル"><a class="header" href="#2-キャッシュ比較テーブル">2. キャッシュ比較テーブル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th></th><th>ProcessorCache</th><th>EncoderCache</th><th>KVプレフィックスキャッシュ</th></tr>
</thead>
<tbody>
<tr><td><strong>場所</strong></td><td>CPU (P0 フロントエンド)</td><td>GPU (P1 バックエンド)</td><td>GPU (P1 バックエンド)</td></tr>
<tr><td><strong>キャッシュキー</strong></td><td>blake3(model_id, 画像ピクセル, processor_kwargs, tokenizer_kwargs)</td><td><code>mm_feature.identifier</code> (= mm_hash or <code>{lora}:{mm_hash}</code>)</td><td>hash(parent_hash, token_ids, extra_keys) — extra_keysにidentifier含む</td></tr>
<tr><td><strong>保存される値</strong></td><td>HF処理済みテンソル (pixel_values, num_patches) + prompt_updates</td><td>エンコーダ出力テンソル (post-Projector, GPU上)</td><td>デコーダ各層のKV状態 (KVCacheブロック)</td></tr>
<tr><td><strong>ヒット時にスキップ</strong></td><td>Step 3全体 (CPU前処理)</td><td>Step 4+5+6 (エンコーダ+プロジェクタ)</td><td>Step 7+8の一部 (プレフィックス分のデコーダ)</td></tr>
<tr><td><strong>Eviction方式</strong></td><td>LRU (サイズベース)</td><td>FIFO (RefCount管理)</td><td>LRU (ブロック単位)</td></tr>
<tr><td><strong>容量設定</strong></td><td><code>mm_processor_cache_gb</code> (default: 4GB)</td><td><code>encoder_cache_size</code> (埋め込み数単位)</td><td>KVCacheの一部 (BlockPool管理)</td></tr>
<tr><td><strong>管理クラス</strong></td><td><code>MultiModalProcessorOnlyCache</code> 等4種</td><td><code>EncoderCacheManager</code> + <code>encoder_cache</code> dict</td><td><code>KVCacheManager</code> (prefix_cache)</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="3-processorcache--cpu側前処理キャッシュ"><a class="header" href="#3-processorcache--cpu側前処理キャッシュ">3. ProcessorCache — CPU側前処理キャッシュ</a></h2>
<h3 id="ハッシュ計算"><a class="header" href="#ハッシュ計算">ハッシュ計算</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/hasher.py:50-162</code>, <code>target/vllm/vllm/multimodal/processing/processor.py:1299-1363</code></p>
<pre><code class="language-python">MultiModalHasher.hash_kwargs(
    model_id=model_id,          # モデル識別子（例: "google/gemma-3-27b-it"）
    image=PIL_Image,            # 画像データ
    **hf_processor_mm_kwargs,   # HF Processorへの追加引数
    **tokenization_kwargs,      # トークナイザ設定
)
</code></pre>
<p>ハッシュに投入されるデータ:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>入力</th><th>シリアライズ方法</th></tr>
</thead>
<tbody>
<tr><td><code>model_id</code> (str)</td><td>UTF-8エンコード</td></tr>
<tr><td><code>image</code> (PIL.Image)</td><td>EXIF ImageID (UUID型) → 16バイト。なければ mode + ピクセルデータ (numpy配列)</td></tr>
<tr><td><code>image</code> (MediaWithBytes)</td><td>EXIF ImageID → 16バイト。なければ original_bytes</td></tr>
<tr><td><code>hf_processor_mm_kwargs</code> (dict)</td><td>キーソート → 再帰的シリアライズ</td></tr>
<tr><td><code>tokenization_kwargs</code> (dict)</td><td>同上</td></tr>
</tbody>
</table>
</div>
<ul>
<li><strong>ハッシュアルゴリズム</strong>: <code>VLLM_MM_HASHER_ALGORITHM</code> 環境変数で設定（<code>blake3</code> デフォルト、<code>sha256</code>/<code>sha512</code> はFIPS準拠用）</li>
<li>キーはアルファベット順にソートされてから逐次ハッシュに投入される（決定的）</li>
</ul>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/hasher.py:154-162</code> (hash_kwargs)</p>
<h3 id="保存される情報"><a class="header" href="#保存される情報">保存される情報</a></h3>
<ul>
<li><strong>テンソルデータ</strong>: <code>pixel_values</code> (形状: <code>(N, 3, 896, 896)</code>), <code>num_patches</code> (形状: <code>(num_images,)</code>)</li>
<li><strong>prompt_updates</strong>: プレースホルダー位置情報、展開パターン</li>
</ul>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/cache.py:326-725</code></p>
<h3 id="cpugpu"><a class="header" href="#cpugpu">CPU/GPU</a></h3>
<p><strong>CPU</strong>。P0フロントエンドプロセスのメモリ上で管理される。</p>
<h3 id="スキップされる処理"><a class="header" href="#スキップされる処理">スキップされる処理</a></h3>
<p><strong>Step 3 全体</strong>（<code>Gemma3Processor.__call__()</code>）:</p>
<ul>
<li>3a: <code>image_processor</code> — resize(896×896), rescale(×1/255), normalize(mean=0.5, std=0.5), Pan-and-Scan時のクロップ生成</li>
<li>3b: <code>num_crops</code> 取得</li>
<li>3c: プロンプト書き換え（Pan-and-Scan時のみ）</li>
<li>3d: <code>boi_token</code> → <code>full_image_sequence</code> 展開</li>
<li>3e: tokenizer による <code>token_ids</code> 変換</li>
<li>3f: <code>token_type_ids</code> 生成</li>
</ul>
<p>さらに、Sender/Shm タイプ使用時は <strong>ZMQ IPC でのテンソルデータ転送もスキップ</strong> される（<code>data=None</code> で送信）。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/processing/processor.py:1513-1596</code> (_cached_apply_hf_processor)</p>
<h3 id="キャッシュフロー詳細"><a class="header" href="#キャッシュフロー詳細">キャッシュフロー詳細</a></h3>
<pre><code>_cached_apply_hf_processor():
  1. _hash_mm_items()          → MultiModalHashes（各画像のblake3ハッシュ）
  2. _get_cache_missing_items() → 各画像がキャッシュにあるか判定
  3. _apply_hf_processor_main() → キャッシュミスの画像だけHF Processor実行
  4. _merge_mm_kwargs()         → キャッシュ済み + 新規処理の結果をマージ
     ※ マージ前に全ハッシュを touch() して LRU Eviction を防止
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/processing/processor.py:1365-1400</code> (_get_cache_missing_items)</p>
<h3 id="4種の実装"><a class="header" href="#4種の実装">4種の実装</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>実装</th><th>用途</th><th>格納先</th><th>ヒット時の動作</th></tr>
</thead>
<tbody>
<tr><td><code>MultiModalProcessorOnlyCache</code></td><td>P0完結（IPC無効時）</td><td>P0メモリ</td><td>テンソル+prompt返却</td></tr>
<tr><td><code>MultiModalProcessorSenderCache</code></td><td>P0→P1（LRUモード）</td><td>P0にメタデータのみ</td><td><code>data=None</code>で送信、IPC転送省略</td></tr>
<tr><td><code>ShmObjectStoreSenderCache</code></td><td>P0→P1（共有メモリ）</td><td>共有メモリ</td><td>共有メモリ参照を返却</td></tr>
<tr><td><code>MultiModalReceiverCache</code></td><td>P1側（LRUモード）</td><td>P1メモリ</td><td>P0と同期したLRUでテンソル取得</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/registry.py:284-320</code> (キャッシュタイプ選択ロジック)</p>
<hr>
<h2 id="4-encodercache--gpuエンコーダ出力キャッシュ"><a class="header" href="#4-encodercache--gpuエンコーダ出力キャッシュ">4. EncoderCache — GPUエンコーダ出力キャッシュ</a></h2>
<h3 id="キャッシュキー"><a class="header" href="#キャッシュキー">キャッシュキー</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/input_processor.py:490-506</code></p>
<pre><code class="language-python">identifier = mm_hash                          # 通常
identifier = f"{lora_name}:{mm_hash}"         # LoRA tower connector有効時
</code></pre>
<p><code>mm_hash</code> は ProcessorCache と同じ blake3 ハッシュ値。LoRA が有効な場合は、同一画像でも LoRA によってエンコーダ出力が変わるため、LoRA名をプレフィックスとして付加する。</p>
<h3 id="保存される情報-1"><a class="header" href="#保存される情報-1">保存される情報</a></h3>
<ul>
<li><strong>GPU上のテンソル</strong>: SiglipVisionModel + Gemma3MultiModalProjector の出力
<ul>
<li>Gemma3の場合: <code>(N×256, 5376)</code> — Projector出力をflattenしたもの</li>
</ul>
</li>
<li><strong>論理管理</strong>: <code>EncoderCacheManager</code> が RefCount + FIFO で管理</li>
<li><strong>物理格納</strong>: <code>gpu_model_runner.encoder_cache: dict[str, torch.Tensor]</code></li>
</ul>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:17-267</code>, <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:439</code></p>
<h3 id="cpugpu-1"><a class="header" href="#cpugpu-1">CPU/GPU</a></h3>
<p><strong>GPU</strong>。エンコーダ出力テンソルはGPUメモリ上に保持される。論理管理（RefCount、Eviction判定）はCPU上の <code>EncoderCacheManager</code> が行う。</p>
<h3 id="スキップされる処理-1"><a class="header" href="#スキップされる処理-1">スキップされる処理</a></h3>
<p><strong>Step 4 + Step 5 + Step 6</strong>:</p>
<ul>
<li>Step 4: <strong>SiglipVisionModel forward</strong> — Conv2d(3→1152) + position_embedding + 27層 Transformer Encoder + post_layernorm</li>
<li>Step 5: <strong>Gemma3MultiModalProjector forward</strong> — reshape + AvgPool2d(k=4, s=4) + GemmaRMSNorm + Linear(1152→5376)</li>
<li>Step 6: <strong>split + flatten</strong> — num_patchesに基づく分割と結合</li>
</ul>
<p>これらはGPU上で最も計算量の大きいビジョン処理であり、特に SiglipEncoder の 27層の双方向 Attention が支配的。</p>
<h3 id="scheduler連携"><a class="header" href="#scheduler連携">Scheduler連携</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:1060-1215</code></p>
<pre><code>Scheduler._get_encoder_budget():
  1. 各 mm_feature について:
  2. encoder_cache_manager.check_and_update_cache(req, i) を呼ぶ
     → True: scheduled_encoder_inputs に含めない（スキップ）
     → False: can_allocate() → allocate() → scheduled_encoder_inputs に追加
  3. SchedulerOutput.scheduled_encoder_inputs = {req_id: [input_ids]}
</code></pre>
<p>GPUModelRunner 側:</p>
<pre><code>_execute_mm_encoder():
  → scheduled_encoder_inputs にあるもののみ model.embed_multimodal() 実行
  → 出力を encoder_cache[mm_hash] に格納

_gather_mm_embeddings():
  → 全ての mm_feature について encoder_cache[mm_hash] からスライス取得
  → キャッシュヒットしたものも、ミスして今回計算したものも、同じキャッシュから取得
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:2293-2447</code> (_execute_mm_encoder), <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:2449-2527</code> (_gather_mm_embeddings)</p>
<hr>
<h2 id="5-kvプレフィックスキャッシュ--デコーダkv状態キャッシュ"><a class="header" href="#5-kvプレフィックスキャッシュ--デコーダkv状態キャッシュ">5. KVプレフィックスキャッシュ — デコーダKV状態キャッシュ</a></h2>
<h3 id="ブロックハッシュ計算"><a class="header" href="#ブロックハッシュ計算">ブロックハッシュ計算</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:525-552</code></p>
<pre><code class="language-python">BlockHash(
    hash_function((parent_block_hash, curr_block_token_ids_tuple, extra_keys))
)
</code></pre>
<p><code>extra_keys</code> は以下の要素の結合:</p>
<pre><code class="language-python">extra_keys = lora_extra_keys + mm_extra_keys + cache_salt_keys + prompt_embeds_keys
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:487-522</code> (generate_block_hash_extra_keys)</p>
<h4 id="mm-extra-keys-の生成"><a class="header" href="#mm-extra-keys-の生成">MM extra keys の生成</a></h4>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:387-448</code></p>
<p>MMトークン（<code>&lt;image&gt;</code> token_id=262144）を含むブロックでは、そのブロックに重なる <code>mm_feature.identifier</code> が <code>extra_keys</code> に追加される。</p>
<pre><code>ブロック [start_token_idx, end_token_idx) が
mm_feature の [offset, offset+length) と重なる場合:
  → extra_keys.append(mm_feature.identifier)
</code></pre>
<p>これにより:</p>
<ul>
<li><strong>同一テキスト・異なる画像</strong> → 異なるブロックハッシュ → キャッシュミス</li>
<li><strong>同一テキスト・同一画像</strong> → 同一ブロックハッシュ → キャッシュヒット</li>
</ul>
<h3 id="保存される情報-2"><a class="header" href="#保存される情報-2">保存される情報</a></h3>
<ul>
<li><strong>GPUメモリ上のKVCacheブロック</strong>: デコーダ62層分のKey/Value状態</li>
<li>BlockPool が物理ブロックを管理、prefix_cache がハッシュ→ブロック対応を管理</li>
</ul>
<h3 id="cpugpu-2"><a class="header" href="#cpugpu-2">CPU/GPU</a></h3>
<p><strong>GPU</strong>。KV状態はGPUメモリ上のブロックに格納される。ハッシュ計算とブロック対応管理はCPU上の <code>KVCacheManager</code> が行う。</p>
<h3 id="スキップされる処理-2"><a class="header" href="#スキップされる処理-2">スキップされる処理</a></h3>
<p><strong>Step 7 + Step 8 の一部</strong>（プレフィックスが一致するトークン分）:</p>
<ul>
<li>Step 7: <strong>embed_input_ids</strong> — テキスト埋め込み × normalizer + masked_scatter_(mm_embeds)</li>
<li>Step 8: <strong>Gemma3 Decoder forward</strong> — 62層 Transformer の KV 計算</li>
</ul>
<p>プレフィックスキャッシュがヒットすると <code>num_computed_tokens</code> が増加し、新規に forward pass が必要なトークン数が減少する。例えば 1000 トークンのプロンプトで 800 トークン分のプレフィックスがヒットすれば、残り 200 トークンだけ計算すればよい。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_manager.py:164-204</code> (get_computed_blocks)</p>
<hr>
<h2 id="6-キャッシュの独立性と相互作用"><a class="header" href="#6-キャッシュの独立性と相互作用">6. キャッシュの独立性と相互作用</a></h2>
<p>3つのキャッシュは独立に動作する。各キャッシュのヒット/ミスは他のキャッシュの判定に影響しない。</p>
<h3 id="典型シナリオ"><a class="header" href="#典型シナリオ">典型シナリオ</a></h3>
<h4 id="シナリオ1-初回リクエスト全ミス"><a class="header" href="#シナリオ1-初回リクエスト全ミス">シナリオ1: 初回リクエスト（全ミス）</a></h4>
<pre><code>画像A + "この画像は何？"  →  全ステップ実行
  ProcessorCache: MISS → Step 3 実行、結果をキャッシュ
  EncoderCache:   MISS → Step 4+5+6 実行、結果をキャッシュ
  KV Prefix:      MISS → Step 7+8 全トークン実行、KVブロック格納
</code></pre>
<h4 id="シナリオ2-同一画像同一プロンプト再送全ヒット"><a class="header" href="#シナリオ2-同一画像同一プロンプト再送全ヒット">シナリオ2: 同一画像・同一プロンプト再送（全ヒット）</a></h4>
<pre><code>画像A + "この画像は何？"（2回目）
  ProcessorCache: HIT  → Step 3 スキップ（pixel_values をキャッシュから取得）
  EncoderCache:   HIT  → Step 4+5+6 スキップ（エンコーダ出力をGPUキャッシュから取得）
  KV Prefix:      HIT  → Step 7+8 のプレフィックス分スキップ（KV状態再利用）
</code></pre>
<h4 id="シナリオ3-同一画像異なるプロンプト"><a class="header" href="#シナリオ3-同一画像異なるプロンプト">シナリオ3: 同一画像・異なるプロンプト</a></h4>
<pre><code>画像A + "この画像を要約して"
  ProcessorCache: HIT  → Step 3 スキップ（同一画像なのでハッシュ一致）
  EncoderCache:   HIT  → Step 4+5+6 スキップ（同一 identifier）
  KV Prefix:      部分HIT → 画像トークン部分（ブロック単位）はヒットする可能性あり
                           テキスト部分は異なるためミス
</code></pre>
<h4 id="シナリオ4-異なる画像全ミス"><a class="header" href="#シナリオ4-異なる画像全ミス">シナリオ4: 異なる画像（全ミス）</a></h4>
<pre><code>画像B + "この画像は何？"
  ProcessorCache: MISS → ピクセルデータが異なるためハッシュ不一致
  EncoderCache:   MISS → identifier が異なる
  KV Prefix:      MISS → extra_keys の identifier が異なりブロックハッシュ不一致
</code></pre>
<h3 id="キャッシュ間のキー共有"><a class="header" href="#キャッシュ間のキー共有">キャッシュ間のキー共有</a></h3>
<p>3つのキャッシュは同一の <strong>mm_hash</strong>（blake3ハッシュ）を基盤として共有している:</p>
<pre><code>MultiModalHasher.hash_kwargs(model_id, image, kwargs...)
        │
        ▼
    mm_hash (blake3 hex digest)
        │
        ├──▶ ProcessorCache のキー（そのまま使用）
        │
        ├──▶ EncoderCache のキー（= identifier = mm_hash or lora:mm_hash）
        │
        └──▶ KV Prefix Cache の extra_keys の一部（= identifier）
</code></pre>
<hr>
<h2 id="7-主要ファイル参照"><a class="header" href="#7-主要ファイル参照">7. 主要ファイル参照</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th><th>行</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/multimodal/hasher.py</code></td><td><code>MultiModalHasher</code>, <code>hash_kwargs()</code>, <code>serialize_item()</code></td><td>L50, L154, L52</td></tr>
<tr><td><code>target/vllm/vllm/multimodal/cache.py</code></td><td><code>MultiModalProcessorOnlyCache</code>, <code>SenderCache</code>, <code>ShmCache</code>, <code>ReceiverCache</code></td><td>L326, L379, L437, L614</td></tr>
<tr><td><code>target/vllm/vllm/multimodal/processing/processor.py</code></td><td><code>_cached_apply_hf_processor()</code>, <code>_hash_mm_items()</code>, <code>_get_cache_missing_items()</code></td><td>L1513, L1299, L1365</td></tr>
<tr><td><code>target/vllm/vllm/multimodal/registry.py</code></td><td><code>processor_cache_from_config()</code></td><td>L305</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/input_processor.py</code></td><td><code>_get_mm_identifier()</code></td><td>L490</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/encoder_cache_manager.py</code></td><td><code>EncoderCacheManager</code>, <code>check_and_update_cache()</code></td><td>L17, L91</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_model_runner.py</code></td><td><code>encoder_cache</code>, <code>_execute_mm_encoder()</code>, <code>_gather_mm_embeddings()</code></td><td>L439, L2293, L2449</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/kv_cache_utils.py</code></td><td><code>_gen_mm_extra_hash_keys()</code>, <code>generate_block_hash_extra_keys()</code>, <code>hash_block_tokens()</code></td><td>L387, L487, L525</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/kv_cache_manager.py</code></td><td><code>get_computed_blocks()</code></td><td>L164</td></tr>
<tr><td><code>target/vllm/vllm/v1/core/sched/scheduler.py</code></td><td><code>_get_encoder_budget()</code></td><td>L1060</td></tr>
</tbody>
</table>
</div>
<h2 id="関連ドキュメント-13"><a class="header" href="#関連ドキュメント-13">関連ドキュメント</a></h2>
<ul>
<li><a href="#gemma3-27b-ビジョンパイプライン-形状フローと数値まとめ">Gemma3 ビジョンパイプライン: 形状フローと数値まとめ</a></li>
<li><a href="#フロントエンド-マルチモーダル処理パス-mediumdeep3-verified">フロントエンド MM処理パス</a></li>
<li><a href="#バックエンド-マルチモーダル処理パス-medium-verified">バックエンド MM処理パス</a></li>
<li><a href="#kvcachemanager-サマリー">KVCacheManager</a> — プレフィックスキャッシュの詳細</li>
<li><a href="#プレフィックスキャッシュ詳細">KVCacheManager: プレフィックスキャッシュ</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="gemma3-27b-ビジョンパイプライン-形状フローと数値まとめ"><a class="header" href="#gemma3-27b-ビジョンパイプライン-形状フローと数値まとめ">Gemma3 27B ビジョンパイプライン: 形状フローと数値まとめ</a></h1>
<h2 id="モデルパラメータconfigjson--preprocessor_configjson"><a class="header" href="#モデルパラメータconfigjson--preprocessor_configjson">モデルパラメータ（config.json + preprocessor_config.json）</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>値</th><th>出典</th></tr>
</thead>
<tbody>
<tr><td>image_size</td><td>896</td><td>vision_config</td></tr>
<tr><td>patch_size</td><td>14</td><td>vision_config</td></tr>
<tr><td>vision hidden_size</td><td>1152</td><td>vision_config</td></tr>
<tr><td>vision num_heads</td><td>16</td><td>vision_config</td></tr>
<tr><td>vision num_layers</td><td>27</td><td>vision_config</td></tr>
<tr><td>text hidden_size</td><td>5376</td><td>text_config</td></tr>
<tr><td>text num_heads</td><td>32</td><td>text_config</td></tr>
<tr><td>text num_layers</td><td>62</td><td>text_config</td></tr>
<tr><td>mm_tokens_per_image</td><td>256</td><td>config.json</td></tr>
<tr><td>image_token_index</td><td>262144</td><td>config.json</td></tr>
<tr><td>boi_token_index</td><td>255999</td><td>config.json</td></tr>
<tr><td>eoi_token_index</td><td>256000</td><td>config.json</td></tr>
</tbody>
</table>
</div>
<h3 id="導出値"><a class="header" href="#導出値">導出値</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>導出パラメータ</th><th>計算</th><th>値</th></tr>
</thead>
<tbody>
<tr><td>patches_per_image</td><td>896 / 14</td><td><strong>64</strong></td></tr>
<tr><td>エンコーダ入力パッチ数</td><td>64²</td><td><strong>4096</strong></td></tr>
<tr><td>tokens_per_side</td><td>√256</td><td><strong>16</strong></td></tr>
<tr><td>AvgPool2d kernel_size</td><td>64 / 16</td><td><strong>4</strong></td></tr>
<tr><td>Projector 出力トークン/画像</td><td>16²</td><td><strong>256</strong> (= mm_tokens_per_image ✅)</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="pan-and-scan-設定"><a class="header" href="#pan-and-scan-設定">Pan-and-Scan 設定</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パラメータ</th><th>preprocessor_config.json</th><th>フォールバックデフォルト</th><th>出典</th></tr>
</thead>
<tbody>
<tr><td>do_pan_and_scan</td><td>null</td><td><strong>False</strong></td><td>processing_gemma3.py L44</td></tr>
<tr><td>pan_and_scan_min_crop_size</td><td>null</td><td><strong>256</strong></td><td>processing_gemma3.py L45</td></tr>
<tr><td>pan_and_scan_max_num_crops</td><td>null</td><td><strong>4</strong></td><td>processing_gemma3.py L46</td></tr>
<tr><td>pan_and_scan_min_ratio_to_activate</td><td>null</td><td><strong>1.2</strong></td><td>processing_gemma3.py L47</td></tr>
</tbody>
</table>
</div>
<ul>
<li>Google はモデル配布時にこれらを<strong>すべて null</strong> にしている</li>
<li>デフォルト値は HF transformers の <code>Gemma3ProcessorKwargs._defaults</code> で定義</li>
<li><strong>デフォルトでは Pan-and-Scan は無効</strong></li>
<li>有効化: vLLM では <code>--hf-overrides '{"do_pan_and_scan": true}'</code></li>
</ul>
<hr>
<h2 id="api-リクエストからデコーダ入力までの全体フロー"><a class="header" href="#api-リクエストからデコーダ入力までの全体フロー">API リクエストからデコーダ入力までの全体フロー</a></h2>
<h3 id="step-1-ユーザーの-api-リクエスト"><a class="header" href="#step-1-ユーザーの-api-リクエスト">Step 1: ユーザーの API リクエスト</a></h3>
<pre><code class="language-json">{
  "model": "gemma-3-27b-it",
  "messages": [
    {
      "role": "user",
      "content": [
        {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}},
        {"type": "text", "text": "この文書を要約して"}
      ]
    }
  ]
}
</code></pre>
<p>ユーザーは画像を1枚渡すだけ。クロップの存在を意識する必要はない。</p>
<h3 id="step-2-chat_template-適用"><a class="header" href="#step-2-chat_template-適用">Step 2: chat_template 適用</a></h3>
<p>vLLM が chat_template を適用してプロンプト文字列を生成:</p>
<pre><code>&lt;start_of_turn&gt;user
&lt;start_of_image&gt;この文書を要約して&lt;end_of_turn&gt;
&lt;start_of_turn&gt;model
</code></pre>
<p><code>&lt;start_of_image&gt;</code> は <code>boi_token</code> (token_id=255999)。この時点ではプレースホルダが <strong>1個だけ</strong>。</p>
<h3 id="step-3-gemma3processorcall--cpu-側前処理"><a class="header" href="#step-3-gemma3processorcall--cpu-側前処理">Step 3: Gemma3Processor.<strong>call</strong>() — CPU 側前処理</a></h3>
<h4 id="3a-image_processor-による画像前処理"><a class="header" href="#3a-image_processor-による画像前処理">3a: image_processor による画像前処理</a></h4>
<pre><code class="language-python">image_inputs = self.image_processor(images, **output_kwargs["images_kwargs"])
</code></pre>
<p>画像をリサイズ・正規化し、Pan-and-Scan が有効ならクロップも生成する。</p>
<h4 id="3b-num_crops-の取得"><a class="header" href="#3b-num_crops-の取得">3b: num_crops の取得</a></h4>
<pre><code class="language-python">num_crops = to_py_obj(image_inputs.pop("num_crops"))
</code></pre>
<h4 id="3c-プロンプトの自動書き換えpan-and-scan-時のみ"><a class="header" href="#3c-プロンプトの自動書き換えpan-and-scan-時のみ">3c: プロンプトの自動書き換え（Pan-and-Scan 時のみ）</a></h4>
<pre><code class="language-python">for num, idx in reversed(list(zip(num_crops, image_indexes))):
    if num:  # num=0 なら falsy → この書き換えは発生しない
        formatted_image_text = (
            f"Here is the original image {self.boi_token} "
            f"and here are some crops to help you see better "
            + " ".join([self.boi_token] * num)
        )
        prompt = prompt[:idx] + formatted_image_text + prompt[idx + len(self.boi_token):]
</code></pre>
<p><strong>Pan-and-Scan 無効（num=0）時</strong>: <code>if num:</code> が falsy なので、書き換えは<strong>一切発生しない</strong>。
<code>&lt;start_of_image&gt;</code> は1個のまま次のステップへ。</p>
<p><strong>Pan-and-Scan 有効（num=2）時</strong>: 1個の <code>&lt;start_of_image&gt;</code> が以下に置き換えられる:</p>
<pre><code>Here is the original image &lt;start_of_image&gt; and here are some crops to help you see better &lt;start_of_image&gt; &lt;start_of_image&gt;
</code></pre>
<h4 id="3d-boi_token--full_image_sequence-への展開"><a class="header" href="#3d-boi_token--full_image_sequence-への展開">3d: boi_token → full_image_sequence への展開</a></h4>
<pre><code class="language-python">self.full_image_sequence = f"\n\n{boi_token}{image_token * 256}{eoi_token}\n\n"
# = "\n\n&lt;start_of_image&gt;&lt;image&gt;×256&lt;end_of_image&gt;\n\n"

text = [prompt.replace(self.boi_token, self.full_image_sequence) for prompt in text]
</code></pre>
<p>全ての <code>&lt;start_of_image&gt;</code> がそれぞれ 256個の <code>&lt;image&gt;</code> トークンを含む <code>full_image_sequence</code> に展開される。</p>
<h4 id="3e-tokenizer-で-token_ids-に変換"><a class="header" href="#3e-tokenizer-で-token_ids-に変換">3e: tokenizer で token_ids に変換</a></h4>
<pre><code class="language-python">text_inputs = self.tokenizer(text=text, **output_kwargs["text_kwargs"])
</code></pre>
<p><code>&lt;image&gt;</code> トークン (token_id=262144) が並んだ input_ids が生成される。</p>
<h4 id="3f-token_type_ids-の生成"><a class="header" href="#3f-token_type_ids-の生成">3f: token_type_ids の生成</a></h4>
<pre><code class="language-python">mm_token_type_ids[array_ids == self.image_token_id] = 1
# → &lt;image&gt; トークン位置が 1、それ以外が 0
</code></pre>
<hr>
<h2 id="ケース1-デフォルトpan-and-scan-無効"><a class="header" href="#ケース1-デフォルトpan-and-scan-無効">ケース1: デフォルト（Pan-and-Scan 無効）</a></h2>
<p>入力例: A4 150dpi 画像 (1240 × 1754 pixel)</p>
<h3 id="プロンプト変換の流れ"><a class="header" href="#プロンプト変換の流れ">プロンプト変換の流れ</a></h3>
<pre><code>ユーザー入力:
  画像1枚 + "この文書を要約して"

chat_template 適用後:
  "...&lt;start_of_image&gt;この文書を要約して..."
                ↑
          boi_token 1個

do_pan_and_scan=False → num_crops=0 → プロンプト書き換えなし

boi_token → full_image_sequence 展開後:
  "...\n\n&lt;start_of_image&gt;&lt;image&gt;×256&lt;end_of_image&gt;\n\nこの文書を要約して..."
           ↑              ↑×256  ↑
         255999         262144  256000

tokenize 後の input_ids (概念的):
  [..., 255999, 262144, 262144, ...(×256)..., 262144, 256000, ..., テキスト, ...]
</code></pre>
<h3 id="cpu-側前処理"><a class="header" href="#cpu-側前処理">CPU 側前処理</a></h3>
<pre><code>元画像 (1240×1754)
    │  resize(896×896, bilinear)   ← アスペクト比無視の正方形リサイズ
    │  rescale(× 1/255)            ← [0,255] → [0,1]
    │  normalize(mean=0.5, std=0.5) ← [0,1] → [-1,1]
    ▼
pixel_values: (1, 3, 896, 896)
num_patches:  tensor([1])
</code></pre>
<h3 id="gpu-側-siglipvisionmodel"><a class="header" href="#gpu-側-siglipvisionmodel">GPU 側: SiglipVisionModel</a></h3>
<pre><code>(1, 3, 896, 896)
    │  Conv2d(3 → 1152, kernel=14, stride=14)
    ▼
(1, 1152, 64, 64)              ← 896/14 = 64
    │  flatten + transpose
    ▼
(1, 4096, 1152)                ← 64² = 4096 パッチ
    │  + position_embedding(4096, 1152)
    ▼
(1, 4096, 1152)
    │  SiglipEncoder × 27層
    │  （双方向 Attention, heads=16, 4096トークン間全対全）
    ▼
(1, 4096, 1152)
    │  post_layernorm
    ▼
(1, 4096, 1152)
</code></pre>
<h3 id="gpu-側-gemma3multimodalprojector"><a class="header" href="#gpu-側-gemma3multimodalprojector">GPU 側: Gemma3MultiModalProjector</a></h3>
<pre><code>(1, 4096, 1152)
    │  transpose → (1, 1152, 4096)
    │  reshape  → (1, 1152, 64, 64)     ← 2Dグリッドに復元
    │
    │  AvgPool2d(kernel_size=4, stride=4)
    ▼
(1, 1152, 16, 16)                       ← 64/4 = 16
    │  flatten(2) → (1, 1152, 256)
    │  transpose  → (1, 256, 1152)       ← 16² = 256 トークン
    │
    │  GemmaRMSNorm(1152)
    ▼
(1, 256, 1152)
    │
    │  matmul(mm_input_projection_weight)  ← shape: (1152, 5376)
    ▼
(1, 256, 5376)                           ← text hidden_size 空間
</code></pre>
<h3 id="gpu-側-split--flatten"><a class="header" href="#gpu-側-split--flatten">GPU 側: split + flatten</a></h3>
<pre><code>(1, 256, 5376)
    │  split by num_patches=[1] → [(1, 256, 5376)]
    │  flatten(0, 1)
    ▼
(256, 5376)                              ← 最終出力
</code></pre>
<h3 id="gpu-側-テキスト埋め込みとマージ"><a class="header" href="#gpu-側-テキスト埋め込みとマージ">GPU 側: テキスト埋め込みとマージ</a></h3>
<pre><code class="language-python">text_embeds = embed_tokens(input_ids) * normalizer   # (seq_len, 5376)
# token_id=262144 は vocab 外 → handle_oov_mm_token=True でゼロ埋め
# is_multimodal: (seq_len,) ← 256箇所が True

merged = masked_scatter_(text_embeds, is_multimodal, mm_embeds)  # (256, 5376)
# → 262144 だった256箇所をビジョン埋め込みで上書き
# ※ ビジョン埋め込みには normalizer スケーリングは適用されない

→ (seq_len, 5376) として Gemma3 Decoder (62層) へ
</code></pre>
<h3 id="消費トークン数-256"><a class="header" href="#消費トークン数-256">消費トークン数: <strong>256</strong></a></h3>
<hr>
<h2 id="ケース2-pan-and-scan-有効"><a class="header" href="#ケース2-pan-and-scan-有効">ケース2: Pan-and-Scan 有効</a></h2>
<p>入力例: 同じ A4 150dpi 画像 (1240 × 1754 pixel)</p>
<h3 id="プロンプト変換の流れ-1"><a class="header" href="#プロンプト変換の流れ-1">プロンプト変換の流れ</a></h3>
<pre><code>ユーザー入力:
  画像1枚 + "この文書を要約して"

chat_template 適用後:
  "...&lt;start_of_image&gt;この文書を要約して..."
                ↑
          boi_token 1個

do_pan_and_scan=True → ratio=1754/1240≈1.415 &gt; 1.2 → num_crops=2

Processor がプロンプトを自動書き換え (Step 3c):
  "...Here is the original image &lt;start_of_image&gt; and here are
   some crops to help you see better &lt;start_of_image&gt; &lt;start_of_image&gt;
   この文書を要約して..."
                                      ↑               ↑              ↑
                                 original 用       crop 0 用      crop 1 用

boi_token → full_image_sequence 展開後:
  "...Here is the original image \n\n&lt;boi&gt;&lt;image&gt;×256&lt;eoi&gt;\n\n and here are
   some crops to help you see better \n\n&lt;boi&gt;&lt;image&gt;×256&lt;eoi&gt;\n\n
   \n\n&lt;boi&gt;&lt;image&gt;×256&lt;eoi&gt;\n\nこの文書を要約して..."

tokenize 後:
  [..., "Here", "is", ...,
   255999, 262144×256, 256000,          ← original
   ..., "and", "here", ...,
   255999, 262144×256, 256000,          ← crop 0
   ...,
   255999, 262144×256, 256000,          ← crop 1
   ..., テキスト, ...]
</code></pre>
<h3 id="cpu-側-pan-and-scan-判定"><a class="header" href="#cpu-側-pan-and-scan-判定">CPU 側: Pan-and-Scan 判定</a></h3>
<pre><code class="language-python"># 縦長画像 (height &gt; width)
ratio = 1754 / 1240 ≈ 1.415
min_ratio_to_activate = 1.2
1.415 &gt; 1.2 → ✅ 発動
</code></pre>
<h3 id="cpu-側-クロップ数計算"><a class="header" href="#cpu-側-クロップ数計算">CPU 側: クロップ数計算</a></h3>
<pre><code class="language-python"># 縦長パス (image_height &gt; image_width)
num_crops_h = min(
    floor(1754 / 256),          # = 6  ← min_crop_size 制約
    floor(1754 / 1240 + 0.5),   # = 1  ← アスペクト比近似
)
# → min(6, 1) = 1
num_crops_h = max(2, 1) = 2    # 最低2クロップに強制
num_crops_h = min(4, 2) = 2    # max_num_crops でクリップ
num_crops_w = 1

# クロップサイズ検証
crop_size_w = ceil(1240 / 1) = 1240
crop_size_h = ceil(1754 / 2) = 877
min(1240, 877) = 877 &gt; 256 (min_crop_size) → ✅ 有効

結果: 1 × 2 = 2 クロップ
</code></pre>
<h3 id="cpu-側-クロップ切り出し--リサイズ"><a class="header" href="#cpu-側-クロップ切り出し--リサイズ">CPU 側: クロップ切り出し + リサイズ</a></h3>
<pre><code>元画像 (1240×1754)
  ├── original  (1240×1754) → resize(896×896) → normalize → (3, 896, 896)
  ├── crop 0    (1240×877)  → resize(896×896) → normalize → (3, 896, 896)
  └── crop 1    (1240×877)  → resize(896×896) → normalize → (3, 896, 896)
                                                              │ stack
                                                pixel_values: (3, 3, 896, 896)
                                                num_patches:  tensor([3])
</code></pre>
<h3 id="gpu-側-siglipvisionmodel-1"><a class="header" href="#gpu-側-siglipvisionmodel-1">GPU 側: SiglipVisionModel</a></h3>
<pre><code>(3, 3, 896, 896)
    │  Conv2d(3 → 1152, kernel=14, stride=14)
    ▼
(3, 1152, 64, 64)
    │  flatten + transpose
    ▼
(3, 4096, 1152)                 ← 3枚 × 4096パッチ
    │  + position_embedding
    ▼
(3, 4096, 1152)
    │  SiglipEncoder × 27層（双方向 Attention）
    ▼
(3, 4096, 1152)
</code></pre>
<h3 id="gpu-側-gemma3multimodalprojector-1"><a class="header" href="#gpu-側-gemma3multimodalprojector-1">GPU 側: Gemma3MultiModalProjector</a></h3>
<pre><code>(3, 4096, 1152)
    │  → reshape → (3, 1152, 64, 64)
    │  AvgPool2d(k=4, s=4)
    ▼
(3, 1152, 16, 16)
    │  flatten + transpose
    ▼
(3, 256, 1152)
    │  RMSNorm → matmul(1152 → 5376)
    ▼
(3, 256, 5376)
</code></pre>
<h3 id="gpu-側-split--flatten-1"><a class="header" href="#gpu-側-split--flatten-1">GPU 側: split + flatten</a></h3>
<pre><code>(3, 256, 5376)
    │  split by num_patches=[3] → [(3, 256, 5376)]
    │  flatten(0, 1)
    ▼
(768, 5376)                     ← 3 × 256 = 768 トークン
</code></pre>
<h3 id="gpu-側-テキスト埋め込みとマージ-1"><a class="header" href="#gpu-側-テキスト埋め込みとマージ-1">GPU 側: テキスト埋め込みとマージ</a></h3>
<pre><code>input_ids 中の token_id=262144 が768箇所
↓ masked_scatter_ で (768, 5376) を順番に書き込み
→ (seq_len, 5376) として Gemma3 Decoder へ
</code></pre>
<h3 id="消費トークン数-768--256--3"><a class="header" href="#消費トークン数-768--256--3">消費トークン数: <strong>768</strong> (= 256 × 3)</a></h3>
<hr>
<h2 id="プロンプト比較"><a class="header" href="#プロンプト比較">プロンプト比較</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th></th><th>Pan-and-Scan 無効（デフォルト）</th><th>Pan-and-Scan 有効</th></tr>
</thead>
<tbody>
<tr><td>プロンプト書き換え</td><td>なし</td><td>“Here is the original image … crops …” 挿入</td></tr>
<tr><td>boi_token 数</td><td>1</td><td>1 + num_crops (= 3)</td></tr>
<tr><td><code>&lt;image&gt;</code> トークン数</td><td>256</td><td>256 × (1 + num_crops) = 768</td></tr>
<tr><td>装飾テキスト</td><td>なし</td><td>“Here is the original image”, “and here are some crops to help you see better”</td></tr>
<tr><td>pixel_values shape</td><td>(1, 3, 896, 896)</td><td>(3, 3, 896, 896)</td></tr>
<tr><td>num_patches</td><td>tensor([1])</td><td>tensor([3])</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="全体データフロー図"><a class="header" href="#全体データフロー図">全体データフロー図</a></h2>
<pre><code>                        ┌─────────────────────────────┐
                        │  OpenAI 互換 API リクエスト    │
                        │  画像1枚 + テキスト           │
                        └────────────┬────────────────┘
                                     │
                        ┌────────────┴────────────────┐
                        │  chat_template 適用           │
                        │  → "&lt;start_of_image&gt;テキスト"  │
                        │    boi_token(255999) が1個    │
                        └────────────┬────────────────┘
                                     │
                        ┌────────────┴────────────────┐
                        │  CPU: Gemma3Processor        │
                        │                              │
                        │  image_processor:             │
                        │    resize(896×896)            │
                        │    rescale(×1/255)            │
                        │    normalize(0.5, 0.5)        │
                        │    Pan-and-Scan 時はクロップ生成│
                        │                              │
                        │  do_pan_and_scan?             │
                        │  ├── False:                   │
                        │  │   書き換えなし              │
                        │  │   boi_token 1個のまま       │
                        │  │   pixel_values: (1,3,896,896)│
                        │  │                            │
                        │  └── True &amp; ratio &gt; 1.2:      │
                        │      "Here is the original    │
                        │       image &lt;boi&gt; and here    │
                        │       are some crops ...      │
                        │       &lt;boi&gt; &lt;boi&gt;"            │
                        │      boi_token 3個に増加       │
                        │      pixel_values: (3,3,896,896)│
                        │                              │
                        │  各 boi_token を展開:          │
                        │  "\n\n&lt;boi&gt;&lt;img&gt;×256&lt;eoi&gt;\n\n" │
                        │                              │
                        │  tokenizer → input_ids        │
                        │  token_type_ids 生成           │
                        └────────────┬────────────────┘
                                     │
                        input_ids:     [..., 262144×256, ...(×N)...]
                        pixel_values:  (total_patches, 3, 896, 896)
                        num_patches:   (num_images,)
                                     │
                        ═════════════╪═══════════════ CPU → GPU
                                     │
                        ┌────────────┴────────────────┐
                        │  GPU: SiglipVisionEmbeddings │
                        │  Conv2d(3→1152, k=14, s=14)  │
                        │  + position_embedding         │
                        └────────────┬────────────────┘
                                     │
                        (total_patches, 4096, 1152)
                                     │
                        ┌────────────┴────────────────┐
                        │  GPU: SiglipEncoder           │
                        │  27層 双方向 Transformer       │
                        │  heads=16, hidden=1152        │
                        └────────────┬────────────────┘
                                     │
                        (total_patches, 4096, 1152)
                                     │
                        ┌────────────┴────────────────┐
                        │  GPU: Gemma3MultiModalProjector│
                        │  reshape → (*, 1152, 64, 64) │
                        │  AvgPool2d(k=4, s=4)          │
                        │  → (*, 1152, 16, 16)          │
                        │  flatten + transpose           │
                        │  → (*, 256, 1152)             │
                        │  GemmaRMSNorm(1152)            │
                        │  matmul(1152 → 5376)           │
                        └────────────┬────────────────┘
                                     │
                        (total_patches, 256, 5376)
                                     │
                        ┌────────────┴────────────────┐
                        │  split by num_patches         │
                        │  flatten(0, 1) per image      │
                        │  → list[(N×256, 5376)]        │
                        └────────────┬────────────────┘
                                     │
                        ┌────────────┴────────────────┐
                        │  GPU: embed_input_ids()       │
                        │                              │
                        │  text = embed_tokens(ids)     │
                        │         × normalizer          │
                        │  ※ 262144 は vocab 外         │
                        │    → handle_oov_mm_token=True │
                        │    → ゼロ埋め                  │
                        │                              │
                        │  masked_scatter_(             │
                        │    text, is_multimodal,       │
                        │    mm_embeds)                 │
                        │                              │
                        │  ※ vision embeds には          │
                        │    normalizer 未適用            │
                        └────────────┬────────────────┘
                                     │
                        (seq_len, 5376)
                                     │
                        ┌────────────┴────────────────┐
                        │  GPU: Gemma3 Decoder          │
                        │  62層, heads=32, kv_heads=16  │
                        │  sliding_window=1024          │
                        │  head_dim=128                 │
                        └──────────────────────────────┘
</code></pre>
<hr>
<h2 id="注意事項-1"><a class="header" href="#注意事項-1">注意事項</a></h2>
<ol>
<li>
<p><strong>ビジョン埋め込みの正規化</strong>: テキスト埋め込みには <code>embed_tokens(ids) × normalizer</code> のスケーリングが適用されるが、ビジョン埋め込みには <code>mm_soft_emb_norm</code>（RMSNorm）のみが適用され、<code>normalizer</code> スケーリングは適用されない。</p>
</li>
<li>
<p><strong>V1 での制限</strong>: Pan-and-Scan 有効時、V1 エンジンでは画像トークン間の双方向アテンションが簡略化されたパターンで実装されており、元モデルのアテンションパターンと完全には一致しない。</p>
</li>
<li>
<p><strong>AvgPool2d の役割</strong>: エンコーダは 4096 パッチ（64×64 グリッド）の高解像度で処理しつつ、AvgPool2d(k=4, s=4) で 256 トークン（16×16）に圧縮して LLM に渡す。これにより計算量と情報量のバランスを取っている。</p>
</li>
<li>
<p><strong>Pan-and-Scan のプロンプト</strong>: クロップありの場合のみ、Processor が “Here is the original image … and here are some crops to help you see better …” という装飾テキストを自動挿入する。クロップなしの場合この装飾テキストは存在せず、<code>&lt;image&gt;</code> トークン列のみとなる。ユーザーはクロップの存在を意識する必要はない。</p>
</li>
<li>
<p><strong>token_id=262144 の扱い</strong>: <code>&lt;image&gt;</code> トークンの token_id=262144 は通常の vocab 範囲外（OOV）。<code>handle_oov_mm_token=True</code> により安全にゼロ埋めされ、後続の <code>masked_scatter_</code> でビジョン埋め込みに上書きされる。</p>
</li>
<li>
<p><strong>Pan-and-Scan のデフォルト値の出典</strong>: <code>min_ratio_to_activate=1.2</code> 等の値は Google がモデルと共に配布した設定ではなく（preprocessor_config.json では全て null）、HF transformers の <code>processing_gemma3.py</code> 内の <code>Gemma3ProcessorKwargs._defaults</code> にハードコードされたフォールバック値。</p>
</li>
</ol>
<hr>
<h2 id="主要ファイル参照"><a class="header" href="#主要ファイル参照">主要ファイル参照</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th></tr>
</thead>
<tbody>
<tr><td>vllm/…/gemma3_mm.py</td><td>Gemma3ForConditionalGeneration, Gemma3MultiModalProjector, Gemma3ProcessingInfo</td></tr>
<tr><td>vllm/…/siglip.py</td><td>SiglipVisionModel, SiglipVisionEmbeddings, SiglipEncoder</td></tr>
<tr><td>vllm/…/utils.py</td><td>_merge_multimodal_embeddings()</td></tr>
<tr><td>HF transformers/…/processing_gemma3.py</td><td>Gemma3Processor, Gemma3ProcessorKwargs (デフォルト値定義)</td></tr>
<tr><td>HF transformers/…/image_processing_gemma3.py</td><td>Gemma3ImageProcessor (Pan-and-Scan 実装)</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="lmcache-統合調査報告-medium-verified"><a class="header" href="#lmcache-統合調査報告-medium-verified">LMCache 統合調査報告 [MEDIUM] [VERIFIED]</a></h1>
<blockquote>
<p><strong>最終更新</strong>: 2026-02-15
<strong>対象ソース</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py</code>, <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/</code>, <code>target/LMCache/lmcache/</code></p>
</blockquote>
<h2 id="調査目的-1"><a class="header" href="#調査目的-1">調査目的</a></h2>
<p>LMCacheがvLLMのKVConnectorBase_V1インタフェースをどのように実装し、チャンク単位のKVキャッシュ保存・取得を実現しているかを明らかにする。</p>
<h2 id="lmcache-概要"><a class="header" href="#lmcache-概要">LMCache 概要</a></h2>
<p>LMCacheはLLM推論のKVキャッシュを外部に保存・共有するためのライブラリ。vLLMと統合して、同じプロンプトプレフィックスのKVキャッシュを再利用したり、Prefill/Decode分離（Disaggregated Serving）でKVキャッシュを転送したりする。</p>
<h3 id="チャンク単位保存"><a class="header" href="#チャンク単位保存">チャンク単位保存</a></h3>
<p>KVキャッシュをトークン列のチャンク（デフォルト256トークン）に分割して保存する。各チャンクは独立したキーで管理される。</p>
<pre><code>プロンプト: [t0, t1, ..., t511]
  → チャンク0: [t0..t255] のKV → CacheEngineKey(chunk_hash=hash([t0..t255]))
  → チャンク1: [t256..t511] のKV → CacheEngineKey(chunk_hash=hash([t0..t511]))
</code></pre>
<p><strong>ハッシュチェーン</strong>: チャンクハッシュはプレフィックス全体のハッシュ（先頭からそのチャンク末尾まで）。これにより、同一プレフィックスを持つ異なるリクエスト間でKVキャッシュが共有可能。</p>
<h3 id="cacheenginekey"><a class="header" href="#cacheenginekey">CacheEngineKey</a></h3>
<pre><code class="language-python">@dataclass(slots=True)
class CacheEngineKey:
    model_name: str       # モデル名
    world_size: int       # TP並列度
    worker_id: int        # TPランク
    chunk_hash: int       # トークン列ハッシュ
    dtype: torch.dtype    # KVキャッシュのデータ型
    request_configs: dict | None  # リクエスト固有タグ
    tags: tuple | None    # (key, value) ペア
</code></pre>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/utils.py:330-410</code></p>
<h3 id="3層ストレージ階層"><a class="header" href="#3層ストレージ階層">3層ストレージ階層</a></h3>
<pre class="mermaid">graph LR
    GPU["GPU&lt;br/&gt;(vLLM paged buffer)"]
    L1["L1: LocalCPU&lt;br/&gt;(ピン留めメモリ)"]
    L2["L2: LocalDisk&lt;br/&gt;(NVMe/SSD)"]
    L3["L3: Remote&lt;br/&gt;(Redis/S3/FS/etc.)"]

    GPU &lt;--&gt;|"GPUConnector"| L1
    L1 &lt;--&gt;|"StorageManager"| L2
    L2 &lt;--&gt;|"StorageManager"| L3
</pre>

<div class="table-wrapper">
<table>
<thead>
<tr><th>層</th><th>バックエンド</th><th>設定</th><th>容量目安</th></tr>
</thead>
<tbody>
<tr><td>L1</td><td>LocalCPUBackend</td><td><code>local_cpu=True, max_local_cpu_size</code></td><td>~5GB</td></tr>
<tr><td>L2</td><td>LocalDiskBackend</td><td><code>local_disk, max_local_disk_size</code></td><td>~数十GB</td></tr>
<tr><td>L3</td><td>RemoteBackend</td><td><code>remote_url</code></td><td>無制限</td></tr>
</tbody>
</table>
</div>
<p><strong>追加バックエンド</strong>: P2PBackend（GPU直接）, GdsBackend（GPU Direct Storage）, NixlStorageBackend（RDMA）, PDBackend（P/D分離用）</p>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/</code></p>
<h3 id="リモートコネクタ15実装"><a class="header" href="#リモートコネクタ15実装">リモートコネクタ（15+実装）</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コネクタ</th><th>URLスキーム</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td>RedisConnector</td><td><code>redis://</code></td><td>Redis単体</td></tr>
<tr><td>RedisSentinelConnector</td><td><code>redis-sentinel://</code></td><td>Redis Sentinel</td></tr>
<tr><td>RedisClusterConnector</td><td><code>redis://</code> (cluster)</td><td>Redisクラスタ</td></tr>
<tr><td>S3Connector</td><td><code>s3://</code></td><td>AWS S3</td></tr>
<tr><td>FSConnector</td><td><code>fs:///</code></td><td>ローカルファイルシステム</td></tr>
<tr><td>MooncakestoreConnector</td><td><code>mooncakestore://</code></td><td>Mooncake</td></tr>
<tr><td>ValkeyConnector</td><td><code>valkey://</code></td><td>Valkey</td></tr>
<tr><td>EICConnector</td><td><code>infinistore://</code></td><td>InfiniStore</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/connector/</code></p>
<h2 id="vllm統合アーキテクチャ"><a class="header" href="#vllm統合アーキテクチャ">vLLM統合アーキテクチャ</a></h2>
<h3 id="2つの実装パス"><a class="header" href="#2つの実装パス">2つの実装パス</a></h3>
<p><code>LMCacheConnectorV1</code>はvLLM側のラッパーで、<code>use_native</code>設定により2つの実装を切り替える。</p>
<pre><code class="language-python"># target/vllm/.../lmcache_connector.py:83-101
use_native = vllm_config.kv_transfer_config.get_from_extra_config("use_native", False)
if use_native:
    # vLLM内蔵の native 実装
    cls = lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl
else:
    # lmcache パッケージの latest 実装
    cls = lmcache.integration.vllm.vllm_v1_adapter.LMCacheConnectorV1Impl
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>パス</th><th>ソース</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td>native</td><td><code>target/vllm/.../lmcache_integration/vllm_v1_adapter.py</code></td><td>vLLM同梱版。安定性重視</td></tr>
<tr><td>latest</td><td><code>target/LMCache/lmcache/integration/vllm/vllm_v1_adapter.py</code></td><td>LMCache最新版。機能追加優先</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py:72-103</code></p>
<h3 id="クラス階層-3"><a class="header" href="#クラス階層-3">クラス階層</a></h3>
<pre class="mermaid">graph TD
    KVBase["KVConnectorBase_V1&lt;br/&gt;(vLLM抽象基底)"]
    LMC["LMCacheConnectorV1&lt;br/&gt;(vLLM側ラッパー)"]
    Impl["LMCacheConnectorV1Impl&lt;br/&gt;(実装本体)"]
    Engine["LMCacheEngine&lt;br/&gt;(チャンク管理)"]
    SM["StorageManager&lt;br/&gt;(バックエンド管理)"]
    GPU["GPUConnector&lt;br/&gt;(paged buffer橋渡し)"]
    TDB["TokenDatabase&lt;br/&gt;(トークン→チャンク)"]
    LC["LookupClient&lt;br/&gt;(Scheduler側キャッシュ問い合わせ)"]

    KVBase --&gt;|"継承"| LMC
    LMC --&gt;|"_lmcache_engine"| Impl
    Impl --&gt;|"lmcache_engine (Worker)"| Engine
    Impl --&gt;|"lookup_client (Scheduler)"| LC
    Engine --&gt; SM
    Engine --&gt; GPU
    Engine --&gt; TDB
</pre>

<h3 id="ロール別初期化"><a class="header" href="#ロール別初期化">ロール別初期化</a></h3>
<p><code>LMCacheConnectorV1Impl.__init__()</code>はロールにより異なるコンポーネントを初期化する。</p>
<p><strong>Scheduler側</strong> (<code>role=SCHEDULER</code>):</p>
<ul>
<li><code>LookupClient</code> — 外部KVキャッシュの存在確認（<code>get_num_new_matched_tokens</code>用）</li>
<li><code>_request_trackers</code> — リクエスト状態管理</li>
<li><code>load_specs</code> — ロード仕様管理</li>
<li><code>lmcache_engine = None</code> — エンジンは持たない</li>
</ul>
<p><strong>Worker側</strong> (<code>role=WORKER</code>):</p>
<ul>
<li><code>LMCacheEngine</code> — KVキャッシュの保存・取得エンジン</li>
<li><code>LookupServer</code> — Scheduler側LookupClientへの応答</li>
<li><code>ZMQOffloadServer</code> — KVオフロードサーバ</li>
<li><code>GPUConnector</code> — vLLMのpaged bufferとの橋渡し（3種: Paged/Layerwise/Buffer）</li>
</ul>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py:570-715</code></p>
<h2 id="主要データ構造"><a class="header" href="#主要データ構造">主要データ構造</a></h2>
<h3 id="requesttracker"><a class="header" href="#requesttracker">RequestTracker</a></h3>
<p>リクエストのライフサイクルを追跡する。</p>
<pre><code class="language-python">@dataclass
class RequestTracker:
    req_id: str
    prompt_len: int                    # プロンプト全長
    token_ids: list[int]               # スケジュール済みトークン列
    allocated_block_ids: list[int]     # 割り当て済みブロックID
    num_saved_tokens: int = 0          # 保存済みトークン数
    disagg_spec: DisaggSpec | None     # P/D分離仕様
    mm_hashes: list[str] | None        # マルチモーダルハッシュ
    mm_positions: list[PlaceholderRange] | None  # MM位置情報
    request_configs: dict | None       # リクエスト固有設定
    is_decode_phase: bool = False      # デコードフェーズか
    skip_save: bool = False            # 保存スキップフラグ
</code></pre>
<p><strong>ライフサイクル</strong>:</p>
<ol>
<li><code>from_new_request()</code> — 新規リクエスト時にSchedulerOutputから生成</li>
<li><code>update()</code> — 各stepで新トークン・新ブロック追加</li>
<li><code>is_decode_phase</code> — 新トークン数=1でデコードフェーズ判定</li>
</ol>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py:121-246</code></p>
<h3 id="reqmeta"><a class="header" href="#reqmeta">ReqMeta</a></h3>
<p>各stepでWorkerに送信されるメタデータ。</p>
<pre><code class="language-python">@dataclass
class ReqMeta:
    req_id: str
    token_ids: list[int]             # 保存/ロード対象トークン
    slot_mapping: torch.Tensor       # vLLM paged bufferへのマッピング
    is_last_prefill: bool = False    # 最終プリフィルステップか
    save_spec: SaveSpec | None       # セーブ仕様
    load_spec: LoadSpec | None       # ロード仕様
    disagg_spec: DisaggSpec | None   # P/D分離仕様
    request_configs: dict | None     # リクエスト固有設定
</code></pre>
<p><strong>slot_mapping計算</strong>: <code>block_id * block_size + offset</code>（vLLMのBlockTable.compute_slot_mappingと同じ方式）</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py:248-398</code></p>
<h3 id="loadspec--savespec"><a class="header" href="#loadspec--savespec">LoadSpec / SaveSpec</a></h3>
<pre><code class="language-python">@dataclass
class LoadSpec:
    vllm_cached_tokens: int      # vLLMローカルキャッシュ済みトークン数
    lmcache_cached_tokens: int   # LMCacheキャッシュ済みトークン数
    can_load: bool               # Schedulerがロードを許可

@dataclass
class SaveSpec:
    skip_leading_tokens: int     # スキップする先頭トークン数（既保存分）
    can_save: bool               # セーブ実行するか
</code></pre>
<h3 id="セーブ判定ロジック"><a class="header" href="#セーブ判定ロジック">セーブ判定ロジック</a></h3>
<p>以下のいずれかに該当する場合、セーブをスキップ:</p>
<ol>
<li>既に保存済み(<code>num_saved_tokens &gt; 0</code>)で、未保存トークンがチャンク境界に達していない</li>
<li>デコードフェーズで<code>save_decode_cache=False</code></li>
<li>リクエスト設定で<code>lmcache.skip_save=True</code></li>
<li>Disagg接続でない場合のみスキップ（Disaggは転送のためスキップ不可）</li>
</ol>
<p>部分チャンク（チャンクサイズ未満）は<code>discard_partial_chunks</code>設定でセーブ可否が決まる。最終プリフィルでない場合は必ず破棄される（次stepでトークンが追加されるため）。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py:295-338</code></p>
<h2 id="kv形状とgpuconnector"><a class="header" href="#kv形状とgpuconnector">KV形状とGPUConnector</a></h2>
<h3 id="kv形状"><a class="header" href="#kv形状">KV形状</a></h3>
<pre><code class="language-python">kv_shape = (num_layer, 1 if use_mla else 2, chunk_size, num_kv_head, head_size)
# 通常: (num_layer, 2, 256, num_kv_heads, head_size)
#   2 = Key + Value
# MLA: (num_layer, 1, 256, 1, aligned_head_size)
#   1 = compressed_kv (KeyとValueが圧縮済み)
</code></pre>
<h3 id="gpuconnector-3種"><a class="header" href="#gpuconnector-3種">GPUConnector 3種</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コネクタ</th><th>用途</th><th>特徴</th></tr>
</thead>
<tbody>
<tr><td><code>VLLMPagedMemGPUConnectorV2</code></td><td>標準</td><td>vLLMのpaged bufferから直接読み書き</td></tr>
<tr><td><code>VLLMPagedMemLayerwiseGPUConnector</code></td><td>レイヤー別</td><td>レイヤーごとに個別処理</td></tr>
<tr><td><code>VLLMBufferLayerwiseGPUConnector</code></td><td>Blending用</td><td>中間バッファ経由でブレンディング</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py:500-541</code></p>
<h2 id="エンドツーエンドフロー"><a class="header" href="#エンドツーエンドフロー">エンドツーエンドフロー</a></h2>
<h3 id="ロード外部kv取得"><a class="header" href="#ロード外部kv取得">ロード（外部KV取得）</a></h3>
<pre class="mermaid">sequenceDiagram
    participant S as Scheduler
    participant LC as LMCache (SCHEDULER)
    participant W as Worker
    participant LW as LMCache (WORKER)
    participant ST as StorageBackend

    Note over S: WAITING処理
    S-&gt;&gt;LC: get_num_new_matched_tokens(request, local_computed)
    LC-&gt;&gt;LC: LookupClientでキャッシュ存在確認
    LC--&gt;&gt;S: (external_tokens, False)

    Note over S: allocate_slots()
    S-&gt;&gt;LC: update_state_after_alloc(request, blocks, external_tokens)
    LC-&gt;&gt;LC: RequestTracker作成、LoadSpec設定

    Note over S: schedule()末尾
    S-&gt;&gt;LC: build_connector_meta(scheduler_output)
    LC-&gt;&gt;LC: 各RequestTrackerからReqMeta生成
    LC--&gt;&gt;S: LMCacheConnectorMetadata

    Note over S: SchedulerOutput送信
    S-&gt;&gt;W: SchedulerOutput (kv_connector_metadata含む)

    Note over W: execute_model()
    W-&gt;&gt;LW: bind_connector_metadata(metadata)
    W-&gt;&gt;LW: start_load_kv(forward_context)
    LW-&gt;&gt;ST: チャンク取得（GPU→CPU→Storage）
    LW-&gt;&gt;LW: slot_mappingでpaged bufferに書き込み

    Note over W: Attention層 forward
    W-&gt;&gt;LW: wait_for_layer_load(layer_name)
    LW--&gt;&gt;W: ロード完了

    Note over W: forward完了
    W-&gt;&gt;LW: get_finished(finished_req_ids)
    LW--&gt;&gt;W: KVConnectorOutput
</pre>

<h3 id="セーブkv保存"><a class="header" href="#セーブkv保存">セーブ（KV保存）</a></h3>
<pre class="mermaid">sequenceDiagram
    participant W as Worker
    participant LW as LMCache (WORKER)
    participant ST as StorageBackend

    Note over W: Attention層 forward
    W-&gt;&gt;LW: save_kv_layer(layer_name, kv_layer, attn_metadata)
    LW-&gt;&gt;LW: ReqMetaからsave_spec確認
    LW-&gt;&gt;LW: slot_mappingでpaged bufferから読み出し
    LW-&gt;&gt;ST: チャンク保存（非同期）

    Note over W: forward完了後
    W-&gt;&gt;LW: wait_for_save()
    LW--&gt;&gt;W: 全チャンク保存完了

    Note over W: リクエスト完了時
    W-&gt;&gt;LW: request_finished(request, block_ids)
    LW--&gt;&gt;W: (delay_free=True, kv_transfer_params)
    Note over W: ブロック解放は送信完了後
</pre>

<h2 id="lmcacheengine-内部"><a class="header" href="#lmcacheengine-内部">LMCacheEngine 内部</a></h2>
<h3 id="storagemanager"><a class="header" href="#storagemanager">StorageManager</a></h3>
<p><code>OrderedDict[name, StorageBackendInterface]</code>でバックエンドを管理。ルックアップは登録順に検索し、最初にヒットしたバックエンドから取得する。保存は全バックエンドに書き込む（write-through）。</p>
<h3 id="tokendatabase"><a class="header" href="#tokendatabase">TokenDatabase</a></h3>
<p>トークン列からチャンクキー（<code>CacheEngineKey</code>）へのマッピングを管理。2つの実装:</p>
<ul>
<li><code>ChunkedTokenDatabase</code> — 固定チャンクサイズで分割</li>
<li><code>SegmentTokenDatabase</code> — セグメント単位で管理</li>
</ul>
<h3 id="kvイベント生成"><a class="header" href="#kvイベント生成">KVイベント生成</a></h3>
<p>LMCacheConnectorV1はWorker側でKV保存時に<code>BlockStored</code>イベントを生成する。これはvLLMのKVイベントシステム（<code>kv_events.py</code>）に変換され、外部のルーティングシステム等に配信される。</p>
<pre><code class="language-python"># LMCacheConnectorV1.get_kv_connector_kv_cache_events()
events = self._lmcache_engine.get_kv_events()
blocks = [BlockStored(block_hashes=e.block_hashes, ...) for e in events]
lmcache_kv_events = LMCacheKVEvents(num_workers=1)
lmcache_kv_events.add_events(blocks)
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py:220-244</code></p>
<h2 id="disaggregated-servingpd分離"><a class="header" href="#disaggregated-servingpd分離">Disaggregated Serving（P/D分離）</a></h2>
<p>LMCacheはDisaggregated Serving（PrefillインスタンスとDecodeインスタンスの分離）もサポートする。</p>
<h3 id="disaggspec"><a class="header" href="#disaggspec">DisaggSpec</a></h3>
<pre><code class="language-python">@dataclass
class DisaggSpec:
    req_id: str
    receiver_id: str          # 受信側エンジンID
    receiver_host: str        # 受信側ホスト
    receiver_init_port: int   # 初期化ポート
    receiver_alloc_port: int  # 割り当てポート
    is_last_prefill: bool = False
    num_transferred_tokens: int = 0
</code></pre>
<p>P/D分離時のフロー:</p>
<ol>
<li>Producerインスタンス（<code>kv_role=kv_producer</code>）がプリフィル実行</li>
<li>KVキャッシュをLMCache経由で保存/転送</li>
<li>Consumerインスタンス（<code>kv_role=kv_consumer</code>）がKVキャッシュをロード</li>
<li>Consumerはデコードのみ実行</li>
</ol>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py:88-99</code></p>
<h2 id="設定-7"><a class="header" href="#設定-7">設定</a></h2>
<h3 id="vllm側kvtransferconfig"><a class="header" href="#vllm側kvtransferconfig">vLLM側（KVTransferConfig）</a></h3>
<pre><code class="language-bash">vllm serve model_name \
  --kv-transfer-config '{"kv_connector": "LMCacheConnectorV1", "kv_role": "kv_both"}'
</code></pre>
<h3 id="lmcache側lmcache_config_file"><a class="header" href="#lmcache側lmcache_config_file">LMCache側（LMCACHE_CONFIG_FILE）</a></h3>
<p>環境変数<code>LMCACHE_CONFIG_FILE</code>でYAML設定ファイルを指定。主要設定:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>設定</th><th>デフォルト</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td><code>chunk_size</code></td><td>256</td><td>チャンクサイズ（トークン数）</td></tr>
<tr><td><code>local_cpu</code></td><td>True</td><td>CPUキャッシュ有効化</td></tr>
<tr><td><code>max_local_cpu_size</code></td><td>5.0</td><td>CPU最大容量（GB）</td></tr>
<tr><td><code>local_disk</code></td><td>None</td><td>ディスクキャッシュパス</td></tr>
<tr><td><code>remote_url</code></td><td>None</td><td>リモートストレージURL</td></tr>
<tr><td><code>enable_async_loading</code></td><td>True</td><td>非同期ロード</td></tr>
<tr><td><code>use_layerwise</code></td><td>False</td><td>レイヤー別処理</td></tr>
<tr><td><code>enable_blending</code></td><td>False</td><td>CacheBlendモード</td></tr>
<tr><td><code>save_decode_cache</code></td><td>False</td><td>デコードKVも保存</td></tr>
<tr><td><code>save_unfull_chunk</code></td><td>True</td><td>部分チャンク保存</td></tr>
<tr><td><code>enable_pd</code></td><td>False</td><td>P/D分離モード</td></tr>
</tbody>
</table>
</div>
<h3 id="vllm-extra_configからの設定伝搬"><a class="header" href="#vllm-extra_configからの設定伝搬">vLLM extra_configからの設定伝搬</a></h3>
<p><code>kv_connector_extra_config</code>で<code>lmcache.</code>プレフィックス付きの設定をLMCacheに渡せる:</p>
<pre><code class="language-json">{
  "kv_connector": "LMCacheConnectorV1",
  "kv_role": "kv_both",
  "kv_connector_extra_config": {
    "lmcache.chunk_size": 512,
    "lmcache.skip_save": true
  }
}
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py:586-601</code></p>
<h2 id="lmcacheディレクトリ構造"><a class="header" href="#lmcacheディレクトリ構造">LMCacheディレクトリ構造</a></h2>
<pre><code>target/LMCache/lmcache/
├── v1/
│   ├── cache_engine.py              # LMCacheEngine（中核エンジン）
│   ├── config.py                    # LMCacheEngineConfig
│   ├── metadata.py                  # LMCacheMetadata
│   ├── manager.py                   # LMCacheManager（ライフサイクル）
│   ├── storage_backend/
│   │   ├── storage_manager.py       # StorageManager
│   │   ├── abstract_backend.py      # StorageBackendInterface
│   │   ├── local_cpu_backend.py     # LocalCPUBackend (L1)
│   │   ├── local_disk_backend.py    # LocalDiskBackend (L2)
│   │   ├── remote_backend.py        # RemoteBackend (L3)
│   │   ├── connector/               # 15+リモートコネクタ
│   │   ├── cache_policy/            # Evictionポリシー
│   │   └── naive_serde/             # シリアライゼーション
│   ├── gpu_connector/               # GPU↔CPU橋渡し
│   ├── compute/                     # CacheBlend等
│   ├── token_database/              # トークン→チャンクマッピング
│   ├── lookup_client/               # Scheduler側問い合わせ
│   └── ...
├── integration/
│   └── vllm/                        # vLLM統合（latest版）
│       ├── lmcache_connector_v1.py  # エントリポイント
│       ├── vllm_v1_adapter.py       # 主実装（~700行）
│       └── utils.py
└── utils.py                         # CacheEngineKey等
</code></pre>
<h2 id="cacheblend"><a class="header" href="#cacheblend">CacheBlend</a></h2>
<p>CacheBlendはRAGシナリオでの非プレフィックスKV再利用を実現する高度な機能。<code>enable_blending=True</code>で有効化。段落（チャンク）単位でKVキャッシュを保存し、異なるコンテキストに挿入されたチャンクのKVを再利用する際に、少量の重要tokenのみを選択的に再計算する。</p>
<p><strong>詳細は別ドキュメント</strong>: <a href="#cacheblend-実装調査報告-medium-verified">CacheBlend実装調査報告</a></p>
<h3 id="要点"><a class="header" href="#要点">要点</a></h3>
<ul>
<li><strong>vLLM本体パッチが必要</strong>: <code>gpu_worker.py</code> にモデルオブジェクト登録コード追加</li>
<li><strong>独自forward path</strong>: vLLMのAttention層をバイパスし、LMCache内で独自にforward計算</li>
<li><strong>対応モデル3種</strong>: Llama, Qwen2, Qwen3のみ</li>
<li><strong>専用GPUコネクタ</strong>: <code>VLLMBufferLayerwiseGPUConnector</code>（中間バッファ＋RoPE位置補正＋パイプライン）</li>
<li><strong>制約多数</strong>: TP/PP未対応、プレフィックスキャッシュ非互換、バッチサイズ1前提</li>
</ul>
<h2 id="ecconnectorとの類似点相違点"><a class="header" href="#ecconnectorとの類似点相違点">ECConnectorとの類似点・相違点</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>観点</th><th>LMCache (KV Transfer)</th><th>ECConnector</th></tr>
</thead>
<tbody>
<tr><td>対象</td><td>デコーダKVキャッシュ</td><td>エンコーダキャッシュ</td></tr>
<tr><td>粒度</td><td>チャンク単位（256トークン）</td><td>エンコーダ出力全体</td></tr>
<tr><td>ストレージ</td><td>3層階層（CPU/Disk/Remote）</td><td>safetensors（参照実装）</td></tr>
<tr><td>問い合わせ</td><td>LookupClient（非同期）</td><td>has_cache_item（同期）</td></tr>
<tr><td>非同期ロード</td><td>あり（WAITING_FOR_REMOTE_KVS）</td><td>なし</td></tr>
<tr><td>P/D分離</td><td>サポート</td><td>なし</td></tr>
<tr><td>KVイベント</td><td>BlockStored/Removed</td><td>なし</td></tr>
<tr><td>成熟度</td><td>本番利用可能（10+コネクタ）</td><td>参照実装段階</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="mm_cache-内部実装調査-medium-verified"><a class="header" href="#mm_cache-内部実装調査-medium-verified">mm_cache 内部実装調査 [MEDIUM] [VERIFIED]</a></h1>
<blockquote>
<p><strong>最終更新</strong>: 2026-02-18</p>
</blockquote>
<p>CPU側（フロントエンドプロセス P0）でHugging FaceのPreprocessorが画像等を処理した結果をキャッシュする <code>mm_cache</code> の内部実装・設定・CPU並列化の調査報告。GPU側の <a href="#encodercacheエンコーダキャッシュ">EncoderCache</a>（<code>dict[str, Tensor]</code>）とは完全に別の機構。</p>
<hr>
<h2 id="1-mm_cacheとは"><a class="header" href="#1-mm_cacheとは">1. mm_cacheとは</a></h2>
<pre class="mermaid">graph LR
    A["API Request&lt;br&gt;(画像バイト列)"] --&gt; B["HF Processor&lt;br&gt;(リサイズ・正規化・パッチ分割)"]
    B --&gt; C["pixel_values テンソル&lt;br&gt;+ prompt_updates"]
    C --&gt; D["ProcessorCache&lt;br&gt;★CPU側 mm_cache ここ★"]
    D --&gt; E["EngineCoreRequest&lt;br&gt;(ZMQ IPC)"]
    E --&gt; F["EncoderCache&lt;br&gt;GPU側"]
</pre>

<p><strong>キャッシュキー</strong>: <code>mm_hash</code>（blake3/sha256によるコンテンツハッシュ、<a href="#フロントエンド-マルチモーダル処理パス-mediumdeep3-verified">mm-processing.md §3</a> 参照）</p>
<p><strong>キャッシュ値</strong>: HF Processor実行結果（<code>pixel_values</code> 等のテンソル群 + <code>prompt_updates</code>）</p>
<p><strong>キャッシュヒット時にスキップされる処理</strong>:</p>
<ol>
<li>HF Processorの実行（画像リサイズ・正規化・パッチ分割）</li>
<li>テンソルデータのZMQ IPC転送（<code>lru</code>/<code>shm</code>モード時）</li>
</ol>
<hr>
<h2 id="2-config設定項目"><a class="header" href="#2-config設定項目">2. Config設定項目</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/config/multimodal.py:103</code>（<code>MultiModalConfig</code>）</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド名</th><th>CLIオプション</th><th>デフォルト</th><th>制約</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>mm_processor_cache_gb</code></td><td><code>--mm-processor-cache-gb</code></td><td><strong>4</strong></td><td><code>≥0</code></td><td>キャッシュ容量（GiB）。<code>0</code> で完全無効</td></tr>
<tr><td><code>mm_processor_cache_type</code></td><td><code>--mm-processor-cache-type</code></td><td><strong>“lru”</strong></td><td><code>"lru"</code> or <code>"shm"</code></td><td>キャッシュ実装タイプ</td></tr>
<tr><td><code>mm_shm_cache_max_object_size_mb</code></td><td>（直接CLIオプションなし）</td><td><strong>128</strong></td><td><code>≥0</code></td><td>shmモード時の1オブジェクト上限（MiB）。<code>shm</code>専用</td></tr>
</tbody>
</table>
</div>
<blockquote>
<p><strong>注意</strong>: キャッシュはプロセスごとに独立して確保される。総メモリ使用量 = <code>mm_processor_cache_gb × (api_server_count + data_parallel_size)</code>。</p>
</blockquote>
<h3 id="設定変更例"><a class="header" href="#設定変更例">設定変更例</a></h3>
<pre><code class="language-bash"># キャッシュを8GBに増量（デフォルト4GB）
vllm serve &lt;model&gt; --mm-processor-cache-gb 8

# 共有メモリキャッシュに切り替え（マルチworker時に効率的）
vllm serve &lt;model&gt; --mm-processor-cache-type shm

# キャッシュを無効化
vllm serve &lt;model&gt; --mm-processor-cache-gb 0
</code></pre>
<pre><code class="language-python"># Python APIから
from vllm import LLM
llm = LLM(model=..., mm_processor_cache_gb=8)
</code></pre>
<hr>
<h2 id="3-キャッシュタイプの選択ロジック"><a class="header" href="#3-キャッシュタイプの選択ロジック">3. キャッシュタイプの選択ロジック</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/registry.py:281</code>（<code>_get_cache_type()</code>）</p>
<pre><code>mm_processor_cache_gb &lt;= 0
  → None（キャッシュ無効）

マルチモーダル非対応モデル
  → None

IPC非対応（以下のいずれか）:
  - api_process_count &gt; 1（マルチAPIプロセス）
  - data_parallel_size &gt; 1 かつ data_parallel_external_lb が False
  → "processor_only"（P0のみのLRU）

IPC対応:
  mm_processor_cache_type == "lru" → "lru"（SenderCache + ReceiverCache）
  mm_processor_cache_type == "shm" → "shm"（共有メモリ）
</code></pre>
<blockquote>
<p><code>processor_only</code> は IPC 不可時の自動フォールバックで、ユーザーが直接指定するオプションではない。</p>
</blockquote>
<hr>
<h2 id="4-キャッシュ実装5クラスの詳細"><a class="header" href="#4-キャッシュ実装5クラスの詳細">4. キャッシュ実装5クラスの詳細</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/cache.py</code></p>
<h3 id="41-p0送信側3実装"><a class="header" href="#41-p0送信側3実装">4.1 P0（送信側）3実装</a></h3>
<h4 id="multimodalprocessoronlycachel326-processor_only-モード"><a class="header" href="#multimodalprocessoronlycachel326-processor_only-モード"><code>MultiModalProcessorOnlyCache</code>（L326）— processor_only モード</a></h4>
<p>IPC非対応時に使用。P0プロセス内でHF処理結果を丸ごとLRUに保持し、ZMQ経由で毎回テンソルを送信。</p>
<pre><code class="language-python">self._cache = LRUCache(
    GiB_bytes * mm_config.mm_processor_cache_gb,
    getsizeof=lambda x: MultiModalCache.get_item_size(x)
)
# キャッシュ値: MultiModalProcessorCacheItem（item テンソル + prompt_updates）
</code></pre>
<h4 id="multimodalprocessorsendercachel379-lru-モード"><a class="header" href="#multimodalprocessorsendercachel379-lru-モード"><code>MultiModalProcessorSenderCache</code>（L379）— lru モード</a></h4>
<p><strong>P0はサイズメタデータのみ保持</strong>（テンソルはP1側に保存）。IPC経路のメモリ使用を削減。</p>
<pre><code class="language-python">self._cache = LRUCache(
    GiB_bytes * mm_config.mm_processor_cache_gb,
    getsizeof=lambda x: MultiModalCache.get_item_size(x)
)
# キャッシュ値: MultiModalProcessorCacheItemMetadata（item_size + prompt_updates）
# ヒット時: data=None で ZMQ 送信（テンソル転送スキップ）
</code></pre>
<h4 id="shmobjectstoresendercachel437-shm-モード"><a class="header" href="#shmobjectstoresendercachel437-shm-モード"><code>ShmObjectStoreSenderCache</code>（L437）— shm モード</a></h4>
<p><strong>テンソルを共有メモリに書き込み</strong>、Worker は直接共有メモリから読む（ZMQ 経由のコピー不要）。</p>
<pre><code class="language-python">ring_buffer = SingleWriterShmRingBuffer(
    data_buffer_size=int(mm_config.mm_processor_cache_gb * GiB_bytes),
    name=envs.VLLM_OBJECT_STORAGE_SHM_BUFFER_NAME,
    create=True,  # P0がWriter
)
self._shm_cache = SingleWriterShmObjectStorage(
    max_object_size=mm_config.mm_shm_cache_max_object_size_mb * MiB_bytes,
    n_readers=self.world_size,
    ring_buffer=ring_buffer,
    serde_class=MsgpackSerde,
)
# P0-private dict に prompt_updates のみ別途保管
self._p0_cache: dict[str, Sequence[ResolvedPromptUpdate]] = {}
</code></pre>
<ul>
<li>エビクション: <strong>FIFO</strong>（LRUではない）</li>
<li>1オブジェクト上限を超えると <code>ValueError</code>/<code>MemoryError</code> → フォールバックで元の mm_input をそのまま返す</li>
</ul>
<h3 id="42-p1受信側2実装"><a class="header" href="#42-p1受信側2実装">4.2 P1（受信側）2実装</a></h3>
<h4 id="multimodalreceivercachel614-lru-モード時-enginecore"><a class="header" href="#multimodalreceivercachel614-lru-モード時-enginecore"><code>MultiModalReceiverCache</code>（L614）— lru モード時 EngineCore</a></h4>
<p>P1でテンソルをLRU保持。P0のSenderCacheと<strong>同じEviction順序を維持</strong>することでキャッシュ状態を同期。</p>
<pre><code class="language-python">self._cache = LRUCache(
    GiB_bytes * mm_config.mm_processor_cache_gb,
    getsizeof=lambda x: MultiModalCache.get_item_size(x)
)
# キャッシュ値: MultiModalKwargsItem（テンソルデータそのもの）
</code></pre>
<h4 id="shmobjectstorereceivercachel662-shm-モード時-worker"><a class="header" href="#shmobjectstorereceivercachel662-shm-モード時-worker"><code>ShmObjectStoreReceiverCache</code>（L662）— shm モード時 Worker</a></h4>
<p>共有メモリ上のアドレスからテンソルを直接読み取る。<code>reader_lock</code>（<code>multiprocessing.Lock</code>）で参照カウントを保護。</p>
<hr>
<h2 id="5-lruキャッシュの内部実装"><a class="header" href="#5-lruキャッシュの内部実装">5. LRUキャッシュの内部実装</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/utils/cache.py:51</code>（<code>vllm.utils.cache.LRUCache</code>）</p>
<p><code>cachetools.LRUCache</code> を継承したサイズベースのLRU実装:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>機能</th><th>実装</th></tr>
</thead>
<tbody>
<tr><td>容量</td><td><code>maxsize = GiB_bytes * mm_processor_cache_gb</code>（バイト数）</td></tr>
<tr><td>サイズ計算</td><td><code>getsizeof()</code> コールバック経由で <code>tensor.nbytes</code> を再帰的に合計</td></tr>
<tr><td>Eviction</td><td>最低使用時間（LRU）順。ピン済みエントリはスキップ</td></tr>
<tr><td>ヒット統計</td><td><code>stat(delta=True)</code> でインターバル別 hits/total を取得可能</td></tr>
<tr><td>ピン機能</td><td><code>pin(key)</code> でEviction対象外にできる</td></tr>
<tr><td><code>touch()</code></td><td>Eviction順序を更新（P0-P1同期用）</td></tr>
</tbody>
</table>
</div>
<p><strong>サイズ計算の詳細</strong>（<code>MultiModalCache.get_item_size()</code>）:</p>
<pre><code class="language-python"># json_map_leaves + json_reduce_leaves で再帰的にリーフ要素のサイズを合計
# leaf ごと: torch.Tensor → tensor.nbytes, str → sys.getsizeof, etc.
</code></pre>
<hr>
<h2 id="6-shmキャッシュsinglewritershmringbufferの内部実装"><a class="header" href="#6-shmキャッシュsinglewritershmringbufferの内部実装">6. SHMキャッシュ（SingleWriterShmRingBuffer）の内部実装</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/device_communicators/shm_object_storage.py:22</code></p>
<h3 id="リングバッファ構造"><a class="header" href="#リングバッファ構造">リングバッファ構造</a></h3>
<pre><code>バッファ全体 (mm_processor_cache_gb GiB)
┌─────────────────────────────────────────────────┐
│ [4B id][4B size][data...] [4B id][4B size][data..] │
│ ^start                     ↑                    ^end│
└─────────────────────────────────────────────────┘
</code></pre>
<ul>
<li><strong>シングルライター</strong>: P0のみが書き込み（<code>allocate_buf</code>）</li>
<li><strong>マルチリーダー</strong>: Worker×TP数が読み取り可能（<code>access_buf</code>）</li>
<li><strong>FIFOエビクション</strong>: 最も古い（start側の）オブジェクトから解放</li>
<li><strong>参照カウント</strong>: 全リーダーが解放を確認してからGC（ライターが管理）</li>
<li><strong>ラップアラウンド</strong>: バッファ末尾に達したら先頭に折り返す</li>
</ul>
<h3 id="singlewritershmobjectstorage-の付加機能"><a class="header" href="#singlewritershmobjectstorage-の付加機能"><code>SingleWriterShmObjectStorage</code> の付加機能</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>機能</th><th>実装</th></tr>
</thead>
<tbody>
<tr><td>キー管理</td><td><code>key_index: dict[str, (address, monotonic_id)]</code>（ライター側のみ）</td></tr>
<tr><td>重複キー</td><td>既存キーはデータ再書き込みなし（アドレス再参照）</td></tr>
<tr><td>シリアライズ</td><td><code>MsgpackSerde</code>（デフォルト）</td></tr>
<tr><td>上限超過</td><td><code>ValueError</code>/<code>MemoryError</code> → SenderCacheがフォールバック</td></tr>
<tr><td>上限チェック</td><td><code>put()</code> 時に <code>max_object_size</code> と比較。超えたらエラー</td></tr>
</tbody>
</table>
</div>
<p><strong>メモリレイアウト（オブジェクト単位）</strong>:</p>
<pre><code>[4-byte ref_count][metadata_size][serialized_object_data]
</code></pre>
<hr>
<h2 id="7-p0p1-キャッシュ整合性の設計"><a class="header" href="#7-p0p1-キャッシュ整合性の設計">7. P0–P1 キャッシュ整合性の設計</a></h2>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/cache.py:175</code>（<code>BaseMultiModalCache</code> docstring）</p>
<pre><code>P0: From API --&gt; is_cached() × N --&gt; get_and_update() --&gt; To P1
P1: From P0  --&gt; get_and_update()                     --&gt; To model
</code></pre>
<p><strong>核心</strong>: <code>get_and_update()</code> は P0 と P1 で<strong>必ず同じ順序</strong>で呼ばれる必要がある。これにより、P0のキャッシュ状態だけを参照してP1のキャッシュ状態を推定でき、IPC通信なしでキャッシュヒット確認が可能。</p>
<ul>
<li><code>is_cached()</code>: Eviction順序を変えない（P0のみ参照）</li>
<li><code>get_and_update()</code>: Eviction順序を更新（P0・P1で順番に呼ぶ）</li>
<li><code>touch_sender_cache_item()</code>: Sender側のEviction順序を手動更新（LoRA切り替え等で使用）</li>
</ul>
<hr>
<h2 id="8-cpu並列処理"><a class="header" href="#8-cpu並列処理">8. CPU並列処理</a></h2>
<h3 id="intra-request-並列化1リクエスト内"><a class="header" href="#intra-request-並列化1リクエスト内">intra-request 並列化（1リクエスト内）</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/input_processor.py:363</code></p>
<pre><code class="language-python">with set_request_id(request_id), set_default_torch_num_threads():
    processed_inputs = self.input_preprocessor.preprocess(...)
</code></pre>
<p><code>set_default_torch_num_threads()</code>（<code>target/vllm/vllm/utils/torch_utils.py:106</code>）:</p>
<pre><code class="language-python"># OMP_NUM_THREADS 環境変数を読んで torch.set_num_threads() を設定
# 未設定時のデフォルト: 1
num_threads = int(os.environ.get("OMP_NUM_THREADS", 1))
torch.set_num_threads(num_threads)
</code></pre>
<p>HF Processorの実行（<code>apply()</code> → <code>_cached_apply_hf_processor()</code> → <code>_call_hf_processor()</code>）はすべて同期処理。<code>OMP_NUM_THREADS</code> を増やすとTorchのOpenMP並列化（行列演算等）が有効になる。</p>
<pre><code class="language-bash">OMP_NUM_THREADS=4 vllm serve &lt;model&gt;  # 1画像あたりのCPU処理を4スレッド化
</code></pre>
<h3 id="inter-request-並列化複数リクエスト間"><a class="header" href="#inter-request-並列化複数リクエスト間">inter-request 並列化（複数リクエスト間）</a></h3>
<p><strong>vLLMには複数リクエストを同時にHF Processorで処理する機構はない。</strong></p>
<ul>
<li><code>process_inputs()</code> は <strong>同期呼び出し</strong>（async/await なし）</li>
<li>AsyncLLMの <code>add_request()</code> 内で呼ばれ、asyncioイベントループで逐次実行される</li>
<li>並列化する唯一の手段: <strong>複数のAPIサーバープロセスを立ち上げる</strong>
<ul>
<li>ただし各プロセスが独立して <code>mm_processor_cache_gb</code> GiBを確保する</li>
<li>IPC非対応になり <code>processor_only</code> モードに自動フォールバック</li>
</ul>
</li>
</ul>
<h3 id="mm_cacheによる実質的な高速化"><a class="header" href="#mm_cacheによる実質的な高速化">mm_cacheによる実質的な高速化</a></h3>
<p>同一画像が何度も送られる場合（例: 同じシステム画像を全リクエストで使用）、キャッシュヒット時は HF Processor の実行を完全にスキップできるため、事実上の並列化効果が得られる。</p>
<hr>
<h2 id="9-キャッシュモード別データフロー比較"><a class="header" href="#9-キャッシュモード別データフロー比較">9. キャッシュモード別データフロー比較</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th></th><th>processor_only</th><th>lru</th><th>shm</th></tr>
</thead>
<tbody>
<tr><td><strong>P0保持</strong></td><td>テンソル + prompt_updates</td><td>サイズメタデータ + prompt_updates</td><td>SHMアドレス + prompt_updates（P0-private dict）</td></tr>
<tr><td><strong>P1保持</strong></td><td>なし</td><td>テンソル（LRU）</td><td>共有メモリ上のテンソル</td></tr>
<tr><td><strong>エビクション</strong></td><td>LRU</td><td>LRU（P0/P1連動）</td><td>FIFO</td></tr>
<tr><td><strong>IPCヒット時</strong></td><td>テンソルをZMQ送信</td><td>data=None 送信（テンソル省略）</td><td>data=None 送信（SHMアドレスのみ）</td></tr>
<tr><td><strong>MissのZMQ転送</strong></td><td>テンソル全体</td><td>テンソル全体</td><td>data=None（SHM経由）</td></tr>
<tr><td><strong>適用場面</strong></td><td>マルチAPIプロセス / DP≥2</td><td>単一APIプロセス + 単一DP（デフォルト）</td><td>同左（マルチWorkerで転送削減）</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="関連ドキュメント-14"><a class="header" href="#関連ドキュメント-14">関連ドキュメント</a></h2>
<ul>
<li><a href="#フロントエンド-マルチモーダル処理パス-mediumdeep3-verified">マルチモーダル処理パイプライン（§3 MMハッシュ DEEP）</a> — ProcessorCacheのキー計算</li>
<li><a href="#マルチモーダル処理パイプライン-サマリー">マルチモーダル処理パイプライン概要</a> — 3層キャッシュ構造図</li>
<li><a href="#encodercacheエンコーダキャッシュ">EncoderCache（GPU側）</a> — GPU側の別キャッシュ機構</li>
<li><a href="#gemma3-ビジョンパイプライン-キャッシュ機構-medium-verified">Gemma3ビジョンパイプラインのキャッシュ機構</a> — 3層キャッシュの具体例</li>
</ul>
<h2 id="主要ファイル-9"><a class="header" href="#主要ファイル-9">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th><th>行</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/config/multimodal.py</code></td><td><code>MultiModalConfig</code>（mm_processor_cache_gb等）</td><td>L103</td></tr>
<tr><td><code>target/vllm/vllm/multimodal/cache.py</code></td><td><code>MultiModalProcessorOnlyCache</code>, <code>SenderCache</code>, <code>ShmObjectStoreSenderCache</code>, <code>ReceiverCache</code>, <code>ShmObjectStoreReceiverCache</code></td><td>L326, L379, L437, L614, L662</td></tr>
<tr><td><code>target/vllm/vllm/multimodal/registry.py</code></td><td><code>_get_cache_type()</code>, <code>processor_cache_from_config()</code>, <code>engine_receiver_cache_from_config()</code>, <code>worker_receiver_cache_from_config()</code></td><td>L281, L307, L335, L348</td></tr>
<tr><td><code>target/vllm/vllm/utils/cache.py</code></td><td><code>LRUCache</code>（cachetools継承、サイズベース、ピン機能）</td><td>L51</td></tr>
<tr><td><code>target/vllm/vllm/distributed/device_communicators/shm_object_storage.py</code></td><td><code>SingleWriterShmRingBuffer</code>, <code>SingleWriterShmObjectStorage</code></td><td>L22, L412</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/input_processor.py</code></td><td><code>process_inputs()</code>（<code>set_default_torch_num_threads</code>呼び出し）</td><td>L363</td></tr>
<tr><td><code>target/vllm/vllm/utils/torch_utils.py</code></td><td><code>set_default_torch_num_threads()</code>（OMP_NUM_THREADS読み取り）</td><td>L106</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="プロセスアーキテクチャtp2構成"><a class="header" href="#プロセスアーキテクチャtp2構成">プロセスアーキテクチャ（TP=2構成）</a></h1>
<blockquote>
<p><strong>深度</strong>: [DEEP]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-14</p>
</blockquote>
<h2 id="概要-22"><a class="header" href="#概要-22">概要</a></h2>
<p>vLLMをGPU2枚・TP=2で起動した場合のプロセス構成、コンポーネント配置、プロセス間通信メカニズムを調査した。</p>
<h2 id="1-プロセス構成合計4プロセス"><a class="header" href="#1-プロセス構成合計4プロセス">1. プロセス構成（合計4プロセス）</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>プロセス名</th><th>生成元</th><th>含まれるコンポーネント</th></tr>
</thead>
<tbody>
<tr><td>Frontend（メインプロセス）</td><td>ユーザー起動</td><td>AsyncLLM, InputProcessor, EngineCoreClient, OutputProcessor</td></tr>
<tr><td>EngineCore (<code>EngineCore_DP0</code>)</td><td>Frontend (<code>mp.Process</code>)</td><td>EngineCore, Scheduler, KVCacheManager, MultiprocExecutor</td></tr>
<tr><td>VllmWorker-0</td><td>EngineCore (<code>mp.Process</code>)</td><td>Worker, GPUModelRunner（GPU 0）</td></tr>
<tr><td>VllmWorker-1</td><td>EngineCore (<code>mp.Process</code>)</td><td>Worker, GPUModelRunner（GPU 1）</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core_client.py:493-507</code> (CoreEngineProcManager)
<strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:147-160</code> (WorkerProc起動)</p>
<h3 id="コンポーネントとプロセスの対応図"><a class="header" href="#コンポーネントとプロセスの対応図">コンポーネントとプロセスの対応図</a></h3>
<pre><code>┌─ Frontend Process ─────────────────────────────────────┐
│  AsyncLLM ─→ InputProcessor                            │
│  EngineCoreClient (ZMQ ROUTER/PULL)                    │
│  OutputProcessor ←─ Detokenizer                        │
└──────────────────────────┬─────────────────────────────┘
                           │ ZMQ (msgpack)
                           ▼
┌─ EngineCore Process ─────────────────────────────────────┐
│  EngineCore.step()                                       │
│  ├─ Scheduler ─→ KVCacheManager                         │
│  └─ MultiprocExecutor                                    │
│       ├─ rpc_broadcast_mq (SharedMemory → 全Worker)      │
│       └─ worker_response_mq × 2 (各Worker → Executor)   │
└──────────┬──────────────────────────────┬────────────────┘
           │ SharedMemory MQ              │ SharedMemory MQ
           ▼                              ▼
┌─ Worker-0 Process ──┐  ┌─ Worker-1 Process ──┐
│  Worker              │  │  Worker              │
│  GPUModelRunner      │  │  GPUModelRunner      │
│  (GPU 0, TP rank 0)  │  │  (GPU 1, TP rank 1)  │
└──────────┬───────────┘  └──────────┬───────────┘
           │         NCCL            │
           └─────────────────────────┘
             (NVLink / PCIe 直接通信)
</code></pre>
<p><strong>注意点</strong>:</p>
<ul>
<li>Scheduler、KVCacheManagerは<strong>EngineCoreプロセス内</strong>で動作し、独立プロセスではない</li>
<li>OutputProcessorは<strong>Frontendプロセス内</strong>で動作する（バックエンドではない）</li>
<li>MultiprocExecutorはEngineCoreプロセス内に存在し、Workerプロセスへの指令管理を行う</li>
</ul>
<h2 id="2-プロセス間通信メカニズム"><a class="header" href="#2-プロセス間通信メカニズム">2. プロセス間通信メカニズム</a></h2>
<h3 id="21-frontend--enginecore-zmq-over-tcp-loopback"><a class="header" href="#21-frontend--enginecore-zmq-over-tcp-loopback">2.1 Frontend ↔ EngineCore: ZMQ over TCP loopback</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>項目</th><th>値</th></tr>
</thead>
<tbody>
<tr><td>プロトコル</td><td>ZMQ over TCP (<code>127.0.0.1:&lt;random_port&gt;</code>)</td></tr>
<tr><td>ソケット型</td><td>Frontend: ROUTER(送信) + PULL(受信), EngineCore: DEALER(受信)</td></tr>
<tr><td>シリアライゼーション</td><td>msgpack（<code>msgspec.Struct(array_like)</code> 対応）</td></tr>
<tr><td>スレッドモデル</td><td>バックグラウンドスレッドでシリアライゼーション/デシリアライゼーション</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core_client.py:510-515</code> (ZMQソケット設定)
<strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:877-950</code> (EngineCoreProc._perform_handshake)</p>
<h3 id="22-enginecore--workers-sharedmemory-messagequeue"><a class="header" href="#22-enginecore--workers-sharedmemory-messagequeue">2.2 EngineCore ↔ Workers: SharedMemory MessageQueue</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>項目</th><th>値</th></tr>
</thead>
<tbody>
<tr><td>プロトコル</td><td>共有メモリ（ShmRingBuffer） + ZMQ PUB/SUB（オーバーフロー時）</td></tr>
<tr><td>キュー</td><td>rpc_broadcast_mq（1対多）+ worker_response_mq（各Worker→Executor）</td></tr>
<tr><td>シリアライゼーション</td><td>pickle（protocol 5, out-of-band buffers対応）</td></tr>
<tr><td>同期方式</td><td>ロックフリー。メモリフェンス（<code>threading.Lock</code> acquire/release, ~20ns）のみ</td></tr>
<tr><td>バッファサイズ</td><td>デフォルト24MiB/チャンク × 10チャンク</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/device_communicators/shm_broadcast.py:127</code> (ShmRingBuffer)
<strong>参照</strong>: <code>target/vllm/vllm/distributed/device_communicators/shm_broadcast.py:272</code> (MessageQueue)
<strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:131-136</code> (rpc_broadcast_mq生成)</p>
<h4 id="shmringbuffer-メモリレイアウト"><a class="header" href="#shmringbuffer-メモリレイアウト">ShmRingBuffer メモリレイアウト</a></h4>
<pre><code>┌─────────────────────────────────┬──────────────────────────────────────┐
│ data: chunk0 | chunk1 | ... | chunkN │ metadata: [written|r0|r1|...|rN] × N │
│ max_chunks × max_chunk_bytes (24MiB) │ max_chunks × (1 + n_reader) bytes    │
└─────────────────────────────────┴──────────────────────────────────────┘
</code></pre>
<p>メタデータの状態遷移:</p>
<ul>
<li><code>0???...???</code>: 未書き込み → 書き込み可</li>
<li><code>1000...000</code>: 書き込み直後 → 全reader読み取り可</li>
<li><code>1???...???</code>: 一部readerが読み取り済み</li>
<li><code>1111...111</code>: 全reader読み取り済み → 書き込み可（再利用）</li>
</ul>
<p><strong>オーバーフロー処理</strong>: データが24MiBを超える場合、ZMQ PUB/SUBソケット（IPC）経由で転送する。ローカルではXPUB/SUBソケット、リモート（マルチノード時）ではTCPソケットを使用。</p>
<h4 id="messagequeue-の詳細設計-deep-verified"><a class="header" href="#messagequeue-の詳細設計-deep-verified">MessageQueue の詳細設計 [DEEP] [VERIFIED]</a></h4>
<p>MessageQueueは<code>ShmRingBuffer</code>をラップし、pickle protocol 5のout-of-bandバッファ対応のシリアライゼーション層を提供する。</p>
<p><strong>ロール分離（Writer / Local Reader / Remote Reader）</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ロール</th><th>判定条件</th><th>通信手段</th></tr>
</thead>
<tbody>
<tr><td>Writer</td><td>コンストラクタで生成した側</td><td>ShmRingBuffer + ZMQ XPUB</td></tr>
<tr><td>Local Reader</td><td><code>rank in handle.local_reader_ranks</code></td><td>ShmRingBuffer + ZMQ SUB</td></tr>
<tr><td>Remote Reader</td><td>上記以外</td><td>ZMQ SUB のみ</td></tr>
</tbody>
</table>
</div>
<p>Writer側のコンストラクタでShmRingBufferとZMQソケット（XPUB）を両方作成する。Local Readerは共有メモリ経由で受信し、オーバーフロー時のみZMQ SUBにフォールバック。Remote Readerは常にZMQ SUBのみで受信する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/device_communicators/shm_broadcast.py:272-354</code> (MessageQueue.<strong>init</strong> / create_from_handle)</p>
<p><strong>enqueue() のデータフォーマット</strong>:</p>
<pre><code>ShmRingBuffer チャンク内のバイトレイアウト:
+------+-------------------+--------------------+--------------------+-----+
| [0]  | [1:3]             | [3:7] [7:7+L0]     | [7+L0:11+L0] ...  | ... |
| flag | buf_count (2byte) | len0+main_pickle   | len1+oob_buffer1   | ... |
+------+-------------------+--------------------+--------------------+-----+
  flag: 0=通常, 1=オーバーフロー（ZMQ経由で後続送信）
</code></pre>
<ul>
<li><strong>pickle protocol 5 + out-of-band buffers</strong>: <code>buffer_callback</code>でサイズ判定。1MiB未満のバッファはインライン化（main pickle内に含む）、1MiB以上はoob bufferとして別管理</li>
<li><strong>オーバーフロー判定</strong>: <code>total_bytes + len(main_pickle) &gt;= max_chunk_bytes</code>（デフォルト24MiB）の場合、ShmRingBufferにはflag=1のみ書き込み、実データはZMQ <code>send_multipart</code>で送信</li>
<li>Remote Readerへは常に<code>send_multipart</code>で送信（ShmRingBufferにアクセスできないため）</li>
</ul>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/device_communicators/shm_broadcast.py:571-612</code> (enqueue)</p>
<p><strong>dequeue() のフロー</strong>:</p>
<ol>
<li><code>acquire_read()</code>でShmRingBufferからチャンクを取得</li>
<li>flag=0（通常）: チャンクからbuf_count→各バッファ長→バッファを順次読み出し、<code>pickle.loads(main, buffers=oob_list)</code>でデシリアライズ</li>
<li>flag=1（オーバーフロー）: <code>acquire_read()</code>のコンテキストを<strong>抜けてから</strong>（readフラグ設定後）、ZMQ SUBソケット経由で<code>recv_multipart</code></li>
</ol>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/device_communicators/shm_broadcast.py:614-640</code> (dequeue)</p>
<p><strong>acquire_write() / acquire_read() の同期プロトコル</strong>:</p>
<p>Writer:</p>
<ol>
<li>メモリフェンスで最新のメタデータを読む</li>
<li><code>written_flag=0</code>（未書き込み）または全readerが読み済み（<code>read_count == n_reader</code>）のチャンクを探す</li>
<li><code>written_flag</code>を0にリセット → データ書き込み → 全readerフラグを0にリセット → <strong>メモリフェンス</strong> → <code>written_flag</code>を1に → <strong>メモリフェンス</strong></li>
<li>フラグ設定順序が重要: 先にreaderフラグをリセット（case 1維持）→最後にwritten=1（case 2へ遷移）。逆順だとcase 3を経由し、readerが不整合なデータを読む危険</li>
</ol>
<p>Reader:</p>
<ol>
<li>メモリフェンスで最新のメタデータを読む</li>
<li><code>written_flag=1</code>かつ自分の<code>read_flag=0</code>のチャンクを探す</li>
<li>データ読み取り → 自分の<code>read_flag</code>を1に → <strong>メモリフェンス</strong></li>
</ol>
<p><strong>SpinTimer / SpinSleepTimer</strong>: Readerのスピン待ち戦略。デフォルトは<code>sched_yield()</code>（CPU譲渡）。<code>VLLM_SLEEP_WHEN_IDLE=1</code>時は3秒間アクティビティがないと100msスリープに移行し、CPU消費を削減する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/device_communicators/shm_broadcast.py:438-504</code> (acquire_write)
<strong>参照</strong>: <code>target/vllm/vllm/distributed/device_communicators/shm_broadcast.py:506-569</code> (acquire_read)</p>
<p><strong>wait_until_ready() ハンドシェイク</strong>:</p>
<p>Writer→各ReaderへZMQ <code>XPUB/SUB</code>経由でREADYメッセージを交換する集合操作。ShmRingBuffer自体にはハンドシェイクがないため、ZMQの<code>XPUB_VERBOSE</code>（全サブスクリプションメッセージ受信）を利用してReader接続完了を確認する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/device_communicators/shm_broadcast.py:405-436</code> (wait_until_ready)</p>
<h4 id="collective_rpc-の動作フロー-1"><a class="header" href="#collective_rpc-の動作フロー-1">collective_rpc の動作フロー</a></h4>
<pre><code>MultiprocExecutor.collective_rpc("execute_model", args=(scheduler_output,))
  │
  ├─ rpc_broadcast_mq.enqueue((method, args, kwargs, output_rank))
  │   → pickle → ShmRingBuffer書き込み → メモリフェンス
  │
  ├─ Worker-0: rpc_broadcast_mq.dequeue() → Worker.execute_model()
  │   → worker_response_mq.enqueue((SUCCESS, output))
  │
  ├─ Worker-1: rpc_broadcast_mq.dequeue() → Worker.execute_model()
  │   → worker_response_mq.enqueue((SUCCESS, output))
  │
  └─ Executor: response_mqs[0].dequeue() → output[0] を返却
      （output_rank=0 の場合、rank 0 の結果のみ返す）
</code></pre>
<h3 id="24-worker--enginecore-結果返却パス-deep-verified"><a class="header" href="#24-worker--enginecore-結果返却パス-deep-verified">2.4 Worker → EngineCore 結果返却パス [DEEP] [VERIFIED]</a></h3>
<h4 id="response_mq-の構成"><a class="header" href="#response_mq-の構成">response_mq の構成</a></h4>
<p>各Workerが<strong>自分専用のwriter側MessageQueue</strong>（<code>worker_response_mq</code>）を持ち、EngineCore側のMultiprocExecutorがそのreaderになる。rpc_broadcast_mq（1→多ブロードキャスト）とは逆方向の<strong>多→1通信</strong>だが、各MQは1 writer : 1 readerの構造。</p>
<pre><code>┌─ EngineCore (MultiprocExecutor) ─────────────────────────┐
│                                                           │
│  response_mqs[0] ◄── reader ─── worker_response_mq (W0)  │
│  response_mqs[1] ◄── reader ─── worker_response_mq (W1)  │
│                                                           │
│  ※ 各MQは独立したShmRingBuffer (n_reader=1, n_local=1)    │
└───────────────────────────────────────────────────────────┘
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:508-509</code> (Worker側: <code>MessageQueue(1, 1)</code>)
<strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:172-185</code> (Executor側: response_mqs構築)</p>
<h4 id="response_mq-のハンドシェイク"><a class="header" href="#response_mq-のハンドシェイク">response_mq のハンドシェイク</a></h4>
<ol>
<li>Worker側: <code>__init__</code>内で<code>MessageQueue(1, 1)</code>を生成（writer兼ShmRingBuffer所有者）</li>
<li>Worker側: READYメッセージと共に<code>worker_response_mq.export_handle()</code>をPipe経由でExecutor側に送信</li>
<li>Executor側: <code>wait_for_ready()</code>内でPipeからhandleを受信し、<code>MessageQueue.create_from_handle(handle, 0)</code>でreader側MQを構築</li>
<li>双方: <code>wait_until_ready()</code>でZMQ XPUB/SUBハンドシェイク完了</li>
</ol>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:757-770</code> (READY送信+ハンドシェイク)
<strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:628-646</code> (wait_for_response_handle_ready)</p>
<h4 id="結果返却の詳細フロー"><a class="header" href="#結果返却の詳細フロー">結果返却の詳細フロー</a></h4>
<pre><code>Worker.worker_busy_loop()
  │
  ├─ rpc_broadcast_mq.dequeue()  ← (method, args, kwargs, output_rank) を受信
  │
  ├─ func = getattr(self.worker, method)  ← "execute_model" 等
  │
  ├─ output = func(*args, **kwargs)  ← Worker.execute_model() 実行
  │
  ├─ if output_rank is None or self.rank == output_rank:
  │     ├─ [sync路] enqueue_output(output)
  │     │     ├─ isinstance(AsyncModelRunnerOutput) → .get_output()  ← GPU→CPU転送待ち
  │     │     ├─ isinstance(Exception) → (FAILURE, str(e))
  │     │     └─ else → (SUCCESS, output)
  │     │     └─ worker_response_mq.enqueue(result)
  │     │
  │     └─ [async路] async_output_queue.put(output)
  │           └─ async_output_busy_loop (別スレッド)
  │                 └─ enqueue_output(output)  ← 同上
  │
  └─ (output_rank != self.rank の場合は何も返さない)
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:845-871</code> (worker_busy_loop)
<strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:814-843</code> (enqueue_output / handle_output / async_output_busy_loop)</p>
<h4 id="output_rank-による結果フィルタリング"><a class="header" href="#output_rank-による結果フィルタリング">output_rank による結果フィルタリング</a></h4>
<p><code>collective_rpc</code>の呼び出し時に<code>output_rank</code>（<code>unique_reply_rank</code>パラメータ）を指定できる:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>output_rank</th><th>Worker側の動作</th><th>Executor側の動作</th></tr>
</thead>
<tbody>
<tr><td><code>None</code></td><td><strong>全Workerが</strong>結果をenqueue</td><td><strong>全response_mqsから</strong>dequeue → リスト返却</td></tr>
<tr><td><code>0</code></td><td><strong>rank 0のみ</strong>enqueue</td><td><strong>response_mqs[0]のみ</strong>dequeue → 単一値返却</td></tr>
<tr><td><code>N</code></td><td><strong>rank Nのみ</strong>enqueue</td><td><strong>response_mqs[N]のみ</strong>dequeue → 単一値返却</td></tr>
</tbody>
</table>
</div>
<p><code>execute_model()</code>は<code>unique_reply_rank=self.output_rank</code>（通常rank 0）で呼ばれるため、<strong>rank 0のWorkerのみが結果を返し</strong>、他のWorkerは結果を破棄する。これはTPモデルでは全Workerが同一の出力を計算するため、1つだけ返せば十分なため。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:270-275</code> (execute_model → unique_reply_rank)
<strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:339-341</code> (response_mqs フィルタリング)</p>
<h4 id="非同期スケジューリングasync_scheduling"><a class="header" href="#非同期スケジューリングasync_scheduling">非同期スケジューリング（async_scheduling）</a></h4>
<p><code>scheduler_config.async_scheduling=True</code>の場合、結果返却が非同期化される:</p>
<ol>
<li><code>worker_busy_loop</code>内の<code>handle_output()</code>が<code>async_output_queue</code>（<code>queue.Queue</code>）に出力を投入</li>
<li>別スレッド<code>async_output_busy_loop</code>（デーモンスレッド <code>WorkerAsyncOutputCopy</code>）がキューから取り出し</li>
<li><code>AsyncModelRunnerOutput.get_output()</code>でGPU→CPU非同期コピー完了を待機</li>
<li><code>worker_response_mq.enqueue()</code>で結果をEngineCore側に送信</li>
</ol>
<p>これにより、worker_busy_loopスレッドは<strong>GPU→CPUコピー完了を待たずに次のRPCを受信</strong>できる。GPU計算と結果転送をパイプライン化する仕組み。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:560-568</code> (async_output_copy_thread起動)
<strong>参照</strong>: <code>target/vllm/vllm/v1/outputs.py:200-209</code> (AsyncModelRunnerOutput)</p>
<h4 id="non_blockfuturewrapper"><a class="header" href="#non_blockfuturewrapper">non_block（FutureWrapper）</a></h4>
<p>Executor側の<code>collective_rpc(non_block=True)</code>では、response_mqからの結果取得を<strong>遅延評価</strong>する:</p>
<ol>
<li><code>get_response</code>クロージャを<code>FutureWrapper</code>に包んで即座に返す</li>
<li>次回の<code>collective_rpc</code>呼び出し時に、pending futuresを先にdrainする（<code>futures_queue</code>から順次pop→<code>wait_for_response</code>）</li>
<li>実際にresponse_mqから<code>dequeue()</code>するのはdrain時</li>
</ol>
<p>これにより、Executor側も結果待ちなしで次のRPCブロードキャストを発行でき、EngineCore.step()内のスケジューリングとWorkerの計算をオーバーラップできる。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:365-375</code> (non_block / FutureWrapper)</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:303-375</code> (collective_rpc)
<strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:845-871</code> (worker_busy_loop)</p>
<h3 id="23-worker--worker-torchdistributed--nccl"><a class="header" href="#23-worker--worker-torchdistributed--nccl">2.3 Worker ↔ Worker: torch.distributed + NCCL</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>項目</th><th>値</th></tr>
</thead>
<tbody>
<tr><td>初期化</td><td><code>torch.distributed.init_process_group(backend="nccl")</code></td></tr>
<tr><td>Rendezvous</td><td>TCP（<code>tcp://127.0.0.1:&lt;random_port&gt;</code>）</td></tr>
<tr><td>通信</td><td>NCCL（NVLink / PCIe によるGPU間直接通信）</td></tr>
<tr><td>用途</td><td>Tensor Parallelの<code>all_reduce()</code>, <code>all_gather()</code>, <code>broadcast()</code></td></tr>
<tr><td>タイミング</td><td>モデル forward pass 内部でレイヤーごとに実行</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_worker.py:263-269</code> (init_worker_distributed_environment)</p>
<p>NCCLの初期化は<code>Worker.init_device()</code>内で、メモリプロファイリング<strong>前</strong>に行われる。これによりNCCLバッファが確保された後の利用可能メモリが正確に計測される。</p>
<h2 id="3-起動シーケンス"><a class="header" href="#3-起動シーケンス">3. 起動シーケンス</a></h2>
<pre><code>1.  ユーザーが AsyncLLM を生成
2.  AsyncLLM → EngineCoreClient.make_async_mp_client()
3.    └─ mp.Process(target=EngineCoreProc.run_engine_core) 起動
4.        └─ EngineCore.__init__() 内で MultiprocExecutor 生成
5.            ├─ distributed_init_method = "tcp://127.0.0.1:&lt;port&gt;" 確保
6.            ├─ rpc_broadcast_mq (ShmRingBuffer, n_reader=2) 作成
7.            └─ for rank in [0, 1]:
8.                mp.Process(target=WorkerProc.worker_main) 起動
9.                  ├─ Worker.init_device():
10.                 │   └─ torch.distributed.init_process_group(backend="nccl")
11.                 ├─ Worker.load_model(): モデルロード
12.                 ├─ _init_message_queues():
13.                 │   ├─ rpc_broadcast_mq = create_from_handle(input_shm_handle, rank)
14.                 │   └─ worker_response_mq = MessageQueue(1, 1)  ← 各Worker独自
15.                 ├─ READY メッセージ + response_mq handle 送信（Pipe経由）
16.                 └─ wait_until_ready() → worker_busy_loop() でRPC待機開始
17.       └─ wait_for_ready():
18.            ├─ Pipeからhandle受信 → response_mqs[rank] 構築
19.            ├─ rpc_broadcast_mq.wait_until_ready()
20.            └─ 各response_mq.wait_until_ready()
21. Frontend ↔ EngineCore ZMQハンドシェイク完了
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:696</code> (WorkerProc.worker_main)
<strong>参照</strong>: <code>target/vllm/vllm/v1/executor/multiproc_executor.py:752-770</code> (READY送信)</p>
<h2 id="4-通信方式の設計判断"><a class="header" href="#4-通信方式の設計判断">4. 通信方式の設計判断</a></h2>
<h3 id="なぜ-frontend--enginecore-は-zmq-なのか"><a class="header" href="#なぜ-frontend--enginecore-は-zmq-なのか">なぜ Frontend ↔ EngineCore は ZMQ なのか</a></h3>
<ol>
<li><strong>疎結合</strong>: Data Parallelism構成では別ノードに配置される可能性がある。ZMQはネットワーク透過</li>
<li><strong>asyncio統合</strong>: Frontendはasyncioイベントループ上で動作し、ZMQのasyncioポーラーと相性がよい</li>
<li><strong>バックグラウンドスレッドでの直列化</strong>: msgpackシリアライゼーションをバックグラウンドスレッドで行い、GPU計算とオーバーラップ可能</li>
<li><strong>メッセージ順序保証</strong>: ROUTER/DEALERソケットで確定的なメッセージ順序を保証</li>
</ol>
<h3 id="なぜ-enginecore--workers-は-sharedmemory-mq-なのかzmqではない理由"><a class="header" href="#なぜ-enginecore--workers-は-sharedmemory-mq-なのかzmqではない理由">なぜ EngineCore ↔ Workers は SharedMemory MQ なのか（ZMQではない理由）</a></h3>
<ol>
<li><strong>低レイテンシ</strong>: 同一ノード内通信に特化。ZMQはネットワークソケット抽象であり、カーネル空間でのバッファコピーとシステムコールのオーバーヘッドがある</li>
<li><strong>ゼロコピー可能</strong>: 共有メモリ上でpickleデータを直接読み書きでき、プロセス間のデータコピーが不要</li>
<li><strong>ロックフリー設計</strong>: リングバッファ + メタデータフラグ + メモリフェンス（~20ns）で同期。ロック競合なし</li>
<li><strong>collective_rpc最適化</strong>: 1対多ブロードキャスト（rpc_broadcast_mq）パターンにリングバッファが最適</li>
</ol>
<h3 id="なぜ-worker--worker-は-nccl-なのか"><a class="header" href="#なぜ-worker--worker-は-nccl-なのか">なぜ Worker ↔ Worker は NCCL なのか</a></h3>
<ol>
<li><strong>GPU間テンソル通信専用</strong>: NCCLはGPUメモリ間の集合通信（all-reduce等）に特化した高性能ライブラリ</li>
<li><strong>NVLink活用</strong>: GPU間直接通信でCPU介在なし。NVLink（最大900GB/s）やPCIe（最大64GB/s）を直接利用</li>
<li><strong>PyTorch統合</strong>: モデルコード内の<code>torch.distributed</code>呼び出しと直接統合</li>
<li><strong>Pythonオブジェクト不可</strong>: NCCLはテンソル転送専用であり、Pythonオブジェクト（SchedulerOutput等）の転送には使えない</li>
</ol>
<h3 id="通信方式比較"><a class="header" href="#通信方式比較">通信方式比較</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>通信路</th><th>方式</th><th>レイテンシ</th><th>帯域幅</th><th>転送対象</th><th>ネットワーク透過</th></tr>
</thead>
<tbody>
<tr><td>Frontend ↔ EngineCore</td><td>ZMQ (TCP)</td><td>~µs</td><td>中</td><td>Pythonオブジェクト (msgpack)</td><td>Yes</td></tr>
<tr><td>EngineCore ↔ Workers</td><td>SharedMemory MQ</td><td>~20ns同期</td><td>高</td><td>Pythonオブジェクト (pickle)</td><td>No（同一ノード限定）</td></tr>
<tr><td>Worker ↔ Worker</td><td>NCCL</td><td>~µs</td><td>最高</td><td>GPUテンソルのみ</td><td>Yes（multi-node NCCL対応）</td></tr>
</tbody>
</table>
</div>
<h2 id="5-tp1単一gpuとの比較"><a class="header" href="#5-tp1単一gpuとの比較">5. TP=1（単一GPU）との比較</a></h2>
<p>TP=1の場合、<code>UniProcExecutor</code>が選択される:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>項目</th><th>TP=1</th><th>TP=2</th></tr>
</thead>
<tbody>
<tr><td>Executor</td><td>UniProcExecutor</td><td>MultiprocExecutor</td></tr>
<tr><td>Workerプロセス</td><td>なし（EngineCoreプロセス内）</td><td>2つの子プロセス</td></tr>
<tr><td>Worker通信</td><td>関数呼び出し（同一プロセス）</td><td>SharedMemory MQ</td></tr>
<tr><td>NCCL</td><td>不要</td><td>必要（Worker間）</td></tr>
<tr><td>合計プロセス数</td><td>2（Frontend + EngineCore）</td><td>4（Frontend + EngineCore + Worker×2）</td></tr>
</tbody>
</table>
</div>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/uniproc_executor.py:26</code> (UniProcExecutor)</p>
<h2 id="主要ファイル-10"><a class="header" href="#主要ファイル-10">主要ファイル</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>主要クラス/関数</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/v1/engine/async_llm.py</code></td><td><code>AsyncLLM</code> — Frontendプロセスのエントリポイント</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core_client.py</code></td><td><code>EngineCoreClient</code> — ZMQ通信, CoreEngineProcManager</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core.py</code></td><td><code>EngineCore</code>, <code>EngineCoreProc</code> — EngineCoreプロセスのエントリポイント</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/abstract.py</code></td><td><code>Executor</code> — collective_rpc(), execute_model()</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/multiproc_executor.py</code></td><td><code>MultiprocExecutor</code>, <code>WorkerProc</code> — Worker起動, MessageQueue管理, worker_busy_loop</td></tr>
<tr><td><code>target/vllm/vllm/v1/executor/uniproc_executor.py</code></td><td><code>UniProcExecutor</code> — 単一GPU用</td></tr>
<tr><td><code>target/vllm/vllm/v1/worker/gpu_worker.py</code></td><td><code>Worker</code> — init_device(), torch.distributed初期化</td></tr>
<tr><td><code>target/vllm/vllm/distributed/device_communicators/shm_broadcast.py</code></td><td><code>ShmRingBuffer</code>, <code>MessageQueue</code> — 共有メモリ通信基盤</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="zmq-通信パターンと信頼性分析"><a class="header" href="#zmq-通信パターンと信頼性分析">ZMQ 通信パターンと信頼性分析</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM]
<strong>確信度</strong>: [VERIFIED]
<strong>日付</strong>: 2026-02-18
<strong>きっかけ</strong>: vLLM全体のプロセス間通信基盤であるZMQの使用パターンを体系的に理解し、メッセージ喪失時の挙動を分析する</p>
</blockquote>
<h2 id="問い"><a class="header" href="#問い">問い</a></h2>
<ol>
<li>vLLMはZMQのどのソケットタイプ・通信パターンを使っているか？</li>
<li>ZMQにはネイティブな到達保証やリトライがないが、メッセージが喪失した場合はどうなるか？</li>
<li>vLLM側で信頼性を担保する仕組みはあるか？</li>
</ol>
<h2 id="zmq使用箇所の全体像"><a class="header" href="#zmq使用箇所の全体像">ZMQ使用箇所の全体像</a></h2>
<p>vLLM（v1）では16ファイルでZMQが使用されており、以下の5カテゴリに分類できる。</p>
<h3 id="カテゴリ一覧"><a class="header" href="#カテゴリ一覧">カテゴリ一覧</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>カテゴリ</th><th>ファイル数</th><th>トランスポート</th><th>用途</th></tr>
</thead>
<tbody>
<tr><td>Frontend↔EngineCore通信</td><td>5</td><td>IPC / TCP</td><td>コアのリクエスト/レスポンス通信</td></tr>
<tr><td>DP Coordinator</td><td>1</td><td>IPC / TCP</td><td>Data Parallel負荷分散・Wave調整</td></tr>
<tr><td>MessageQueue (ShmRingBuffer)</td><td>1</td><td>IPC</td><td>SharedMemoryオーバーフロー時のフォールバック</td></tr>
<tr><td>KV Cache Events</td><td>1</td><td>TCP / IPC</td><td>外部サービスへのKVイベント配信</td></tr>
<tr><td>KV Transfer コネクタ</td><td>8</td><td>TCP</td><td>ノード間KVキャッシュ転送の制御チャネル</td></tr>
</tbody>
</table>
</div>
<h2 id="1-frontendenginecore通信-verified"><a class="header" href="#1-frontendenginecore通信-verified">1. Frontend↔EngineCore通信 [VERIFIED]</a></h2>
<p>最も重要な通信パス。フロントエンド（AsyncLLM/LLM）とEngineCore間のリクエスト送信・レスポンス受信を担う。</p>
<h3 id="ソケット構成"><a class="header" href="#ソケット構成">ソケット構成</a></h3>
<pre><code>Frontend (MPClient)              EngineCore (EngineCoreProc)
┌─────────────────┐              ┌─────────────────┐
│  input_socket    │ ──ROUTER──► │  input_socket    │
│  (zmq.ROUTER,    │              │  (zmq.DEALER,    │
│   bind=True)     │              │   bind=False)    │
│                  │              │                  │
│  output_socket   │ ◄──PULL──── │  output_socket   │
│  (zmq.PULL,      │              │  (zmq.PUSH,      │
│   bind=False)    │              │   bind=True)     │ ← linger=4000ms
└─────────────────┘              └─────────────────┘
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core_client.py:510-514</code> (ソケット作成)
<strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:1199-1206</code> (EngineCore側input)
<strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:1286-1296</code> (EngineCore側output)</p>
<h3 id="パターン-routerdealer"><a class="header" href="#パターン-routerdealer">パターン: ROUTER/DEALER</a></h3>
<ul>
<li>
<p><strong>リクエスト送信</strong>: Frontend(ROUTER) → EngineCore(DEALER)</p>
<ul>
<li>ROUTERはidentityベースのルーティングを行う。DPモードでは複数EngineCoreへの振り分けに使用</li>
<li>EngineCoreのidentityはDP rankの2バイトリトルエンディアン表現</li>
<li>メッセージ形式: <code>(identity, request_type, serialized_data, [oob_buffers...])</code></li>
</ul>
</li>
<li>
<p><strong>レスポンス返却</strong>: EngineCore(PUSH) → Frontend(PULL)</p>
<ul>
<li>PUSH/PULLはunidirectionalで、identityルーティングなし</li>
<li>複数API serverがある場合、各API serverに別々のPUSH→PULLペア</li>
<li><code>client_index</code>で宛先のPUSHソケットを選択</li>
</ul>
</li>
</ul>
<h3 id="hwmhigh-water-mark設定"><a class="header" href="#hwmhigh-water-mark設定">HWM（High Water Mark）設定</a></h3>
<p><strong>参照</strong>: <code>target/vllm/vllm/utils/network_utils.py:260-313</code> (make_zmq_socket)</p>
<pre><code class="language-python"># PULL, DEALER, ROUTER
socket.setsockopt(zmq.RCVHWM, 0)  # 受信HWM無制限
socket.setsockopt(zmq.RCVBUF, buf_size)  # 0.5GB or system default

# PUSH, DEALER, ROUTER
socket.setsockopt(zmq.SNDHWM, 0)  # 送信HWM無制限
socket.setsockopt(zmq.SNDBUF, buf_size)  # 0.5GB or system default
</code></pre>
<p><strong>重要</strong>: HWMが0（無制限）に設定されているため、<strong>送信側でのメッセージドロップは発生しない</strong>。ZMQはHWMに達した場合にメッセージをドロップするが、HWM=0ではカーネルバッファが許す限りキューイングされる。</p>
<h3 id="ハンドシェイクプロトコル"><a class="header" href="#ハンドシェイクプロトコル">ハンドシェイクプロトコル</a></h3>
<p>起動時の3段階ハンドシェイク:</p>
<ol>
<li><strong>EngineCore→Frontend</strong>: DEALER→ROUTERで空メッセージ<code>b""</code>送信（ROUTER側がidentityを認識するため必須）</li>
<li><strong>EngineCore→Frontend</strong>: <code>"HELLO"</code>メッセージ送信（DP rank、local/remote情報）</li>
<li><strong>Frontend→EngineCore</strong>: <code>EngineHandshakeMetadata</code>返却（ZMQアドレス、parallel_config）</li>
<li><strong>EngineCore→Frontend</strong>: <code>"READY"</code>メッセージ送信（初期化完了、num_gpu_blocks報告）</li>
</ol>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/utils.py:937-1091</code> (wait_for_engine_startup)
<strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:870-920</code> (EngineCore側ハンドシェイク)</p>
<h3 id="ゼロコピー送信とmessagetracker"><a class="header" href="#ゼロコピー送信とmessagetracker">ゼロコピー送信とMessageTracker</a></h3>
<p>メッセージにテンソルのバッキングバッファが含まれる場合、<code>send_multipart(copy=False, track=True)</code>でゼロコピー送信を行う。<code>zmq.MessageTracker</code>でZMQがバッファを使い終わるまで参照を保持する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core_client.py:581-587</code> (pending_messages管理)
<strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:1322-1332</code> (output側のpending管理+バッファ再利用)</p>
<h2 id="2-dp-coordinator通信-verified"><a class="header" href="#2-dp-coordinator通信-verified">2. DP Coordinator通信 [VERIFIED]</a></h2>
<p>Data Parallel環境での負荷分散統計の集約・配信とWave調整を担う。</p>
<h3 id="ソケット構成-1"><a class="header" href="#ソケット構成-1">ソケット構成</a></h3>
<pre><code>Frontend(s)                DPCoordinator            EngineCore(s)
┌──────────┐              ┌──────────────┐          ┌──────────┐
│stats_upd │◄──XSUB────── │publish_front │          │          │
│(zmq.XSUB)│              │(zmq.XPUB,    │          │          │
│          │──────────────►│ bind=True)   │          │          │
│          │ 新リクエスト通知│              │          │          │
│          │              │              │          │          │
│          │              │output_back   │◄──PUSH── │coord_out │
│          │              │(zmq.PULL,    │          │(zmq.PUSH)│
│          │              │ bind=True)   │          │          │
│          │              │              │          │          │
│          │              │publish_back  │──XPUB──► │coord_in  │
│          │              │(zmq.XPUB,    │          │(zmq.XSUB)│
│          │              │ bind=True)   │          │          │
└──────────┘              └──────────────┘          └──────────┘
</code></pre>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/coordinator.py:113-395</code></p>
<h3 id="3つの通信チャネル"><a class="header" href="#3つの通信チャネル">3つの通信チャネル</a></h3>
<ol>
<li><strong>publish_front (XPUB)</strong>: Coordinator→Frontend。統計情報（各エンジンのwaiting/running数）とwave状態を配信</li>
<li><strong>output_back (PULL)</strong>: EngineCore→Coordinator。各エンジンのScheduler統計とwave完了通知</li>
<li><strong>publish_back (XPUB)</strong>: Coordinator→EngineCore。wave開始指示のブロードキャスト</li>
</ol>
<h3 id="xpubxsubパターン"><a class="header" href="#xpubxsubパターン">XPUB/XSUBパターン</a></h3>
<p>通常のPUB/SUBと異なり、XPUB/XSUBはサブスクリプションメッセージを可視化できる:</p>
<ul>
<li><strong>XPUB</strong>: subscription/unsubscriptionメッセージを受信可能 → 全サブスクライバの接続確認に使用</li>
<li><strong>XSUB</strong>: 明示的にsubscriptionメッセージを送信可能 → 動的なsubscribe制御に使用</li>
</ul>
<h2 id="3-messagequeue-shmringbuffer-zmqフォールバック-verified"><a class="header" href="#3-messagequeue-shmringbuffer-zmqフォールバック-verified">3. MessageQueue (ShmRingBuffer) ZMQフォールバック [VERIFIED]</a></h2>
<p>EngineCore↔Worker間のSharedMemory通信で、メッセージがShmRingBufferの最大チャンクサイズ（デフォルト24MiB）を超えた場合にZMQ PUB/SUBへフォールバックする。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/device_communicators/shm_broadcast.py:590-594</code></p>
<h3 id="オーバーフロー判定"><a class="header" href="#オーバーフロー判定">オーバーフロー判定</a></h3>
<pre><code class="language-python">if total_bytes + len(all_buffers[0]) &gt;= self.buffer.max_chunk_bytes:
    with self.acquire_write(timeout) as buf:
        buf[0] = 1  # overflow flag
    self.local_socket.send_multipart(all_buffers, copy=False)  # ZMQ XPUB
</code></pre>
<ol>
<li>ShmRingBufferのメタデータブロックに <code>overflow=1</code> フラグを書き込み</li>
<li>実データはXPUB→SUBソケット経由で送信</li>
<li>Reader側はメタデータのoverflowフラグを確認し、ZMQソケットから読み取る</li>
</ol>
<h3 id="ソケット構成-2"><a class="header" href="#ソケット構成-2">ソケット構成</a></h3>
<ul>
<li><strong>Writer</strong>: <code>XPUB</code> (bind、IPC)。ローカルリーダー向け + リモートリーダー向けの2つ</li>
<li><strong>Local Reader</strong>: <code>SUB</code> (connect、IPC)。ShmRingBufferとペア</li>
<li><strong>Remote Reader</strong>: <code>SUB</code> (connect、TCP)。ShmRingBufferなし、常にZMQ経由</li>
</ul>
<h3 id="接続確認"><a class="header" href="#接続確認">接続確認</a></h3>
<p><code>wait_until_ready()</code>で全リーダーのサブスクリプション受信→<code>b"READY"</code>送信で双方向の接続を確認後に通信開始。</p>
<h2 id="4-kv-cache-events-verified"><a class="header" href="#4-kv-cache-events-verified">4. KV Cache Events [VERIFIED]</a></h2>
<p>KVキャッシュの変更イベント（BlockStored/BlockRemoved/AllBlocksCleared）を外部サービスに配信する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_events.py:270-400</code></p>
<h3 id="ソケット構成-3"><a class="header" href="#ソケット構成-3">ソケット構成</a></h3>
<ul>
<li><strong>PUB</strong> (bind、TCP <code>tcp://*:5557</code>): イベントストリームの配信</li>
<li><strong>ROUTER</strong> (bind): リプレイリクエストの受け付け（過去イベントの再送要求用）</li>
</ul>
<h3 id="特徴"><a class="header" href="#特徴">特徴</a></h3>
<ul>
<li><strong>HWM設定あり</strong>: <code>set_hwm(100_000)</code> — PUBソケットにHWMが設定されている。サブスクライバが遅い場合、HWMを超えたメッセージはドロップされる</li>
<li><strong>シーケンス番号</strong>: 各イベントバッチにシーケンス番号を付与</li>
<li><strong>リプレイ機能</strong>: ROUTERソケットでリプレイリクエストを受け付け、バッファリングされた過去イベントを再送可能（deque、maxlen=10,000ステップ）</li>
<li><strong>バックグラウンドスレッド</strong>: パブリッシャーは専用スレッドで動作</li>
</ul>
<h2 id="5-kv-transfer-コネクタ-verified"><a class="header" href="#5-kv-transfer-コネクタ-verified">5. KV Transfer コネクタ [VERIFIED]</a></h2>
<p>ノード間KVキャッシュ転送の制御プレーンにZMQを使用。データプレーンは各コネクタ固有（RDMA、NCCL等）。</p>
<h3 id="共通パターン-routerdealer"><a class="header" href="#共通パターン-routerdealer">共通パターン: ROUTER/DEALER</a></h3>
<p>全コネクタで共通して ROUTER（サーバー側、bind）/ DEALER（クライアント側、connect）パターンを使用。</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>コネクタ</th><th>ZMQ用途</th><th>ソケットタイプ</th></tr>
</thead>
<tbody>
<tr><td>NIXL</td><td>メタデータハンドシェイク</td><td>ROUTER/REQ</td></tr>
<tr><td>P2P NCCL</td><td>転送要求・応答</td><td>ROUTER/DEALER</td></tr>
<tr><td>Mooncake</td><td>サイドチャネル通知</td><td>ROUTER/DEALER</td></tr>
<tr><td>MoRIIO</td><td>メタデータ交換・通知</td><td>ROUTER/DEALER</td></tr>
<tr><td>LMCache MP</td><td>LookupClient/Server通信</td><td>ZMQ経由（LMCache内部）</td></tr>
</tbody>
</table>
</div>
<h3 id="nixl特有-reqrep風ハンドシェイク"><a class="header" href="#nixl特有-reqrep風ハンドシェイク">NIXL特有: REQ/REP風ハンドシェイク</a></h3>
<p>NIXLはROUTER/REQパターンを使用し、RDMAメモリ登録のためのメタデータ交換を行う。<code>RCVTIMEO=5000ms</code>でタイムアウトを設定。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py:615-618</code> (ROUTER側)
<strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py:1062-1076</code> (REQ側)</p>
<h2 id="信頼性分析"><a class="header" href="#信頼性分析">信頼性分析</a></h2>
<h3 id="zmqの特性前提知識"><a class="header" href="#zmqの特性前提知識">ZMQの特性（前提知識）</a></h3>
<p>ZMQは<strong>メッセージ到達保証を提供しない</strong>メッセージングライブラリ:</p>
<ul>
<li>TCPの上に構築されているが、接続のライフサイクル管理は自動（再接続含む）</li>
<li><strong>PUB/SUB</strong>: サブスクライバが遅い場合、HWMを超えたメッセージはサイレントにドロップされる</li>
<li><strong>PUSH/PULL</strong>: HWMに達するとブロック（またはEAGAIN）</li>
<li><strong>ROUTER/DEALER</strong>: HWMに達するとROUTER側はメッセージをドロップ（DEALERはブロック）</li>
<li>IPC（同一ホスト内）はTCPより信頼性が高い（ネットワーク障害なし）</li>
</ul>
<h3 id="各通信パスの信頼性評価"><a class="header" href="#各通信パスの信頼性評価">各通信パスの信頼性評価</a></h3>
<h4 id="1-frontendenginecore最重要パス"><a class="header" href="#1-frontendenginecore最重要パス">1. Frontend↔EngineCore（最重要パス）</a></h4>
<p><strong>リスク</strong>: 低</p>
<p>理由:</p>
<ul>
<li><strong>HWM=0（無制限）</strong>: 送信側でのメッセージドロップは発生しない</li>
<li><strong>同一ホスト内IPC</strong>: ネットワーク障害のリスクなし（DP TCPモード除く）</li>
<li><strong>プロセス死活監視</strong>: <code>MPClientEngineMonitor</code>スレッドがEngineCoreプロセスのsentinelを監視。プロセス死亡→<code>engine_dead=True</code>→以降の操作は<code>EngineDeadError</code></li>
<li><strong>ENGINE_CORE_DEAD通知</strong>: EngineCore異常終了時にFrontendへ通知。<code>linger=4000ms</code>で送信完了を待つ</li>
<li><strong>validate_alive()</strong>: 受信メッセージがENGINE_CORE_DEADか毎回チェック</li>
</ul>
<p><strong>メッセージ喪失時の影響</strong>:</p>
<ul>
<li><strong>リクエスト喪失（Frontend→EngineCore）</strong>: リクエストが処理されず、クライアントがタイムアウトするまで待機。vLLM内部でのリトライ機構はない</li>
<li><strong>レスポンス喪失（EngineCore→Frontend）</strong>: リクエスト結果が返却されず、クライアントがタイムアウト。Schedulerのリクエストは残り続け、abort時に解放</li>
</ul>
<p><strong>実際にメッセージ喪失が起きうるか</strong>: IPCかつHWM=0の環境では、プロセスが正常動作している限りメッセージ喪失は極めて起きにくい。主なリスクはプロセスクラッシュ。</p>
<h4 id="2-dp-coordinator通信"><a class="header" href="#2-dp-coordinator通信">2. DP Coordinator通信</a></h4>
<p><strong>リスク</strong>: 低〜中</p>
<p>理由:</p>
<ul>
<li><strong>XPUBの統計配信はベストエフォート</strong>: 統計メッセージの一部が失われても負荷分散の精度が一時的に低下するだけ。定期的に再送されるため自己回復する</li>
<li><strong>Wave開始指示の喪失</strong>: エンジンが一時的にidle状態のまま留まる可能性がある。ただしフロントエンド側からの新リクエスト送信で再度waveが開始されるため、長時間のデッドロックにはならない</li>
<li><strong>接続確認</strong>: 全サブスクライバのsubscription受信を待ってから通信開始</li>
</ul>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/coordinator.py:189-198</code> (サブスクリプション待ち)</p>
<h4 id="3-shmringbufferフォールバック"><a class="header" href="#3-shmringbufferフォールバック">3. ShmRingBufferフォールバック</a></h4>
<p><strong>リスク</strong>: 低</p>
<p>理由:</p>
<ul>
<li><strong>XPUB/SUBだがHWM未設定（デフォルト1000）</strong>: 理論上、非常に大きなメッセージが連続すると滞留可能</li>
<li><strong>同一ホスト内IPC</strong>: ネットワーク障害なし</li>
<li><strong>頻度が低い</strong>: 24MiB超のメッセージは稀（通常のSchedulerOutput、ModelRunnerOutputは小さい）</li>
<li><strong>オーバーフロー自体がレアパス</strong>: 通常はShmRingBuffer直接で完結</li>
</ul>
<p><strong>メッセージ喪失時の影響</strong>: Worker側でRPCレスポンスが受信できず、EngineCore側でハングする可能性</p>
<h4 id="4-kv-cache-events"><a class="header" href="#4-kv-cache-events">4. KV Cache Events</a></h4>
<p><strong>リスク</strong>: 中（設計上許容）</p>
<p>理由:</p>
<ul>
<li><strong>PUBにHWM=100,000</strong>: サブスクライバが遅い場合、メッセージがドロップされる</li>
<li><strong>TCP経由</strong>: ネットワーク障害のリスクあり</li>
<li><strong>リプレイ機能で緩和</strong>: シーケンス番号ギャップを検出し、ROUTERソケット経由で過去イベントを再取得可能</li>
<li><strong>外部サービス向け</strong>: vLLMのコア動作には影響しない。あくまでKVイベントの外部通知</li>
</ul>
<p><strong>メッセージ喪失時の影響</strong>: 外部キャッシュマネージャがイベントを見逃す。リプレイで回復可能（バッファ範囲内）</p>
<h4 id="5-kv-transfer-コネクタ"><a class="header" href="#5-kv-transfer-コネクタ">5. KV Transfer コネクタ</a></h4>
<p><strong>リスク</strong>: 中</p>
<p>理由:</p>
<ul>
<li><strong>TCP経由（ノード間通信）</strong>: ネットワーク障害のリスクあり</li>
<li><strong>NIXL: RCVTIMEO設定あり</strong>: 1000msまたは5000msのタイムアウト。<code>zmq.Again</code>例外をキャッチしてリトライロジックを実装</li>
<li><strong>P2P NCCL</strong>: Pollerでの待機、明示的なタイムアウトなし（ブロッキング）</li>
<li><strong>Mooncake</strong>: DEALER側にlinger=0設定。タイムアウト付きで転送</li>
</ul>
<p><strong>メッセージ喪失時の影響</strong>:</p>
<ul>
<li>ハンドシェイク失敗→コネクタ初期化失敗→ログエラー、該当リクエストは通常の計算パスにフォールバック</li>
<li>転送通知失敗→送信側がブロックをタイムアウト解放→受信側はprefillを再実行</li>
</ul>
<h3 id="信頼性設計のまとめ"><a class="header" href="#信頼性設計のまとめ">信頼性設計のまとめ</a></h3>
<p>vLLMのZMQ使用における信頼性戦略:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>戦略</th><th>適用箇所</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><strong>HWM=0（無制限バッファ）</strong></td><td>Frontend↔EngineCore</td><td>メッセージドロップを完全に防止</td></tr>
<tr><td><strong>IPC優先</strong></td><td>同一ホスト内通信</td><td>ネットワーク障害を排除</td></tr>
<tr><td><strong>プロセス死活監視</strong></td><td>Frontend→EngineCore</td><td>sentinelによる即座のクラッシュ検出</td></tr>
<tr><td><strong>ENGINE_CORE_DEAD通知 + linger</strong></td><td>EngineCore→Frontend</td><td>異常終了の明示的通知を保証</td></tr>
<tr><td><strong>ハンドシェイク</strong></td><td>起動時</td><td>通信確立の確認後に運用開始</td></tr>
<tr><td><strong>リプレイ機能</strong></td><td>KV Events</td><td>メッセージドロップ後の回復手段</td></tr>
<tr><td><strong>タイムアウト + リトライ</strong></td><td>KV Transfer</td><td>ネットワーク障害時のフォールバック</td></tr>
<tr><td><strong>ベストエフォート + 自己回復</strong></td><td>DP Coordinator</td><td>統計は定期再送、waveは再トリガー</td></tr>
</tbody>
</table>
</div>
<p><strong>結論</strong>: vLLMは<strong>コア通信パスでは実質的にメッセージ喪失が起きない設計</strong>（HWM=0 + IPC + プロセス監視）を採用し、<strong>補助的な通信パスではベストエフォート + リカバリ機構</strong>（リプレイ、タイムアウト、再トリガー）で対処している。ZMQの「到達保証なし」の弱点は、使用パターンの選択（IPC、HWM=0）とアプリケーション層の監視で効果的に緩和されている。</p>
<h2 id="ソケットタイプ使用一覧"><a class="header" href="#ソケットタイプ使用一覧">ソケットタイプ使用一覧</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ソケットタイプ</th><th>使用箇所</th><th>方向</th><th>特徴</th></tr>
</thead>
<tbody>
<tr><td><code>ROUTER</code></td><td>Frontend input, NIXL server, P2P server, Mooncake server, MoRIIO server, KV Events replay, ハンドシェイク</td><td>bind（サーバー）</td><td>identityベースルーティング</td></tr>
<tr><td><code>DEALER</code></td><td>EngineCore input, NIXL client, P2P client, Mooncake client, MoRIIO client</td><td>connect（クライアント）</td><td>透過的なidentity送信</td></tr>
<tr><td><code>PUSH</code></td><td>EngineCore output, EngineCore→Coordinator</td><td>connect</td><td>単方向、ブロック型</td></tr>
<tr><td><code>PULL</code></td><td>Frontend output, Coordinator←EngineCore</td><td>bind</td><td>単方向、フェアキューイング</td></tr>
<tr><td><code>XPUB</code></td><td>Coordinator→Frontend, Coordinator→EngineCore, MessageQueue writer</td><td>bind</td><td>サブスクリプション可視化</td></tr>
<tr><td><code>XSUB</code></td><td>EngineCore←Coordinator, Frontend←Coordinator</td><td>connect</td><td>明示的サブスクリプション</td></tr>
<tr><td><code>SUB</code></td><td>MessageQueue reader</td><td>connect</td><td>自動サブスクリプション</td></tr>
<tr><td><code>PUB</code></td><td>KV Events</td><td>bind</td><td>ブロードキャスト、HWMドロップ</td></tr>
<tr><td><code>PAIR</code></td><td>Frontend内部（shutdown通知、first_req通知）</td><td>bind/connect</td><td>排他的1:1ペア</td></tr>
<tr><td><code>REQ</code></td><td>NIXL ハンドシェイクclient</td><td>connect</td><td>同期的リクエスト/レスポンス</td></tr>
</tbody>
</table>
</div>
<h2 id="参照"><a class="header" href="#参照">参照</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ファイル</th><th>行</th><th>内容</th></tr>
</thead>
<tbody>
<tr><td><code>target/vllm/vllm/v1/engine/core_client.py</code></td><td>L510-514</td><td>Frontend側ZMQソケット作成（ROUTER/PULL）</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core_client.py</code></td><td>L539-549</td><td>ROUTER初期メッセージ待ち（poll + タイムアウト）</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core_client.py</code></td><td>L581-587</td><td>MessageTracker管理（ゼロコピー参照保持）</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core_client.py</code></td><td>L684-720</td><td>SyncMPClientの出力処理スレッド（Poller + PAIR shutdown）</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core_client.py</code></td><td>L877-901</td><td>AsyncMPClientの出力処理タスク</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core_client.py</code></td><td>L1080-1186</td><td>DPClient統計購読（XSUB + PAIR first_req）</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core.py</code></td><td>L870-920</td><td>EngineCore側ハンドシェイク（DEALER→ROUTER）</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core.py</code></td><td>L1186-1265</td><td>EngineCore入力スレッド（DEALER + XSUB + Poller）</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/core.py</code></td><td>L1267-1335</td><td>EngineCore出力スレッド（PUSH, tracker, linger=4000）</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/coordinator.py</code></td><td>L113-395</td><td>DPCoordinator（XPUB×2 + PULL, Wave調整）</td></tr>
<tr><td><code>target/vllm/vllm/v1/engine/utils.py</code></td><td>L937-1091</td><td>ハンドシェイクプロトコル（HELLO→metadata→READY）</td></tr>
<tr><td><code>target/vllm/vllm/utils/network_utils.py</code></td><td>L260-313</td><td>make_zmq_socket（HWM=0, buf_size, IPv6対応）</td></tr>
<tr><td><code>target/vllm/vllm/distributed/device_communicators/shm_broadcast.py</code></td><td>L280-403</td><td>MessageQueue（XPUB/SUB, ShmRingBufferフォールバック）</td></tr>
<tr><td><code>target/vllm/vllm/distributed/device_communicators/shm_broadcast.py</code></td><td>L571-594</td><td>enqueue（オーバーフロー判定→ZMQ送信）</td></tr>
<tr><td><code>target/vllm/vllm/distributed/kv_events.py</code></td><td>L270-400</td><td>ZmqEventPublisher（PUB + ROUTER replay）</td></tr>
<tr><td><code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py</code></td><td>L615-618</td><td>NIXL ROUTER（RCVTIMEO=1000ms）</td></tr>
<tr><td><code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py</code></td><td>L124-130</td><td>P2P NCCL ROUTER/DEALER</td></tr>
</tbody>
</table>
</div>
<h2 id="関連ドキュメント-15"><a class="header" href="#関連ドキュメント-15">関連ドキュメント</a></h2>
<ul>
<li><a href="#プロセスアーキテクチャtp2構成">プロセスアーキテクチャ（TP=2構成）</a> — ShmRingBuffer、通信方式選択理由の詳細</li>
<li><a href="#executor">Executorコンポーネント</a> — MessageQueue、WorkerProc busy loop</li>
<li><a href="#enginecoreclient-サマリー">EngineCoreClientコンポーネント</a> — Frontend側の通信クライアント階層</li>
<li><a href="#kv-transfer-medium-verified">KV Transferコンポーネント</a> — KVConnectorBase_V1、各コネクタの概要</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="用語集"><a class="header" href="#用語集">用語集</a></h1>
<!-- 調査中に発見した対象OSS固有の用語をここに蓄積する -->
<h3 id="pagedattention-1"><a class="header" href="#pagedattention-1">PagedAttention</a></h3>
<p>KVキャッシュをOSの仮想メモリページングに着想を得て、固定サイズのブロック単位で管理する技術。連続したGPUメモリ確保が不要になり、メモリ断片化を大幅に抑制する。SOSP 2023論文で提案。</p>
<p><strong>参照</strong>: <code>target/vllm/csrc/attention/</code> (カーネル実装)</p>
<h3 id="continuous-batching-1"><a class="header" href="#continuous-batching-1">Continuous Batching</a></h3>
<p>リクエストの到着・完了に応じてバッチを動的に更新する手法。固定バッチサイズと異なり、GPU稼働率を最大化できる。vLLMのSchedulerが担う。</p>
<h3 id="prefill"><a class="header" href="#prefill">Prefill</a></h3>
<p>プロンプト入力トークン全体を処理してKVキャッシュに書き込む最初のフェーズ。計算量が多く、GPUの並列性を活かしやすい。</p>
<h3 id="decode"><a class="header" href="#decode">Decode</a></h3>
<p>生成済みコンテキストのKVキャッシュを参照しながら次のトークンを1つずつ逐次生成するフェーズ。メモリバウンドになりやすい。</p>
<h3 id="chunked-prefill"><a class="header" href="#chunked-prefill">Chunked Prefill</a></h3>
<p>Prefillフェーズをチャンクに分割してDecodeフェーズと交互実行する手法。長いプロンプトがDecodeのレイテンシを増加させるのを防ぐ。</p>
<h3 id="enginecore"><a class="header" href="#enginecore">EngineCore</a></h3>
<p>vLLMの推論ループの内側コンポーネント。別プロセス（<code>EngineCoreProc</code>）として動作し、ZeroMQソケットで上位エンジン層と通信する。Scheduler、KVCacheManager、Executorを統括する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/core.py:79</code> (<code>EngineCore</code>)</p>
<h3 id="kvcachemanager"><a class="header" href="#kvcachemanager">KVCacheManager</a></h3>
<p>KVキャッシュブロックの割り当て・解放・プレフィックスキャッシュを管理するクラス。BlockPoolを内部で使用する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_manager.py:94</code> (<code>KVCacheManager</code>)</p>
<h3 id="kvcacheblock"><a class="header" href="#kvcacheblock">KVCacheBlock</a></h3>
<p>PagedAttentionで管理するKVキャッシュの最小単位。固定サイズ（block_sizeトークン分）のGPUメモリブロック。</p>
<h3 id="blockpool"><a class="header" href="#blockpool">BlockPool</a></h3>
<p>KVCacheBlockの空きブロックをプール管理するクラス。ブロックの割り当て・返却を効率的に行う。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py</code></p>
<h3 id="vllmconfig"><a class="header" href="#vllmconfig">VllmConfig</a></h3>
<p>全設定を集約するトップレベルクラス。<code>ModelConfig</code>、<code>CacheConfig</code>、<code>SchedulerConfig</code>、<code>ParallelConfig</code>等を内包する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/config/vllm.py</code></p>
<h3 id="gpumodelrunner-1"><a class="header" href="#gpumodelrunner-1">GPUModelRunner</a></h3>
<p>GPU上でモデルのフォワードパスを実際に実行するクラス。LoRA、KVConnector、ECConnectorのMixinを持つ。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:329</code> (<code>GPUModelRunner</code>)</p>
<h3 id="executor-1"><a class="header" href="#executor-1">Executor</a></h3>
<p>Worker群を管理する抽象層。シングルプロセス（<code>UniProcExecutor</code>）、マルチプロセス（<code>MultiprocExecutor</code>）、Ray分散（<code>RayDistributedExecutor</code>）の実装がある。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py</code></p>
<h3 id="worker"><a class="header" href="#worker">Worker</a></h3>
<p>1つのGPU（またはCPU/XPU）デバイスを担当するプロセス。GPUModelRunnerを保持し、Executorから呼び出される。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_worker.py:70</code> (<code>Worker</code>)</p>
<h3 id="speculative-decoding"><a class="header" href="#speculative-decoding">Speculative Decoding</a></h3>
<p>ドラフトモデル（小さいモデル）で複数トークンを仮生成し、メインモデルで一括検証することで推論を高速化する手法。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/spec_decode/</code></p>
<h3 id="lora-low-rank-adaptation"><a class="header" href="#lora-low-rank-adaptation">LoRA (Low-Rank Adaptation)</a></h3>
<p>少量の追加パラメータでLLMをファインチューニングする手法。vLLMは複数LoRAの動的切替（Multi-LoRA）をランタイムでサポートする。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/lora/</code></p>
<h3 id="kv-transfer"><a class="header" href="#kv-transfer">KV Transfer</a></h3>
<p>複数のvLLMインスタンス間またはストレージ間でデコーダKVキャッシュを転送するプラグインフレームワーク。KVConnectorBase_V1抽象基底クラス（7 abstractメソッド）と KVConnectorFactory（10個の登録済みコネクタ）で構成。Scheduler側（外部キャッシュ問い合わせ）とWorker側（レイヤー別非同期ロード/セーブ）の2ロール分離。ECConnector（エンコーダキャッシュ用）とは完全に独立した系統。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/base.py:147</code> (KVConnectorBase_V1), <code>docs/src/components/kv-transfer/summary.md</code></p>
<h3 id="kvconnectorbase_v1"><a class="header" href="#kvconnectorbase_v1">KVConnectorBase_V1</a></h3>
<p>KV Transferのプラグイン基底クラス。Worker側4メソッド（start_load_kv, wait_for_layer_load, save_kv_layer, wait_for_save）とScheduler側3メソッド（get_num_new_matched_tokens, update_state_after_alloc, build_connector_meta）を定義する。KVConnectorMetadataでScheduler→Worker間の通信を行う。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/base.py:147</code></p>
<h3 id="kvconnectorfactory-1"><a class="header" href="#kvconnectorfactory-1">KVConnectorFactory</a></h3>
<p>KVコネクタの登録・発見・生成を行うファクトリクラス。遅延ロードパターン（module_path + class_name）で10個のコネクタを登録。<code>kv_connector_module_path</code>設定で動的ロードも可能。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/factory.py:27</code></p>
<h3 id="kvtransferconfig-1"><a class="header" href="#kvtransferconfig-1">KVTransferConfig</a></h3>
<p>KV Transferの設定クラス。kv_connector（コネクタ名）、kv_role（producer/consumer/both）、kv_connector_extra_config（コネクタ固有設定）等を保持。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/config/kv_transfer.py:17</code></p>
<h3 id="kvconnectormodelrunnermixin-1"><a class="header" href="#kvconnectormodelrunnermixin-1">KVConnectorModelRunnerMixin</a></h3>
<p>GPUModelRunnerにミックスインされるKVコネクタ統合クラス。<code>_get_kv_connector_output()</code>コンテキストマネージャでbind→start_load→yield→wait_for_save→get_finished→clearのライフサイクルを管理する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/kv_connector_model_runner_mixin.py:40</code></p>
<h3 id="lmcache"><a class="header" href="#lmcache">LMCache</a></h3>
<p>vLLMと統合可能な外部KVキャッシュライブラリ。チャンク単位（デフォルト256トークン）でKVキャッシュを保存し、3層ストレージ階層（CPU→Disk→Remote）を持つ。15+のリモートコネクタ（Redis, S3, FS等）をサポート。Disaggregated Serving（P/D分離）にも対応。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py:72</code>, <code>docs/src/investigations/lmcache-integration.md</code></p>
<h3 id="disaggregated-prefill-pd分離"><a class="header" href="#disaggregated-prefill-pd分離">Disaggregated Prefill (P/D分離)</a></h3>
<p>PrefillフェーズとDecodeフェーズを異なるvLLMインスタンスで実行するアーキテクチャ。Producerインスタンスがプリフィルを実行してKVキャッシュをKV Transfer経由で転送し、ConsumerインスタンスがKVキャッシュをロードしてデコードのみ実行する。</p>
<h3 id="waiting_for_remote_kvs"><a class="header" href="#waiting_for_remote_kvs">WAITING_FOR_REMOTE_KVS</a></h3>
<p>KV Transferの非同期ロード中のリクエスト状態。Schedulerがブロックを割り当て後、Worker側のKVロード完了を待つ間この状態に置かれる。Worker側コネクタの<code>get_finished()</code>で受信完了が報告されると、WAITING状態に戻されて次のスケジューリングサイクルでRUNNINGに昇格する。</p>
<h3 id="kv-cache-events-1"><a class="header" href="#kv-cache-events-1">KV Cache Events</a></h3>
<p>KVキャッシュのブロック保存・削除を外部システムに通知するイベントシステム。BlockStored、BlockRemoved、AllBlocksClearedの3イベント型。KVEventAggregatorで全Worker共通イベントを集約し、EventPublisher（ZMQ等）で配信。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/distributed/kv_events.py:49-84</code></p>
<h3 id="cacheenginekey-lmcache"><a class="header" href="#cacheenginekey-lmcache">CacheEngineKey (LMCache)</a></h3>
<p>LMCacheのチャンクを一意に識別するキー。model_name、world_size、worker_id、chunk_hash（トークン列のプレフィックスハッシュ）、dtype、タグで構成。</p>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/utils.py:330</code></p>
<h3 id="multimodal-マルチモーダル"><a class="header" href="#multimodal-マルチモーダル">Multimodal (マルチモーダル)</a></h3>
<p>テキスト以外の入力（画像・動画・音声）を扱うモデル機能。<code>vllm/multimodal/</code> にプロセッサ・レジストリ等が実装されている。Gemma3等のマルチモーダルモデルが対応。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/</code></p>
<h3 id="unified-compute-model-1"><a class="header" href="#unified-compute-model-1">Unified Compute Model</a></h3>
<p>vLLM v1のSchedulerが採用するスケジューリングアプローチ。PrefillフェーズとDecodeフェーズを明示的に区別せず、各リクエストの<code>num_computed_tokens</code>（計算済みトークン数）が目標に追いつくまでトークンを割り当てる。これにより、Chunked Prefill、Prefix Caching、Speculative Decodingを統一的に扱える。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/sched/scheduler.py:322</code> (コメント)</p>
<h3 id="collective_rpc-1"><a class="header" href="#collective_rpc-1">collective_rpc</a></h3>
<p>Executor層が全Workerに対して同一メソッドを実行するRPCパターン。メソッド名（文字列）または関数を受け取り、全Workerで並列実行後、出力ランクのWorkerの結果を返す。<code>non_block=True</code>でFuture返却も可能。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/executor/abstract.py:180</code> (<code>collective_rpc</code>)</p>
<h3 id="executemodelstate-1"><a class="header" href="#executemodelstate-1">ExecuteModelState</a></h3>
<p>GPUModelRunnerの2フェーズ実行パターンで使用される一時状態。execute_model()がlogitsやhidden_statesなどのGPUテンソルを保存し、sample_tokens()が復元してサンプリングを行う。NamedTuple。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu_model_runner.py:313</code> (<code>ExecuteModelState</code>)</p>
<h3 id="outputprocessor-1"><a class="header" href="#outputprocessor-1">OutputProcessor</a></h3>
<p>フロントエンドプロセスで動作し、EngineCoreOutputをRequestOutputに変換するコンポーネント。インクリメンタルデトークナイズ、停止文字列判定、logprobs処理を行う。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/output_processor.py:73</code> (<code>OutputProcessor</code>)</p>
<h3 id="incrementaldetokenizer"><a class="header" href="#incrementaldetokenizer">IncrementalDetokenizer</a></h3>
<p>トークンIDからテキストへのインクリメンタル変換を行うクラス。FastIncrementalDetokenizer（HF DecodeStream）とSlowIncrementalDetokenizer（Python実装）の2種がある。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/engine/detokenizer.py:30</code> (<code>IncrementalDetokenizer</code>)</p>
<h3 id="requestoutputkind"><a class="header" href="#requestoutputkind">RequestOutputKind</a></h3>
<p>出力モードを定義するEnum。CUMULATIVE（毎回全出力）、DELTA（差分のみ、ストリーミング向け）、FINAL_ONLY（完了時のみ）の3値。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/sampling_params.py:108</code> (<code>RequestOutputKind</code>)</p>
<h3 id="mm_cache-マルチモーダルキャッシュ"><a class="header" href="#mm_cache-マルチモーダルキャッシュ">mm_cache (マルチモーダルキャッシュ)</a></h3>
<p>マルチモーダル入力（画像エンコーダ出力等）のキャッシュ機構。同一画像の繰り返し処理を避けるため、エンコーダ出力をキャッシュする。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/worker/gpu/mm/encoder_runner.py</code></p>
<h3 id="freekvcacheblockqueue"><a class="header" href="#freekvcacheblockqueue">FreeKVCacheBlockQueue</a></h3>
<p>空きKVキャッシュブロックをLRU順序で管理する双方向リンクリスト。Pythonの<code>deque</code>ではなく独自実装を採用し、O(1)の中間要素削除をサポートする。センチネルノード（fake_head/fake_tail）を使用。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:156</code></p>
<h3 id="blockhashwithgroupid"><a class="header" href="#blockhashwithgroupid">BlockHashWithGroupId</a></h3>
<p><code>BlockHash</code>（ブロックのハッシュ値）にKVキャッシュグループID（4バイトBE）を結合したバイト列。Tuple生成を避けてGC負荷を低減する。プレフィックスキャッシュのキーとして使用。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_utils.py:39</code></p>
<h3 id="null_block"><a class="header" href="#null_block">null_block</a></h3>
<p>BlockPoolが保持する特殊なKVCacheBlock（block_id=0, is_null=True）。Sliding Window Attentionのウィンドウ外位置やMambaのスキップ位置を埋めるプレースホルダ。物理メモリを消費せず、解放・Eviction対象外。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/block_pool.py:174</code></p>
<h3 id="kvcachecoordinator"><a class="header" href="#kvcachecoordinator">KVCacheCoordinator</a></h3>
<p>複数のKVキャッシュグループ（異なるアテンションタイプのレイヤー群）を統括する抽象クラス。NoPrefixCache、Unitary（単一グループ）、Hybrid（複数グループ）の3実装がある。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/kv_cache_coordinator.py:28</code></p>
<h3 id="singletypekvcachemanager"><a class="header" href="#singletypekvcachemanager">SingleTypeKVCacheManager</a></h3>
<p>1種類のアテンションタイプのKVキャッシュ管理ロジックを担当する抽象基底クラス。FullAttention、SlidingWindow、ChunkedLocalAttention、Mamba、CrossAttention、SinkFullAttentionの7実装がある。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/single_type_kv_cache_manager.py:24</code></p>
<h3 id="cascade-attention"><a class="header" href="#cascade-attention">Cascade Attention</a></h3>
<p>全リクエストで共有される共通プレフィックスの再計算をスキップする最適化。<code>get_num_common_prefix_blocks()</code>で共通ブロック数を判定し、アテンション計算から除外する。</p>
<h3 id="sliding-window-attention"><a class="header" href="#sliding-window-attention">Sliding Window Attention</a></h3>
<p>各トークンが直近のN個のトークンにのみアテンションするメカニズム。ウィンドウ外のKVキャッシュブロックは<code>null_block</code>で置換されメモリを節約する。</p>
<h3 id="attention-sink-streamingllm"><a class="header" href="#attention-sink-streamingllm">Attention Sink (StreamingLLM)</a></h3>
<p>先頭の少数トークン（sink tokens）のKVキャッシュを常に保持しつつ、中間トークンを捨てて長いシーケンスを処理する手法。<code>SinkFullAttentionManager</code>が実装。</p>
<h3 id="multimodalfeaturespec"><a class="header" href="#multimodalfeaturespec">MultiModalFeatureSpec</a></h3>
<p>マルチモーダル入力1つ分のメタデータとテンソルデータを保持するデータクラス。<code>data</code>（処理済みテンソル、キャッシュヒット時はNone）、<code>identifier</code>（エンコーダキャッシュ用ハッシュ）、<code>mm_position</code>（PlaceholderRange）、<code>modality</code>（“image“等）を含む。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/inputs.py:337</code></p>
<h3 id="placeholderrange"><a class="header" href="#placeholderrange">PlaceholderRange</a></h3>
<p>プロンプト内のマルチモーダルプレースホルダーの位置情報。<code>offset</code>（開始位置）、<code>length</code>（トークン数）、<code>is_embed</code>（埋め込みマスク）を保持。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/inputs.py:170</code></p>
<h3 id="multimodalhasher"><a class="header" href="#multimodalhasher">MultiModalHasher</a></h3>
<p>マルチモーダルデータのコンテンツベースハッシュを計算するクラス。PIL Image、Tensor、ndarray等を決定的にシリアライズし、blake3（デフォルト）でハッシュ化する。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/hasher.py:50</code></p>
<h3 id="processorcache-mm"><a class="header" href="#processorcache-mm">ProcessorCache (MM)</a></h3>
<p>フロントエンド（P0）でHF Processor処理結果をキャッシュする仕組み。4種類の実装（processor_only/lru/shm/none）があり、P0-P1間のキャッシュEviction順序を同期させることでIPCなしにキャッシュ状態を推定できる。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/multimodal/cache.py</code></p>
<h3 id="encodercachemanager"><a class="header" href="#encodercachemanager">EncoderCacheManager</a></h3>
<p>バックエンド（P1）でビジョンエンコーダ出力のライフサイクルを管理するクラス。リファレンスカウント方式で複数リクエスト間のキャッシュ共有を実現し、FIFO順の遅延Evictionを行う。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/v1/core/encoder_cache_manager.py:17</code></p>
<h3 id="siglipvisionmodel"><a class="header" href="#siglipvisionmodel">SiglipVisionModel</a></h3>
<p>SIGLIP（Sigmoid Loss for Language Image Pre-training）ベースのViTビジョンエンコーダ。Gemma3のビジョンタワーとして使用される。パッチ埋め込み + 位置埋め込み → Transformer Encoder（双方向Attention）の構造。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/siglip.py:848</code></p>
<h3 id="pan-and-scan"><a class="header" href="#pan-and-scan">Pan-and-Scan</a></h3>
<p>アスペクト比が大きい画像を複数のクロップに分割して詳細認識を向上させるGemma3の仕組み。V1では簡略化されたアテンションパターンのため最適でない結果になりうる。</p>
<p><strong>参照</strong>: <code>target/vllm/vllm/model_executor/models/gemma3_mm.py:109</code></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ファイル索引"><a class="header" href="#ファイル索引">ファイル索引</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="lmcache-1"><a class="header" href="#lmcache-1">LMCache</a></h1>
<p>KVキャッシュの保存・共有・再利用ライブラリ <a href="https://github.com/LMCache/LMCache">LMCache</a> のコードリーディング。</p>
<ul>
<li><strong>ソースコード</strong>: <code>target/LMCache/</code></li>
<li><strong>現在のPhase</strong>: Phase 0a（オリエンテーション）</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="データフロー-1"><a class="header" href="#データフロー-1">データフロー</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM] / <strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-16（Phase 1 セッション2）</p>
</blockquote>
<p>本ドキュメントはLMCacheのKVキャッシュ store / retrieve パスを追跡する。</p>
<h2 id="store-パス概要"><a class="header" href="#store-パス概要">Store パス概要</a></h2>
<p>vLLMのattentionレイヤー実行中に、KVキャッシュをGPUからCPU（および後段ストレージ）に退避するパス。
<strong>レイヤーワイズ方式</strong>（<code>use_layerwise=True</code>）が主要パスであり、各attentionレイヤーの実行直後にそのレイヤーのKVデータを転送する。</p>
<pre class="mermaid">sequenceDiagram
    participant vLLM as vLLM Attention Layer
    participant Adapter as V1Impl (adapter)
    participant Engine as LMCacheEngine
    participant TDB as TokenDatabase
    participant GPU as GPUConnector
    participant SM as StorageManager
    participant CPU as LocalCPUBackend

    Note over vLLM,Adapter: Layer 0 開始
    vLLM-&gt;&gt;Adapter: save_kv_layer(layer_name, kv_layer, attn_metadata)

    Note over Adapter: layer==0 のとき、各リクエストに対して Generator 生成
    Adapter-&gt;&gt;Engine: store_layer(token_ids, mask, kvcaches, slot_mapping, ...)
    activate Engine
    Engine-&gt;&gt;TDB: process_tokens(tokens, mask)
    TDB--&gt;&gt;Engine: [(start, end, CacheEngineKey), ...]
    Engine-&gt;&gt;SM: batched_allocate(shape, dtype, batch_size=num_layers)
    SM--&gt;&gt;Engine: List[MemoryObj] × num_layers
    Engine-&gt;&gt;GPU: batched_from_gpu(memory_objs, starts, ends, ...)
    Note over GPU: GPU Generator 初期化
    GPU--&gt;&gt;Engine: Generator (primed)
    Note over Engine: yield (Layer 0 の DMA 準備完了)
    deactivate Engine

    Adapter-&gt;&gt;Engine: next(generator) — Layer 0
    activate Engine
    Engine-&gt;&gt;GPU: next(mem_obj_generator)
    Note over GPU: lmc_ops.single_layer_kv_transfer&lt;br/&gt;(paged GPU → buffer → pinned CPU)
    GPU--&gt;&gt;Engine: yield
    Engine-&gt;&gt;SM: batched_put(keys[0], memory_objs[0])
    SM-&gt;&gt;CPU: batched_submit_put_task(keys, objs)
    Note over CPU: hot_cache[key] = memory_obj
    Note over Engine: yield (Layer 0 完了)
    deactivate Engine

    Note over vLLM,Adapter: Layer 1 以降も同じパターン繰り返し
    vLLM-&gt;&gt;Adapter: save_kv_layer(...)
    Adapter-&gt;&gt;Engine: next(generator) — Layer N
    Note over Engine: GPU転送 → StorageManager.batched_put
</pre>

<h2 id="各コンポーネントの役割"><a class="header" href="#各コンポーネントの役割">各コンポーネントの役割</a></h2>
<h3 id="1-lmcacheconnectorv1impladapter"><a class="header" href="#1-lmcacheconnectorv1impladapter">1. LMCacheConnectorV1Impl（adapter）</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/integration/vllm/vllm_v1_adapter.py:964</code> (<code>save_kv_layer</code>)</p>
<p>vLLMの<code>KVConnectorBase_V1.save_kv_layer()</code>フックから呼ばれるアダプタ。
<code>LMCacheConnectorV1Dynamic</code>は純粋な委譲シェルであり、実装は<code>V1Impl</code>に集約。</p>
<p><strong>Layer 0での処理</strong>:</p>
<ol>
<li><code>connector_metadata.requests</code>を走査し、<code>save_spec.can_save</code>がTrueのリクエストを処理</li>
<li><code>skip_leading_tokens</code>をLMCacheのchunk_size（256）の倍数に切り下げてマスク境界を整合</li>
<li><code>store_mask</code>を構築：プレフィックス部分=False、新規部分=True</li>
<li><code>LMCacheEngine.store_layer()</code>を呼んでGeneratorを取得、<code>self.layerwise_storers</code>に追加</li>
<li>最初のリクエストのみ<code>sync=True</code>でCUDAストリームを同期</li>
</ol>
<p><strong>全レイヤー共通</strong>: <code>self.layerwise_storers</code>内の全Generatorを<code>next()</code>で1ステップ進める。</p>
<h3 id="2-lmcacheenginestore_layer"><a class="header" href="#2-lmcacheenginestore_layer">2. LMCacheEngine.store_layer()</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/cache_engine.py:528</code></p>
<p><strong>Generator関数</strong>であり、呼び出し側（adapter）が1レイヤーごとに<code>next()</code>で進める。</p>
<p><strong>初期化フェーズ</strong>（最初のyieldまで）:</p>
<ol>
<li><code>TokenDatabase.process_tokens()</code>でトークン列をチャンク分割し、各チャンクのCacheEngineKeyを取得</li>
<li><code>StorageManager.contains()</code>で既存チャンクをスキップ（layer 0のキーで判定）</li>
<li><code>StorageManager.batched_allocate()</code>で各チャンク×全レイヤー分のMemoryObjを確保</li>
<li>チャンク×レイヤー → レイヤー×チャンクに転置</li>
<li><code>GPUConnector.batched_from_gpu()</code>でGPU転送Generatorを生成・prime</li>
</ol>
<p><strong>レイヤーループ</strong>（<code>num_layers</code>回yield）:</p>
<pre><code>yield → next(mem_obj_generator) → batched_put(keys[layer_id], memory_objs[layer_id])
</code></pre>
<p>各レイヤーで「GPU→CPU DMA」→「ストレージ書き込み」を実行。</p>
<p><strong>重要</strong>: メモリ確保失敗時（<code>batched_allocate</code>がNone）は<code>break</code>で即座にstore中止。yieldだけ行ってストレージには書かない。</p>
<h3 id="3-chunkedtokendatabaseprocess_tokens"><a class="header" href="#3-chunkedtokendatabaseprocess_tokens">3. ChunkedTokenDatabase.process_tokens()</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/token_database.py:309</code></p>
<p>トークン列をチャンク（デフォルト256トークン）に分割し、プレフィックスチェーンハッシュを計算。</p>
<p><strong>ハッシュアルゴリズム</strong>: vLLMと<strong>完全に同一</strong>。</p>
<ul>
<li><code>vllm.utils.hashing.get_hash_fn_by_name("sha256_cbor")</code>を直接利用</li>
<li>NONE_HASHも<code>vllm.v1.core.kv_cache_utils.NONE_HASH</code>から取得</li>
<li>ハッシュ入力: <code>(prefix_hash, token_tuple, extra_keys)</code></li>
</ul>
<p><strong>マスク処理</strong>: <code>mask</code>のFalse数（=already-cached prefix長）がchunk_sizeの倍数であることを検証。False区間のチャンクはスキップ。</p>
<p><strong>CacheEngineKey生成</strong>: <code>_make_key_by_hash()</code>で<code>(model_name, world_size, worker_id, chunk_hash, kv_dtype, request_configs)</code>の6タプルを構築。その後<code>split_layers()</code>でレイヤーIDを付与した<code>LayerCacheEngineKey</code>に分割。</p>
<h3 id="4-vllmpagedmemlayerwisegpuconnectorbatched_from_gpu"><a class="header" href="#4-vllmpagedmemlayerwisegpuconnectorbatched_from_gpu">4. VLLMPagedMemLayerwiseGPUConnector.batched_from_gpu()</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/gpu_connector/gpu_connectors.py:1212</code></p>
<p>GPU上のページドKVキャッシュからCPU上のMemoryObjにデータを転送するGenerator関数。</p>
<p><strong>2段転送パス</strong>（<code>use_gpu=True</code>時）:</p>
<ol>
<li><strong>Paged GPU → 中間GPUバッファ</strong>: <code>lmc_ops.single_layer_kv_transfer()</code>（CUDAカーネル）でslot_mappingに基づきscatter→gatherコピー</li>
<li><strong>GPUバッファ → Pinned CPU</strong>: <code>memory_obj.tensor.copy_(..., non_blocking=True)</code>で非同期DMA</li>
</ol>
<p><strong>直接転送パス</strong>（<code>use_gpu=False</code>時）:</p>
<ul>
<li><code>lmc_ops.single_layer_kv_transfer()</code>でpaged GPUから直接pinned CPUへ（チャンク単位）</li>
</ul>
<p><strong>CUDAストリーム</strong>: <code>self.store_stream</code>（専用ストリーム）を使用し、計算ストリームとオーバーラップ可能。<code>sync=True</code>の場合のみ<code>store_stream.synchronize()</code>で同期。</p>
<p><strong>出力形式</strong>: <code>MemoryFormat.KV_T2D</code> = <code>[num_tokens, 2, hidden_dim]</code>（token-major、K/Vインターリーブ）。MLAの場合は<code>KV_MLA_FMT</code> = <code>[num_tokens, hidden_dim]</code>。</p>
<h3 id="5-storagemanagerbatched_put"><a class="header" href="#5-storagemanagerbatched_put">5. StorageManager.batched_put()</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/storage_manager.py:388</code></p>
<p>登録された全ストレージバックエンドにデータを配布するディスパッチャ。</p>
<p><strong>処理フロー</strong>:</p>
<ol>
<li><code>allocator_backend</code>（通常LocalCPUBackend）の元データをそのまま利用</li>
<li><code>OrderedDict</code>順に全バックエンド（L1→L2→L3）を走査</li>
<li>異なるallocatorを持つバックエンドには<code>allocate_and_copy_objects()</code>で新たにメモリ確保＋コピー</li>
<li>各バックエンドの<code>batched_submit_put_task()</code>を呼び出し</li>
<li>最後にref_countをデクリメント</li>
</ol>
<p><strong>注意</strong>: <code>put()</code>メソッドは<strong>非推奨</strong>（<code>RuntimeError</code>を投げる）。<code>batched_put()</code>が唯一のエントリポイント。</p>
<h3 id="6-localcpubackendsubmit_put_task"><a class="header" href="#6-localcpubackendsubmit_put_task">6. LocalCPUBackend.submit_put_task()</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/local_cpu_backend.py:141</code></p>
<p><strong>同期実行</strong>（バックグラウンドスレッドなし）。<code>cpu_lock</code>下で以下を実行:</p>
<ol>
<li>既存キーの重複チェック</li>
<li><code>memory_obj.ref_count_up()</code>でrefcount増加</li>
<li><code>hot_cache[key] = memory_obj</code>で保存</li>
<li><code>cache_policy.update_on_put(key)</code>でEvictionポリシー更新（LRU: OrderedDictの末尾に移動、等）</li>
<li>必要に応じてcontrollerへADMITメッセージ送信（<code>batched_msg_sender</code>経由）</li>
</ol>
<h2 id="パイプライン動作の詳細"><a class="header" href="#パイプライン動作の詳細">パイプライン動作の詳細</a></h2>
<p>store_layerとbatched_from_gpuは2つの入れ子Generatorで<strong>パイプライン動作</strong>する:</p>
<pre><code>store_layer Generator:     [初期化] → yield → [L0転送+保存] → yield → [L1転送+保存] → yield → ...
batched_from_gpu Generator: [初期化] → yield → [L0 DMA]     → yield → [L1 DMA]     → yield → ...
</code></pre>
<p><strong>タイミング</strong>（<code>num_layers=N</code>の場合）:</p>
<ul>
<li><code>store_layer</code>は<code>N+1</code>回yield（初期化1回 + レイヤーN回）</li>
<li><code>batched_from_gpu</code>は<code>N+1</code>回yield（初期化prime + レイヤーN回）</li>
<li>adapterは合計<code>N</code>回<code>next()</code>を呼ぶ（各attentionレイヤー実行後）</li>
</ul>
<p><strong>パイプラインのステップ</strong>: Layer Lの<code>next()</code>呼び出しで、<code>batched_from_gpu</code>がLayer LのDMAを実行し、<code>store_layer</code>がLayer LのStorageManager書き込みを行う。つまり<strong>DMAとストレージ書き込みは同一レイヤーで連続実行</strong>される。</p>
<h2 id="データ構造-2"><a class="header" href="#データ構造-2">データ構造</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>構造</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td>CacheEngineKey</td><td><code>(model_name, world_size, worker_id, chunk_hash, kv_dtype, request_configs)</code></td><td>チャンク単位のキー（レイヤー横断）</td></tr>
<tr><td>LayerCacheEngineKey</td><td>CacheEngineKey + layer_id</td><td>レイヤー単位のキー</td></tr>
<tr><td>MemoryObj</td><td>pinned CPU tensor wrapper</td><td>ref_count管理、MemoryObjMetadata付き</td></tr>
<tr><td>MemoryFormat.KV_T2D</td><td><code>[num_tokens, 2, hidden_dim]</code></td><td>レイヤーワイズ形式（token-major）</td></tr>
<tr><td>MemoryFormat.KV_MLA_FMT</td><td><code>[num_tokens, hidden_dim]</code></td><td>MLA形式（K/V統合）</td></tr>
<tr><td>store_mask</td><td><code>torch.Tensor[bool]</code></td><td>False=キャッシュ済みprefix、True=新規トークン</td></tr>
<tr><td>slot_mapping</td><td><code>torch.Tensor[long]</code></td><td>トークン位置→vLLMページドメモリのflat slot</td></tr>
<tr><td>hot_cache</td><td><code>OrderedDict[CacheEngineKey, MemoryObj]</code></td><td>L1 CPUキャッシュ（Evictionポリシー付き）</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="retrieve-パス概要"><a class="header" href="#retrieve-パス概要">Retrieve パス概要</a></h2>
<p>KVキャッシュをストレージ（CPU/Disk/Remote）からGPUのvLLMページドメモリに復元するパス。
<strong>2フェーズ設計</strong>: Scheduler側の<strong>lookup</strong>（ヒット判定+prefetch指示）と、Worker側の<strong>load</strong>（実際のGPU転送）に分離。</p>
<h3 id="schedulerworker間の情報伝達"><a class="header" href="#schedulerworker間の情報伝達">Scheduler→Worker間の情報伝達</a></h3>
<pre class="mermaid">sequenceDiagram
    participant Sched as V1Impl (Scheduler側)
    participant LC as LookupClient
    participant Worker as V1Impl (Worker側)
    participant Engine as LMCacheEngine

    Note over Sched: vLLM Scheduler.schedule() から呼ばれる
    Sched-&gt;&gt;LC: lookup(token_ids, req_id)
    LC--&gt;&gt;Sched: num_external_hit_tokens
    Note over Sched: LoadSpec(vllm_cached, lmcache_cached) を生成
    Sched-&gt;&gt;Sched: update_state_after_alloc() → can_load=True
    Sched-&gt;&gt;Sched: build_connector_meta() → ReqMeta(load_spec) を構築
    Note over Sched: ConnectorMetadata を SchedulerOutput に添付

    Note over Worker: Forward開始時
    Worker-&gt;&gt;Engine: start_load_kv(forward_context)
    Note over Engine: Bulk or Layerwise retrieve 実行
</pre>

<h3 id="lookupclient-の動作"><a class="header" href="#lookupclient-の動作">LookupClient の動作</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/lookup_client/lmcache_lookup_client.py:28</code></p>
<p><code>LMCacheLookupClient</code>はvLLMのSchedulerプロセスで動作する。LMCacheEngine（Worker側）とは<strong>ZMQ IPC</strong>（REQ/REP）で通信。</p>
<p><strong>処理フロー</strong>:</p>
<ol>
<li><code>process_tokens()</code>でトークン列をチャンクハッシュに分割</li>
<li>ハッシュ列をmsgpackシリアライズし、ZMQで<code>LookupServer</code>に送信</li>
<li><code>LookupServer</code>（Worker側）が<code>StorageManager.contains()</code>で存在チェック</li>
<li>ヒットトークン数を返却</li>
</ol>
<p><strong>キャッシュ</strong>: 同一リクエストの2回目以降のlookupは<code>reqs_status</code>辞書から即座に返す。</p>
<h3 id="retrieve-パスの2モード"><a class="header" href="#retrieve-パスの2モード">Retrieve パスの2モード</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>モード</th><th>条件</th><th>エントリポイント</th><th>特徴</th></tr>
</thead>
<tbody>
<tr><td><strong>Bulk</strong></td><td><code>use_layerwise=False</code>（デフォルト）</td><td><code>LMCacheEngine.retrieve()</code></td><td>全レイヤー一括取得→一括GPU転送</td></tr>
<tr><td><strong>Layerwise</strong></td><td><code>use_layerwise=True</code></td><td><code>LMCacheEngine.retrieve_layer()</code></td><td>レイヤー単位Generator、パイプライン可能</td></tr>
</tbody>
</table>
</div>
<h3 id="bulk-retrieve-パス"><a class="header" href="#bulk-retrieve-パス">Bulk Retrieve パス</a></h3>
<pre class="mermaid">sequenceDiagram
    participant Adapter as V1Impl (Worker側)
    participant Engine as LMCacheEngine
    participant TDB as TokenDatabase
    participant SM as StorageManager
    participant CPU as LocalCPUBackend
    participant GPU as GPUConnector (V2)

    Adapter-&gt;&gt;Engine: retrieve(tokens, mask, kvcaches, slot_mapping, ...)
    activate Engine

    Engine-&gt;&gt;TDB: process_tokens(tokens, mask)
    TDB--&gt;&gt;Engine: [(start, end, CacheEngineKey), ...]

    alt async_loading == True
        Note over Engine: event_managerからprefetch済みMemoryObjを取得
        Engine-&gt;&gt;Engine: _async_process_tokens_internal()
    else sync loading
        Engine-&gt;&gt;SM: get_block_mapping(chunk_infos)
        SM--&gt;&gt;Engine: {backend_name: [(key, start, end)]}
        Engine-&gt;&gt;SM: batched_get(keys, location)
        SM-&gt;&gt;CPU: batched_get_blocking(keys)
        CPU--&gt;&gt;SM: List[MemoryObj]（ref_count_up済み）
        SM--&gt;&gt;Engine: List[MemoryObj]
    end

    Engine-&gt;&gt;GPU: batched_to_gpu(memory_objs, starts, ends, slot_mapping=...)
    Note over GPU: load_stream上で全チャンクをGPU転送
    GPU-&gt;&gt;GPU: lmc_ops.multi_layer_kv_transfer(memobj→paged KV)
    GPU--&gt;&gt;Engine: 完了

    Note over Engine: memory_obj.ref_count_down() で解放
    Engine--&gt;&gt;Adapter: ret_mask (bool tensor)
    deactivate Engine
</pre>

<h4 id="_process_tokens_internal-の詳細"><a class="header" href="#_process_tokens_internal-の詳細">_process_tokens_internal() の詳細</a></h4>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/cache_engine.py:1527</code></p>
<ol>
<li><code>process_tokens()</code>でチャンク分割・ハッシュ計算</li>
<li><code>StorageManager.get_block_mapping()</code>でチャンクの<strong>所在バックエンド</strong>を特定
<ul>
<li>各バックエンドの<code>batched_contains()</code>をprefix match方式で呼び出し</li>
<li>チャンクを所在バックエンドごとにグルーピング</li>
</ul>
</li>
<li>バックエンドごとに<code>batched_get()</code>で<code>MemoryObj</code>を取得</li>
<li>取得失敗時は<code>last_failed_block_start</code>以降の<code>ret_mask</code>をFalseに戻し、チャンクリストを切り詰め</li>
</ol>
<h4 id="_async_process_tokens_internal-の詳細"><a class="header" href="#_async_process_tokens_internal-の詳細">_async_process_tokens_internal() の詳細</a></h4>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/cache_engine.py:1463</code></p>
<p>非同期プリフェッチ済みの結果を<code>event_manager</code>から取得するパス。</p>
<ol>
<li><code>event_manager.pop_event(EventType.LOADING, req_id)</code>でprefetch結果の<code>Future</code>を取得</li>
<li><code>future.result()</code>で<code>list[list[tuple[CacheEngineKey, MemoryObj]]]</code>（tier×chunk）を取得</li>
<li><code>process_tokens()</code>で再度チャンク分割し、<code>memory_obj_map</code>からマッチングしてチャンクリストを構築</li>
<li>未使用の<code>MemoryObj</code>は<code>ref_count_down()</code>で即座に解放</li>
</ol>
<h4 id="storagemanagerbatched_get-のwrite-back"><a class="header" href="#storagemanagerbatched_get-のwrite-back">StorageManager.batched_get() のwrite-back</a></h4>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/storage_manager.py:484</code></p>
<p>リモートバックエンド（Disk/Remote）からデータを取得した場合、<strong>自動的にLocalCPUBackendにwrite-back</strong>する。</p>
<ul>
<li><code>LocalCPUBackend</code>以外から取得 &amp;&amp; <code>LocalCPUBackend</code>が存在 &amp;&amp; 全MemoryObjがnon-None → <code>batched_submit_put_task()</code>でL1にコピー</li>
</ul>
<h3 id="layerwise-retrieve-パス"><a class="header" href="#layerwise-retrieve-パス">Layerwise Retrieve パス</a></h3>
<pre class="mermaid">sequenceDiagram
    participant Adapter as V1Impl (Worker側)
    participant Engine as LMCacheEngine
    participant TDB as TokenDatabase
    participant SM as StorageManager
    participant GPU as GPUConnector (Layerwise)

    Note over Adapter: start_load_kv() 内
    Adapter-&gt;&gt;Engine: retrieve_layer(tokens, mask, kvcaches, slot_mapping, sync)
    activate Engine
    Engine-&gt;&gt;TDB: process_tokens(tokens, mask)
    TDB--&gt;&gt;Engine: [(start, end, CacheEngineKey), ...]
    Note over Engine: contains(layer0_key) でヒット判定 + location統一チェック

    Engine-&gt;&gt;SM: layerwise_batched_get(keys_layer_major, location)
    Note over SM: Layer 0 の get_non_blocking を asyncio.create_task() で投入
    Engine-&gt;&gt;GPU: batched_to_gpu(starts, ends, ...) → mem_obj_consumer Generator 生成
    GPU--&gt;&gt;Engine: mem_obj_consumer primed (yield)

    Engine--&gt;&gt;Adapter: yield torch.sum(ret_mask) — Layer 0 のヒット数
    deactivate Engine

    Note over Adapter: next(retriever) で Layer 0 データ受領
    Adapter-&gt;&gt;Engine: next(retriever)
    activate Engine
    Note over Engine: Layer 0 の task.result() を取得
    Engine-&gt;&gt;GPU: mem_obj_consumer.send(mem_objs_layer0)
    Note over GPU: CPU→GPUバッファ copy（load_stream）
    SM--&gt;&gt;Engine: Layer 1 の task yield
    Engine--&gt;&gt;Adapter: yield None
    deactivate Engine

    Note over Adapter: wait_for_layer_load(layer_name) で同期
    Adapter-&gt;&gt;Engine: next(retriever)
    activate Engine
    Note over Engine: Layer N-1 処理...
    Engine--&gt;&gt;Adapter: yield ret_mask（最終レイヤー後）
    deactivate Engine
</pre>

<h4 id="retrieve_layer-の-generator-構造"><a class="header" href="#retrieve_layer-の-generator-構造">retrieve_layer() の Generator 構造</a></h4>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/cache_engine.py:851</code></p>
<p><code>num_layers + 3</code>回yieldする（ヒットあり時）:</p>
<ol>
<li><strong>yield 0</strong>: <code>torch.sum(ret_mask)</code> — ヒットトークン数（sglang統合向け）</li>
<li><strong>yield 1 ~ N-1</strong>: <code>None</code> — 各レイヤーのGPU転送進行中</li>
<li><strong>yield N</strong>: <code>None</code> — 最終レイヤー処理中</li>
<li><strong>yield N+1</strong>: <code>next(mem_obj_consumer)</code> で同期</li>
<li><strong>yield N+2</strong>: <code>ret_mask</code> — 最終結果</li>
</ol>
<p>各レイヤーで:</p>
<ul>
<li><code>next(get_generator)</code>で<code>StorageManager</code>から非同期取得した<code>Future</code>を受け取る</li>
<li><code>task.result()</code>で<code>List[MemoryObj]</code>を取得（ブロッキング）</li>
<li><code>mem_obj_consumer.send(mem_objs_layer)</code>でGPUコネクタにデータを渡す</li>
<li><code>MemoryObj.ref_count_down()</code>は全レイヤー完了後にバッチで実行</li>
</ul>
<h4 id="layerwise-gpuconnectorbatched_to_gpu-のパイプライン"><a class="header" href="#layerwise-gpuconnectorbatched_to_gpu-のパイプライン">Layerwise GPUConnector.batched_to_gpu() のパイプライン</a></h4>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/gpu_connector/gpu_connectors.py:683</code></p>
<p><code>VLLMBufferLayerwiseGPUConnector</code>は<code>num_layers + 2</code>回のイテレーションで<strong>3段パイプライン</strong>を実行:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>イテレーション i</th><th>操作1: paged書き込み</th><th>操作2: RoPE補正+gap zeroing</th><th>操作3: CPU→GPU load</th></tr>
</thead>
<tbody>
<tr><td>i = 0</td><td>—</td><td>—</td><td><code>yield</code>で<code>mem_objs_layer0</code>受領、load_stream上でcopy</td></tr>
<tr><td>i = 1</td><td>—</td><td>Layer 0のRoPE補正</td><td><code>yield</code>で<code>mem_objs_layer1</code>受領、load_stream上でcopy</td></tr>
<tr><td>i = 2</td><td>Layer 0をpagedメモリに書き込み</td><td>Layer 1のRoPE補正</td><td><code>yield</code>で<code>mem_objs_layer2</code>受領</td></tr>
<tr><td>…</td><td>Layer i-2</td><td>Layer i-1</td><td>Layer i</td></tr>
<tr><td>i = N</td><td>Layer N-2</td><td>Layer N-1</td><td><code>yield</code>（同期用、データなし）</td></tr>
<tr><td>i = N+1</td><td>Layer N-1</td><td>—</td><td>—</td></tr>
</tbody>
</table>
</div>
<p><strong>ダブルバッファ</strong>: <code>compute_gpu_buffer_obj</code>と<code>load_gpu_buffer_obj</code>をping-pongして、RoPE計算とDMAをオーバーラップ。</p>
<p><strong>RoPE位置補正</strong>: <code>cache_positions=True</code>の場合、保存時の位置と現在の位置の差分で<code>fused_rotary_emb()</code>を適用。保存時位置は<code>MemoryObjMetadata.cached_positions</code>から取得。</p>
<p><strong>gap zeroing</strong>: チャンク間のギャップ位置（連続しないstart/endの隙間）をゼロ埋め。</p>
<h3 id="非同期プリフェッチの全体フロー"><a class="header" href="#非同期プリフェッチの全体フロー">非同期プリフェッチの全体フロー</a></h3>
<pre class="mermaid">sequenceDiagram
    participant Sched as V1Impl (Scheduler)
    participant LC as LookupClient
    participant LS as LookupServer (Worker)
    participant SM as StorageManager
    participant EM as EventManager
    participant Worker as V1Impl (Worker)
    participant Engine as LMCacheEngine

    Note over Sched: get_num_new_matched_tokens() 内
    Sched-&gt;&gt;LC: lookup(token_ids, req_id)
    LC-&gt;&gt;LS: ZMQ REQ (hashes + offsets + req_id)
    LS-&gt;&gt;SM: async_lookup_and_prefetch(lookup_id, keys, ...)
    Note over SM: 各バックエンドに batched_async_contains → batched_get_non_blocking
    SM-&gt;&gt;EM: add_event(LOADING, lookup_id, all_done_task)
    LS--&gt;&gt;LC: num_hit_tokens
    LC--&gt;&gt;Sched: num_external_hit_tokens

    Note over Sched: build_connector_meta() で LoadSpec を ConnectorMetadata に格納

    Note over Worker: start_load_kv() 内
    Worker-&gt;&gt;Engine: retrieve(tokens, mask, ..., req_id=req_id)
    Engine-&gt;&gt;EM: pop_event(LOADING, req_id)
    Note over Engine: future.result() で prefetch 済み MemoryObj を取得
    Engine-&gt;&gt;Engine: _async_process_tokens_internal()
    Engine-&gt;&gt;GPU: batched_to_gpu(memory_objs, ...)
</pre>

<p><strong>ポイント</strong>:</p>
<ul>
<li>Scheduler側のlookupがprefetchを<strong>トリガー</strong>し、Worker側のretrieveがprefetch結果を<strong>消費</strong>する</li>
<li><code>EventManager</code>が両者を<code>lookup_id</code>（=req_id）で紐付け</li>
<li>prefetchは<code>asyncio.create_task()</code>で非同期実行され、Worker側のretrieveまでに完了していればブロッキングなし</li>
</ul>
<h3 id="token_mask-と-ret_mask-の意味"><a class="header" href="#token_mask-と-ret_mask-の意味">token_mask と ret_mask の意味</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>マスク</th><th>用途</th><th>値の意味</th></tr>
</thead>
<tbody>
<tr><td><code>token_mask</code></td><td>adapter側で構築</td><td><code>False</code>=vLLMがキャッシュ済み（チャンク境界まで切り詰め）、<code>True</code>=LMCacheから要ロード</td></tr>
<tr><td><code>ret_mask</code></td><td>Engine内部で構築</td><td><code>True</code>=実際にLMCacheから取得成功、<code>False</code>=未取得</td></tr>
<tr><td><code>mask</code>（Engine引数）</td><td>token_maskと同じ</td><td><code>process_tokens()</code>のFalseプレフィックス=スキップ対象</td></tr>
</tbody>
</table>
</div>
<p><code>token_mask</code>のFalse区間は<code>vllm_cached_tokens</code>をchunk_sizeの倍数に切り下げた範囲。これにより、vLLMとLMCacheのキャッシュ境界がチャンク単位で整合する（オーバーラップ領域はLMCacheが上書き）。</p>
<h3 id="エラーハンドリング"><a class="header" href="#エラーハンドリング">エラーハンドリング</a></h3>
<ul>
<li><strong>部分的取得失敗</strong>: <code>ret_mask.sum() &lt; expected</code>の場合、<code>record_failed_blocks()</code>で失敗ブロックIDを計算し、<code>_invalid_block_ids</code>に蓄積。vLLMのSchedulerが次stepで<code>get_block_ids_with_load_errors()</code>で回収し、再計算を指示</li>
<li><strong>StorageManager.batched_get()</strong>: <code>memory_obj=None</code>が返された場合、<code>last_failed_block_start</code>以降を切り捨て（prefix matchの性質上、途中の欠損以降は全て無効）</li>
<li><strong>健全性チェック</strong>: <code>is_healthy()==False</code>の場合、retrieve自体をスキップ（ゼロマスクを返す）</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="lmcache-アーキテクチャ概要"><a class="header" href="#lmcache-アーキテクチャ概要">LMCache アーキテクチャ概要</a></h1>
<blockquote>
<p><strong>深度</strong>: [SHALLOW]
<strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-16（Phase 0a）</p>
</blockquote>
<h2 id="プロジェクト概要"><a class="header" href="#プロジェクト概要">プロジェクト概要</a></h2>
<p>LMCacheは、LLMサービングエンジン（vLLM, SGLang等）の<strong>KVキャッシュを外部に保存・再利用</strong>することで、TTFT（Time To First Token）削減とスループット向上を実現する拡張ライブラリ。</p>
<ul>
<li><strong>コア機能</strong>: KVキャッシュのチャンク単位（256トークン）保存・検索・ロード</li>
<li><strong>ストレージ階層</strong>: GPU → CPU → Disk → Remote（Redis/S3/Mooncake/NIXL等）</li>
<li><strong>統合方式</strong>: vLLMの<code>KVConnectorBase_V1</code>を実装して接続</li>
<li><strong>追加機能</strong>: CacheBlend（非プレフィックス再利用）、P2P転送、Disaggregated Prefill</li>
</ul>
<h2 id="リポジトリ規模"><a class="header" href="#リポジトリ規模">リポジトリ規模</a></h2>
<ul>
<li><strong>Python</strong>: 約62,000行（<code>lmcache/</code>配下）</li>
<li><strong>C++/CUDA</strong>: 20ファイル（<code>csrc/</code>）— CacheGen圧縮、メモリ操作、位置エンコーディングカーネル</li>
<li><strong>Rust</strong>: 1ファイル（<code>rust/raw_block/</code>）— Raw Block ストレージバックエンド</li>
</ul>
<h2 id="全体アーキテクチャ-shallow"><a class="header" href="#全体アーキテクチャ-shallow">全体アーキテクチャ [SHALLOW]</a></h2>
<pre class="mermaid">graph TD
    subgraph "サービングエンジン（vLLM）"
        SE[GPUModelRunner]
    end

    subgraph "Integration Layer"
        CONN[LMCacheConnectorV1Dynamic&lt;br/&gt;KVConnectorBase_V1実装]
        ADAPT[LMCacheConnectorV1Impl&lt;br/&gt;vllm_v1_adapter.py]
    end

    subgraph "LMCache Core"
        MGR[LMCacheManager&lt;br/&gt;ライフサイクル管理]
        ENG[LMCacheEngine&lt;br/&gt;メインエンジン]
        TDB[TokenDatabase&lt;br/&gt;トークン→チャンクキー変換]
        GPU_CONN[GPUConnector&lt;br/&gt;GPU↔CPU転送]
        SM[StorageManager&lt;br/&gt;ストレージ階層管理]
    end

    subgraph "Storage Backends"
        CPU[LocalCPUBackend]
        DISK[LocalDiskBackend]
        RMT[RemoteBackend&lt;br/&gt;Redis/S3/Valkey等]
        P2P[P2PBackend]
        NIXL[NIXLBackend]
        GDS[GdsBackend]
    end

    subgraph "Optional Components"
        CTRL[CacheController&lt;br/&gt;クラスタ管理]
        BLEND[CacheBlend&lt;br/&gt;非プレフィックス融合]
        LOOKUP[LookupClient/Server&lt;br/&gt;キャッシュ存在確認]
        MP[MultiProcess Server&lt;br/&gt;IPC/ZMQ]
    end

    SE --&gt; CONN --&gt; ADAPT --&gt; MGR
    MGR --&gt; ENG
    ENG --&gt; TDB
    ENG --&gt; GPU_CONN
    ENG --&gt; SM
    SM --&gt; CPU
    SM --&gt; DISK
    SM --&gt; RMT
    SM --&gt; P2P
    SM --&gt; NIXL
    SM --&gt; GDS
    ENG --&gt; CTRL
    ENG --&gt; BLEND
    MGR --&gt; LOOKUP
</pre>

<h2 id="パッケージ構造-verified"><a class="header" href="#パッケージ構造-verified">パッケージ構造 [VERIFIED]</a></h2>
<pre><code>lmcache/
├── __init__.py
├── config.py              # レガシー設定（旧API）
├── connections.py         # 接続ヘルパー
├── logging.py             # ログ初期化
├── observability.py       # Prometheus/メトリクス（1,839行）
├── protocol.py            # LMCacheModelRequest（msgspec）
├── usage_context.py       # 利用状況トラッキング
├── utils.py               # CacheEngineKey, ユーティリティ（652行）
├── non_cuda_equivalents.py # CUDA非依存フォールバック
│
├── integration/           # サービングエンジン統合
│   ├── vllm/              # vLLM統合
│   │   ├── lmcache_connector_v1.py      # KVConnectorBase_V1実装（latest版）
│   │   ├── lmcache_connector_v1_085.py  # vLLM 0.8.5互換版
│   │   ├── vllm_v1_adapter.py           # 実装本体（LMCacheConnectorV1Impl）
│   │   ├── vllm_multi_process_adapter.py # マルチプロセス対応
│   │   └── utils.py                     # vLLM固有ユーティリティ
│   └── sglang/            # SGLang統合
│       ├── sglang_adapter.py
│       └── utils.py
│
├── server/                # レガシーサーバー
│
├── storage_backend/       # レガシーストレージ
│   ├── evictor/           # Eviction（LRU）
│   └── serde/             # シリアライゼーション（CacheGen等）
│
├── tools/                 # ベンチマーク等
│
└── v1/                    # ★ メインコード（v1アーキテクチャ）
    ├── cache_engine.py          # LMCacheEngine（1,949行）★中核
    ├── cache_interface.py       # LMCacheModelRequest
    ├── config.py                # LMCacheEngineConfig
    ├── config_base.py           # 設定基盤
    ├── manager.py               # LMCacheManager（694行）
    ├── metadata.py              # LMCacheMetadata
    ├── token_database.py        # Chunked/Segment TokenDatabase
    ├── memory_management.py     # MemoryObj, MemoryAllocator（2,339行）
    ├── event_manager.py         # イベント管理
    ├── kv_layer_groups.py       # レイヤーグループ管理
    ├── lazy_memory_allocator.py # 遅延メモリ確保
    ├── pin_monitor.py           # ピン監視
    ├── periodic_thread.py       # 定期実行スレッド
    ├── protocol.py              # MPプロトコル定義
    ├── rpc_utils.py             # ZMQ RPC
    ├── system_detection.py      # NUMA検出
    │
    ├── gpu_connector/           # GPU↔CPUデータ転送
    │   ├── gpu_connectors.py    # GPUConnectorInterface + 実装
    │   ├── gpu_ops.py           # CUDA memcpy操作
    │   └── utils.py
    │
    ├── storage_backend/         # ストレージ階層
    │   ├── abstract_backend.py  # StorageBackendInterface
    │   ├── storage_manager.py   # StorageManager（1,145行）
    │   ├── local_cpu_backend.py # CPU (L1)
    │   ├── local_disk_backend.py# Disk (L2)
    │   ├── remote_backend.py    # Remote (L3)
    │   ├── p2p_backend.py       # P2P転送
    │   ├── pd_backend.py        # Prefill-Decode分離
    │   ├── nixl_storage_backend.py # NIXL
    │   ├── gds_backend.py       # GPUDirect Storage
    │   ├── connector/           # 15+リモートコネクタ
    │   │   ├── base_connector.py
    │   │   ├── redis_connector.py
    │   │   ├── s3_connector.py
    │   │   ├── valkey_connector.py
    │   │   ├── mooncakestore_connector.py
    │   │   ├── infinistore_connector.py
    │   │   ├── fs_connector.py
    │   │   └── ...
    │   ├── cache_policy/        # Eviction方針
    │   │   ├── fifo.py / lru.py / lfu.py / mru.py
    │   │   └── base_policy.py
    │   ├── naive_serde/         # シリアライゼーション
    │   │   ├── naive_serde.py / cachegen_*.py / kivi_serde.py
    │   │   └── serde.py
    │   └── job_executor/        # 非同期ジョブ実行
    │
    ├── compute/                 # 計算コンポーネント
    │   ├── attention/           # Attention計算
    │   │   ├── flash_attn.py / flash_infer_sparse.py
    │   │   └── metadata.py
    │   ├── blend/               # CacheBlend
    │   │   ├── blender.py / metadata.py / utils.py
    │   │   └── __init__.py
    │   ├── models/              # モデル固有（Llama/Qwen3）
    │   │   ├── base.py / llama.py / qwen3.py
    │   │   └── utils.py
    │   └── positional_encoding.py # RoPE等
    │
    ├── multiprocess/            # マルチプロセスアーキテクチャ
    │   ├── server.py            # MPサーバー
    │   ├── blend_server.py      # CacheBlend用サーバー
    │   ├── mq.py                # メッセージキュー
    │   ├── session.py           # セッション管理
    │   ├── token_hasher.py      # トークンハッシュ
    │   ├── gpu_context.py       # GPUコンテキスト
    │   ├── protocols/           # プロトコル定義
    │   └── ...
    │
    ├── cache_controller/        # クラスタキャッシュ管理
    │   ├── controller_manager.py # メイン管理
    │   ├── worker.py            # LMCacheWorker
    │   ├── executor.py          # クラスタ実行
    │   ├── controllers/         # KV/Registration制御
    │   └── commands/            # コマンド（FullSync等）
    │
    ├── lookup_client/           # キャッシュ存在確認
    │   ├── abstract_client.py   # LookupClientInterface
    │   ├── lmcache_lookup_client.py  # ZMQベース
    │   ├── lmcache_async_lookup_client.py
    │   └── record_strategies/   # 記録戦略（BloomFilter等）
    │
    ├── distributed/             # 分散ストレージ管理（MPモード用）
    │   ├── storage_manager.py   # 分散StorageManager
    │   ├── l1_manager.py        # L1管理
    │   ├── memory_manager.py    # メモリ管理
    │   └── eviction_policy/     # 分散Eviction
    │
    ├── transfer_channel/        # データ転送チャネル
    │   ├── abstract.py
    │   ├── nixl_channel.py
    │   └── py_socket_channel.py
    │
    ├── offload_server/          # オフロードサーバー
    │   ├── abstract_server.py
    │   └── zmq_server.py
    │
    ├── health_monitor/          # ヘルスチェック
    ├── internal_api_server/     # 内部API（FastAPI）
    ├── standalone/              # スタンドアロンモード
    └── plugin/                  # ランタイムプラグイン
</code></pre>
<h2 id="主要エントリポイント-verified"><a class="header" href="#主要エントリポイント-verified">主要エントリポイント [VERIFIED]</a></h2>
<h3 id="1-vllm統合メインパス"><a class="header" href="#1-vllm統合メインパス">1. vLLM統合（メインパス）</a></h3>
<ul>
<li><strong>参照</strong>: <code>target/LMCache/lmcache/integration/vllm/lmcache_connector_v1.py:30</code> (LMCacheConnectorV1Dynamic)</li>
<li>vLLMの<code>KVConnectorBase_V1</code>を実装。内部で<code>LMCacheConnectorV1Impl</code>に委譲</li>
<li><code>LMCacheManager</code>がライフサイクル全体を管理</li>
</ul>
<h3 id="2-lmcacheengineコア"><a class="header" href="#2-lmcacheengineコア">2. LMCacheEngine（コア）</a></h3>
<ul>
<li><strong>参照</strong>: <code>target/LMCache/lmcache/v1/cache_engine.py:78</code> (LMCacheEngine)</li>
<li>KVキャッシュのstore/retrieve/prefetchを統合するメインクラス</li>
<li><code>TokenDatabase</code>でトークン列→チャンクキー変換</li>
<li><code>GPUConnector</code>でGPU↔CPU転送</li>
<li><code>StorageManager</code>でストレージ階層管理</li>
</ul>
<h3 id="3-storagemanagerストレージ階層"><a class="header" href="#3-storagemanagerストレージ階層">3. StorageManager（ストレージ階層）</a></h3>
<ul>
<li><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/storage_manager.py:1</code></li>
<li><code>OrderedDict</code>でバックエンド登録順管理（L1→L2→L3）</li>
<li>非同期put/getで階層間データ移動</li>
</ul>
<h3 id="4-multiprocess-server"><a class="header" href="#4-multiprocess-server">4. MultiProcess Server</a></h3>
<ul>
<li><strong>参照</strong>: <code>target/LMCache/lmcache/v1/multiprocess/server.py:1</code></li>
<li>ZMQ IPCベースのマルチプロセスアーキテクチャ</li>
<li>GPUコンテキスト管理、セッション管理</li>
</ul>
<h3 id="5-cachecontrollerクラスタ管理"><a class="header" href="#5-cachecontrollerクラスタ管理">5. CacheController（クラスタ管理）</a></h3>
<ul>
<li><strong>参照</strong>: <code>target/LMCache/lmcache/v1/cache_controller/controller_manager.py:1</code></li>
<li>複数インスタンス間のキャッシュ状態管理</li>
<li>Register/Deregister/Heartbeat/P2P Lookup</li>
</ul>
<h3 id="6-lmcache-apiサーバー"><a class="header" href="#6-lmcache-apiサーバー">6. LMCache APIサーバー</a></h3>
<ul>
<li><strong>参照</strong>: <code>target/LMCache/lmcache/v1/api_server/__main__.py</code></li>
<li>スタンドアロンAPIサーバーモード</li>
</ul>
<h2 id="新旧アーキテクチャ-verified"><a class="header" href="#新旧アーキテクチャ-verified">新旧アーキテクチャ [VERIFIED]</a></h2>
<ul>
<li><strong><code>lmcache/v1/</code></strong>: 現行アーキテクチャ（★フォーカス対象）</li>
<li><strong><code>lmcache/server/</code></strong>, <strong><code>lmcache/storage_backend/</code></strong>: レガシー（旧API互換）</li>
<li>v1がメイン。レガシーは基本的にスキップしてよい</li>
</ul>
<h2 id="2つの動作モード-shallow"><a class="header" href="#2つの動作モード-shallow">2つの動作モード [SHALLOW]</a></h2>
<h3 id="in-process-モード"><a class="header" href="#in-process-モード">In-Process モード</a></h3>
<ul>
<li>vLLMプロセス内で直接<code>LMCacheEngine</code>を動作</li>
<li><code>LMCacheConnectorV1Dynamic</code> → <code>LMCacheConnectorV1Impl</code> → <code>LMCacheManager</code> → <code>LMCacheEngine</code></li>
</ul>
<h3 id="multiprocess-mp-モード"><a class="header" href="#multiprocess-mp-モード">MultiProcess (MP) モード</a></h3>
<ul>
<li>別プロセスでLMCacheサーバーを起動</li>
<li>ZMQ IPCで通信</li>
<li><code>multiprocess/server.py</code>がメイン</li>
<li>分散StorageManager（<code>distributed/storage_manager.py</code>）を使用</li>
</ul>
<h2 id="データフロー概要-shallow"><a class="header" href="#データフロー概要-shallow">データフロー概要 [SHALLOW]</a></h2>
<h3 id="store-パスgpu--storage"><a class="header" href="#store-パスgpu--storage">Store パス（GPU → Storage）</a></h3>
<ol>
<li>vLLMのforward完了後、<code>wait_for_save()</code>が呼ばれる</li>
<li><code>GPUConnector.from_gpu()</code> でGPU KVキャッシュ → CPU MemoryObj</li>
<li><code>TokenDatabase</code>でトークン列をチャンクキーに変換</li>
<li><code>StorageManager</code>が各バックエンドに非同期put</li>
</ol>
<h3 id="retrieve-パスstorage--gpu"><a class="header" href="#retrieve-パスstorage--gpu">Retrieve パス（Storage → GPU）</a></h3>
<ol>
<li>Scheduler側で<code>LookupClient</code>がキャッシュ存在確認</li>
<li>Worker側で<code>start_load_kv()</code>が呼ばれる</li>
<li><code>StorageManager</code>がバックエンドからMemoryObjを取得</li>
<li><code>GPUConnector.to_gpu()</code> でCPU MemoryObj → GPU KVキャッシュ</li>
</ol>
<h2 id="重要な設計判断-shallow"><a class="header" href="#重要な設計判断-shallow">重要な設計判断 [SHALLOW]</a></h2>
<ul>
<li><strong>チャンク単位保存</strong>: 256トークン（設定可）単位でKVキャッシュを分割保存</li>
<li><strong>プレフィックスハッシュ</strong>: vLLMと同じsha256ベースのハッシュチェーンでチャンク識別</li>
<li><strong>非同期ストレージ</strong>: put操作は非同期で実行（Futureベース）</li>
<li><strong>Eviction方針</strong>: FIFO/LRU/LFU/MRUから選択可能</li>
<li><strong>Layerwise GPUConnector</strong>: レイヤー単位でのKVキャッシュ転送をサポート</li>
<li><strong>CacheBlend</strong>: 非プレフィックス部分のKVキャッシュも再利用（セパレータベースセグメント分割）</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="lmcacheengine"><a class="header" href="#lmcacheengine">LMCacheEngine</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM] / <strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-16（Phase 1 セッション1）</p>
</blockquote>
<h2 id="概要-23"><a class="header" href="#概要-23">概要</a></h2>
<p>LMCacheのコアコンポーネント（1,949行）。TokenDatabase、GPUConnector、StorageManagerを統合し、
KVキャッシュのstore/retrieveオペレーションを実行する。</p>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/cache_engine.py</code></p>
<h2 id="store-api2つのエントリポイント"><a class="header" href="#store-api2つのエントリポイント">Store API（2つのエントリポイント）</a></h2>
<h3 id="store_layer--レイヤーワイズ主要パス"><a class="header" href="#store_layer--レイヤーワイズ主要パス">store_layer() — レイヤーワイズ（主要パス）</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/cache_engine.py:528</code></p>
<pre><code class="language-python">def store_layer(
    tokens: Union[Tensor, list[int]],
    mask: Optional[Tensor] = None,
    **kwargs,  # kvcaches, slot_mapping, offset, sync, req_id
) -&gt; Generator[None, None, None]
</code></pre>
<p><strong>Generator関数</strong>。呼び出し側が各attentionレイヤー実行後に<code>next()</code>で進める。</p>
<p><strong>初期化フェーズ（最初のyieldまで）</strong>:</p>
<ol>
<li><code>TokenDatabase.process_tokens(tokens, mask)</code> → <code>(start, end, CacheEngineKey)</code>のイテラブル</li>
<li><code>key.split_layers(num_layers)</code> → <code>LayerCacheEngineKey</code>のリスト</li>
<li><code>StorageManager.contains(keys[0])</code> で既存チェック（layer 0のキーで判定）</li>
<li><code>StorageManager.batched_allocate(shape, dtype, batch_size=num_layers)</code> でMemoryObj確保</li>
<li>チャンク×レイヤー → レイヤー×チャンクに転置</li>
<li><code>GPUConnector.batched_from_gpu(memory_objs, starts, ends, ...)</code> でGPU転送Generator生成</li>
</ol>
<p><strong>レイヤーループ（num_layers回yield）</strong>:</p>
<pre><code>yield → next(mem_obj_generator) → StorageManager.batched_put(keys[layer_id], memory_objs[layer_id])
</code></pre>
<p><strong>エラーハンドリング</strong>:</p>
<ul>
<li><code>batched_allocate</code>がNone → メモリ不足、storeを中止（yieldだけ行う）</li>
<li><code>is_healthy()</code> False → 全操作スキップ</li>
<li><code>is_frozen()</code> True → freeze mode、yieldだけ行う</li>
</ul>
<h3 id="store--非レイヤーワイズ"><a class="header" href="#store--非レイヤーワイズ">store() — 非レイヤーワイズ</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/cache_engine.py:335</code></p>
<p>全レイヤー一括転送。<code>GPUConnector.from_gpu()</code>で全レイヤーをまとめてコピーし、<code>StorageManager.batched_put()</code>で保存。レイヤーワイズが無効の場合に使用。</p>
<h2 id="主要な内部状態"><a class="header" href="#主要な内部状態">主要な内部状態</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>フィールド</th><th>型</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>token_database</code></td><td>ChunkedTokenDatabase</td><td>トークン→チャンクハッシュ変換</td></tr>
<tr><td><code>gpu_connector</code></td><td>GPUConnectorInterface</td><td>GPU↔CPU転送</td></tr>
<tr><td><code>storage_manager</code></td><td>StorageManager</td><td>多段バックエンド管理</td></tr>
<tr><td><code>num_layers</code></td><td>int</td><td>モデルのレイヤー数</td></tr>
<tr><td><code>metadata</code></td><td>LMCacheMetadata</td><td>model_name, world_size等</td></tr>
<tr><td><code>fmt</code></td><td>MemoryFormat</td><td>KV_T2D or KV_2LTD</td></tr>
<tr><td><code>kv_events</code></td><td>list</td><td>BlockStored等のイベントキュー</td></tr>
</tbody>
</table>
</div>
<h2 id="retrieve-api2つのエントリポイント"><a class="header" href="#retrieve-api2つのエントリポイント">Retrieve API（2つのエントリポイント）</a></h2>
<h3 id="retrieve--bulkデフォルト"><a class="header" href="#retrieve--bulkデフォルト">retrieve() — Bulk（デフォルト）</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/cache_engine.py:708</code></p>
<pre><code class="language-python">def retrieve(
    tokens: Union[Tensor, list[int]],
    mask: Optional[Tensor] = None,
    **kwargs,  # kvcaches, slot_mapping, request_configs, req_id
) -&gt; torch.Tensor  # ret_mask (bool, CPU)
</code></pre>
<p>全レイヤーのKVキャッシュを一括取得し、GPUのページドメモリに書き戻す。</p>
<p><strong>処理フロー</strong>:</p>
<ol>
<li><code>_process_tokens_internal()</code>（同期）or <code>_async_process_tokens_internal()</code>（非同期prefetch済み）でMemoryObjを取得</li>
<li><code>save_only_first_rank</code>時は<code>_broadcast_or_receive_memory_objs()</code>で他ランクにブロードキャスト</li>
<li><code>GPUConnector.batched_to_gpu(memory_objs, starts, ends, ...)</code>で一括GPU転送</li>
<li><code>memory_obj.ref_count_down()</code>で解放</li>
<li><code>remove_after_retrieve</code>時は<code>StorageManager.remove(key)</code>で即座に削除</li>
</ol>
<p><strong>_process_tokens_internal()</strong>（同期パス）:</p>
<ol>
<li><code>process_tokens()</code>でチャンク分割</li>
<li><code>get_block_mapping()</code>でチャンクの所在バックエンドをprefix matchで特定</li>
<li><code>batched_get(keys, location)</code>でバックエンドからMemoryObj取得</li>
<li>取得失敗時は<code>last_failed_block_start</code>以降を全て無効化</li>
</ol>
<p><strong>_async_process_tokens_internal()</strong>（非同期パス）:</p>
<ol>
<li><code>event_manager.pop_event(LOADING, req_id)</code>でprefetch済みFutureを取得</li>
<li><code>future.result()</code>でtier×chunkのMemoryObjマップを構築</li>
<li><code>process_tokens()</code>で再チャンク分割しマッチング</li>
<li>未使用MemoryObjは即座に<code>ref_count_down()</code></li>
</ol>
<h3 id="retrieve_layer--layerwise"><a class="header" href="#retrieve_layer--layerwise">retrieve_layer() — Layerwise</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/cache_engine.py:851</code></p>
<pre><code class="language-python">def retrieve_layer(
    tokens: Union[Tensor, list[int]],
    mask: Optional[Tensor] = None,
    **kwargs,  # kvcaches, slot_mapping, sync
) -&gt; Generator[Optional[Tensor], None, None]
</code></pre>
<p>レイヤー単位でKVキャッシュを取得するGenerator関数。</p>
<p><strong>初期化フェーズ</strong>:</p>
<ol>
<li><code>process_tokens()</code>でチャンク分割</li>
<li><code>StorageManager.contains(layer0_key)</code>でヒット＋location統一チェック</li>
<li>キーをlayer-major形式に転置: <code>keys[chunk][layer]</code> → <code>keys_layer_major[layer][chunk]</code></li>
<li><code>StorageManager.layerwise_batched_get(keys_layer_major, location)</code> → <code>get_generator</code></li>
<li><code>GPUConnector.batched_to_gpu(starts, ends, ...)</code> → <code>mem_obj_consumer</code> Generator</li>
</ol>
<p><strong>レイヤーループ</strong>:</p>
<pre><code>yield → task = next(get_generator) → mem_objs = task.result() → mem_obj_consumer.send(mem_objs)
</code></pre>
<p>最終yield時に<code>ret_mask</code>を返す。<code>ref_count_down()</code>は全レイヤー完了後にバッチ実行。</p>
<h3 id="lookup--ヒット数問い合わせ"><a class="header" href="#lookup--ヒット数問い合わせ">lookup() — ヒット数問い合わせ</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/cache_engine.py:992</code></p>
<p>Scheduler側から呼ばれるヒット数チェック。<code>process_tokens()</code>でチャンク分割し、<code>StorageManager</code>の<code>contains()</code>/<code>batched_contains()</code>でプレフィックスマッチ。</p>
<h2 id="上流下流-1"><a class="header" href="#上流下流-1">上流・下流</a></h2>
<ul>
<li><strong>上流</strong>: LMCacheConnectorV1Impl（store_layer/retrieve呼び出し）</li>
<li><strong>下流</strong>: TokenDatabase、GPUConnector、StorageManager</li>
<li><strong>ライフサイクル</strong>: LMCacheManagerが生成・管理</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="gpuconnector"><a class="header" href="#gpuconnector">GPUConnector</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM] / <strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-16（Phase 1 セッション1）</p>
</blockquote>
<h2 id="概要-24"><a class="header" href="#概要-24">概要</a></h2>
<p>GPU上のKVキャッシュとCPU上のMemoryObj間でデータを転送するコンポーネント。
vLLMのページドメモリレイアウトからslot_mappingを使って正しいデータを抽出する。</p>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/gpu_connector/gpu_connectors.py</code></p>
<h2 id="クラス階層-4"><a class="header" href="#クラス階層-4">クラス階層</a></h2>
<pre><code>GPUConnectorInterface (ABC)
  ├── VLLMPagedMemGPUConnectorV2        ← 非レイヤーワイズ（全レイヤー一括）
  ├── VLLMPagedMemLayerwiseGPUConnector ← レイヤーワイズ（主要パス）
  ├── VLLMBufferLayerwiseGPUConnector   ← CacheBlend用（中間バッファ経由）
  ├── VLLMGPUConnectorXPU               ← Intel XPU用
  └── SGLangGPUConnector                ← SGLang用
</code></pre>
<h2 id="主要メソッドstore方向"><a class="header" href="#主要メソッドstore方向">主要メソッド（Store方向）</a></h2>
<h3 id="batched_from_gpuvllmpagedmemlayerwisegpuconnector"><a class="header" href="#batched_from_gpuvllmpagedmemlayerwisegpuconnector">batched_from_gpu()（VLLMPagedMemLayerwiseGPUConnector）</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/gpu_connector/gpu_connectors.py:1212</code></p>
<pre><code class="language-python">def batched_from_gpu(
    memory_objs: List[List[MemoryObj]],  # [num_layers][num_chunks]
    starts: List[int],
    ends: List[int],
    **kwargs,  # slot_mapping, sync, kvcaches
) -&gt; Generator
</code></pre>
<p><strong>Generator関数</strong>。<code>num_layers + 1</code>回yield。</p>
<p><strong>セットアップフェーズ</strong>:</p>
<ol>
<li>slot_mapping_chunksを結合して<code>slot_mapping_full</code>を構築</li>
<li><code>use_gpu=True</code>時: <code>gpu_buffer_allocator</code>から中間GPUバッファを確保</li>
</ol>
<p><strong>レイヤーループ（各yield間）</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>ステップ</th><th>use_gpu=True</th><th>use_gpu=False</th></tr>
</thead>
<tbody>
<tr><td>1</td><td><code>lmc_ops.single_layer_kv_transfer()</code><br />paged GPU → 中間GPUバッファ</td><td><code>lmc_ops.single_layer_kv_transfer()</code><br />paged GPU → 直接pinned CPU</td></tr>
<tr><td>2</td><td><code>memory_obj.tensor.copy_(..., non_blocking=True)</code><br />GPUバッファ → pinned CPU</td><td>（不要）</td></tr>
</tbody>
</table>
</div>
<p><strong>lmc_ops.single_layer_kv_transfer引数</strong>:</p>
<pre><code class="language-python">lmc_ops.single_layer_kv_transfer(
    dst_tensor,          # 出力先
    kvcaches[layer_id],  # vLLMのページドKVキャッシュ（1レイヤー分）
    slot_mapping,        # トークン位置→flat slot
    True,                # store=True（GPU→dst方向）
    True,                # token_major=True（KV_T2D形式）
    vllm_two_major,      # vLLMの2-major形式フラグ
    use_mla,             # MLA形式フラグ
)
</code></pre>
<p><strong>CUDAストリーム設計</strong>:</p>
<ul>
<li><code>self.store_stream</code>: 専用CUDAストリーム。メイン計算ストリームとオーバーラップ可能</li>
<li><code>store_stream.wait_stream(current_stream)</code>: 計算が完了してからDMA開始</li>
<li><code>sync=True</code>時のみ<code>store_stream.synchronize()</code>で同期（最初のリクエストのみ）</li>
</ul>
<p><strong>出力形式</strong>:</p>
<ul>
<li>標準: <code>MemoryFormat.KV_T2D</code> = <code>[num_tokens, 2, hidden_dim]</code></li>
<li>MLA: <code>MemoryFormat.KV_MLA_FMT</code> = <code>[num_tokens, hidden_dim]</code></li>
</ul>
<h3 id="get_shape"><a class="header" href="#get_shape">get_shape()</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/gpu_connector/gpu_connectors.py:1331</code></p>
<pre><code class="language-python">def get_shape(num_tokens: int) -&gt; torch.Size:
    # 標準: [num_tokens, 2, hidden_dim_size]
    # MLA:  [num_tokens, hidden_dim_size]
</code></pre>
<h2 id="主要メソッドretrieve方向"><a class="header" href="#主要メソッドretrieve方向">主要メソッド（Retrieve方向）</a></h2>
<h3 id="batched_to_gpuvllmpagedmemgpuconnectorv2--bulk"><a class="header" href="#batched_to_gpuvllmpagedmemgpuconnectorv2--bulk">batched_to_gpu()（VLLMPagedMemGPUConnectorV2 — Bulk）</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/gpu_connector/gpu_connectors.py:359</code></p>
<pre><code class="language-python">def batched_to_gpu(memory_objs, starts, ends, **kwargs)
</code></pre>
<p><code>load_stream</code>上で全チャンクの<code>to_gpu()</code>を順次実行し、最後に<code>load_stream.synchronize()</code>。
<code>to_gpu()</code>は<code>lmc_ops.multi_layer_kv_transfer()</code>でMemoryObj → pagedメモリに<strong>全レイヤー一括</strong>転送。</p>
<h3 id="batched_to_gpuvllmbufferlayerwisegpuconnector--layerwise"><a class="header" href="#batched_to_gpuvllmbufferlayerwisegpuconnector--layerwise">batched_to_gpu()（VLLMBufferLayerwiseGPUConnector — Layerwise）</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/gpu_connector/gpu_connectors.py:683</code></p>
<p><strong>Generator関数</strong>。<code>num_layers + 2</code>回イテレーションの<strong>3段パイプライン</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>段</th><th>操作</th><th>ストリーム</th></tr>
</thead>
<tbody>
<tr><td>Load</td><td>CPU pinned → GPUバッファに<code>copy_(non_blocking=True)</code></td><td><code>load_stream</code></td></tr>
<tr><td>Compute</td><td>RoPE位置補正 + gap zeroing</td><td>default stream</td></tr>
<tr><td>Write</td><td><code>lmc_ops.single_layer_kv_transfer()</code>でバッファ→pagedメモリ</td><td>default stream</td></tr>
</tbody>
</table>
</div>
<p><strong>ダブルバッファ</strong>: <code>compute_gpu_buffer_obj</code>と<code>load_gpu_buffer_obj</code>をping-pongし、DMAとRoPE計算をオーバーラップ。</p>
<p><strong>RoPE位置補正</strong>（<code>cache_positions=True</code>時）:</p>
<ul>
<li><code>MemoryObjMetadata.cached_positions</code>から保存時位置を取得</li>
<li><code>fused_rotary_emb(old_positions, new_positions, K_tensor)</code>で差分補正</li>
<li>これにより異なるコンテキスト位置でもKVキャッシュを再利用可能</li>
</ul>
<p><strong>gap zeroing</strong>: 連続しないチャンク間のギャップ位置をゼロ埋め。</p>
<p><strong><code>mem_obj_consumer.send()</code>パターン</strong>: Engine側から<code>send(mem_objs_layer)</code>でデータを受け取る（Generator.send）。<code>yield</code>でデータ受領→次イテレーションで処理。</p>
<h2 id="上流下流-2"><a class="header" href="#上流下流-2">上流・下流</a></h2>
<ul>
<li><strong>上流</strong>: LMCacheEngine（store_layer/store/retrieve等で呼び出し）</li>
<li><strong>下流</strong>: vLLMのページドKVキャッシュ（<code>self.kvcaches</code>）、lmc_ops CUDAカーネル</li>
<li><strong>依存</strong>: lmcache.c_ops（single_layer_kv_transfer / multi_layer_kv_transfer）</li>
</ul>
<h2 id="設計上の注意点"><a class="header" href="#設計上の注意点">設計上の注意点</a></h2>
<ul>
<li>中間GPUバッファ（<code>use_gpu=True</code>）は<strong>全チャンクを結合してから一括転送</strong>するため、チャンクごとのカーネル起動オーバーヘッドを削減</li>
<li><code>kvcaches</code>はvLLMの<code>kv_cache</code>リスト（<code>list[Tensor]</code>、レイヤーごとに1テンソル）を<code>initialize_kvcaches_ptr()</code>で受け取る</li>
<li>GPUバッファは<code>gpu_buffer_allocator</code>から確保し、使用後に<code>ref_count_down()</code>で解放</li>
<li><strong>Retrieve時のCUDAカーネル</strong>: Bulk=<code>multi_layer_kv_transfer</code>（全レイヤー一括）、Layerwise=<code>single_layer_kv_transfer</code>（store=Falseで逆方向=メモリ→paged）</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="storagemanager--localcpubackend"><a class="header" href="#storagemanager--localcpubackend">StorageManager + LocalCPUBackend</a></h1>
<blockquote>
<p><strong>深度</strong>: [DEEP] / <strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-16（Phase 2 セッション1）</p>
</blockquote>
<h2 id="概要-25"><a class="header" href="#概要-25">概要</a></h2>
<p>多段ストレージバックエンドを管理するディスパッチャ（StorageManager）と、
L1 CPUメモリキャッシュの実装（LocalCPUBackend）。</p>
<p><strong>参照</strong>:</p>
<ul>
<li><code>target/LMCache/lmcache/v1/storage_backend/storage_manager.py</code>（StorageManager）</li>
<li><code>target/LMCache/lmcache/v1/storage_backend/local_cpu_backend.py</code>（LocalCPUBackend）</li>
<li><code>target/LMCache/lmcache/v1/storage_backend/abstract_backend.py</code>（インターフェース定義）</li>
</ul>
<p><strong>サブドキュメント</strong>:</p>
<ul>
<li><a href="#メモリアロケータ階層">memory-allocator.md</a> — メモリアロケータ階層と物理メモリ管理</li>
<li><a href="#cachepolicy--eviction戦略">cache-policy.md</a> — Eviction戦略（FIFO/LRU/LFU/MRU）</li>
<li><a href="#localdiskbackend--l2ディスクバックエンド">local-disk-backend.md</a> — L2ディスクバックエンドと階層化動作</li>
</ul>
<h2 id="storagemanager-1"><a class="header" href="#storagemanager-1">StorageManager</a></h2>
<h3 id="バックエンド登録と優先度"><a class="header" href="#バックエンド登録と優先度">バックエンド登録と優先度</a></h3>
<p><code>storage_backends</code>は<code>OrderedDict</code>で登録順=優先度:</p>
<pre><code>LocalCPUBackend (L1) → LocalDiskBackend (L2) → RemoteBackend (L3)
</code></pre>
<p><strong>allocator_backend</strong>: メモリ確保の責務を持つバックエンド。通常は<code>LocalCPUBackend</code>（PD有効時は<code>PDBackend</code>）。
全バックエンドは<code>get_allocator_backend()</code>で自身のallocator元を返す:</p>
<ul>
<li>LocalCPUBackend → 自身（<code>AllocatorBackendInterface</code>実装）</li>
<li>LocalDiskBackend → <code>local_cpu_backend</code>参照（CPU上に確保してからディスクに書く）</li>
<li>RemoteBackend → <code>local_cpu_backend</code>参照</li>
</ul>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/storage_manager.py:321</code></p>
<h3 id="batched_allocate"><a class="header" href="#batched_allocate">batched_allocate()</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/storage_manager.py:352</code></p>
<pre><code class="language-python">def batched_allocate(
    shapes: Union[torch.Size, list[torch.Size]],
    dtypes: Union[torch.dtype, list[torch.dtype]],
    batch_size: int,              # = num_layers
    fmt: MemoryFormat = KV_2LTD,
    eviction: bool = True,
    busy_loop: bool = True,
) -&gt; Optional[list[MemoryObj]]
</code></pre>
<p><code>allocator_backend</code>に委譲。LocalCPUBackendが内部でEviction→再確保のループを行う。</p>
<h3 id="batched_put"><a class="header" href="#batched_put">batched_put()</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/storage_manager.py:388</code></p>
<p><strong>処理フロー</strong>:</p>
<ol>
<li><code>allocator_backend</code>のデータをそのまま利用（コピー不要）</li>
<li><code>OrderedDict</code>順に全バックエンド（L1→L2→L3）を走査</li>
<li>異なるallocatorを持つバックエンドには<code>allocate_and_copy_objects()</code>で新メモリ確保＋コピー
<ul>
<li>実際にはLocalDiskBackendもRemoteBackendも<code>get_allocator_backend()</code>→LocalCPUBackendなので、同一allocator＝コピー不要</li>
</ul>
</li>
<li>各バックエンドの<code>batched_submit_put_task()</code>を呼び出し</li>
<li>全バックエンド処理後、各obj_dictの<code>ref_count_down()</code>で解放</li>
</ol>
<p><strong>注意</strong>: <code>put()</code>は非推奨（<code>RuntimeError</code>を投げる）。<code>batched_put()</code>が唯一のエントリポイント。</p>
<h3 id="運用機能"><a class="header" href="#運用機能">運用機能</a></h3>
<ul>
<li><strong>freeze mode</strong>: <code>_freeze=True</code>でリモートバックエンドをスキップ（LocalCPUのみ使用）</li>
<li><strong>bypass mode</strong>: ヘルスチェック失敗時に特定バックエンドを一時的にバイパス</li>
<li><strong>internal_copy_stream</strong>: put時の異なるallocator間コピー用CUDAストリーム</li>
</ul>
<h2 id="localcpubackend"><a class="header" href="#localcpubackend">LocalCPUBackend</a></h2>
<p><code>AllocatorBackendInterface</code>を実装。<strong>メモリ確保</strong>と<strong>キャッシュストレージ</strong>の2つの役割を持つ。</p>
<h3 id="submit_put_task"><a class="header" href="#submit_put_task">submit_put_task()</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/local_cpu_backend.py:141</code></p>
<p><strong>同期実行</strong>（バックグラウンドスレッドなし）。<code>cpu_lock</code>下で:</p>
<ol>
<li>重複チェック: <code>key in hot_cache</code> → スキップ</li>
<li><code>memory_obj.ref_count_up()</code></li>
<li><code>hot_cache[key] = memory_obj</code></li>
<li><code>cache_policy.update_on_put(key)</code> — Evictionポリシー更新</li>
<li><code>batched_msg_sender.add_kv_op(ADMIT, key.chunk_hash)</code> — controller通知（オプション）</li>
<li>ロック外でon_complete_callback実行</li>
</ol>
<h3 id="allocate--batched_allocate--evictionループ"><a class="header" href="#allocate--batched_allocate--evictionループ">allocate() / batched_allocate() — Evictionループ</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/local_cpu_backend.py:426</code></p>
<pre><code>memory_allocatorに確保試行
  ↓ 失敗
cache_policy.get_evict_candidates(hot_cache, num_candidates=1)
  ↓ 候補あり
batched_remove(evict_keys)  ← hot_cacheから除去 + ref_count_down → allocatorに返却
  ↓
memory_allocatorに再確保試行
  ↓ 失敗 &amp;&amp; busy_loop=True
0.1秒待機して再試行（他のstore完了によるメモリ解放を待つ）
</code></pre>
<p><strong>batched_allocateの特殊処理</strong>: Layerwise時、1チャンクの全レイヤーをまとめて追い出す
（<code>evict_key.split_layers(batch_size)</code>で全レイヤーキーを生成→一括free）。</p>
<p><strong>busy_loopの用途</strong>:</p>
<ul>
<li>store（書き込み）: <code>busy_loop=False</code> — 並行storeがデッドロックするため</li>
<li>retrieve（読み出し）: <code>busy_loop=True</code> — storeの完了でメモリが解放されるのを待つ</li>
</ul>
<h3 id="hot_cache"><a class="header" href="#hot_cache">hot_cache</a></h3>
<p><code>cache_policy.init_mutable_mapping()</code>が返すマッピング:</p>
<ul>
<li>FIFO: <code>dict</code>（Python dictは挿入順を保持）</li>
<li>LRU/MRU: <code>OrderedDict</code></li>
<li>LFU: <code>dict</code>（freq_to_keysで別途管理）</li>
</ul>
<h3 id="touch_cache"><a class="header" href="#touch_cache">touch_cache()</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/local_cpu_backend.py:128</code></p>
<p><code>keys_in_request</code>を<strong>逆順</strong>にupdate_on_hit()。suffix→prefix順にlookupされたキーを、
prefix→suffix順（正しい時系列順）に修正してアクセス順序を更新。</p>
<h3 id="contains-with-pin"><a class="header" href="#contains-with-pin">contains() with pin</a></h3>
<p>lookup時に<code>pin=True</code>で呼ばれると:</p>
<ol>
<li><code>hot_cache[key].pin()</code> → Eviction対象外にマーク</li>
<li><code>keys_in_request</code>に追加 → retrieve完了後にtouch_cache()で解除</li>
</ol>
<h3 id="initialize_allocator"><a class="header" href="#initialize_allocator">initialize_allocator()</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/local_cpu_backend.py:346</code></p>
<p>設定に応じてアロケータを選択:</p>
<ul>
<li><strong>P2P有効時</strong>: <code>PagedCpuGpuMemoryAllocator</code>（NIXL連携用ページアロケータ）</li>
<li><strong>通常時</strong>: <code>MixedMemoryAllocator</code>（テンソル用PinMemory + バイナリ用BufferAllocator）</li>
<li>NUMA対応: GPU→NUMAマッピングでNUMA-awareなpinned memory確保</li>
<li>MLA first rank: 最初のrankのみ大容量CPU確保</li>
<li>reserve_cpu_size: システム利用可能メモリから予約サイズを差し引き</li>
</ul>
<h2 id="storagemanagerretrieve方向"><a class="header" href="#storagemanagerretrieve方向">StorageManager（Retrieve方向）</a></h2>
<h3 id="batched_get"><a class="header" href="#batched_get">batched_get()</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/storage_manager.py:484</code></p>
<p>指定locationのバックエンドから<code>batched_get_blocking(keys)</code>でMemoryObjを取得。
<strong>write-back</strong>: リモートバックエンドから取得した場合、<code>LocalCPUBackend</code>が存在すれば自動的にL1にコピー。</p>
<h3 id="layerwise_batched_get"><a class="header" href="#layerwise_batched_get">layerwise_batched_get()</a></h3>
<p>レイヤー単位で非同期取得。各レイヤーの<code>batched_get_non_blocking()</code>をasyncio.create_taskで投入し、Futureをyield。</p>
<h3 id="get_block_mapping"><a class="header" href="#get_block_mapping">get_block_mapping()</a></h3>
<p>チャンクリストを受け取り、各チャンクの所在バックエンドを特定。<strong>prefix match方式</strong>: 各バックエンドの<code>batched_contains()</code>で先頭からの連続ヒット数を取得し、残りを次のバックエンドに渡す。</p>
<h3 id="async_lookup_and_prefetch"><a class="header" href="#async_lookup_and_prefetch">async_lookup_and_prefetch()</a></h3>
<p>非同期プリフェッチの中核。LookupServerから呼ばれ、全バックエンドに対してprefix match方式で<code>batched_async_contains()</code>→<code>batched_get_non_blocking()</code>を実行。結果は<code>EventManager</code>にFutureとして登録。</p>
<h2 id="バックエンドインターフェース階層"><a class="header" href="#バックエンドインターフェース階層">バックエンドインターフェース階層</a></h2>
<pre><code>StorageBackendInterface (abstract)
├── AllocatorBackendInterface (abstract)  — メモリ確保能力あり
│   └── LocalCPUBackend (concrete)
├── StoragePluginInterface (abstract)     — 独自バックエンド実装用
│   └── (ユーザー定義バックエンド)
├── LocalDiskBackend (concrete)
└── RemoteBackend (concrete)
</code></pre>
<p>独自バックエンド実装の詳細は <a href="#localdiskbackend--l2ディスクバックエンド">local-disk-backend.md</a> 末尾の「独自バックエンド実装ガイド」を参照。</p>
<h2 id="上流下流-3"><a class="header" href="#上流下流-3">上流・下流</a></h2>
<ul>
<li><strong>上流</strong>: LMCacheEngine（batched_allocate/batched_put/contains等）</li>
<li><strong>下流</strong>:
<ul>
<li>LocalCPUBackend（L1 CPUメモリ）— <a href="#メモリアロケータ階層">memory-allocator.md</a>, <a href="#cachepolicy--eviction戦略">cache-policy.md</a></li>
<li>LocalDiskBackend（L2 ディスク）— <a href="#localdiskbackend--l2ディスクバックエンド">local-disk-backend.md</a></li>
<li>RemoteBackend（L3 リモート、Redis/S3等）</li>
</ul>
</li>
<li><strong>依存</strong>: CachePolicy（Eviction戦略）、MemoryAllocator（メモリプール）、EventManager（非同期prefetch）</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cachepolicy--eviction戦略"><a class="header" href="#cachepolicy--eviction戦略">CachePolicy — Eviction戦略</a></h1>
<blockquote>
<p><strong>深度</strong>: [DEEP] / <strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-16（Phase 2 セッション1）</p>
</blockquote>
<h2 id="概要-26"><a class="header" href="#概要-26">概要</a></h2>
<p>キャッシュのEviction（追い出し）戦略を抽象化するポリシーフレームワーク。
LocalCPUBackendとLocalDiskBackendが独立したCachePolicyインスタンスを持つ。</p>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/cache_policy/</code></p>
<h2 id="basecachepolicy-インターフェース"><a class="header" href="#basecachepolicy-インターフェース">BaseCachePolicy インターフェース</a></h2>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/cache_policy/base_policy.py</code></p>
<pre><code class="language-python">class BaseCachePolicy(Generic[KeyType, MapType]):
    def init_mutable_mapping(self) -&gt; MapType
    def update_on_hit(self, key, cache_dict) -&gt; None
    def update_on_put(self, key) -&gt; None
    def update_on_force_evict(self, key) -&gt; None
    def get_evict_candidates(self, cache_dict, num_candidates=1) -&gt; list[KeyType]
</code></pre>
<p><strong>重要な設計判断</strong>:</p>
<ul>
<li><code>init_mutable_mapping()</code>がhot_cache/dictの型を決定（dict, OrderedDict等）</li>
<li><code>get_evict_candidates()</code>は<strong>best effort</strong>: <code>can_evict</code>チェックでpinned/参照中のオブジェクトをスキップ</li>
<li><code>cache_dict</code>の値がMemoryObj（hot_cache）またはDiskCacheMetadata（disk dict）</li>
</ul>
<h2 id="fifo--first-in-first-out"><a class="header" href="#fifo--first-in-first-out">FIFO — First In, First Out</a></h2>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/cache_policy/fifo.py</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>実装</th></tr>
</thead>
<tbody>
<tr><td><code>init_mutable_mapping()</code></td><td><code>dict</code>（Python dictは挿入順保持）</td></tr>
<tr><td><code>update_on_hit()</code></td><td><strong>何もしない</strong>（FIFOはアクセスで順序不変）</td></tr>
<tr><td><code>update_on_put()</code></td><td><strong>何もしない</strong>（dictの末尾に自然追加）</td></tr>
<tr><td><code>update_on_force_evict()</code></td><td><strong>何もしない</strong></td></tr>
<tr><td><code>get_evict_candidates()</code></td><td>dict先頭からイテレート、<code>can_evict</code>なものを返す</td></tr>
</tbody>
</table>
</div>
<p>最もシンプル。追い出し候補の選定はO(k)（kは先頭のnon-evictableエントリ数）。</p>
<h2 id="lru--least-recently-used"><a class="header" href="#lru--least-recently-used">LRU — Least Recently Used</a></h2>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/cache_policy/lru.py</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>実装</th></tr>
</thead>
<tbody>
<tr><td><code>init_mutable_mapping()</code></td><td><code>OrderedDict</code></td></tr>
<tr><td><code>update_on_hit()</code></td><td><code>cache_dict.move_to_end(key)</code> + chunk再利用時間追跡</td></tr>
<tr><td><code>update_on_put()</code></td><td>chunk初回タイムスタンプ記録</td></tr>
<tr><td><code>update_on_force_evict()</code></td><td><strong>何もしない</strong></td></tr>
<tr><td><code>get_evict_candidates()</code></td><td>OrderedDict先頭（最も古いアクセス）からイテレート</td></tr>
</tbody>
</table>
</div>
<p><strong>追加機能</strong>: <code>chunk_hash_to_init_timestamp</code>でチャンクの再利用間隔をPrometheusメトリクスに報告。
メモリ上限 <code>max_num_chunk_hash=12,500,000</code>、超過時は辞書をclear()。</p>
<h2 id="lfu--least-frequently-used"><a class="header" href="#lfu--least-frequently-used">LFU — Least Frequently Used</a></h2>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/cache_policy/lfu.py</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>実装</th></tr>
</thead>
<tbody>
<tr><td><code>init_mutable_mapping()</code></td><td><code>dict</code></td></tr>
<tr><td><code>update_on_hit()</code></td><td>freq++。freq_to_keys[old_freq]→freq_to_keys[new_freq]に移動</td></tr>
<tr><td><code>update_on_put()</code></td><td><code>key_to_freq[key] = 1</code>、freq_to_keys[1]に登録</td></tr>
<tr><td><code>update_on_force_evict()</code></td><td>key_to_freq/freq_to_keys両方からkey除去</td></tr>
<tr><td><code>get_evict_candidates()</code></td><td>最低freq→高freqの順にイテレート、同freq内はFIFO</td></tr>
</tbody>
</table>
</div>
<p><strong>データ構造</strong>:</p>
<ul>
<li><code>freq_to_keys</code>: <code>SortedDict[int, dict[key, None]]</code> — freq順ソート</li>
<li><code>key_to_freq</code>: <code>dict[key, int]</code> — O(1)でfreq逆引き</li>
<li>計算量: update_on_hit = O(log N)（SortedDictの操作）</li>
</ul>
<p><strong>注意</strong>: get_evict_candidates()内でkey_to_freq.pop()を直接実行（副作用あり）。</p>
<h2 id="mru--most-recently-used"><a class="header" href="#mru--most-recently-used">MRU — Most Recently Used</a></h2>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/cache_policy/mru.py</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>実装</th></tr>
</thead>
<tbody>
<tr><td><code>init_mutable_mapping()</code></td><td><code>OrderedDict</code></td></tr>
<tr><td><code>update_on_hit()</code></td><td><code>cache_dict.move_to_end(key, last=True)</code></td></tr>
<tr><td><code>update_on_put()</code></td><td><strong>何もしない</strong></td></tr>
<tr><td><code>update_on_force_evict()</code></td><td><strong>何もしない</strong></td></tr>
<tr><td><code>get_evict_candidates()</code></td><td><code>reversed(cache_dict.items())</code>でOrderedDict末尾（最新アクセス）から</td></tr>
</tbody>
</table>
</div>
<p>LRUの逆。ストリーミング的アクセスパターン（同じチャンクが二度使われない場合）に有効。</p>
<h2 id="eviction発動フロー"><a class="header" href="#eviction発動フロー">Eviction発動フロー</a></h2>
<pre class="mermaid">graph TD
    A[allocate() 失敗] --&gt; B{use_hot?}
    B --&gt;|Yes| C[cache_policy.get_evict_candidates\nhot_cache, num=1]
    C --&gt; D{候補あり?}
    D --&gt;|Yes| E[batched_remove\nevict_keys]
    E --&gt; F[allocate再試行]
    F --&gt; G{成功?}
    G --&gt;|No| C
    D --&gt;|No| H{busy_loop?}
    H --&gt;|Yes| I[0.1秒待機]
    I --&gt; C
    H --&gt;|No| J[None返却]
    G --&gt;|Yes| K[MemoryObj返却]
    B --&gt;|No| H
</pre>

<p><strong>can_evict条件</strong>: <code>not is_pinned and ref_count == 1</code></p>
<ul>
<li>pinned: lookup中のチャンク（retrieve完了まで保護）</li>
<li>ref_count &gt; 1: 他のバックエンドやGPU転送が参照中</li>
</ul>
<h2 id="設定-8"><a class="header" href="#設定-8">設定</a></h2>
<p><code>LMCacheEngineConfig.cache_policy</code>で指定:</p>
<pre><code class="language-yaml">cache_policy: "lru"  # fifo | lru | lfu | mru
</code></pre>
<p><code>get_cache_policy()</code>ファクトリ関数で対応するポリシーインスタンスを生成。</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="localdiskbackend--l2ディスクバックエンド"><a class="header" href="#localdiskbackend--l2ディスクバックエンド">LocalDiskBackend — L2ディスクバックエンド</a></h1>
<blockquote>
<p><strong>深度</strong>: [DEEP] / <strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-16（Phase 2 セッション1）</p>
</blockquote>
<h2 id="概要-27"><a class="header" href="#概要-27">概要</a></h2>
<p>ローカルディスクにKVキャッシュを永続化するL2バックエンド。
CPU上のMemoryObjをバイト列としてファイルに書き出し/読み出しする。
メモリ確保は<code>local_cpu_backend</code>に委譲（自身はAllocatorBackendInterfaceではない）。</p>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/local_disk_backend.py</code></p>
<h2 id="アーキテクチャ-11"><a class="header" href="#アーキテクチャ-11">アーキテクチャ</a></h2>
<pre><code>StorageManager
  ├── LocalCPUBackend (L1)  ← hot_cache + MemoryAllocator
  ├── LocalDiskBackend (L2) ← ファイル永続化 + local_cpu_backendからメモリ借用
  └── RemoteBackend (L3)    ← ネットワーク永続化
</code></pre>
<p>LocalDiskBackendは<code>StorageBackendInterface</code>を直接実装（<code>AllocatorBackendInterface</code>ではない）。
<code>get_allocator_backend()</code> → <code>self.local_cpu_backend</code>を返す。</p>
<h2 id="store方向"><a class="header" href="#store方向">Store方向</a></h2>
<h3 id="submit_put_task-1"><a class="header" href="#submit_put_task-1">submit_put_task()</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/local_disk_backend.py:291</code></p>
<ol>
<li>重複チェック: <code>exists_in_put_tasks(key)</code> → スキップ</li>
<li><code>disk_worker.insert_put_task(key)</code> — 進行中タスクリストに登録</li>
<li><strong>ディスク容量Eviction</strong>: <code>disk_lock</code>下で <code>current_cache_size + required_size &gt; max_cache_size</code> の間、
<code>cache_policy.get_evict_candidates()</code> → <code>batched_remove()</code> + <code>os.remove(path)</code></li>
<li><code>memory_obj.ref_count_up()</code> — 非同期書き込み中の保護</li>
<li><code>asyncio.run_coroutine_threadsafe()</code> で <code>async_save_bytes_to_disk()</code> を投入</li>
</ol>
<h3 id="async_save_bytes_to_disk"><a class="header" href="#async_save_bytes_to_disk">async_save_bytes_to_disk()</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/local_disk_backend.py:479</code></p>
<p><code>AsyncPQThreadPoolExecutor</code>上で実行（max_workers=4のスレッドプール）。</p>
<ol>
<li><code>memory_obj.byte_array</code> → <code>write_file(buffer, path)</code></li>
<li><code>memory_obj.ref_count_down()</code> — 参照解放</li>
<li><code>insert_key(key, size, shape, dtype, fmt)</code> — <code>self.dict</code>にDiskCacheMetadata登録</li>
<li><code>disk_worker.remove_put_task(key)</code></li>
<li><code>on_complete_callback(key)</code> 実行（ロック外）</li>
</ol>
<h3 id="write_file"><a class="header" href="#write_file">write_file()</a></h3>
<ul>
<li><strong>通常</strong>: <code>open(path, "wb").write(buffer)</code></li>
<li><strong>O_DIRECT</strong>: サイズがディスクブロックサイズの倍数の場合、<code>os.O_DIRECT</code>フラグで直接I/O</li>
</ul>
<h3 id="diskcachemetadata"><a class="header" href="#diskcachemetadata">DiskCacheMetadata</a></h3>
<pre><code class="language-python">DiskCacheMetadata(path, size, shape, dtype, cached_positions, fmt, pin_count)
</code></pre>
<p>MemoryObjとは異なり、ディスク上のファイルパスとメタデータのみ保持。</p>
<h2 id="retrieve方向"><a class="header" href="#retrieve方向">Retrieve方向</a></h2>
<h3 id="get_blocking"><a class="header" href="#get_blocking">get_blocking()</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/local_disk_backend.py:380</code></p>
<ol>
<li><code>disk_lock</code>下で<code>self.dict[key]</code>からpath/shape/dtype/fmt取得</li>
<li><code>cache_policy.update_on_hit(key, self.dict)</code> — アクセス順更新</li>
<li><code>local_cpu_backend.allocate(shape, dtype, fmt)</code> — <strong>CPUメモリに確保</strong></li>
<li><code>read_file(key, buffer, path)</code> — ファイルからバッファに読み込み</li>
<li><code>metadata.cached_positions</code>をDiskCacheMetadataから復元</li>
</ol>
<h3 id="batched_get_non_blocking--非同期プリフェッチ"><a class="header" href="#batched_get_non_blocking--非同期プリフェッチ">batched_get_non_blocking() — 非同期プリフェッチ</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/local_disk_backend.py:410</code></p>
<ol>
<li>各キーについて:
<ul>
<li><code>local_cpu_backend.allocate()</code> でCPUメモリ確保</li>
<li><code>self.dict[key].pin()</code> — 読み込み中のEviction防止</li>
<li><code>cache_policy.update_on_hit()</code> — アクセス順更新</li>
</ul>
</li>
<li><code>disk_worker.submit_task("prefetch", batched_async_load_bytes_from_disk, ...)</code>
<ul>
<li>priority=0（最高優先度）でスレッドプールに投入</li>
</ul>
</li>
</ol>
<h3 id="batched_async_load_bytes_from_disk"><a class="header" href="#batched_async_load_bytes_from_disk">batched_async_load_bytes_from_disk()</a></h3>
<p>各ファイルを<code>read_file()</code>で読み込み後、<code>self.dict[key].unpin()</code>。</p>
<h3 id="read_file"><a class="header" href="#read_file">read_file()</a></h3>
<ul>
<li><strong>通常</strong>: <code>open(path, "rb").readinto(buffer)</code></li>
<li><strong>O_DIRECT</strong>: ブロック整列時のみ <code>os.O_DIRECT | os.O_RDONLY</code></li>
<li>FileNotFoundError時: 警告ログ + dictからキー除去</li>
</ul>
<h2 id="localdiskworker--優先度付きスレッドプール"><a class="header" href="#localdiskworker--優先度付きスレッドプール">LocalDiskWorker — 優先度付きスレッドプール</a></h2>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/local_disk_backend.py:37</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>タスク種別</th><th>優先度</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td>prefetch</td><td>0 (最高)</td><td>ディスク→CPUメモリ読み込み</td></tr>
<tr><td>delete</td><td>1</td><td>ファイル削除</td></tr>
<tr><td>put</td><td>2 (最低)</td><td>CPUメモリ→ディスク書き込み</td></tr>
</tbody>
</table>
</div>
<p><code>AsyncPQThreadPoolExecutor</code>（Priority Queue付き非同期スレッドプール）で管理。
prefetchが最優先なのは、retrieve（推論の待ち時間に直結）をstoreより優先するため。</p>
<h2 id="容量管理"><a class="header" href="#容量管理">容量管理</a></h2>
<ul>
<li><code>max_cache_size</code>: <code>config.max_local_disk_size * 1024^3</code>（バイト単位）</li>
<li><code>current_cache_size</code>: 現在のディスク使用量（書き込み時に加算、削除時に減算）</li>
<li>Eviction: store時にmax超過なら<code>cache_policy.get_evict_candidates()</code> → <code>os.remove()</code></li>
</ul>
<p><strong>注意</strong>: ディスクフラグメンテーションは未考慮（TODO）。</p>
<h2 id="独自ストレージバックエンド実装ガイド"><a class="header" href="#独自ストレージバックエンド実装ガイド">独自ストレージバックエンド実装ガイド</a></h2>
<h3 id="storageplugininterface"><a class="header" href="#storageplugininterface">StoragePluginInterface</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/abstract_backend.py:394</code></p>
<p>独自バックエンドを実装する場合は<code>StoragePluginInterface</code>を継承する:</p>
<pre><code class="language-python">class MyBackend(StoragePluginInterface):
    def __init__(self, dst_device, config, metadata, local_cpu_backend, loop):
        super().__init__(dst_device, config, metadata, local_cpu_backend, loop)
</code></pre>
<h3 id="必須メソッドstoragebackendinterface由来"><a class="header" href="#必須メソッドstoragebackendinterface由来">必須メソッド（StorageBackendInterface由来）</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>役割</th></tr>
</thead>
<tbody>
<tr><td><code>contains(key, pin)</code></td><td>キーの存在確認。pin=Trueで追い出し保護</td></tr>
<tr><td><code>exists_in_put_tasks(key)</code></td><td>書き込み進行中のキー確認（重複store防止）</td></tr>
<tr><td><code>batched_submit_put_task(keys, objs, ...)</code></td><td>非同期書き込み投入</td></tr>
<tr><td><code>get_blocking(key)</code></td><td>同期的なKVデータ取得</td></tr>
<tr><td><code>pin(key)</code> / <code>unpin(key)</code></td><td>Eviction保護の制御</td></tr>
<tr><td><code>remove(key, force)</code></td><td>エントリ削除</td></tr>
<tr><td><code>get_allocator_backend()</code></td><td>メモリ確保先（通常<code>local_cpu_backend</code>を返す）</td></tr>
<tr><td><code>close()</code></td><td>リソース解放</td></tr>
</tbody>
</table>
</div>
<h3 id="オプショナルメソッド"><a class="header" href="#オプショナルメソッド">オプショナルメソッド</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>メソッド</th><th>デフォルト動作</th><th>オーバーライド推奨</th></tr>
</thead>
<tbody>
<tr><td><code>batched_get_blocking(keys)</code></td><td>個別get_blocking()のループ</td><td>バッチ取得が効率的な場合</td></tr>
<tr><td><code>batched_async_contains(lookup_id, keys, pin)</code></td><td>NotImplementedError</td><td>非同期prefetch対応時</td></tr>
<tr><td><code>batched_get_non_blocking(lookup_id, keys, ...)</code></td><td>NotImplementedError</td><td>非同期prefetch対応時</td></tr>
<tr><td><code>batched_contains(keys, pin)</code></td><td>個別contains()のループ（prefix match方式）</td><td>バッチ判定が効率的な場合</td></tr>
<tr><td><code>touch_cache()</code></td><td>（定義なし）</td><td>LRU等のアクセス順更新が必要な場合</td></tr>
</tbody>
</table>
</div>
<h3 id="メモリ確保パターン"><a class="header" href="#メモリ確保パターン">メモリ確保パターン</a></h3>
<p>独自バックエンドからデータを取得する場合:</p>
<ol>
<li><code>local_cpu_backend.allocate(shape, dtype, fmt)</code> でCPUメモリを確保</li>
<li>データをMemoryObjの<code>byte_array</code>に書き込み（<code>readinto()</code>等）</li>
<li><code>metadata.cached_positions</code>等のメタデータを復元</li>
<li>MemoryObjを返却（呼び出し元がref_count管理）</li>
</ol>
<h3 id="on_complete_callback"><a class="header" href="#on_complete_callback">on_complete_callback</a></h3>
<p><code>batched_submit_put_task()</code>のオプションパラメータ。
各キーのstore完了時にコールバックを実行（バッチ単位ではなくキー単位）。
他バックエンドへの連鎖store等に使用可能。実装側でcatch/logすること。</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="メモリアロケータ階層"><a class="header" href="#メモリアロケータ階層">メモリアロケータ階層</a></h1>
<blockquote>
<p><strong>深度</strong>: [DEEP] / <strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-16（Phase 2 セッション1）</p>
</blockquote>
<h2 id="概要-28"><a class="header" href="#概要-28">概要</a></h2>
<p>LMCacheのメモリ管理は、事前確保された大きなバッファ上で仮想アドレス空間管理を行う
カスタムアロケータで実現されている。pinned CPUメモリ上に確保することで、
GPU⇔CPU間のDMA転送を高速化する。</p>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/memory_management.py</code></p>
<h2 id="アロケータ階層"><a class="header" href="#アロケータ階層">アロケータ階層</a></h2>
<pre><code>MemoryAllocatorInterface (abstract)
├── TensorMemoryAllocator         ← explicit free list方式
├── PagedTensorMemoryAllocator    ← ページ単位の固定サイズスロット
├── BufferAllocator               ← バイト配列用（GC任せ）
├── HostMemoryAllocator           ← 非pinned CPU + 内部委譲
├── PinMemoryAllocator            ← pinned CPU + 内部委譲
├── MixedMemoryAllocator          ← テンソル用Pin + バイナリ用Buffer
├── GPUMemoryAllocator            ← GPU VRAM + 内部委譲
├── CuFileMemoryAllocator         ← GPUDirect Storage対応
├── PagedCpuGpuMemoryAllocator   ← CPU+GPU両方のページアロケータ（NIXL/P2P用）
└── AdHocMemoryAllocator          ← テスト用ダミー
</code></pre>
<h2 id="tensormemoryallocator--explicit-free-list方式"><a class="header" href="#tensormemoryallocator--explicit-free-list方式">TensorMemoryAllocator — explicit free list方式</a></h2>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/memory_management.py:1135</code></p>
<p>事前確保されたフラットテンソル(<code>buffer</code>)上で、AddressManagerが仮想アドレス空間を管理。</p>
<h3 id="addressmanager"><a class="header" href="#addressmanager">AddressManager</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/memory_management.py:903</code></p>
<ul>
<li><strong>データ構造</strong>: <code>SortedList[FreeBlock]</code>（<code>sortedcontainers</code>ライブラリ）</li>
<li><strong>FreeBlock</strong>: <code>(start, size)</code> タプル。startでソート</li>
<li><strong>アライメント</strong>: デフォルト4,096バイト（<code>ALIGN_BYTES</code>）</li>
<li><strong>確保</strong>: first-fit方式。ソートリストを先頭から走査し最初に十分な空きブロックを選択</li>
<li><strong>解放</strong>: bisect_leftで挿入位置を特定し、前後のブロックとcoalesce（結合）</li>
<li><strong>sbrk()</strong>: アドレス空間の動的拡張（LazyAllocator向け）</li>
<li><strong>スレッドセーフ</strong>: <code>@synchronized("_lock")</code>デコレータで全操作をロック保護</li>
</ul>
<h3 id="allocate-の動作"><a class="header" href="#allocate-の動作">allocate() の動作</a></h3>
<pre><code class="language-python">aligned_size = (raw_size + 4095) &amp; ~4095  # 4KB境界に切り上げ
# SortedListからfirst-fitで空きブロックを探索
# ブロックを分割: [確保分 | 残余→フリーリストに戻す]
# buffer[start:start+raw_size] をraw_dataとしてTensorMemoryObjを生成
</code></pre>
<h3 id="batched_allocate-の最適化"><a class="header" href="#batched_allocate-の最適化">batched_allocate() の最適化</a></h3>
<p>通常のallocateはブロックごとに個別確保だが、batched_allocateは
<code>unit_aligned_size * batch_size</code> の<strong>1つの大きなブロック</strong>として確保し、
<code>torch.chunk()</code>で分割。これによりフラグメンテーションを軽減。</p>
<h3 id="free--batched_free"><a class="header" href="#free--batched_free">free() / batched_free()</a></h3>
<ul>
<li><code>free()</code>: AddressManager.free()でブロックを返却→前後のFreeBlockとcoalesce</li>
<li><code>batched_free()</code>: メモリブロックをアドレス順にソートし、隣接ブロックを事前にcoalesceしてから
AddressManager.free()を呼ぶ。フリーリスト操作回数を削減</li>
</ul>
<h2 id="pagedtensormemoryallocator--ページスロット方式"><a class="header" href="#pagedtensormemoryallocator--ページスロット方式">PagedTensorMemoryAllocator — ページスロット方式</a></h2>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/memory_management.py:1404</code></p>
<p>固定サイズのスロット（ページ）に分割し、<code>deque[TensorMemoryObj]</code>でフリーリストを管理。</p>
<ul>
<li><strong>初期化</strong>: <code>buffer</code>を<code>align_bytes</code>（=1チャンク分のKVデータサイズ）で分割</li>
<li><strong>allocate()</strong>: <code>free_blocks.popleft()</code> — O(1)</li>
<li><strong>free()</strong>: <code>free_blocks.append(mem_obj)</code> — O(1)、<code>invalidate()</code>しない（再利用）</li>
<li><strong>スレッドセーフ</strong>: <code>deque</code>のCPython実装がアトミックなため、ロック不要</li>
<li><strong>用途</strong>: P2P/PD共有時のNIXL連携（ページ単位のアドレス管理が必要）</li>
</ul>
<h2 id="mixedmemoryallocator--通常時のデフォルト"><a class="header" href="#mixedmemoryallocator--通常時のデフォルト">MixedMemoryAllocator — 通常時のデフォルト</a></h2>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/memory_management.py:1892</code></p>
<p>LocalCPUBackendの<code>initialize_allocator()</code>がデフォルトで生成するアロケータ。</p>
<pre><code>MixedMemoryAllocator
├── pin_allocator: TensorMemoryAllocator or PagedTensorMemoryAllocator
│   └── buffer: pinned CPU memory（NUMA-aware確保可能）
└── buffer_allocator: BufferAllocator
    └── 個別bytearrayを都度確保
</code></pre>
<ul>
<li><strong>MemoryFormat分岐</strong>: KV_2LTD/KV_T2D/KV_2TD/KV_MLA_FMT → pin_allocator、BINARY_BUFFER → buffer_allocator</li>
<li><strong>pinned memory確保</strong>: <code>lmc_ops.alloc_pinned_ptr()</code>（CUDAのcudaHostAlloc相当）
<ul>
<li>NUMA対応時: <code>lmc_ops.alloc_pinned_numa_ptr(size, numa_id)</code></li>
</ul>
</li>
<li><strong>close()</strong>: <code>lmc_ops.free_pinned_ptr()</code>で明示解放</li>
</ul>
<h2 id="memoryobj--メモリオブジェクトの抽象"><a class="header" href="#memoryobj--メモリオブジェクトの抽象">MemoryObj — メモリオブジェクトの抽象</a></h2>
<h3 id="tensormemoryobj"><a class="header" href="#tensormemoryobj">TensorMemoryObj</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/memory_management.py:431</code></p>
<ul>
<li><strong>raw_data</strong>: フラットな<code>torch.Tensor</code>（uint8ビュー）。アロケータのバッファスライス</li>
<li><strong>metadata</strong>: <code>MemoryObjMetadata</code>（shape, dtype, address, phy_size, ref_count, pin_count, fmt, cached_positions）</li>
<li><strong>group_prefix_sum</strong>: 複数グループ（shapes/dtypes）のバイトオフセットプレフィックスサム</li>
<li><strong>tensor プロパティ</strong>: <code>raw_data[:logical_size].view(dtype).view(shape)</code> でテンソルビューを返す</li>
<li><strong>get_tensor(index)</strong>: グループ別テンソルビュー（MLA等の複数形状対応）</li>
</ul>
<h3 id="ref_count--pin_count-ライフサイクル"><a class="header" href="#ref_count--pin_count-ライフサイクル">ref_count / pin_count ライフサイクル</a></h3>
<pre><code>確保時: ref_count=1, pin_count=0
  ↓
hot_cacheに登録: ref_count_up() → ref_count=2
  ↓
GPU転送中: ref_count_up() → ref_count=3
  ↓
GPU転送完了: ref_count_down() → ref_count=2
  ↓
hot_cacheから追い出し: ref_count_down() → ref_count=1
  ↓
batched_put完了: ref_count_down() → ref_count=0 → allocator.free()
</code></pre>
<p><strong>pin_count</strong>: lookup時にpin()でEviction対象外にマーク。</p>
<ul>
<li><code>can_evict = not is_pinned and ref_count == 1</code>（ref_count=1=hot_cacheのみが保持）</li>
<li>PinMonitor: タイムアウト追跡。異常なpin長期化を検出</li>
</ul>
<h3 id="bytesbuffermemoryobj"><a class="header" href="#bytesbuffermemoryobj">BytesBufferMemoryObj</a></h3>
<p><code>bytes</code>/<code>bytearray</code>のラッパー。ref_count操作はno-op（GC任せ）。
BINARY_BUFFER形式用。Serde圧縮結果の格納に使用。</p>
<h2 id="メモリサイズ計算"><a class="header" href="#メモリサイズ計算">メモリサイズ計算</a></h2>
<h3 id="calculate_chunk_budget"><a class="header" href="#calculate_chunk_budget">calculate_chunk_budget()</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/storage_backend/local_cpu_backend.py:688</code></p>
<pre><code class="language-python">max_chunks = total_memory // aligned_chunk_bytes
</code></pre>
<p>非同期ローディングシステムでの同時確保数上限を算出。
デッドロック防止のためのチャンクバジェット管理に使用。</p>
<h3 id="get_full_chunk_size"><a class="header" href="#get_full_chunk_size">get_full_chunk_size()</a></h3>
<p>Layerwise時: <code>chunk_tokens * kv_size * hidden_dim * dtype_size</code>（1レイヤー分）
Bulk時: <code>kv_size * num_layers * chunk_tokens * hidden_dim * dtype_size</code>（全レイヤー分）</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="tokendatabasechunkedtokendatabase"><a class="header" href="#tokendatabasechunkedtokendatabase">TokenDatabase（ChunkedTokenDatabase）</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM] / <strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-16（Phase 1 セッション1）</p>
</blockquote>
<h2 id="概要-29"><a class="header" href="#概要-29">概要</a></h2>
<p>トークン列をチャンクに分割し、プレフィックスチェーンハッシュを計算してCacheEngineKeyを生成する。
vLLMのプレフィックスキャッシュと<strong>同一のハッシュアルゴリズム</strong>を使用し、キーの互換性を保証する。</p>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/token_database.py</code></p>
<h2 id="クラス階層-5"><a class="header" href="#クラス階層-5">クラス階層</a></h2>
<pre><code>TokenDatabase (ABC)
  ├── ChunkedTokenDatabase    ← 標準実装（固定サイズチャンク）
  └── SegmentTokenDatabase    ← CacheBlend用（セパレータベースの可変長チャンク）
</code></pre>
<h2 id="主要メソッド-8"><a class="header" href="#主要メソッド-8">主要メソッド</a></h2>
<h3 id="process_tokens"><a class="header" href="#process_tokens">process_tokens()</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/token_database.py:309</code></p>
<pre><code class="language-python">def process_tokens(
    tokens: Optional[Union[Tensor, List[int]]] = None,
    hashes: Optional[List[int]] = None,
    offsets: Optional[List[int]] = None,
    mask: Optional[Tensor] = None,
    make_key: bool = True,
    request_configs: Optional[dict] = None,
) -&gt; Iterable[ProcessTokensResult]  # (start, end, CacheEngineKey|hash)
</code></pre>
<p><strong>2つの入力モード</strong>:</p>
<ol>
<li><strong>tokens入力</strong>: トークン列を受け取り、チャンク分割→ハッシュ計算</li>
<li><strong>hashes入力</strong>: 事前計算済みハッシュ+offsetsを受け取り、キー生成のみ</li>
</ol>
<p><strong>チャンク分割アルゴリズム</strong>（tokens入力時）:</p>
<ol>
<li><code>_chunk_tokens()</code>: chunk_size（デフォルト256）単位で分割
<ul>
<li><code>save_unfull_chunk=True</code>（デフォルト）: 端数チャンクも保存</li>
<li><code>save_unfull_chunk=False</code>: 端数は切り捨て</li>
</ul>
</li>
<li><code>_prefix_hash()</code>: プレフィックスチェーンハッシュを計算
<ul>
<li>初期値: <code>NONE_HASH</code>（vLLMから取得、<code>kv_cache_utils.init_none_hash()</code>）</li>
<li>各チャンク: <code>hash_func((previous_hash, token_tuple, extra_keys))</code></li>
</ul>
</li>
<li>maskのFalse区間（=already-cached prefix）のチャンクをスキップ
<ul>
<li><strong>制約</strong>: False数はchunk_sizeの倍数でなければならない（ValueError）</li>
</ul>
</li>
</ol>
<h3 id="ハッシュ関数"><a class="header" href="#ハッシュ関数">ハッシュ関数</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/token_database.py:97</code> (<code>_get_vllm_hash_func</code>)</p>
<p>vLLMの<code>get_hash_fn_by_name("sha256_cbor")</code>を直接利用。
複数のインポートパスを試行し、vLLMバージョン互換性を確保:</p>
<ul>
<li><code>vllm.utils.hashing.get_hash_fn_by_name</code>（PR#27151以降）</li>
<li><code>vllm.utils.get_hash_fn_by_name</code>（PR#27151以前）</li>
<li><code>sha256_cbor_64bit</code>→<code>sha256_cbor</code>リネーム対応（PR#23673）</li>
<li>フォールバック: Python組み込み<code>hash()</code>（非推奨、分散キャッシュで不整合の可能性）</li>
</ul>
<h3 id="cacheenginekey生成"><a class="header" href="#cacheenginekey生成">CacheEngineKey生成</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/v1/token_database.py:207</code> (<code>_make_key_by_hash</code>)</p>
<pre><code class="language-python">CacheEngineKey(
    model_name,         # メタデータから
    world_size,         # save_only_first_rank時は1に固定
    worker_id,          # GPUランク
    chunk_hash,         # プレフィックスチェーンハッシュ
    kv_dtype,           # e.g. bfloat16
    request_configs,    # オプション（per-requestの設定dict）
)
</code></pre>
<p><code>save_only_first_rank</code>はMLA（Multi-head Latent Attention）使用時に有効。world_sizeを1に固定することで、異なるTP並列度でもキーが一致する。</p>
<h2 id="retrieve時の利用"><a class="header" href="#retrieve時の利用">Retrieve時の利用</a></h2>
<p>Retrieveパスでも同じ<code>process_tokens()</code>が使用される:</p>
<ul>
<li><strong>Bulk retrieve</strong>: <code>_process_tokens_internal()</code>と<code>_async_process_tokens_internal()</code>の両方で呼ばれ、チャンクキーを生成</li>
<li><strong>Layerwise retrieve</strong>: <code>retrieve_layer()</code>内で呼ばれ、layer 0のキーで<code>StorageManager.contains()</code>判定</li>
<li><strong>LookupClient</strong>: Schedulerプロセス内でも独自の<code>ChunkedTokenDatabase</code>インスタンスを持ち、<code>process_tokens(make_key=False)</code>でハッシュのみ計算してZMQ経由でLookupServerに送信</li>
<li><strong>非同期prefetch</strong>: <code>async_process_tokens_internal()</code>ではハッシュ再計算が発生する（TODO: スキップ最適化の余地あり）</li>
</ul>
<h2 id="上流下流-4"><a class="header" href="#上流下流-4">上流・下流</a></h2>
<ul>
<li><strong>上流</strong>: LMCacheEngine（store_layer/store/retrieve等で呼び出し）、LookupClient（lookup時）</li>
<li><strong>下流</strong>: なし（純粋な変換コンポーネント）</li>
<li><strong>依存</strong>: vLLMのハッシュ関数ライブラリ</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="vllm統合lmcacheconnector"><a class="header" href="#vllm統合lmcacheconnector">vLLM統合（LMCacheConnector）</a></h1>
<blockquote>
<p><strong>深度</strong>: [MEDIUM] / <strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-16（Phase 1 セッション1）</p>
</blockquote>
<h2 id="概要-30"><a class="header" href="#概要-30">概要</a></h2>
<p>LMCacheとvLLMを接続するアダプタ層。vLLMの<code>KVConnectorBase_V1</code>インターフェースを実装し、
attentionレイヤー実行中のKVキャッシュstore/retrieveをLMCacheに委譲する。</p>
<h2 id="クラス階層-6"><a class="header" href="#クラス階層-6">クラス階層</a></h2>
<pre><code>KVConnectorBase_V1 (vLLM)
  └── LMCacheConnectorV1Dynamic       ← vLLMに登録される外殻
        └── LMCacheConnectorV1Impl    ← 実装本体（Worker側）
              └── LMCacheEngine       ← 直接参照（LMCacheManager経由で取得）
</code></pre>
<p><strong>参照</strong>:</p>
<ul>
<li><code>target/LMCache/lmcache/integration/vllm/lmcache_connector_v1.py</code> (Dynamic)</li>
<li><code>target/LMCache/lmcache/integration/vllm/vllm_v1_adapter.py:964</code> (Impl.save_kv_layer)</li>
</ul>
<h2 id="主要メソッドstore方向-1"><a class="header" href="#主要メソッドstore方向-1">主要メソッド（Store方向）</a></h2>
<h3 id="save_kv_layer"><a class="header" href="#save_kv_layer">save_kv_layer()</a></h3>
<p><strong>シグネチャ</strong>: <code>save_kv_layer(layer_name: str, kv_layer: Tensor, attn_metadata: AttentionMetadata, **kwargs)</code></p>
<p>vLLMの各attentionレイヤー実行後に呼ばれる。</p>
<ul>
<li><code>use_layerwise=False</code>の場合は即座にreturn（非レイヤーワイズパスは別経路）</li>
<li><code>kv_role="kv_consumer"</code>の場合もreturn（consumeのみ、storeしない）</li>
</ul>
<p><strong>Layer 0の処理</strong>:</p>
<ol>
<li><code>connector_metadata.requests</code>から<code>save_spec.can_save=True</code>のリクエストを抽出</li>
<li><code>skip_leading_tokens</code>をchunk_size（256）の倍数に切り下げてマスク整合</li>
<li><code>store_mask</code>を構築：prefix部分=False、新規部分=True</li>
<li><code>LMCacheEngine.store_layer()</code>でGenerator生成、<code>self.layerwise_storers</code>に追加</li>
<li>最初のリクエストのみ<code>sync=True</code>（CUDAストリーム同期）</li>
</ol>
<p><strong>全レイヤー共通</strong>: <code>layerwise_storers</code>内の全Generatorを<code>next()</code>で1ステップ進行。</p>
<h3 id="connectormetadata"><a class="header" href="#connectormetadata">ConnectorMetadata</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/integration/vllm/vllm_v1_adapter.py</code> (LMCacheConnectorMetadata)</p>
<p>Scheduler側で構築され、Worker側に渡される。各リクエストの<code>token_ids</code>、<code>slot_mapping</code>、<code>save_spec</code>（<code>can_save</code>, <code>skip_leading_tokens</code>）を含む。</p>
<h2 id="上流下流-5"><a class="header" href="#上流下流-5">上流・下流</a></h2>
<ul>
<li><strong>上流</strong>: vLLM GPUModelRunner（<code>save_kv_layer</code>フック）</li>
<li><strong>下流</strong>: LMCacheEngine（<code>store_layer</code> / <code>store</code> / <code>retrieve</code> / <code>retrieve_layer</code>）</li>
<li><strong>関連</strong>: LMCacheManager（ライフサイクル管理、Engine取得）</li>
</ul>
<h2 id="主要メソッドretrieve方向-1"><a class="header" href="#主要メソッドretrieve方向-1">主要メソッド（Retrieve方向）</a></h2>
<h3 id="scheduler側-get_num_new_matched_tokens"><a class="header" href="#scheduler側-get_num_new_matched_tokens">Scheduler側: get_num_new_matched_tokens()</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/integration/vllm/vllm_v1_adapter.py:1193</code></p>
<p>vLLM Schedulerの<code>schedule()</code>から呼ばれ、外部KVキャッシュのヒット数を返す。</p>
<ol>
<li><code>LookupClient.lookup_cache(req_id)</code>で既存キャッシュ確認（2回目以降）</li>
<li>未キャッシュなら<code>LookupClient.lookup(token_ids, req_id)</code>でZMQ経由でWorker側に問い合わせ</li>
<li><code>LoadSpec(vllm_cached_tokens, lmcache_cached_tokens, can_load=False)</code>を生成</li>
<li><code>update_state_after_alloc()</code>で<code>can_load=True</code>に更新（ブロック確保成功時）</li>
<li><code>build_connector_meta()</code>で<code>ReqMeta(load_spec=LoadSpec)</code>を<code>LMCacheConnectorMetadata</code>に格納</li>
</ol>
<h3 id="worker側-start_load_kv"><a class="header" href="#worker側-start_load_kv">Worker側: start_load_kv()</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/integration/vllm/vllm_v1_adapter.py:737</code></p>
<p>vLLMのforward実行<strong>前</strong>に<code>ForwardContext</code>から呼ばれ、KVキャッシュのGPU復元を開始。</p>
<p><strong>token_maskの構築</strong>:</p>
<ol>
<li><code>request.load_spec.vllm_cached_tokens</code>をchunk_sizeの倍数に切り下げ → <code>masked_token_count</code></li>
<li><code>token_mask[:masked_token_count] = False</code>（vLLM既キャッシュ分）、残り=True</li>
</ol>
<p><strong>2モード分岐</strong>:</p>
<ul>
<li><strong>Layerwise</strong> (<code>use_layerwise=True</code>):
<ol>
<li><code>LMCacheEngine.retrieve_layer()</code>でGenerator取得</li>
<li><code>next()</code> × 2回で先行2レイヤー分をキック</li>
<li><code>self.layerwise_retrievers</code>にGenerator追加</li>
</ol>
</li>
<li><strong>Bulk</strong> (<code>use_layerwise=False</code>):
<ol>
<li><code>LMCacheEngine.retrieve()</code>を呼び出し、<code>ret_mask</code>を取得</li>
<li>取得失敗時は<code>record_failed_blocks()</code>で失敗ブロックIDを<code>_invalid_block_ids</code>に記録</li>
</ol>
</li>
</ul>
<h3 id="worker側-wait_for_layer_load"><a class="header" href="#worker側-wait_for_layer_load">Worker側: wait_for_layer_load()</a></h3>
<p><strong>参照</strong>: <code>target/LMCache/lmcache/integration/vllm/vllm_v1_adapter.py:940</code></p>
<p>各attentionレイヤー実行<strong>前</strong>に呼ばれ、該当レイヤーのKVロード完了を待機。
<code>layerwise_retrievers</code>内の全Generatorを<code>next()</code>で1ステップ進行。
最終レイヤーでは<code>ret_mask</code>を取得して検証。</p>
<h2 id="設計上の注意点-1"><a class="header" href="#設計上の注意点-1">設計上の注意点</a></h2>
<ul>
<li><code>LMCacheConnectorV1Dynamic</code>は純粋な委譲シェル。全メソッドが<code>self._lmcache_engine</code>（V1Impl）に転送</li>
<li><strong>LMCacheManagerにstore()メソッドは存在しない</strong>。V1ImplがEngineを直接呼ぶ</li>
<li><code>kv_role</code>は<code>"kv_both"</code>（default）/<code>"kv_producer"</code>/<code>"kv_consumer"</code>の3値。producer時はskip_leading_tokens=0（全トークンstore）</li>
<li><code>current_layer</code>カウンタでレイヤー追跡。wait_for_save()でリセット</li>
<li><strong>Retrieve 2モード</strong>: <code>use_layerwise</code>（デフォルトFalse）でBulk/Layerwiseを切替</li>
<li><strong>LookupClient-LookupServer分離</strong>: SchedulerプロセスのLookupClientからWorkerプロセスのLookupServerにZMQ IPC通信</li>
<li><strong>LoadSpec</strong>: Scheduler→Worker間でlookup結果を伝達するデータ構造（ConnectorMetadata経由）</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="lmcache-用語集"><a class="header" href="#lmcache-用語集">LMCache 用語集</a></h1>
<blockquote>
<p><strong>確信度</strong>: [VERIFIED]
<strong>最終更新</strong>: 2026-02-16（Phase 1 セッション1）</p>
</blockquote>
<h2 id="コアコンセプト"><a class="header" href="#コアコンセプト">コアコンセプト</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>用語</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><strong>LMCacheEngine</strong></td><td>KVキャッシュのstore/retrieve/prefetchを統合するメインエンジン。<code>lmcache/v1/cache_engine.py</code></td></tr>
<tr><td><strong>LMCacheManager</strong></td><td>LMCacheの内部コンポーネント（Engine, LookupClient, OffloadServer等）のライフサイクル管理。<code>lmcache/v1/manager.py</code></td></tr>
<tr><td><strong>CacheEngineKey</strong></td><td>KVキャッシュチャンクの一意識別子。(model_name, world_size, worker_id, chunk_hash, dtype, request_configs)の6タプル。<code>lmcache/utils.py:333</code></td></tr>
<tr><td><strong>LayerCacheEngineKey</strong></td><td>CacheEngineKey + layer_id。レイヤー単位保存時のキー。<code>split_layers()</code>で生成。<code>lmcache/utils.py:392</code></td></tr>
<tr><td><strong>MemoryObj</strong></td><td>KVキャッシュデータを保持する抽象メモリオブジェクト。フォーマット情報とテンソルデータを包含。<code>lmcache/v1/memory_management.py</code></td></tr>
<tr><td><strong>MemoryFormat</strong></td><td>KVキャッシュのメモリレイアウト種別。KV_2LTD, KV_T2D, KV_2TD, BINARY, KV_MLA_FMT等。</td></tr>
<tr><td><strong>LMCacheMetadata</strong></td><td>モデル名、world_size、worker_id、kv_dtype、kv_shape等のメタ情報。サービングエンジンから抽出。</td></tr>
<tr><td><strong>LMCacheEngineConfig</strong></td><td>YAML/環境変数ベースの設定。chunk_size, ストレージ設定, blend設定, P2P設定等。</td></tr>
</tbody>
</table>
</div>
<h2 id="トークン処理"><a class="header" href="#トークン処理">トークン処理</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>用語</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><strong>TokenDatabase</strong></td><td>トークン列をチャンクキー列に変換する抽象基底クラス。</td></tr>
<tr><td><strong>ChunkedTokenDatabase</strong></td><td>固定サイズ（default 256トークン）チャンクでプレフィックスハッシュを計算。標準の実装。</td></tr>
<tr><td><strong>SegmentTokenDatabase</strong></td><td>セパレータベースでセグメント分割。CacheBlend時に使用。</td></tr>
<tr><td><strong>chunk_size</strong></td><td>チャンクのトークン数。デフォルト256。</td></tr>
<tr><td><strong>chunk_hash</strong></td><td>チャンクのプレフィックスハッシュ値。vLLMの<code>sha256_cbor</code>ハッシュ関数を直接利用（完全互換）。</td></tr>
<tr><td><strong>NONE_HASH</strong></td><td>プレフィックスハッシュチェーンの初期値。vLLMの<code>kv_cache_utils.init_none_hash()</code>で初期化。</td></tr>
<tr><td><strong>store_mask</strong></td><td>store時のマスク。False=already-cached prefix、True=新規トークン。False数はchunk_sizeの倍数必須。</td></tr>
</tbody>
</table>
</div>
<h2 id="ストレージ"><a class="header" href="#ストレージ">ストレージ</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>用語</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><strong>StorageManager</strong></td><td>複数のストレージバックエンドを階層管理。put/get要求を各バックエンドに振り分け。</td></tr>
<tr><td><strong>StorageBackendInterface</strong></td><td>ストレージバックエンドの抽象インターフェース。contains/put/get等。</td></tr>
<tr><td><strong>LocalCPUBackend</strong></td><td>CPU メモリ上のKVキャッシュストレージ（L1）。hot_cache（OrderedDict）で管理。同期書き込み。</td></tr>
<tr><td><strong>hot_cache</strong></td><td>LocalCPUBackendの<code>OrderedDict[CacheEngineKey, MemoryObj]</code>。CachePolicyでEviction管理。</td></tr>
<tr><td><strong>allocator_backend</strong></td><td>MemoryObj確保を担当するバックエンド。通常はLocalCPUBackend。</td></tr>
<tr><td><strong>LocalDiskBackend</strong></td><td>ディスク上のKVキャッシュストレージ（L2）。</td></tr>
<tr><td><strong>RemoteBackend</strong></td><td>リモートストレージ（L3）。connector経由でRedis/S3/Valkey等に接続。</td></tr>
<tr><td><strong>P2PBackend</strong></td><td>インスタンス間のP2P KVキャッシュ転送。</td></tr>
<tr><td><strong>NIXLBackend</strong></td><td>NVIDIA NIXL経由の高速転送。</td></tr>
<tr><td><strong>GdsBackend</strong></td><td>GPUDirect Storage経由の転送。</td></tr>
<tr><td><strong>CachePolicy</strong></td><td>Eviction方針。FIFO/LRU/LFU/MRUから選択可能。</td></tr>
<tr><td><strong>Serde</strong></td><td>シリアライゼーション/デシリアライゼーション。naive（無圧縮）、CacheGen（圧縮）、KIVI等。</td></tr>
</tbody>
</table>
</div>
<h2 id="gpu連携"><a class="header" href="#gpu連携">GPU連携</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>用語</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><strong>GPUConnectorInterface</strong></td><td>GPU KVキャッシュとCPU MemoryObj間のデータ転送抽象インターフェース。to_gpu/from_gpu。</td></tr>
<tr><td><strong>VLLMPagedMemGPUConnectorV2</strong></td><td>vLLMのPaged KVキャッシュ向けGPUコネクタ（非レイヤーワイズ）。全レイヤー一括転送。</td></tr>
<tr><td><strong>VLLMPagedMemLayerwiseGPUConnector</strong></td><td>レイヤー単位でKVキャッシュを転送するコネクタ。ジェネレータパターン使用。主要パス。</td></tr>
<tr><td><strong>lmc_ops.single_layer_kv_transfer</strong></td><td>CUDAカーネル。vLLMのページドKVキャッシュからslot_mapping経由でデータを抽出/書き戻し。</td></tr>
<tr><td><strong>slot_mapping</strong></td><td>トークン位置→vLLMページドメモリのflat slot位置へのマッピング。GPU Tensor。</td></tr>
<tr><td><strong>store_stream</strong></td><td>GPU→CPU転送専用CUDAストリーム。メイン計算ストリームとオーバーラップ可能。</td></tr>
<tr><td><strong>load_stream</strong></td><td>CPU→GPU転送専用CUDAストリーム。retrieve時にメイン計算とオーバーラップ。</td></tr>
<tr><td><strong>lmc_ops.multi_layer_kv_transfer</strong></td><td>CUDAカーネル。全レイヤー一括でMemoryObj→paged KVキャッシュに転送（Bulk retrieve用）。</td></tr>
<tr><td><strong>fused_rotary_emb</strong></td><td>RoPE位置補正関数。Layerwise retrieve時に保存時と現在のposition差分を補正。</td></tr>
<tr><td><strong>VLLMBufferLayerwiseGPUConnector</strong></td><td>CacheBlend対応のLayerwiseコネクタ。ダブルバッファ+RoPE補正+gap zeroing。</td></tr>
</tbody>
</table>
</div>
<h2 id="統合"><a class="header" href="#統合">統合</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>用語</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><strong>LMCacheConnectorV1Dynamic</strong></td><td>vLLMの<code>KVConnectorBase_V1</code>実装。<code>LMCacheConnectorV1Impl</code>に委譲。</td></tr>
<tr><td><strong>LMCacheConnectorV1Impl</strong></td><td>vLLM統合の実装本体（<code>vllm_v1_adapter.py</code>）。LoadSpec/SaveSpecでロード・保存を管理。</td></tr>
<tr><td><strong>LoadSpec</strong></td><td>ロード仕様。vLLMキャッシュ済みトークン数、LMCacheキャッシュ済みトークン数、ロード可否。</td></tr>
<tr><td><strong>SaveSpec</strong></td><td>保存仕様。<code>skip_leading_tokens</code>（キャッシュ済みプレフィックス長）、<code>can_save</code>（保存可否）。</td></tr>
<tr><td><strong>ConnectorMetadata</strong></td><td>Scheduler→Worker間で渡されるメタデータ。各リクエストのtoken_ids, slot_mapping, LoadSpec, SaveSpecを含む。</td></tr>
<tr><td><strong>kv_role</strong></td><td><code>"kv_both"</code>（default）/<code>"kv_producer"</code>/<code>"kv_consumer"</code>。producer時はskip_leading_tokens=0。</td></tr>
<tr><td><strong>LookupClient</strong></td><td>Scheduler側でキャッシュ存在確認を行うZMQベースクライアント。<code>lmcache_lookup_client.py</code></td></tr>
<tr><td><strong>LookupServer</strong></td><td>Worker側でLookupClientからのZMQ REQ/REPを受け付け、StorageManager.async_lookup_and_prefetchを実行。</td></tr>
<tr><td><strong>EventManager</strong></td><td>非同期イベント（LOADING等）のFutureを管理。lookup_idでprefetch結果とretrieve消費を紐付け。</td></tr>
<tr><td><strong>token_mask</strong></td><td>retrieve時のマスク。False=vLLMがキャッシュ済み（chunk_sizeの倍数に切り下げ）、True=LMCacheからロード対象。</td></tr>
<tr><td><strong>ret_mask</strong></td><td>retrieve結果のマスク。True=LMCacheから実際に取得成功、False=未取得。Engine内部で構築。</td></tr>
<tr><td><strong>write-back</strong></td><td>StorageManager.batched_get()がリモートバックエンドから取得した場合、自動的にLocalCPUBackendにコピーする動作。</td></tr>
<tr><td><strong>get_block_mapping</strong></td><td>チャンクの所在バックエンドをprefix match方式で特定するStorageManagerメソッド。</td></tr>
</tbody>
</table>
</div>
<h2 id="cacheblend-1"><a class="header" href="#cacheblend-1">CacheBlend</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>用語</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><strong>CacheBlend</strong></td><td>非プレフィックス部分のKVキャッシュも再利用する技術。重要トークンを再計算して品質保持。</td></tr>
<tr><td><strong>Blender</strong></td><td>CacheBlendのblending計算を実行するコンポーネント。<code>lmcache/v1/compute/blend/</code></td></tr>
<tr><td><strong>blend_recompute_ratios</strong></td><td>再計算するトークンの割合。</td></tr>
<tr><td><strong>blend_special_str</strong></td><td>セグメント分割用セパレータ文字列。デフォルト<code>" # # "</code>。</td></tr>
</tbody>
</table>
</div>
<h2 id="分散マルチプロセス"><a class="header" href="#分散マルチプロセス">分散・マルチプロセス</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>用語</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><strong>CacheController</strong></td><td>複数LMCacheインスタンス間のキャッシュ状態を中央管理するコントローラ。</td></tr>
<tr><td><strong>LMCacheWorker</strong></td><td>CacheControllerと通信するワーカー。Heartbeat/Register/P2P Lookup。</td></tr>
<tr><td><strong>MultiProcess Server</strong></td><td>ZMQ IPCベースの別プロセスLMCacheサーバー。SessionManager, GPUCacheContext管理。</td></tr>
<tr><td><strong>BlendServer</strong></td><td>CacheBlend用MPサーバー。MPCacheEngine継承。</td></tr>
<tr><td><strong>OffloadServer</strong></td><td>KVキャッシュオフロード用ZMQサーバー。</td></tr>
<tr><td><strong>Disaggregated Prefill (PD)</strong></td><td>Prefill/Decode分離アーキテクチャ。NIXL経由でPD間転送。</td></tr>
</tbody>
</table>
</div>
<h2 id="設定キー主要"><a class="header" href="#設定キー主要">設定キー（主要）</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>設定名</th><th>デフォルト</th><th>説明</th></tr>
</thead>
<tbody>
<tr><td><code>chunk_size</code></td><td>256</td><td>チャンクのトークン数</td></tr>
<tr><td><code>local_cpu</code></td><td>true</td><td>CPU バックエンド有効化</td></tr>
<tr><td><code>max_local_cpu_size</code></td><td>5.0 (GB)</td><td>CPUストレージ上限</td></tr>
<tr><td><code>local_disk</code></td><td>None</td><td>ディスクパス（Noneで無効）</td></tr>
<tr><td><code>remote_url</code></td><td>None</td><td>リモートストレージURL</td></tr>
<tr><td><code>remote_serde</code></td><td>“naive”</td><td>リモート用Serde</td></tr>
<tr><td><code>use_layerwise</code></td><td>false</td><td>レイヤー単位転送</td></tr>
<tr><td><code>enable_blending</code></td><td>false</td><td>CacheBlend有効化</td></tr>
<tr><td><code>enable_p2p</code></td><td>false</td><td>P2P転送有効化</td></tr>
<tr><td><code>enable_pd</code></td><td>false</td><td>Disaggregated Prefill</td></tr>
<tr><td><code>enable_controller</code></td><td>false</td><td>CacheController有効化</td></tr>
<tr><td><code>save_decode_cache</code></td><td>false</td><td>Decodeフェーズのキャッシュも保存</td></tr>
</tbody>
</table>
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->
        <script src="mermaid-eefea253.min.js"></script>
        <script src="mermaid-init-ccf746f1.js"></script>

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
